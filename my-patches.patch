From 9b368adbfc5693a654a049e640d9f2a085557942 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Mon, 7 Jan 2019 14:36:22 -0800
Subject: [PATCH 01/20] Squashed commit of the following:
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

commit 701b2d7c3a95ef971191423e9a351bb07f76678f
Merge: ccb45ad3c5ed 4f21dd61b3b2
Author: Mathew Prokos <mprokos@anki.com>
Date:   Mon Jan 7 14:09:32 2019 -0800

    Merge remote-tracking branch 'linux-stable-rt/v3.18.66-rt72' into r00014.1

    Change-Id: I91b8c0962cf0f7415f45e0c809d26bdb2b7b6ed3

commit 4f21dd61b3b246b7c62fb043f3ac255189a0db54
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:34:21 2017 -0400

    Linux 3.18.66-rt72

commit da8d6f852194fe7bb8ec5a30ab8b274afc1660b4
Merge: 109cfed4faca a82d5fd0c3a5
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:34:06 2017 -0400

    Merge tag 'v3.18.66' into v3.18-rt

    This is the 3.18.66 stable release

commit 109cfed4facae8d013cb547e2053b8a5b41814a7
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:33:55 2017 -0400

    Linux 3.18.65-rt71

commit d39c62231ea38bf02fc61fec687ea35fdf2cc046
Merge: 964b38130264 945f1d358141
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:33:42 2017 -0400

    Merge tag 'v3.18.65' into v3.18-rt

    This is the 3.18.65 stable release

commit 964b381302644ac6e38fc861ad077b57946a402c
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:33:06 2017 -0400

    Linux 3.18.64-rt70

commit 7ad3c15eccb03b0a35cf810d71a3c69fe5f132ef
Merge: 82bd3ba348bb e831353e20d7
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:32:56 2017 -0400

    Merge tag 'v3.18.64' into v3.18-rt

    This is the 3.18.64 stable release

commit 82bd3ba348bb74228f535c56caaa701b82daa7fa
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:32:44 2017 -0400

    Linux 3.18.63-rt69

commit a76bd8b3862daa8ffd558e825029dfaefbdffabe
Merge: 2b926b414ad6 8c13fcce2c66
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:32:35 2017 -0400

    Merge tag 'v3.18.63' into v3.18-rt

    This is the 3.18.63 stable release

commit 2b926b414ad66a0315048156d55b966641227647
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:32:21 2017 -0400

    Linux 3.18.62-rt68

commit 4682ad55742e99aa1d90f239e676e6d350eb7879
Merge: ff64a1650d12 dd8b674caeef
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:32:06 2017 -0400

    Merge tag 'v3.18.62' into v3.18-rt

    This is the 3.18.62 stable release

     Conflicts:
    	kernel/trace/trace.c

commit ff64a1650d1285619da0748bfca40045deb01eea
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:30:16 2017 -0400

    Linux 3.18.61-rt67

commit 9cdb0ba552705be20fcfbe379f9b062b380ae643
Merge: ec929ea7a82f 4d29e8c0e931
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:29:58 2017 -0400

    Merge tag 'v3.18.61' into v3.18-rt

    This is the 3.18.61 stable release

commit ec929ea7a82f8ba00806d3264843c26bb211b9bb
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:29:25 2017 -0400

    Linux 3.18.60-rt66

commit f3257fd8c87e19bc50199f81aa7e3350de235a75
Merge: d5b42d415fd5 8686d7d2e93b
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Sep 5 17:29:11 2017 -0400

    Merge tag 'v3.18.60' into v3.18-rt

    This is the 3.18.60 stable release

commit d5b42d415fd58d56734a79fb997606488eba7ef0
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jul 3 20:10:29 2017 -0400

    Linux 3.18.59-rt65

commit 4bae4875775588939d1dff0d1f6a3612308b0470
Merge: 80a9e9a3add7 985c6fe6e035
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jul 3 17:43:40 2017 -0400

    Merge tag 'v3.18.59' into v3.18-rt

    This is the 3.18.59 stable release

     Conflicts:
    	kernel/signal.c

commit 80a9e9a3add7309caa89fa028a9904e2a642647b
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jul 3 17:42:54 2017 -0400

    Linux 3.18.58-rt64

commit 53946b8857a114506e084cefc959e97593aa3bdd
Merge: 8ae68e0845d7 31c34faca7a7
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jul 3 17:42:45 2017 -0400

    Merge tag 'v3.18.58' into v3.18-rt

    This is the 3.18.58 stable release

commit 8ae68e0845d7bd99e875b00a13500a39f6d08811
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jul 3 17:42:24 2017 -0400

    Linux 3.18.57-rt63

commit d9840300e6333689f7032ccdc426461aa207a393
Merge: 9c975e7c0691 8366868460f8
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jul 3 17:42:13 2017 -0400

    Merge tag 'v3.18.57' into v3.18-rt

    This is the 3.18.57 stable release

commit 9c975e7c06918c4fdd5c21c07f5b18d2e7603cec
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jul 3 17:41:36 2017 -0400

    Linux 3.18.56-rt62

commit 09917abd49078f74740eb281f5abdccbb38878ca
Merge: a2896fc9069f 88ff45d07559
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jul 3 17:41:22 2017 -0400

    Merge tag 'v3.18.56' into v3.18-rt

    This is the 3.18.56 stable release

commit a2896fc9069ff60b38334cfff9c2f98b4978790d
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Jun 2 04:26:36 2017 -0400

    Linux 3.18.55-rt61

commit 747b473ac140b2541d014bd0b05a8656b7428340
Merge: 198fe8520639 6b65a8f64f4f
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 1 20:39:52 2017 -0400

    Merge tag 'v3.18.55' into v3.18-rt

    This is the 3.18.55 stable release

commit 198fe852063947215dfa12ff9aa96dcf51a02e35
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 1 20:39:19 2017 -0400

    Linux 3.18.54-rt60

commit 2d0f605732d2cd640450beae14ba47f665a5c502
Merge: 6b8d7a8a070f 7b9d239f0d9f
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 1 20:38:54 2017 -0400

    Merge tag 'v3.18.54' into v3.18-rt

    This is the 3.18.54 stable release

commit 6b8d7a8a070fbc637599dc84c447735b5de54dc8
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 1 20:38:19 2017 -0400

    Linux 3.18.53-rt59

commit 8edc0a28d469548ed9dacb6ad27b53de0fe2a62b
Merge: 5612a2b39723 b3eba07a079e
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 1 20:38:03 2017 -0400

    Merge tag 'v3.18.53' into v3.18-rt

    This is the 3.18.53 stable release

commit 5612a2b397233aa01588fb7b1b08e83d657842f8
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 1 20:36:12 2017 -0400

    Linux 3.18.52-rt58

commit 615dd3941412e8f4f5c87f0912a6529f49c7bdd9
Merge: 6a22e2fa746c 68e50dad01f4
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 1 20:35:13 2017 -0400

    Merge tag 'v3.18.52' into v3.18-rt

    This is the 3.18.52 stable release

commit 6a22e2fa746cc117253522d7abc9c8baafd2eaf1
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 8 21:04:15 2017 -0400

    Linux 3.18.51-rt57

commit 0358881d956e2ef7844b80e4176eac7b00650053
Merge: b7882c56563c ce88f0271483
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 8 17:30:30 2017 -0400

    Merge tag 'v3.18.51' into v3.18-rt

    This is the 3.18.51 stable release

commit b7882c56563c219900a41262b1cd2dd20731f112
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 8 17:29:33 2017 -0400

    Linux 3.18.50-rt56

commit b9dc00596b94d50a272d32a62c9d069f0e74109c
Merge: 26cdaf0cbb2d 630b59cde7be
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 8 17:29:19 2017 -0400

    Merge tag 'v3.18.50' into v3.18-rt

    This is the 3.18.50 stable release

     Conflicts:
    	drivers/block/zram/zram_drv.c

commit 26cdaf0cbb2d15abc8b551c1f26c860dc9cd656c
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 8 17:28:55 2017 -0400

    Linux 3.18.49-rt55

commit db07e461cdc3fd6d866806d7db28a2eddd21d5e7
Merge: 7da5438755e5 e6ff2eed0d08
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 8 17:27:15 2017 -0400

    Merge tag 'v3.18.49' into v3.18-rt

    This is the 3.18.49 stable release

     Conflicts:
    	kernel/futex.c

commit 7da5438755e5c4ac20b7b6a3e689f5c6ecc18c49
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Mar 10 09:57:41 2017 -0500

    Linux 3.18.48-rt54

commit 6b69740c21d5504f11917c5587d236cf761e45fb
Author: Dan Murphy <dmurphy@ti.com>
Date:   Fri Feb 24 08:41:49 2017 -0600

    lockdep: Fix compilation error for !CONFIG_MODULES and !CONFIG_SMP

    When CONFIG_MODULES is not set then it fails to compile in lockdep:

    |kernel/locking/lockdep.c: In function 'look_up_lock_class':
    |kernel/locking/lockdep.c:684:12: error: implicit declaration of function
    | '__is_module_percpu_address' [-Werror=implicit-function-declaration]

    If CONFIG_MODULES is set but CONFIG_SMP is not, then it compiles but
    fails link at the end:

    |kernel/locking/lockdep.c:684: undefined reference to `__is_module_percpu_address'
    |kernel/built-in.o:(.debug_addr+0x1e674): undefined reference to `__is_module_percpu_address'

    This patch adds the function for both cases.

    Signed-off-by: Dan Murphy <dmurphy@ti.com>
    [bigeasy: merge the two patches from Dan into one, adapt changelog]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit 418cc8b2ff330b8532ed4792dad0bf9494ac5f16
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sun Feb 19 16:24:04 2017 +0100

    rt: Drop the removal of _GPL from rt_mutex_destroy()'s EXPORT_SYMBOL

    What we have now should be enough, the EXPORT_SYMBOL statement for
    rt_mutex_destroy() is not required.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit 819fd137a8beefd3470a8d39aebfb52feb953a0a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 17 19:44:39 2017 +0100

    lockdep: Handle statically initialized PER_CPU locks proper

    If a PER_CPU struct which contains a spin_lock is statically initialized
    via:

    DEFINE_PER_CPU(struct foo, bla) = {
    	.lock = __SPIN_LOCK_UNLOCKED(bla.lock)
    };

    then lockdep assigns a seperate key to each lock because the logic for
    assigning a key to statically initialized locks is to use the address as
    the key. With per CPU locks the address is obvioulsy different on each CPU.

    That's wrong, because all locks should have the same key.

    To solve this the following modifications are required:

     1) Extend the is_kernel/module_percpu_addr() functions to hand back the
        canonical address of the per CPU address, i.e. the per CPU address
        minus the per CPU offset.

     2) Check the lock address with these functions and if the per CPU check
        matches use the returned canonical address as the lock key, so all per
        CPU locks have the same key.

     3) Move the static_obj(key) check into look_up_lock_class() so this check
        can be avoided for statically initialized per CPU locks.  That's
        required because the canonical address fails the static_obj(key) check
        for obvious reasons.

    Reported-by: Mike Galbraith <efault@gmx.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit 881b6e83122ec49e210daa5e88142380a327a5de
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Feb 10 18:21:04 2017 +0100

    rt: Drop mutex_disable() on !DEBUG configs and the GPL suffix from export symbol

    Alex Goins reported that mutex_destroy() on RT will force a GPL only symbol
    which won't link and therefore fail on a non-GPL kernel module.
    This does not happen on !RT and is a regression on RT which we would like to
    avoid.
    I try here the easy thing and to not use rt_mutex_destroy() if
    CONFIG_DEBUG_MUTEXES is not enabled. This will still break for the DEBUG
    configs so instead of adding a wrapper around rt_mutex_destroy() (which we have
    for rt_mutex_lock() for instance) I am simply dropping the GPL part from the
    export.

    Reported-by: Alex Goins <agoins@nvidia.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit 50c685d89fd8408fba351e522c718dab9477f244
Author: John Ogness <john.ogness@linutronix.de>
Date:   Mon Jan 30 09:41:21 2017 +0100

    x86/mm/cpa: avoid wbinvd() for PREEMPT

    Although wbinvd() is faster than flushing many individual pages, it
    blocks the memory bus for "long" periods of time (>100us), thus
    directly causing unusually large latencies on all CPUs, regardless
    of any CPU isolation features that may be active.

    For 1024 pages, flushing those pages individually can take up to
    2200us, but the task remains fully preemptible during that time.

    Cc: stable-rt@vger.kernel.org
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: John Ogness <john.ogness@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit 7e7487e94ec971a55c476f354cf759a66b0c76a9
Author: Julia Cartwright <julia@ni.com>
Date:   Fri Jan 20 10:13:47 2017 -0600

    pinctrl: qcom: Use raw spinlock variants

    The MSM pinctrl driver currently implements an irq_chip for handling
    GPIO interrupts; due to how irq_chip handling is done, it's necessary
    for the irq_chip methods to be invoked from hardirq context, even on a
    a real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.

    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw
    spinlock.

    On real-time kernels, this fixes an OOPs which looks like the following,
    as reported by Brian Wrenn:

        kernel BUG at kernel/locking/rtmutex.c:1014!
        Internal error: Oops - BUG: 0 [#1] PREEMPT SMP
        Modules linked in: spidev_irq(O) smsc75xx wcn36xx [last unloaded: spidev]
        CPU: 0 PID: 1163 Comm: irq/144-mmc0 Tainted: G        W  O    4.4.9-linaro-lt-qcom #1
        PC is at rt_spin_lock_slowlock+0x80/0x2d8
        LR is at rt_spin_lock_slowlock+0x68/0x2d8
        [..]
      Call trace:
        rt_spin_lock_slowlock
        rt_spin_lock
        msm_gpio_irq_ack
        handle_edge_irq
        generic_handle_irq
        msm_gpio_irq_handler
        generic_handle_irq
        __handle_domain_irq
        gic_handle_irq

    Cc: stable-rt@vger.kernel.org
    Cc: Bjorn Andersson <bjorn.andersson@linaro.org>
    Reported-by: Brian Wrenn <dcbrianw@gmail.com>
    Tested-by: Brian Wrenn <dcbrianw@gmail.com>
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit 0f82cfd6cbdde000dfdd429b0a1ea72acf038f0b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jan 25 16:34:27 2017 +0100

    radix-tree: use local locks

    The preload functionality uses per-CPU variables and preempt-disable to
    ensure that it does not switch CPUs during its usage. This patch adds
    local_locks() instead preempt_disable() for the same purpose and to
    remain preemptible on -RT.

    Cc: stable-rt@vger.kernel.org
    Reported-and-debugged-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit e433cbc1fa41e70f7fef5038f69ee8e92154c78d
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Feb 22 10:30:08 2017 -0500

    Linux 3.18.48-rt53

commit bf35777b608e62b5f5c4d4b9697f48143c73fd35
Merge: f5cd67daa1a9 53752ea21001
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Feb 22 00:41:40 2017 -0500

    Merge tag 'v3.18.48' into v3.18-rt

    This is the 3.18.48 stable release

commit f5cd67daa1a9e2f3be2c70e715e50b03c8dd0df1
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Feb 13 16:26:56 2017 -0500

    Linux 3.18.47-rt52

commit 146721f0515306b91032da11f4c9eda27ba357f6
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Dec 12 16:14:18 2016 +0100

    workqueue: use rcu_readlock() in put_pwq_unlocked()

    The RCU sched protection was changed to RCU only and so all IRQ-off and
    preempt-off disabled region were changed to the relevant rcu-read-lock
    primitives. One was missed and triggered:
    |[ BUG: bad unlock balance detected! ]
    |4.4.30-rt41 #51 Tainted: G        W
    |btattach/345 is trying to release lock (
    |Unable to handle kernel paging request at virtual address 6b6b6bbb
    |Backtrace:
    |[<c016b5a0>] (lock_release) from [<c0804844>] (rt_spin_unlock+0x20/0x30)
    |[<c0804824>] (rt_spin_unlock) from [<c0138954>] (put_pwq_unlocked+0xa4/0x118)
    |[<c01388b0>] (put_pwq_unlocked) from [<c0138b2c>] (destroy_workqueue+0x164/0x1b0)
    |[<c01389c8>] (destroy_workqueue) from [<c078e1ac>] (hci_unregister_dev+0x120/0x21c)
    |[<c078e08c>] (hci_unregister_dev) from [<c054f658>] (hci_uart_tty_close+0x90/0xbc)
    |[<c054f5c8>] (hci_uart_tty_close) from [<c03a2be8>] (tty_ldisc_close+0x50/0x58)
    |[<c03a2b98>] (tty_ldisc_close) from [<c03a2cb4>] (tty_ldisc_kill+0x18/0x78)
    |[<c03a2c9c>] (tty_ldisc_kill) from [<c03a3528>] (tty_ldisc_release+0x100/0x134)
    |[<c03a3428>] (tty_ldisc_release) from [<c039cd68>] (tty_release+0x3bc/0x460)
    |[<c039c9ac>] (tty_release) from [<c020cc08>] (__fput+0xe0/0x1b4)
    |[<c020cb28>] (__fput) from [<c020cd3c>] (____fput+0x10/0x14)
    |[<c020cd2c>] (____fput) from [<c013e0d4>] (task_work_run+0xa4/0xb8)
    |[<c013e030>] (task_work_run) from [<c0121754>] (do_exit+0x40c/0x8b0)
    |[<c0121348>] (do_exit) from [<c0122ff8>] (do_group_exit+0x54/0xc4)

    Cc: stable-rt@vger.kernel.org
    Reported-by: John Keeping <john@metanate.com>
    Tested-by: John Keeping <john@metanate.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit 88d4125bae8d9234c7a1c21b3b3ebb2dfea0c356
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Feb 7 19:11:03 2017 -0500

    Linux 3.18.47-rt51

commit add36afb8e9c1250eceb62b2a0dd4f72819a70d0
Merge: cd7973372c31 8433e5c9c830
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Feb 7 19:10:51 2017 -0500

    Merge tag 'v3.18.47' into v3.18-rt

    Linux 3.18.47

commit cd7973372c310ab7ed8cd4db79d96ec29fcf9a17
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 3 23:33:43 2017 -0500

    Linux 3.18.46-rt50

commit 17f59e9a15b9ab8a589855c39dc16db19556ada0
Merge: f52196834661 1e20e732ffa1
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 3 17:49:24 2017 -0500

    Merge tag 'v3.18.46' into v3.18-rt

    Linux 3.18.46

commit f5219683466143e4ec38f53b52ab7786616cf72e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Sat Dec 10 07:53:06 2016 -0500

    Linux 3.18.45-rt49

commit 605bd69aa83949ad18319f660a2faa4623e061e5
Merge: f80743a0e8c2 ac3d826bef90
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Dec 9 21:54:14 2016 -0500

    Merge tag 'v3.18.45' into v3.18-rt

    Linux 3.18.45

commit f80743a0e8c29b73350e31ac4b99b6b6e6e324ba
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Oct 28 16:54:58 2016 -0400

    Linux 3.18.44-rt48

commit c9197e51410adc27581fd0f5685744f05a897bac
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Oct 28 16:51:14 2016 -0400

    x86: Fix bad backport for should_resched() update

    Backporting "x86/preempt-lazy: fixup should_resched()", I missed
    initializing the tmp variable with __preempt_count, and was using it
    uninitialized. Strangely, my gcc just optimized it completely out and never
    warned. Luckily, someone else had a gcc that would give a proper warning.

    Reported-by: "Koehrer Mathias (ETAS/ESW5)" <mathias.koehrer@etas.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 22432ff27e420eb48d90ad90235b26397e21333f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Oct 21 12:21:07 2016 +0200

    kbuild: add -fno-PIE

    Debian started to build the gcc with -fPIE by default so the kernel
    build ends before it starts properly with:
    |kernel/bounds.c:1:0: error: code model kernel does not support PIC mode

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 116c818bbd2b0b1a3375e595fc1c5cc4de294664
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Oct 20 12:49:10 2016 +0200

    zsmalloc: turn that get_cpu_light() into a local_lock()

    We might get preempted, grab the same ressource again and then corrupt
    the memory.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c978ab254b46b9a425071c4bb00adf8848348a2e
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Sun Oct 16 05:08:30 2016 +0200

    ftrace: Fix trace header alignment

    Line up helper arrows to the right column.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    [bigeasy: fixup function tracer header]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 6f5042e7314887c56de5f0441ac585b549f1f90f
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Oct 26 10:50:37 2016 -0400

    Linux 3.18.44-rt47

commit e3126ea0a5be07f562ea66811c7529bb4bfd2321
Merge: 970149ffe7b4 a6846cfd266b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Oct 25 22:31:20 2016 -0400

    Merge tag 'v3.18.44' into v3.18-rt

    Linux 3.18.44

commit 970149ffe7b4f8e027989a8a7d4696b8d9cc64d1
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Oct 19 10:32:04 2016 -0400

    Linux 3.18.43-rt46

commit a59ebcb8677ed8ccf4f07f024da8c47322a3f443
Merge: 940c8067f2f5 3cab355c2ff3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Oct 18 21:05:07 2016 -0400

    Merge tag 'v3.18.43' into v3.18-rt

    Linux 3.18.43

commit 940c8067f2f58aab66fa27a6a64bd1d5eeecdab9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Sep 28 08:11:11 2016 -0400

    Linux 3.18.42-rt45

commit f70b46ac21677259e810086dcf322a68b5a6b76c
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Sep 15 18:25:47 2016 +0200

    fs/dcache: incremental fixup of the retry routine

    It has been pointed out by tglx that on UP the non-RT task could spin
    its entire time slice because the lock owner is preempted. This won't
    happen on !RT. So we back to "chill" if we can't cond_resched() did not
    work.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f2e9dc2a4e29f74b6a7715fedd49825558a91861
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Sep 14 19:18:47 2016 +0200

    x86/preempt-lazy: fixup should_resched()

    should_resched() returns true if NEED_RESCHED is set and the
    preempt_count is 0 _or_ if NEED_RESCHED_LAZY is set ignoring the preempt
    counter. Ignoring the preemp counter is wrong. This patch adds this into
    account.
    While at it, __preempt_count_dec_and_test() ignores preempt_lazy_count
    while checking TIF_NEED_RESCHED_LAZY so we this check, too.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit dfd016a504433e72b71fa9b36ea8c19f0606e606
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Sep 8 18:33:52 2016 +0200

    fs/dcache: resched/chill only if we make no progress

    Upstream commit 47be61845c77 ("fs/dcache.c: avoid soft-lockup in
    dput()") changed the condition _when_ cpu_relax() / cond_resched() was
    invoked. This change was adapted in -RT into mostly the same thing
    except that if cond_resched() did nothing we had to do cpu_chill() to
    force the task off CPU for a tiny little bit in case the task had RT
    priority and did not want to leave the CPU.
    This change resulted in a performance regression (in my testcase the
    build time on /dev/shm increased from 19min to 24min). The reason is
    that with this change cpu_chill() was invoked even dput() made progress
    (dentry_kill() returned a different dentry) instead only if we were
    trying this operation on the same dentry over and over again.

    This patch brings back to the old behavior back to cond_resched() &
    chill if we make no progress. A little improvement is to invoke
    cpu_chill() only if we are a RT task (and avoid the sleep otherwise).
    Otherwise the scheduler should remove us from the CPU if we make no
    progress.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 29307f4388a733261a88a5dd05924f4c35a15e39
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Aug 31 17:54:09 2016 +0200

    net: add a lock around icmp_sk()

    It looks like the this_cpu_ptr() access in icmp_sk() is protected with
    local_bh_disable(). To avoid missing serialization in -RT I am adding
    here a local lock. No crash has been observed, this is just precaution.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 92982f9a6f0c66317d2fc5923a36d14a6151450e
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Aug 31 17:21:56 2016 +0200

    net: add back the missing serialization in ip_send_unicast_reply()

    Some time ago Sami Pietik√§inen reported a crash on -RT in
    ip_send_unicast_reply() which was later fixed by Nicholas Mc Guire
    (v3.12.8-rt11). Later (v3.18.8) the code was reworked and I dropped the
    patch. As it turns out it was mistake.
    I have reports that the same crash is possible with a similar backtrace.
    It seems that vanilla protects access to this_cpu_ptr() via
    local_bh_disable(). This does not work the on -RT since we can have
    NET_RX and NET_TX running in parallel on the same CPU.
    This is brings back the old locks.

    |Unable to handle kernel NULL pointer dereference at virtual address 00000010
    |PC is at __ip_make_skb+0x198/0x3e8
    |[<c04e39d8>] (__ip_make_skb) from [<c04e3ca8>] (ip_push_pending_frames+0x20/0x40)
    |[<c04e3ca8>] (ip_push_pending_frames) from [<c04e3ff0>] (ip_send_unicast_reply+0x210/0x22c)
    |[<c04e3ff0>] (ip_send_unicast_reply) from [<c04fbb54>] (tcp_v4_send_reset+0x190/0x1c0)
    |[<c04fbb54>] (tcp_v4_send_reset) from [<c04fcc1c>] (tcp_v4_do_rcv+0x22c/0x288)
    |[<c04fcc1c>] (tcp_v4_do_rcv) from [<c0474364>] (release_sock+0xb4/0x150)
    |[<c0474364>] (release_sock) from [<c04ed904>] (tcp_close+0x240/0x454)
    |[<c04ed904>] (tcp_close) from [<c0511408>] (inet_release+0x74/0x7c)
    |[<c0511408>] (inet_release) from [<c0470728>] (sock_release+0x30/0xb0)
    |[<c0470728>] (sock_release) from [<c0470abc>] (sock_close+0x1c/0x24)
    |[<c0470abc>] (sock_close) from [<c0115ec4>] (__fput+0xe8/0x20c)
    |[<c0115ec4>] (__fput) from [<c0116050>] (____fput+0x18/0x1c)
    |[<c0116050>] (____fput) from [<c0058138>] (task_work_run+0xa4/0xb8)
    |[<c0058138>] (task_work_run) from [<c0011478>] (do_work_pending+0xd0/0xe4)
    |[<c0011478>] (do_work_pending) from [<c000e740>] (work_pending+0xc/0x20)
    |Code: e3530001 8a000001 e3a00040 ea000011 (e5973010)

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit eefb360bd07502b8d2ceddb9b545da54f40ceeaf
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Thu Jul 28 06:04:49 2016 +0200

    scsi/fcoe: Fix get_cpu()/put_cpu_light() imbalance in fcoe_recv_frame()

    During master->rt merge, I stumbled across the buglet below.

    Fix get_cpu()/put_cpu_light() imbalance.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Mike Gabraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 59352ada04fe8114769e3ba6078f6724cd66d751
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jul 14 14:57:07 2016 +0200

    sched: lazy_preempt: avoid a warning in the !RT case

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f46dfabbd312c2929d763ba045a89dbedc20406b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jul 13 17:13:23 2016 +0200

    timers: wakeup all timer waiters without holding the base lock

    There should be no need to hold the base lock during the wakeup. There
    should be no boosting involved, the wakeup list has its own lock so it
    should be safe to do this without the lock.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit d0306bfb0afc607e451a973ae1896e4de8f44467
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jul 13 17:13:23 2016 +0200

    timers: wakeup all timer waiters

    The base lock is dropped during the invocation if the timer. That means
    it is possible that we have one waiter while timer1 is running and once
    this one finished, we get another waiter while timer2 is running. Since
    we wake up only one waiter it is possible that we miss the other one.
    This will probably heal itself over time because most of the time we
    complete timers without an active wake up.
    To avoid the scenario where we don't wake up all waiters at once,
    wake_up_all() is used.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0b1264494b1a295a74c301530124835e6841918b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 20 22:20:44 2016 -0400

    Linux 3.18.42-rt44

commit bcc77dc018550ac48f27ca1e39d72c442f3eaa63
Merge: 76c157a5bb69 a8e202812b52
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 20 18:34:08 2016 -0400

    Merge tag 'v3.18.42' into v3.18-rt

    Linux 3.18.42

commit 76c157a5bb6910d4c47d1d8de45a0751bda8f983
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 20 18:33:37 2016 -0400

    Linux 3.18.41-rt43

commit 943929cf9ca9f611b1398a20456bdce03cfac095
Merge: 329f98847810 e9e1b43e9f91
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 20 18:31:46 2016 -0400

    Merge tag 'v3.18.41' into v3.18-rt

    Linux 3.18.41

commit 329f9884781004ddccffb2bb1c2bc3987189e914
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Aug 30 19:07:45 2016 -0400

    Linux 3.18.40-rt42

commit 51e94468c5b87f5305bece7957cbc3e606b55320
Merge: b62a4b1349e4 d45da77ced2a
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Aug 30 17:23:19 2016 -0400

    Merge tag 'v3.18.40' into v3.18-rt

    Linux 3.18.40

     Conflicts:
    	fs/dcache.c

commit b62a4b1349e4a84a15ba56b85d9c4ed8fde889ef
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Aug 15 13:40:18 2016 -0400

    Linux 3.18.39-rt41

commit 1b9f9af2da4828d4fc00eae8ef45dabf27ad7397
Merge: b5e7b7f952f8 6e0f6268e3d1
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Aug 15 11:46:48 2016 -0400

    Merge tag 'v3.18.39' into v3.18-rt

    Linux 3.18.39

commit b5e7b7f952f8643af05c20f08acb175e7d45ac94
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Aug 15 10:48:59 2016 -0400

    Linux 3.18.38-rt40

commit 66cd7bc4ba92c9c09073769327a3b48be6b7a636
Merge: 3b6bd95954c1 0d1097ae44c2
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Aug 15 10:09:53 2016 -0400

    Merge tag 'v3.18.38' into v3.18-rt

    Linux 3.18.38

commit 3b6bd95954c182f7e27cd348677de7aad5170383
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Sat Aug 13 00:17:00 2016 -0400

    Linux 3.18.37-rt39

commit 14d54e3b33d8c06ad301a8bbac6e0611c2d68f3f
Merge: cf3a38958bb4 0ac0a856d986
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Sat Aug 13 00:16:45 2016 -0400

    Merge tag 'v3.18.37' into v3.18-rt

    Linux 3.18.37

     Conflicts:
    	mm/swap.c

commit cf3a38958bb4b88e877e89b7e323d1f26cd35b46
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jul 14 10:05:07 2016 -0400

    Linux 3.18.36-rt38

commit 0e1f182a04bace1d8da3b21bc1f6213861a2b461
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed May 25 14:03:50 2016 +0200

    trace: correct off by one while recording the trace-event

    Trace events like raw_syscalls show always a preempt code of one. The
    reason is that on PREEMPT kernels rcu_read_lock_sched_notrace()
    increases the preemption counter and the function recording the counter
    is caller within the RCU section.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    [ Changed this to upstream version. See commit e947841c0dce ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 222788a4688e305cbb3992f69baf8a88a5853c47
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Fri May 27 15:03:28 2016 +0200

    mm: perform lru_add_drain_all() remotely

    lru_add_drain_all() works by scheduling lru_add_drain_cpu() to run
    on all CPUs that have non-empty LRU pagevecs and then waiting for
    the scheduled work to complete. However, workqueue threads may never
    have the chance to run on a CPU that's running a SCHED_FIFO task.
    This causes lru_add_drain_all() to block forever.

    This commit solves this problem by changing lru_add_drain_all()
    to drain the LRU pagevecs of remote CPUs. This is done by grabbing
    swapvec_lock and calling lru_add_drain_cpu().

    PS: This is based on an idea and initial implementation by
        Rik van Riel.

    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit d90a745aa361ecba9051870de4d6fae88069677f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri May 27 15:11:51 2016 +0200

    locallock: add local_lock_on()

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 31a1d2c85694e09d6f74d1ed2f2d1fd9e45f0f63
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue May 24 12:56:38 2016 +0200

    arm: lazy preempt: correct resched condition

    If we get out of preempt_schedule_irq() then we check for NEED_RESCHED
    and call the former function again if set because the preemption counter
    has be zero at this point.
    However the counter for lazy-preempt might not be zero therefore we have
    to check the counter before looking at the need_resched_lazy flag.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 208fdb2ea86be15413e47062659f0e4b3bfd17b6
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu May 19 17:45:27 2016 +0200

    kernel/printk: Don't try to print from IRQ/NMI region

    On -RT we try to acquire sleeping locks which might lead to warnings
    from lockdep or a warn_on() from spin_try_lock() (which is a rtmutex on
    RT).
    We don't print in general from a IRQ off region so we should not try
    this via console_unblank() / bust_spinlocks() as well.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 34b2b7c497616789ecbae5e4b11f7bff14d6dcb3
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 16 15:01:11 2016 +0200

    sched,preempt: Fix preempt_count manipulations

    Vikram reported that his ARM64 compiler managed to 'optimize' away the
    preempt_count manipulations in code like:

    	preempt_enable_no_resched();
    	put_user();
    	preempt_disable();

    Irrespective of that fact that that is horrible code that should be
    fixed for many reasons, it does highlight a deficiency in the generic
    preempt_count manipulators. As it is never right to combine/elide
    preempt_count manipulations like this.

    Therefore sprinkle some volatile in the two generic accessors to
    ensure the compiler is aware of the fact that the preempt_count is
    observed outside of the regular program-order view and thus cannot be
    optimized away like this.

    x86; the only arch not using the generic code is not affected as we
    do all this in asm in order to use the segment base per-cpu stuff.

    Cc: stable@vger.kernel.org
    Cc: stable-rt@vger.kernel.org
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: a787870924db ("sched, arch: Create asm/preempt.h")
    Reported-by: Vikram Mulukutla <markivx@codeaurora.org>
    Tested-by: Vikram Mulukutla <markivx@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ea7b63a71e3f8bd3b06ddfca5402f34a3a6991d4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 22 22:19:25 2016 +0000

    perf/x86/intel/rapl: Make PMU lock raw

    Upstream commit: a208749c6426 ("perf/x86/intel/rapl: Make PMU lock raw")

    This lock is taken in hard interrupt context even on Preempt-RT. Make it raw
    so RT does not have to patch it.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Harish Chegondi <harish.chegondi@intel.com>
    Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: linux-kernel@vger.kernel.org
    Cc: stable-rt@vger.kernel.org
    Link: http://lkml.kernel.org/r/20160222221012.669411833@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 56528cf9b1ceb44603743cfcd59ddfeea7d21209
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Apr 8 16:18:24 2016 +0200

    drivers/block/zram: fixup compile for !RT

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 5b2c3c7909c9428123587c08b1b651a2e31eb46e
Author: Josh Cartwright <joshc@ni.com>
Date:   Thu Mar 31 00:04:25 2016 -0500

    list_bl: fixup bogus lockdep warning

    At first glance, the use of 'static inline' seems appropriate for
    INIT_HLIST_BL_HEAD().

    However, when a 'static inline' function invocation is inlined by gcc,
    all callers share any static local data declared within that inline
    function.

    This presents a problem for how lockdep classes are setup.  raw_spinlocks, for
    example, when CONFIG_DEBUG_SPINLOCK,

    	# define raw_spin_lock_init(lock)				\
    	do {								\
    		static struct lock_class_key __key;			\
    									\
    		__raw_spin_lock_init((lock), #lock, &__key);		\
    	} while (0)

    When this macro is expanded into a 'static inline' caller, like
    INIT_HLIST_BL_HEAD():

    	static inline INIT_HLIST_BL_HEAD(struct hlist_bl_head *h)
    	{
    		h->first = NULL;
    		raw_spin_lock_init(&h->lock);
    	}

    ...the static local lock_class_key object is made a function static.

    For compilation units which initialize invoke INIT_HLIST_BL_HEAD() more
    than once, then, all of the invocations share this same static local
    object.

    This can lead to some very confusing lockdep splats (example below).
    Solve this problem by forcing the INIT_HLIST_BL_HEAD() to be a macro,
    which prevents the lockdep class object sharing.

     =============================================
     [ INFO: possible recursive locking detected ]
     4.4.4-rt11 #4 Not tainted
     ---------------------------------------------
     kswapd0/59 is trying to acquire lock:
      (&h->lock#2){+.+.-.}, at: mb_cache_shrink_scan

     but task is already holding lock:
      (&h->lock#2){+.+.-.}, at:  mb_cache_shrink_scan

     other info that might help us debug this:
      Possible unsafe locking scenario:

            CPU0
            ----
       lock(&h->lock#2);
       lock(&h->lock#2);

      *** DEADLOCK ***

      May be due to missing lock nesting notation

     2 locks held by kswapd0/59:
      #0:  (shrinker_rwsem){+.+...}, at: rt_down_read_trylock
      #1:  (&h->lock#2){+.+.-.}, at: mb_cache_shrink_scan

    Reported-by: Luis Claudio R. Goncalves <lclaudio@uudg.org>
    Tested-by: Luis Claudio R. Goncalves <lclaudio@uudg.org>
    Signed-off-by: Josh Cartwright <joshc@ni.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1044df71c86c05ce4376d8c6d3238e2405c09cf0
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Thu Mar 31 04:08:28 2016 +0200

    drivers/block/zram: Replace bit spinlocks with rtmutex for -rt

    They're nondeterministic, and lead to ___might_sleep() splats in -rt.
    OTOH, they're a lot less wasteful than an rtmutex per page.

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b17f44a4aff93cfb88ea034f8e35edab6ea74bce
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Mar 30 13:36:29 2016 +0200

    net: dev: always take qdisc's busylock in __dev_xmit_skb()

    The root-lock is dropped before dev_hard_start_xmit() is invoked and after
    setting the __QDISC___STATE_RUNNING bit. If this task is now pushed away
    by a task with a higher priority then the task with the higher priority
    won't be able to submit packets to the NIC directly instead they will be
    enqueued into the Qdisc. The NIC will remain idle until the task(s) with
    higher priority leave the CPU and the task with lower priority gets back
    and finishes the job.

    If we take always the busylock we ensure that the RT task can boost the
    low-prio task and submit the packet.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit acdcdcd553b6900c5d480d2dbf53b637d354f1c6
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Tue Mar 22 11:16:09 2016 +0100

    mm/zsmalloc: Use get/put_cpu_light in zs_map_object()/zs_unmap_object()

    Otherwise, we get a ___might_sleep() splat.

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3d705a8c61feeba2edcdb9456858100f8ad06614
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Mar 21 15:13:27 2016 +0100

    kvm, rt: change async pagefault code locking for PREEMPT_RT

    The async pagefault wake code can run from the idle task in exception
    context, so everything here needs to be made non-preemptible.

    Conversion to a simple wait queue and raw spinlock does the trick.

    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 102f914b209d29682139970bc815e25fbf6a1c94
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 19:19:16 2016 -0400

    Linux 3.18.36-rt37

commit b713e130ade55639a63be292497738418d75317d
Merge: 18e4efd246ee d420f00c7bfb
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:29:25 2016 -0400

    Merge tag 'v3.18.36' into v3.18-rt

    Linux 3.18.36

commit 18e4efd246eed835e5a1832963f245aa08f53e40
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:29:11 2016 -0400

    Linux 3.18.35-rt36

commit bfd659ba48983558db9f3d2cbbd5a4a3dccde9b0
Merge: 9658e68f7677 b5076139991c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:29:02 2016 -0400

    Merge tag 'v3.18.35' into v3.18-rt

    Linux 3.18.35

commit 9658e68f7677820407a6cf6575e7bfa0a5ca8f13
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:28:48 2016 -0400

    Linux 3.18.34-rt35

commit 2247d3fb0e36be462054e438909992967fa5d949
Merge: 794e782487a0 3b6aa07b936b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:28:38 2016 -0400

    Merge tag 'v3.18.34' into v3.18-rt

    Linux 3.18.34

commit 794e782487a0e832cc18b99a6ffd5830c837a039
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:28:23 2016 -0400

    Linux 3.18.33-rt34

commit a3a174ec5e0009ead70c3e399ee604aee6c7ba61
Merge: 7c49fbfbadb5 6b12ebc0ecce
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:28:13 2016 -0400

    Merge tag 'v3.18.33' into v3.18-rt

    Linux 3.18.33

commit 7c49fbfbadb5967fd5b8bcc07d06df4f649cc0b9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:27:56 2016 -0400

    Linux 3.18.32-rt33

commit 098570e37f5042006e740a5bce87f1d21dc8d7d6
Merge: 34d9300b3676 834125557e0a
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:27:49 2016 -0400

    Merge tag 'v3.18.32' into v3.18-rt

    Linux 3.18.32

commit 34d9300b367655f58f428acf5f75468c6e3c6f9e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:27:16 2016 -0400

    Linux 3.18.31-rt32

commit de3b543ade5b28228ff76185a61dea3fcba76f76
Merge: d7050ae964b7 f151e73cea45
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:26:41 2016 -0400

    Merge tag 'v3.18.31' into v3.18-rt

    Linux 3.18.31

commit d7050ae964b720bb384ed352e76b1d53b09e0d59
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:26:18 2016 -0400

    Linux 3.18.30-rt31

commit b8513e0addfcfc7612945914e64cc721a758960e
Merge: 18303cd31e84 b36eba9b4dd4
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 17:25:59 2016 -0400

    Merge tag 'v3.18.30' into v3.18-rt

    Linux 3.18.30

commit 18303cd31e84f06eb780dd58ea213e4a515609fc
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Mar 28 20:50:51 2016 -0400

    Linux 3.18.29-rt30

commit 4aea7c27d5874066791e921d8e014e87c9568865
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Sep 19 10:25:18 2015 +0200

    Revert d04ea10ba1ea mmc: sdhci: don't provide hard irq handler

    This workaround cannot work because the logic in the SDHCI driver is:

    interrupt
      hard_interrupt_handler()
        wake_thread_handler()

    thread_handler()
      poke_hardware()
      wait_for_interrupt()

    Now the reverted commit changed that to:

    interrupt
        wake_thread()

    thread()
      hard_interrupt_handler()
        thread_handler()
        poke_hardware()
        wait_for_interrupt()

    Doesn't really work well because thread handler waits for
    hard_interrupt_handler() to wake it up again.....

    This revert reintroduces the problem which was tried to be solved by
    this patch. This can't be solved at the driver level, this needs a
    solution at the irq core code.

    Reported-by: Nathan Sullivan <nathan.sullivan@ni.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 10e4d1d556f442e792cf2dcaf1fdcfa942fbed53
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Mar 28 17:39:10 2016 -0400

    Linux 3.18.29-rt29

commit 4b6b018364975ef8aa845197d40387b3db097a21
Merge: 700b7080eb2e d439e869d612
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Mar 28 17:38:58 2016 -0400

    Merge tag 'v3.18.29' into v3.18-rt

    Linux 3.18.29

commit 700b7080eb2e8a941a3b03f3b4f1d19142a97cfd
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Mar 10 21:03:52 2016 -0500

    Linux 3.18.28-rt28

commit f023eebbc840300319282c09299916243c910a13
Merge: a69d68687e04 0f67c5beb42a
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Mar 10 17:12:01 2016 -0500

    Merge tag 'v3.18.28' into v3.18-rt

    Linux 3.18.28

commit a69d68687e04e6f5e9b79a5ac8dacc9d86f195c7
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Mar 7 15:09:30 2016 -0500

    Linux 3.18.27-rt27

commit e71ba400ce79f35e79863fa440259736125cfa12
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Fri Mar 4 05:04:06 2016 +0100

    tracing: Fix probe_wakeup_latency_hist_start() prototype

    Drop 'success' arg from probe_wakeup_latency_hist_start().

    Link: http://lkml.kernel.org/r/1457064246.3501.2.camel@gmail.com
    Fixes: cf1dd658 sched: Introduce the trace_sched_waking tracepoint
    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a4d0d3583b65861f008dcb92df19b4fd4f7c78a4
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Feb 11 22:06:28 2016 +0100

    kernel: sched: Fix preempt_disable_ip recodring for preempt_disable()

    preempt_disable() invokes preempt_count_add() which saves the caller in
    current->preempt_disable_ip. It uses CALLER_ADDR1 which does not look for its
    caller but for the parent of the caller. Which means we get the correct caller
    for something like spin_lock() unless the architectures inlines those
    invocations. It is always wrong for preempt_disable() or local_bh_disable().

    This patch makes the function get_parent_ip() which tries CALLER_ADDR0,1,2 if
    the former is a locking function.  This seems to record the preempt_disable()
    caller properly for preempt_disable() itself as well as for get_cpu_var() or
    local_bh_disable().

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 08d80d860afd23fb7253b4a47261e95cda254f2f
Author: Clark Williams <williams@redhat.com>
Date:   Fri Feb 26 13:19:20 2016 -0600

    rcu/torture: Comment out rcu_bh ops on PREEMPT_RT_FULL

    RT has dropped support of rcu_bh, comment out in rcutorture.

    Signed-off-by: Clark Williams <williams@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ed9c517cdc65cb47f3e788a0f0744031ca0d8458
Author: Yang Shi <yang.shi@linaro.org>
Date:   Fri Feb 26 16:25:25 2016 -0800

    f2fs: Mutex can't be used by down_write_nest_lock()

    fsf2_lock_all() calls down_write_nest_lock() to acquire a rw_sem and check
    a mutex, but down_write_nest_lock() is designed for two rw_sem accoring to the
    comment in include/linux/rwsem.h. And, other than f2fs, it is just called in
    mm/mmap.c with two rwsem.

    So, it looks it is used wrongly by f2fs. And, it causes the below compile
    warning on -rt kernel too.

    In file included from fs/f2fs/xattr.c:25:0:
    fs/f2fs/f2fs.h: In function 'f2fs_lock_all':
    fs/f2fs/f2fs.h:962:34: warning: passing argument 2 of 'down_write_nest_lock' from
    		       incompatible pointer type [-Wincompatible-pointer-types]
      f2fs_down_write(&sbi->cp_rwsem, &sbi->cp_mutex);
                                      ^

    The nest annotation was anyway bogus as nested annotations for lockdep are
    only required if one nests two locks of the same lock class, which is not the
    case here.

    Signed-off-by: Yang Shi <yang.shi@linaro.org>
    Cc: cm224.lee@samsung.com
    Cc: chao2.yu@samsung.com
    Cc: linaro-kernel@lists.linaro.org
    Cc: linux-rt-users@vger.kernel.org
    Cc: bigeasy@linutronix.de
    Cc: rostedt@goodmis.org
    Cc: linux-f2fs-devel@lists.sourceforge.net
    Cc: linux-fsdevel@vger.kernel.org
    Cc: jaegeuk@kernel.org
    Link: http://lkml.kernel.org/r/1456532725-4126-1-git-send-email-yang.shi@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit aec688118ebc6e29cd5d805b1d3f23d5fe0d6690
Author: Yang Shi <yang.shi@windriver.com>
Date:   Tue Feb 23 13:23:23 2016 -0800

    trace: Use rcuidle version for preemptoff_hist trace point

    When running -rt kernel with both PREEMPT_OFF_HIST and LOCKDEP enabled,
    the below error is reported:

     [ INFO: suspicious RCU usage. ]
     4.4.1-rt6 #1 Not tainted
     include/trace/events/hist.h:31 suspicious rcu_dereference_check() usage!

     other info that might help us debug this:

     RCU used illegally from idle CPU!
     rcu_scheduler_active = 1, debug_locks = 0
     RCU used illegally from extended quiescent state!
     no locks held by swapper/0/0.

     stack backtrace:
     CPU: 0 PID: 0 Comm: swapper/0 Not tainted 4.4.1-rt6-WR8.0.0.0_standard #1
     Stack : 0000000000000006 0000000000000000 ffffffff81ca8c38 ffffffff81c8fc80
        ffffffff811bdd68 ffffffff81cb0000 0000000000000000 ffffffff81cb0000
        0000000000000000 0000000000000000 0000000000000004 0000000000000000
        0000000000000004 ffffffff811bdf50 0000000000000000 ffffffff82b60000
        0000000000000000 ffffffff812897ac ffffffff819f0000 000000000000000b
        ffffffff811be460 ffffffff81b7c588 ffffffff81c8fc80 0000000000000000
        0000000000000000 ffffffff81ec7f88 ffffffff81d70000 ffffffff81b70000
        ffffffff81c90000 ffffffff81c3fb00 ffffffff81c3fc28 ffffffff815e6f98
        0000000000000000 ffffffff81c8fa87 ffffffff81b70958 ffffffff811bf2c4
        0707fe32e8d60ca5 ffffffff81126d60 0000000000000000 0000000000000000
        ...
     Call Trace:
     [<ffffffff81126d60>] show_stack+0xe8/0x108
     [<ffffffff815e6f98>] dump_stack+0x88/0xb0
     [<ffffffff8124b88c>] time_hardirqs_off+0x204/0x300
     [<ffffffff811aa5dc>] trace_hardirqs_off_caller+0x24/0xe8
     [<ffffffff811a4ec4>] cpu_startup_entry+0x39c/0x508
     [<ffffffff81d7dc68>] start_kernel+0x584/0x5a0

    Replace regular trace_preemptoff_hist to rcuidle version to avoid the error.

    Signed-off-by: Yang Shi <yang.shi@windriver.com>
    Cc: bigeasy@linutronix.de
    Cc: rostedt@goodmis.org
    Cc: linux-rt-users@vger.kernel.org
    Link: http://lkml.kernel.org/r/1456262603-10075-1-git-send-email-yang.shi@windriver.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 439a2c76e7534ad6122559bbb567c437801ea96f
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Sat Feb 27 09:01:42 2016 +0100

    drm,i915: Use local_lock/unlock_irq() in intel_pipe_update_start/end()

    [    8.014039] BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:918
    [    8.014041] in_atomic(): 0, irqs_disabled(): 1, pid: 78, name: kworker/u4:4
    [    8.014045] CPU: 1 PID: 78 Comm: kworker/u4:4 Not tainted 4.1.7-rt7 #5
    [    8.014055] Workqueue: events_unbound async_run_entry_fn
    [    8.014059]  0000000000000000 ffff880037153748 ffffffff815f32c9 0000000000000002
    [    8.014063]  ffff88013a50e380 ffff880037153768 ffffffff815ef075 ffff8800372c06c8
    [    8.014066]  ffff8800372c06c8 ffff880037153778 ffffffff8107c0b3 ffff880037153798
    [    8.014067] Call Trace:
    [    8.014074]  [<ffffffff815f32c9>] dump_stack+0x4a/0x61
    [    8.014078]  [<ffffffff815ef075>] ___might_sleep.part.93+0xe9/0xee
    [    8.014082]  [<ffffffff8107c0b3>] ___might_sleep+0x53/0x80
    [    8.014086]  [<ffffffff815f9064>] rt_spin_lock+0x24/0x50
    [    8.014090]  [<ffffffff8109368b>] prepare_to_wait+0x2b/0xa0
    [    8.014152]  [<ffffffffa016c04c>] intel_pipe_update_start+0x17c/0x300 [i915]
    [    8.014156]  [<ffffffff81093b40>] ? prepare_to_wait_event+0x120/0x120
    [    8.014201]  [<ffffffffa0158f36>] intel_begin_crtc_commit+0x166/0x1e0 [i915]
    [    8.014215]  [<ffffffffa00c806d>] drm_atomic_helper_commit_planes+0x5d/0x1a0 [drm_kms_helper]
    [    8.014260]  [<ffffffffa0171e9b>] intel_atomic_commit+0xab/0xf0 [i915]
    [    8.014288]  [<ffffffffa00654c7>] drm_atomic_commit+0x37/0x60 [drm]
    [    8.014298]  [<ffffffffa00c6fcd>] drm_atomic_helper_plane_set_property+0x8d/0xd0 [drm_kms_helper]
    [    8.014301]  [<ffffffff815f77d9>] ? __ww_mutex_lock+0x39/0x40
    [    8.014319]  [<ffffffffa0053b3d>] drm_mode_plane_set_obj_prop+0x2d/0x90 [drm]
    [    8.014328]  [<ffffffffa00c8edb>] restore_fbdev_mode+0x6b/0xf0 [drm_kms_helper]
    [    8.014337]  [<ffffffffa00cae49>] drm_fb_helper_restore_fbdev_mode_unlocked+0x29/0x80 [drm_kms_helper]
    [    8.014346]  [<ffffffffa00caec2>] drm_fb_helper_set_par+0x22/0x50 [drm_kms_helper]
    [    8.014390]  [<ffffffffa016dfba>] intel_fbdev_set_par+0x1a/0x60 [i915]
    [    8.014394]  [<ffffffff81327dc4>] fbcon_init+0x4f4/0x580
    [    8.014398]  [<ffffffff8139ef4c>] visual_init+0xbc/0x120
    [    8.014401]  [<ffffffff813a1623>] do_bind_con_driver+0x163/0x330
    [    8.014405]  [<ffffffff813a1b2c>] do_take_over_console+0x11c/0x1c0
    [    8.014408]  [<ffffffff813236e3>] do_fbcon_takeover+0x63/0xd0
    [    8.014410]  [<ffffffff81328965>] fbcon_event_notify+0x785/0x8d0
    [    8.014413]  [<ffffffff8107c12d>] ? __might_sleep+0x4d/0x90
    [    8.014416]  [<ffffffff810775fe>] notifier_call_chain+0x4e/0x80
    [    8.014419]  [<ffffffff810779cd>] __blocking_notifier_call_chain+0x4d/0x70
    [    8.014422]  [<ffffffff81077a06>] blocking_notifier_call_chain+0x16/0x20
    [    8.014425]  [<ffffffff8132b48b>] fb_notifier_call_chain+0x1b/0x20
    [    8.014428]  [<ffffffff8132d8fa>] register_framebuffer+0x21a/0x350
    [    8.014439]  [<ffffffffa00cb164>] drm_fb_helper_initial_config+0x274/0x3e0 [drm_kms_helper]
    [    8.014483]  [<ffffffffa016f1cb>] intel_fbdev_initial_config+0x1b/0x20 [i915]
    [    8.014486]  [<ffffffff8107912c>] async_run_entry_fn+0x4c/0x160
    [    8.014490]  [<ffffffff81070ffa>] process_one_work+0x14a/0x470
    [    8.014493]  [<ffffffff81071489>] worker_thread+0x169/0x4c0
    [    8.014496]  [<ffffffff81071320>] ? process_one_work+0x470/0x470
    [    8.014499]  [<ffffffff81076606>] kthread+0xc6/0xe0
    [    8.014502]  [<ffffffff81070000>] ? queue_work_on+0x80/0x110
    [    8.014506]  [<ffffffff81076540>] ? kthread_worker_fn+0x1c0/0x1c0

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: linux-rt-users <linux-rt-users@vger.kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit d9e9e9f62ca100d41854637c6c6ec0209145c02b
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Sat Feb 27 08:09:11 2016 +0100

    drm,radeon,i915: Use preempt_disable/enable_rt() where recommended

    DRM folks identified the spots, so use them.

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: linux-rt-users <linux-rt-users@vger.kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b74d4206bb076e54d9bddf190c72440e444dd519
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Sun Feb 21 16:11:30 2016 +0100

    sched,rt: __always_inline preemptible_lazy()

    homer: # nm kernel/sched/core.o|grep preemptible_lazy
    00000000000000b5 t preemptible_lazy

    echo wakeup_rt > current_tracer ==> Welcome to infinity.

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: linux-rt-users <linux-rt-users@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1456067490.3771.2.camel@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b1bd523bc988ab868acc1fbd7a721c8a27e6e845
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Feb 27 10:47:10 2016 +0100

    tick/broadcast: Make broadcast hrtimer irqsafe

    Otherwise we end up with the following:

    |=================================
    |[ INFO: inconsistent lock state ]
    |4.4.2-rt7+ #5 Not tainted
    |---------------------------------
    |inconsistent {IN-HARDIRQ-W} -> {HARDIRQ-ON-W} usage.
    |ktimersoftd/0/4 [HC0[0]:SC0[0]:HE1:SE1] takes:
    | (tick_broadcast_lock){?.....}, at: [<ffffffc000150db4>] tick_handle_oneshot_broadcast+0x58/0x27c
    |{IN-HARDIRQ-W} state was registered at:
    |  [<ffffffc000118198>] mark_lock+0x19c/0x6a0
    |  [<ffffffc000119728>] __lock_acquire+0xb1c/0x2100
    |  [<ffffffc00011b560>] lock_acquire+0xf8/0x230
    |  [<ffffffc00061bf08>] _raw_spin_lock_irqsave+0x50/0x68
    |  [<ffffffc000152188>] tick_broadcast_switch_to_oneshot+0x20/0x60
    |  [<ffffffc0001529f4>] tick_switch_to_oneshot+0x64/0xd8
    |  [<ffffffc000152b00>] tick_init_highres+0x1c/0x24
    |  [<ffffffc000141e58>] hrtimer_run_queues+0x78/0x100
    |  [<ffffffc00013f804>] update_process_times+0x38/0x74
    |  [<ffffffc00014fc5c>] tick_periodic+0x60/0x140
    |  [<ffffffc00014fd68>] tick_handle_periodic+0x2c/0x94
    |  [<ffffffc00052b878>] arch_timer_handler_phys+0x3c/0x48
    |  [<ffffffc00012d078>] handle_percpu_devid_irq+0x100/0x390
    |  [<ffffffc000127f34>] generic_handle_irq+0x34/0x4c
    |  [<ffffffc000128300>] __handle_domain_irq+0x90/0xf8
    |  [<ffffffc000082554>] gic_handle_irq+0x5c/0xa4
    |  [<ffffffc0000855ac>] el1_irq+0x6c/0xec
    |  [<ffffffc000112bec>] default_idle_call+0x2c/0x44
    |  [<ffffffc000113058>] cpu_startup_entry+0x3cc/0x410
    |  [<ffffffc0006169f8>] rest_init+0x158/0x168
    |  [<ffffffc000888954>] start_kernel+0x3a0/0x3b4
    |  [<0000000080621000>] 0x80621000
    |irq event stamp: 18723
    |hardirqs last  enabled at (18723): [<ffffffc00061c188>] _raw_spin_unlock_irq+0x38/0x80
    |hardirqs last disabled at (18722): [<ffffffc000140a4c>] run_hrtimer_softirq+0x2c/0x2f4
    |softirqs last  enabled at (0): [<ffffffc0000c4744>] copy_process.isra.50+0x300/0x16d4
    |softirqs last disabled at (0): [<          (null)>]           (null)

    Reported-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 5738c062214e7512b3b177789c6d59629a7a3307
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 10 18:25:16 2016 +0100

    kernel/stop_machine: partly revert "stop_machine: Use raw spinlocks"

    With completion using swait and so rawlocks we don't need this anymore.
    Further, bisect thinks this patch is responsible for:

    |BUG: unable to handle kernel NULL pointer dereference at           (null)
    |IP: [<ffffffff81082123>] sched_cpu_active+0x53/0x70
    |PGD 0
    |Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC
    |Dumping ftrace buffer:
    |   (ftrace buffer empty)
    |Modules linked in:
    |CPU: 1 PID: 0 Comm: swapper/1 Not tainted 4.4.1+ #330
    |Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS Debian-1.8.2-1 04/01/2014
    |task: ffff88013ae64b00 ti: ffff88013ae74000 task.ti: ffff88013ae74000
    |RIP: 0010:[<ffffffff81082123>]  [<ffffffff81082123>] sched_cpu_active+0x53/0x70
    |RSP: 0000:ffff88013ae77eb8  EFLAGS: 00010082
    |RAX: 0000000000000001 RBX: ffffffff81c2cf20 RCX: 0000001050fb52fb
    |RDX: 0000001050fb52fb RSI: 000000105117ca1e RDI: 00000000001c7723
    |RBP: 0000000000000000 R08: 0000000000000000 R09: 0000000000000001
    |R10: 0000000000000000 R11: 0000000000000001 R12: 00000000ffffffff
    |R13: ffffffff81c2cee0 R14: 0000000000000000 R15: 0000000000000001
    |FS:  0000000000000000(0000) GS:ffff88013b200000(0000) knlGS:0000000000000000
    |CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    |CR2: 0000000000000000 CR3: 0000000001c09000 CR4: 00000000000006e0
    |Stack:
    | ffffffff810c446d ffff88013ae77f00 ffffffff8107d8dd 000000000000000a
    | 0000000000000001 0000000000000000 0000000000000000 0000000000000000
    | 0000000000000000 ffff88013ae77f10 ffffffff8107d90e ffff88013ae77f20
    |Call Trace:
    | [<ffffffff810c446d>] ? debug_lockdep_rcu_enabled+0x1d/0x20
    | [<ffffffff8107d8dd>] ? notifier_call_chain+0x5d/0x80
    | [<ffffffff8107d90e>] ? __raw_notifier_call_chain+0xe/0x10
    | [<ffffffff810598a3>] ? cpu_notify+0x23/0x40
    | [<ffffffff8105a7b8>] ? notify_cpu_starting+0x28/0x30

    during hotplug. The rawlocks need to remain however.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 8d51d3a296b6ec4aebd0d6d7e1b7162cd9bf6662
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Feb 9 18:17:18 2016 +0100

    kernel: softirq: unlock with irqs on

    We unlock the lock while the interrupts are off. This isn't a problem
    now but will get because the migrate_disable() + enable are not
    symmetrical in regard to the status of interrupts.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 62044e554f14547061afcfef7f0aceda43e28982
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Feb 9 18:18:01 2016 +0100

    kernel: migrate_disable() do fastpath in atomic & irqs-off

    With interrupts off it makes no sense to do the long path since we can't
    leave the CPU anyway. Also we might end up in a recursion with lockdep.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 193ecd3fb80e18929c16b0340b9ca5a27d74a26b
Author: Yang Shi <yang.shi@linaro.org>
Date:   Mon Feb 8 14:49:24 2016 -0800

    arm64: replace read_lock to rcu lock in call_step_hook

    BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:917
    in_atomic(): 1, irqs_disabled(): 128, pid: 383, name: sh
    Preemption disabled at:[<ffff800000124c18>] kgdb_cpu_enter+0x158/0x6b8

    CPU: 3 PID: 383 Comm: sh Tainted: G        W       4.1.13-rt13 #2
    Hardware name: Freescale Layerscape 2085a RDB Board (DT)
    Call trace:
    [<ffff8000000885e8>] dump_backtrace+0x0/0x128
    [<ffff800000088734>] show_stack+0x24/0x30
    [<ffff80000079a7c4>] dump_stack+0x80/0xa0
    [<ffff8000000bd324>] ___might_sleep+0x18c/0x1a0
    [<ffff8000007a20ac>] __rt_spin_lock+0x2c/0x40
    [<ffff8000007a2268>] rt_read_lock+0x40/0x58
    [<ffff800000085328>] single_step_handler+0x38/0xd8
    [<ffff800000082368>] do_debug_exception+0x58/0xb8
    Exception stack(0xffff80834a1e7c80 to 0xffff80834a1e7da0)
    7c80: ffffff9c ffffffff 92c23ba0 0000ffff 4a1e7e40 ffff8083 001bfcc4 ffff8000
    7ca0: f2000400 00000000 00000000 00000000 4a1e7d80 ffff8083 0049501c ffff8000
    7cc0: 00005402 00000000 00aaa210 ffff8000 4a1e7ea0 ffff8083 000833f4 ffff8000
    7ce0: ffffff9c ffffffff 92c23ba0 0000ffff 4a1e7ea0 ffff8083 001bfcc0 ffff8000
    7d00: 4a0fc400 ffff8083 00005402 00000000 4a1e7d40 ffff8083 00490324 ffff8000
    7d20: ffffff9c 00000000 92c23ba0 0000ffff 000a0000 00000000 00000000 00000000
    7d40: 00000008 00000000 00080000 00000000 92c23b8b 0000ffff 92c23b8e 0000ffff
    7d60: 00000038 00000000 00001cb2 00000000 00000005 00000000 92d7b498 0000ffff
    7d80: 01010101 01010101 92be9000 0000ffff 00000000 00000000 00000030 00000000
    [<ffff8000000833f4>] el1_dbg+0x18/0x6c

    This issue is similar with 62c6c61("arm64: replace read_lock to rcu lock in
    call_break_hook"), but comes to single_step_handler.

    This also solves kgdbts boot test silent hang issue on 4.4 -rt kernel.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Yang Shi <yang.shi@linaro.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit efeb85566b27d0b1e2d2f5236c719d1a615e3fba
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Feb 4 14:08:06 2016 +0100

    latencyhist: disable jump-labels

    Atleast on X86 we die a recursive death

    |CPU: 3 PID: 585 Comm: bash Not tainted 4.4.1-rt4+ #198
    |Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS Debian-1.8.2-1 04/01/2014
    |task: ffff88007ab4cd00 ti: ffff88007ab94000 task.ti: ffff88007ab94000
    |RIP: 0010:[<ffffffff81684870>]  [<ffffffff81684870>] int3+0x0/0x10
    |RSP: 0018:ffff88013c107fd8  EFLAGS: 00010082
    |RAX: ffff88007ab4cd00 RBX: ffffffff8100ceab RCX: 0000000080202001
    |RDX: 0000000000000000 RSI: ffffffff8100ceab RDI: ffffffff810c78b2
    |RBP: ffff88007ab97c10 R08: ffffffffff57b000 R09: 0000000000000000
    |R10: ffff88013bb64790 R11: ffff88007ab4cd68 R12: ffffffff8100ceab
    |R13: ffffffff810c78b2 R14: ffffffff810f8158 R15: ffffffff810f9120
    |FS:  0000000000000000(0000) GS:ffff88013c100000(0063) knlGS:00000000f74e3940
    |CS:  0010 DS: 002b ES: 002b CR0: 000000008005003b
    |CR2: 0000000008cf6008 CR3: 000000013b169000 CR4: 00000000000006e0
    |Call Trace:
    | <#DB>
    | [<ffffffff810f8158>] ? trace_preempt_off+0x18/0x170
    | <<EOE>>
    | [<ffffffff81077745>] preempt_count_add+0xa5/0xc0
    | [<ffffffff810c78b2>] on_each_cpu+0x22/0x90
    | [<ffffffff8100ceab>] text_poke_bp+0x5b/0xc0
    | [<ffffffff8100a29c>] arch_jump_label_transform+0x8c/0xf0
    | [<ffffffff8111c77c>] __jump_label_update+0x6c/0x80
    | [<ffffffff8111c83a>] jump_label_update+0xaa/0xc0
    | [<ffffffff8111ca54>] static_key_slow_inc+0x94/0xa0
    | [<ffffffff810e0d8d>] tracepoint_probe_register_prio+0x26d/0x2c0
    | [<ffffffff810e0df3>] tracepoint_probe_register+0x13/0x20
    | [<ffffffff810fca78>] trace_event_reg+0x98/0xd0
    | [<ffffffff810fcc8b>] __ftrace_event_enable_disable+0x6b/0x180
    | [<ffffffff810fd5b8>] event_enable_write+0x78/0xc0
    | [<ffffffff8117a768>] __vfs_write+0x28/0xe0
    | [<ffffffff8117b025>] vfs_write+0xa5/0x180
    | [<ffffffff8117bb76>] SyS_write+0x46/0xa0
    | [<ffffffff81002c91>] do_fast_syscall_32+0xa1/0x1d0
    | [<ffffffff81684d57>] sysenter_flags_fixed+0xd/0x17

    during
     echo 1 > /sys/kernel/debug/tracing/events/hist/preemptirqsoff_hist/enable

    Reported-By: Christoph Mathys <eraserix@gmail.com>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0c2639f1d06f9c3cc15f6d988832122d8ed85f8d
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jan 20 15:39:05 2016 +0100

    net: provide a way to delegate processing a softirq to ksoftirqd

    If the NET_RX uses up all of his budget it moves the following NAPI
    invocations into the `ksoftirqd`. On -RT it does not do so. Instead it
    rises the NET_RX softirq in its current context again.

    In order to get closer to mainline's behaviour this patch provides
    __raise_softirq_irqoff_ksoft() which raises the softirq in the ksoftird.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 4eaead515e290dd8968f09245e9f5024fa7bc936
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jan 20 16:34:17 2016 +0100

    softirq: split timer softirqs out of ksoftirqd

    The softirqd runs in -RT with SCHED_FIFO (prio 1) and deals mostly with
    timer wakeup which can not happen in hardirq context. The prio has been
    risen from the normal SCHED_OTHER so the timer wakeup does not happen
    too late.
    With enough networking load it is possible that the system never goes
    idle and schedules ksoftirqd and everything else with a higher priority.
    One of the tasks left behind is one of RCU's threads and so we see stalls
    and eventually run out of memory.
    This patch moves the TIMER and HRTIMER softirqs out of the `ksoftirqd`
    thread into its own `ktimersoftd`. The former can now run SCHED_OTHER
    (same as mainline) and the latter at SCHED_FIFO due to the wakeups.

    From networking point of view: The NAPI callback runs after the network
    interrupt thread completes. If its run time takes too long the NAPI code
    itself schedules the `ksoftirqd`. Here in the thread it can run at
    SCHED_OTHER priority and it won't defer RCU anymore.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c929fa7b8464e40f809f837c81bd282870b4d556
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jan 20 15:13:30 2016 +0100

    preempt-lazy: Add the lazy-preemption check to preempt_schedule()

    Probably in the rebase onto v4.1 this check got moved into less commonly used
    preempt_schedule_notrace(). This patch ensures that both functions use it.

    Reported-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit fd7ce3b6ea7e7756b7127db4f604954de1f19315
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jan 13 15:55:02 2016 +0100

    net: move xmit_recursion to per-task variable on -RT

    A softirq on -RT can be preempted. That means one task is in
    __dev_queue_xmit(), gets preempted and another task may enter
    __dev_queue_xmit() aw well. netperf together with a bridge device
    will then trigger the `recursion alert` because each task increments
    the xmit_recursion variable which is per-CPU.
    A virtual device like br0 is required to trigger this warning.

    This patch moves the counter to per task instead per-CPU so it counts
    the recursion properly on -RT.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ed4baa243c64e486cf043ba13c4cd6e5e0ba5238
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jan 13 14:09:05 2016 +0100

    ptrace: don't open IRQs in ptrace_freeze_traced() too early

    In the non-RT case the spin_lock_irq() here disables interrupts as well
    as raw_spin_lock_irq(). So in the unlock case the interrupts are enabled
    too early.

    Reported-by: kernel test robot <ying.huang@linux.intel.com>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3bb69ff99b4032d6ec5d35bedc6fbf1e4b482194
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Dec 21 18:17:10 2015 +0100

    sched: reset task's lockless wake-queues on fork()

    In 7675104990ed ("sched: Implement lockless wake-queues") we gained
    lockless wake-queues. -RT managed to lockup itself with those. There
    could be multiple attempts for task X to enqueue it for a wakeup
    _even_ if task X is already running.
    The reason is that task X could be runnable but not yet on CPU. The the
    task performing the wakeup did not leave the CPU it could performe
    multiple wakeups.
    With the proper timming task X could be running and enqueued for a
    wakeup. If this happens while X is performing a fork() then its its
    child will have a !NULL `wake_q` member copied.
    This is not a problem as long as the child task does not participate in
    lockless wakeups :)

    Fixes: 7675104990ed ("sched: Implement lockless wake-queues")
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9df6942317afb556a87b905a0c1137c06d6b53a3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Feb 29 11:22:19 2016 -0500

    Linux 3.18.27-rt26

commit 5f0f37cdffa3308edac01d4c581da61957c81809
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sun Oct 25 16:35:24 2015 -0400

    sched: Introduce the trace_sched_waking tracepoint

    Upstream commit fbd705a0c6184580d0e2fbcbd47a37b6e5822511

    Mathieu reported that since 317f394160e9 ("sched: Move the second half
    of ttwu() to the remote cpu") trace_sched_wakeup() can happen out of
    context of the waker.

    This is a problem when you want to analyse wakeup paths because it is
    now very hard to correlate the wakeup event to whoever issued the
    wakeup.

    OTOH trace_sched_wakeup() is issued at the point where we set
    p->state = TASK_RUNNING, which is right were we hand the task off to
    the scheduler, so this is an important point when looking at
    scheduling behaviour, up to here its been the wakeup path everything
    hereafter is due to scheduler policy.

    To bridge this gap, introduce a second tracepoint: trace_sched_waking.
    It is guaranteed to be called in the waker context.

    [ Ported to linux-4.1.y-rt kernel by Mathieu Desnoyers. Resolved
      conflict: try_to_wake_up_local() does not exist in -rt kernel. Removed
      its instrumentation hunk. ]

    Reported-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    CC: Julien Desfossez <jdesfossez@efficios.com>
    CC: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Francis Giraldeau <francis.giraldeau@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    CC: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/20150609091336.GQ3644@twins.programming.kicks-ass.net
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e068ec7031c17da0e640df1f1fc6b40bd77eaa81
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Nov 15 18:40:17 2015 +0100

    irqwork: Move irq safe work to irq context

    On architectures where arch_irq_work_has_interrupt() returns false, we
    end up running the irq safe work from the softirq context. That
    results in a potential deadlock in the scheduler irq work which
    expects that function to be called with interrupts disabled.

    Split the irq_work_tick() function into a hard and soft variant. Call
    the hard variant from the tick interrupt and add the soft variant to
    the timer softirq.

    Reported-and-tested-by: Yanjiang Jin <yanjiang.jin@windriver.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 6983d62f5d225e43b7cc6ccd7517e8674b08f73f
Author: Grygorii Strashko <grygorii.strashko@ti.com>
Date:   Fri Oct 9 09:25:49 2015 -0500

    net/core/cpuhotplug: Drain input_pkt_queue lockless

    I can constantly see below error report with 4.1 RT-kernel on TI ARM dra7-evm
    if I'm trying to unplug cpu1:

    [   57.737589] CPU1: shutdown
    [   57.767537] BUG: spinlock bad magic on CPU#0, sh/137
    [   57.767546]  lock: 0xee994730, .magic: 00000000, .owner: <none>/-1, .owner_cpu: 0
    [   57.767552] CPU: 0 PID: 137 Comm: sh Not tainted 4.1.10-rt8-01700-g2c38702-dirty #55
    [   57.767555] Hardware name: Generic DRA74X (Flattened Device Tree)
    [   57.767568] [<c001acd0>] (unwind_backtrace) from [<c001534c>] (show_stack+0x20/0x24)
    [   57.767579] [<c001534c>] (show_stack) from [<c075560c>] (dump_stack+0x84/0xa0)
    [   57.767593] [<c075560c>] (dump_stack) from [<c00aca48>] (spin_dump+0x84/0xac)
    [   57.767603] [<c00aca48>] (spin_dump) from [<c00acaa4>] (spin_bug+0x34/0x38)
    [   57.767614] [<c00acaa4>] (spin_bug) from [<c00acc10>] (do_raw_spin_lock+0x168/0x1c0)
    [   57.767624] [<c00acc10>] (do_raw_spin_lock) from [<c075b4cc>] (_raw_spin_lock+0x4c/0x54)
    [   57.767631] [<c075b4cc>] (_raw_spin_lock) from [<c07599fc>] (rt_spin_lock_slowlock+0x5c/0x374)
    [   57.767638] [<c07599fc>] (rt_spin_lock_slowlock) from [<c075bcf4>] (rt_spin_lock+0x38/0x70)
    [   57.767649] [<c075bcf4>] (rt_spin_lock) from [<c06333c0>] (skb_dequeue+0x28/0x7c)
    [   57.767662] [<c06333c0>] (skb_dequeue) from [<c06476ec>] (dev_cpu_callback+0x1b8/0x240)
    [   57.767673] [<c06476ec>] (dev_cpu_callback) from [<c007566c>] (notifier_call_chain+0x3c/0xb4)

    The reason is that skb_dequeue is taking skb->lock, but RT changed the
    core code to use a raw spinlock. The non-raw lock is not initialized
    on purpose to catch exactly this kind of problem.

    Fixes: 91df05da13a6 'net: Use skbufhead with raw lock'
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 056c50bb9f02b32247ee7892596a4f14c13546f5
Author: Josh Cartwright <joshc@ni.com>
Date:   Tue Oct 27 07:31:53 2015 -0500

    net: Make synchronize_rcu_expedited() conditional on !RT_FULL

    While the use of synchronize_rcu_expedited() might make
    synchronize_net() "faster", it does so at significant cost on RT
    systems, as expediting a grace period forcibly preempts any
    high-priority RT tasks (via the stop_machine() mechanism).

    Without this change, we can observe a latency spike up to 30us with
    cyclictest by rapidly unplugging/reestablishing an ethernet link.

    Suggested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Josh Cartwright <joshc@ni.com>
    Cc: bigeasy@linutronix.de
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20151027123153.GG8245@jcartwri.amer.corp.natinst.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f339ae7c9600399776bb4f840b6f1c58a4293aa1
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sun Aug 16 14:27:50 2015 +0200

    dump stack: don't disable preemption during trace

    I see here large latencies during a stack dump on x86. The
    preempt_disable() and get_cpu() should forbid moving the task to another
    CPU during a stack dump and avoiding two stack traces in parallel on the
    same CPU. However a stack trace from a second CPU may still happen in
    parallel. Also nesting is allowed so a stack trace happens in
    process-context and we may have another one from IRQ context. With migrate
    disable we keep this code preemptible and allow a second backtrace on
    the same CPU by another task.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 171045bdda5e7da85013e478105f4bdf20fb092a
Author: bmouring@ni.com <bmouring@ni.com>
Date:   Tue Dec 15 17:07:30 2015 -0600

    rtmutex: Use chainwalking control enum

    In 8930ed80 (rtmutex: Cleanup deadlock detector debug logic),
    chainwalking control enums were introduced to limit the deadlock
    detection logic. One of the calls to task_blocks_on_rt_mutex was
    missed when converting to use the enums.

    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Brad Mouring <brad.mouring@ni.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 83f58210e745c535348c250b49c56dea2f8a6e74
Author: Wolfgang M. Reimer <linuxball@gmail.com>
Date:   Tue Jul 21 16:20:07 2015 +0200

    locking: locktorture: Do NOT include rwlock.h directly

    Including rwlock.h directly will cause kernel builds to fail
    if CONFIG_PREEMPT_RT_FULL is defined. The correct header file
    (rwlock_rt.h OR rwlock.h) will be included by spinlock.h which
    is included by locktorture.c anyway.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Wolfgang M. Reimer <linuxball@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7bb128748606a5ed70095ef548a29d00c86562d3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Nov 6 18:51:03 2015 +0100

    rtmutex: Handle non enqueued waiters gracefully

    Yimin debugged that in case of a PI wakeup in progress when
    rt_mutex_start_proxy_lock() calls task_blocks_on_rt_mutex() the latter
    returns -EAGAIN and in consequence the remove_waiter() call runs into
    a BUG_ON() because there is nothing to remove.

    Guard it with rt_mutex_has_waiters(). This is a quick fix which is
    easy to backport. The proper fix is to have a central check in
    remove_waiter() so we can call it unconditionally.

    Reported-and-debugged-by: Yimin Deng <yimin11.deng@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3ff367f166d45ed14b8daeb917b09411611df33b
Author: Grygorii Strashko <grygorii.strashko@ti.com>
Date:   Fri Sep 11 21:21:23 2015 +0300

    ARM: smp: Move clear_tasks_mm_cpumask() call to __cpu_die()

    When running with the RT-kernel (4.1.5-rt5) on TI OMAP dra7-evm and trying
    to do Suspend to RAM, the following backtrace occurs:

     Disabling non-boot CPUs ...
     PM: noirq suspend of devices complete after 7.295 msecs
     Disabling non-boot CPUs ...
     BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:917
     in_atomic(): 1, irqs_disabled(): 128, pid: 18, name: migration/1
     INFO: lockdep is turned off.
     irq event stamp: 122
     hardirqs last  enabled at (121): [<c06ac0ac>] _raw_spin_unlock_irqrestore+0x88/0x90
     hardirqs last disabled at (122): [<c06abed0>] _raw_spin_lock_irq+0x28/0x5c
     softirqs last  enabled at (0): [<c003d294>] copy_process.part.52+0x410/0x19d8
     softirqs last disabled at (0): [<  (null)>]   (null)
     Preemption disabled at:[<  (null)>]   (null)
      CPU: 1 PID: 18 Comm: migration/1 Tainted: G        W       4.1.4-rt3-01046-g96ac8da #204
     Hardware name: Generic DRA74X (Flattened Device Tree)
     [<c0019134>] (unwind_backtrace) from [<c0014774>] (show_stack+0x20/0x24)
     [<c0014774>] (show_stack) from [<c06a70f4>] (dump_stack+0x88/0xdc)
     [<c06a70f4>] (dump_stack) from [<c006cab8>] (___might_sleep+0x198/0x2a8)
     [<c006cab8>] (___might_sleep) from [<c06ac4dc>] (rt_spin_lock+0x30/0x70)
     [<c06ac4dc>] (rt_spin_lock) from [<c013f790>] (find_lock_task_mm+0x9c/0x174)
     [<c013f790>] (find_lock_task_mm) from [<c00409ac>] (clear_tasks_mm_cpumask+0xb4/0x1ac)
     [<c00409ac>] (clear_tasks_mm_cpumask) from [<c00166a4>] (__cpu_disable+0x98/0xbc)
     [<c00166a4>] (__cpu_disable) from [<c06a2e8c>] (take_cpu_down+0x1c/0x50)
     [<c06a2e8c>] (take_cpu_down) from [<c00f2600>] (multi_cpu_stop+0x11c/0x158)
     [<c00f2600>] (multi_cpu_stop) from [<c00f2a9c>] (cpu_stopper_thread+0xc4/0x184)
     [<c00f2a9c>] (cpu_stopper_thread) from [<c0069058>] (smpboot_thread_fn+0x18c/0x324)
     [<c0069058>] (smpboot_thread_fn) from [<c00649c4>] (kthread+0xe8/0x104)
     [<c00649c4>] (kthread) from [<c0010058>] (ret_from_fork+0x14/0x3c)
     CPU1: shutdown
     PM: Calling sched_clock_suspend+0x0/0x40
     PM: Calling timekeeping_suspend+0x0/0x2e0
     PM: Calling irq_gc_suspend+0x0/0x68
     PM: Calling fw_suspend+0x0/0x2c
     PM: Calling cpu_pm_suspend+0x0/0x28

    Also, sometimes system stucks right after displaying "Disabling non-boot
    CPUs ...". The root cause of above backtrace is task_lock() which takes
    a sleeping lock on -RT.

    To fix the issue, move clear_tasks_mm_cpumask() call from __cpu_disable()
    to __cpu_die() which is called on the thread which is asking for a target
    CPU to be shutdown. In addition, this change restores CPUhotplug functionality
    on TI OMAP dra7-evm and CPU1 can be unplugged/plugged many times.

    Signed-off-by: Grygorii Strashko <grygorii.strashko@ti.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: <linux-arm-kernel@lists.infradead.org>
    Cc: Sekhar Nori <nsekhar@ti.com>
    Cc: Austin Schuh <austin@peloton-tech.com>
    Cc: <philipp@peloton-tech.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: <bigeasy@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Link: http://lkml.kernel.org/r/1441995683-30817-1-git-send-email-grygorii.strashko@ti.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 374a09b1fcc9333df4d97b75f18f4701d96bc494
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Sep 19 11:56:20 2015 +0200

    genirq: Handle force threading of interrupts with primary and thread handler

    Force threading of interrupts does not deal with interrupts which are
    requested with a primary and a threaded handler. The current policy is
    to leave them alone and let the primary handler run in interrupt
    context, but we set the ONESHOT flag for those interrupts as well.

    Kohji Okuno debugged a problem with the SDHCI driver where the
    interrupt thread waits for a hardware interrupt to trigger, which cant
    work well because the hardware interrupt is masked due to the ONESHOT
    flag being set. He proposed to set the ONESHOT flag only if the
    interrupt does not provide a thread handler.

    Though that does not work either because these interrupts can be
    shared. So the other interrupt would rightfully get the ONESHOT flag
    set and therefor the same situation would happen again.

    To deal with this proper, we need to force thread the primary handler
    of such interrupts as well. That means that the primary interrupt
    handler is treated as any other primary interrupt handler which is not
    marked IRQF_NO_THREAD. The threaded handler becomes a separate thread
    so the SDHCI flow logic can be handled gracefully.

    The same issue was reported against 4.1-rt.

    Reported-by: Kohji Okuno <okuno.kohji@jp.panasonic.com>
    Reported-By: Michal ≈†mucr <msmucr@gmail.com>
    Reported-and-tested-by: Nathan Sullivan <nathan.sullivan@ni.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 642012ea1db5db96f059757ee605ca9412c8968d
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Jul 21 15:28:49 2015 +0200

    cpufreq: Remove cpufreq_rwsem

    cpufreq_rwsem was introduced in commit 6eed9404ab3c4 ("cpufreq: Use
    rwsem for protecting critical sections) in order to replace
    try_module_get() on the cpu-freq driver. That try_module_get() worked
    well until the refcount was so heavily used that module removal became
    more or less impossible.

    Though when looking at the various (undocumented) protection
    mechanisms in that code, the randomly sprinkeled around cpufreq_rwsem
    locking sites are superfluous.

    The policy, which is acquired in cpufreq_cpu_get() and released in
    cpufreq_cpu_put() is sufficiently protected already.

      cpufreq_cpu_get(cpu)
        /* Protects against concurrent driver removal */
        read_lock_irqsave(&cpufreq_driver_lock, flags);
        policy = per_cpu(cpufreq_cpu_data, cpu);
        kobject_get(&policy->kobj);
        read_unlock_irqrestore(&cpufreq_driver_lock, flags);

    The reference on the policy serializes versus module unload already:

      cpufreq_unregister_driver()
        subsys_interface_unregister()
          __cpufreq_remove_dev_finish()
            per_cpu(cpufreq_cpu_data) = NULL;
    	cpufreq_policy_put_kobj()

    If there is a reference held on the policy, i.e. obtained prior to the
    unregister call, then cpufreq_policy_put_kobj() will wait until that
    reference is dropped. So once subsys_interface_unregister() returns
    there is no policy pointer in flight and no new reference can be
    obtained. So that rwsem protection is useless.

    The other usage of cpufreq_rwsem in show()/store() of the sysfs
    interface is redundant as well because sysfs already does the proper
    kobject_get()/put() pairs.

    That leaves CPU hotplug versus module removal. The current
    down_write() around the write_lock() in cpufreq_unregister_driver() is
    silly at best as it protects actually nothing.

    The trivial solution to this is to prevent hotplug across
    cpufreq_unregister_driver completely.

    [upstream: rafael/linux-pm 454d3a2500a4eb33be85dde3bfba9e5f6b5efadc]
    [fixes: "cpufreq_stat_notifier_trans: No policy found" since v4.0-rt]
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c5734a9683d0e73727626dd47abe59ef9a70205c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Feb 23 13:09:27 2016 -0500

    Linux 3.18.27-rt25

commit 1b4ddba80ff3d7c60356043d39c1a1e7ea5fff27
Merge: 5f7fae6ca9cf 2c07053b8e1e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Feb 23 13:08:20 2016 -0500

    Merge tag 'v3.18.27' into v3.18-rt

    Linux 3.18.27

     Conflicts:
    	kernel/futex.c
    	kernel/printk/printk.c

commit 5f7fae6ca9cf49253acddc5aa78dc94e030d906e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Feb 23 10:17:21 2016 -0500

    Linux 3.18.26-rt24

commit 15c51f0e1518fb6cf426411528cac20130a07352
Merge: 90a3d747490f 707e840c5e24
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Feb 23 10:16:51 2016 -0500

    Merge tag 'v3.18.26' into v3.18-rt

    Linux 3.18.26

commit 90a3d747490fbedbed10fe2c75b1954c5360613c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Jan 11 17:45:01 2016 -0500

    Linux 3.18.25-rt23

commit e2ec61c75d01396a413dab033870fe26b303ebd0
Merge: b190b7282cda 60917545df1f
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Jan 11 10:35:27 2016 -0500

    Merge tag 'v3.18.25' into v3.18-rt

    Linux 3.18.25

commit b190b7282cda249579b3f8c0fb8c7d3084944efd
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 10 14:04:42 2015 -0500

    Linux 3.18.24-rt22

commit 7c10c3e227691b36a2a22dc208068f31594a0fd3
Merge: 53c2d11cc4af b12403044336
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 10 11:55:03 2015 -0500

    Merge tag 'v3.18.24' into v3.18-rt

    Linux 3.18.24

commit 53c2d11cc4af6f11e06838e0259ada781696bc5b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 10 11:54:33 2015 -0500

    Linux 3.18.23-rt21

commit bb2aac11e310892e5aa3eebf6414055d933e9c15
Merge: 0f213c29b252 8341455f7f2b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 10 11:54:19 2015 -0500

    Merge tag 'v3.18.23' into v3.18-rt

    Linux 3.18.23

commit 0f213c29b2522d32371de02ef8aff0806d15ed9a
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 10 11:54:14 2015 -0500

    Linux 3.18.22-rt20

commit 48d47250a48697b242c2c2b0c869166d24b05a55
Merge: 21eff94c39d3 ac6d8ef9174f
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 10 11:53:57 2015 -0500

    Merge tag 'v3.18.22' into v3.18-rt

    Linux 3.18.22

commit 21eff94c39d37c3b292d31f6608fb2188568f414
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Sep 4 12:05:35 2015 -0400

    Linux 3.18.21-rt19

commit a280001e32ca3322de7d40d1bf72f3c8d70545d8
Merge: 786cc916226f fcd9bfdb9d88
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Sep 4 10:23:31 2015 -0400

    Merge tag 'v3.18.21' into v3.18-rt

    Linux 3.18.21

commit 786cc916226f416b3a18a42512256593d3e1407b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Aug 10 10:47:57 2015 -0400

    Linux 3.18.20-rt18

commit 74a94238e090d5d3a227294f6faa61f243e8d507
Merge: d90f6eaa167e e9fd6ddcabf8
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Aug 10 10:47:07 2015 -0400

    Merge tag 'v3.18.20' into v3.18-rt

    Linux 3.18.20

commit d90f6eaa167e2d01b8ad3c90d98b7ad998aa0dac
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Aug 10 10:46:52 2015 -0400

    Linux 3.18.19-rt17

commit 24846f4f3771344e3be0b341bb62318e51bfdfa8
Merge: b628ab744a38 22a6cbf9f36e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Aug 10 10:46:33 2015 -0400

    Merge tag 'v3.18.19' into v3.18-rt

    Linux 3.18.19

commit b628ab744a38ced6fbb48d6d3d32303782daf141
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Aug 10 10:11:02 2015 -0400

    Linux 3.18.18-rt16

commit 9e1c39d9dff009e3cf69bc000df331d03769d0ac
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jun 11 17:31:40 2015 +0200

    kernel/irq_work: fix non RT case

    After the deadlock fixed, the checked got somehow away and broke the non-RT
    case which could invoke IRQ-work from softirq context.

    Cc: stable-rt@vger.kernel.org
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3f1084676c05a083afe995b03f42b6e361a44fc0
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Mon May 4 07:02:46 2015 -0700

    ipc/mqueue: Implement lockless pipelined wakeups

    This patch moves the wakeup_process() invocation so it is not done under
    the info->lock by making use of a lockless wake_q. With this change, the
    waiter is woken up once it is STATE_READY and it does not need to loop
    on SMP if it is still in STATE_PENDING. In the timeout case we still need
    to grab the info->lock to verify the state.

    This change should also avoid the introduction of preempt_disable() in -rt
    which avoids a busy-loop which pools for the STATE_PENDING -> STATE_READY
    change if the waiter has a higher priority compared to the waker.

    Additionally, this patch micro-optimizes wq_sleep by using the cheaper
    cousin of set_current_state(TASK_INTERRUPTABLE) as we will block no
    matter what, thus get rid of the implied barrier.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: George Spelvin <linux@horizon.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Mason <clm@fb.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: dave@stgolabs.net
    Link: http://lkml.kernel.org/r/1430748166.1940.17.camel@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f2e2dc85880aef470475d06f323c540b616118ad
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri May 1 08:27:51 2015 -0700

    futex: Implement lockless wakeups

    Given the overall futex architecture, any chance of reducing
    hb->lock contention is welcome. In this particular case, using
    wake-queues to enable lockless wakeups addresses very much real
    world performance concerns, even cases of soft-lockups in cases
    of large amounts of blocked tasks (which is not hard to find in
    large boxes, using but just a handful of futex).

    At the lowest level, this patch can reduce latency of a single thread
    attempting to acquire hb->lock in highly contended scenarios by a
    up to 2x. At lower counts of nr_wake there are no regressions,
    confirming, of course, that the wake_q handling overhead is practically
    non existent. For instance, while a fair amount of variation,
    the extended pef-bench wakeup benchmark shows for a 20 core machine
    the following avg per-thread time to wakeup its share of tasks:

    	nr_thr	ms-before	ms-after
    	16 	0.0590		0.0215
    	32 	0.0396		0.0220
    	48 	0.0417		0.0182
    	64 	0.0536		0.0236
    	80 	0.0414		0.0097
    	96 	0.0672		0.0152

    Naturally, this can cause spurious wakeups. However there is no core code
    that cannot handle them afaict, and furthermore tglx does have the point
    that other events can already trigger them anyway.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Mason <clm@fb.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: George Spelvin <linux@horizon.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1430494072-30283-3-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a7a81c9cce2a7d271f7a29f28d0e92f1bb1efc54
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 1 08:27:50 2015 -0700

    sched: Implement lockless wake-queues

    This is useful for locking primitives that can effect multiple
    wakeups per operation and want to avoid lock internal lock contention
    by delaying the wakeups until we've released the lock internal locks.

    Alternatively it can be used to avoid issuing multiple wakeups, and
    thus save a few cycles, in packet processing. Queue all target tasks
    and wakeup once you've processed all packets. That way you avoid
    waking the target task multiple times if there were multiple packets
    for the same task.

    Properties of a wake_q are:
    - Lockless, as queue head must reside on the stack.
    - Being a queue, maintains wakeup order passed by the callers. This can
      be important for otherwise, in scenarios where highly contended locks
      could affect any reliance on lock fairness.
    - A queued task cannot be added again until it is woken up.

    This patch adds the needed infrastructure into the scheduler code
    and uses the new wake_list to delay the futex wakeups until
    after we've released the hash bucket locks.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    [tweaks, adjustments, comments, etc.]
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Mason <clm@fb.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: George Spelvin <linux@horizon.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1430494072-30283-2-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 3f44b543a813734eb998e8ce2996fdc2f7b6fc4c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 14 14:26:34 2015 +0200

    mm/slub: move slab initialization into irq enabled region

    Initializing a new slab can introduce rather large latencies because most
    of the initialization runs always with interrupts disabled.

    There is no point in doing so.  The newly allocated slab is not visible
    yet, so there is no reason to protect it against concurrent alloc/free.

    Move the expensive parts of the initialization into allocate_slab(), so
    for all allocations with GFP_WAIT set, interrupts are enabled.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c9fe2283414849f0b0f397d20ec11fe566fffee4
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Jul 13 13:08:25 2015 +0200

    Revert "slub: delay ctor until the object is requested"

    This approach is broken with SLAB_DESTROY_BY_RCU allocations.
    Reported by Steven Rostedt and Koehrer Mathias.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1094c21471dc03e268b9d5cc780703eee19eb9a0
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Jul 20 17:28:20 2015 -0400

    Linux 3.18.18-rt15

commit 6bb0e2dc64953ce9b8d09a0aa32b04c20fdcdacf
Merge: bba903c2bb20 866cebe251f4
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Jul 20 15:21:33 2015 -0400

    Merge tag 'v3.18.18' into v3.18-rt

    Linux 3.18.18

    Conflicts:
    	kernel/softirq.c

commit bba903c2bb20dbb6c57a8ba3889f9b8f09dcc6e0
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Jul 1 15:44:34 2015 -0400

    Linux 3.18.17-rt14

commit 30cdb329708c5b82d15567af3db7a92362165b89
Merge: 2ab403919b5b ea5dd38e93b3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Jul 1 11:41:29 2015 -0400

    Merge tag 'v3.18.17' into v3.18-rt

    Linux 3.18.17

commit 2ab403919b5b80b0fbce1f99cf023fecd13d7b95
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 18 18:25:25 2015 -0400

    Linux 3.18.16-rt13

commit 1a0c685375ad3ccfdbd75c1781e94223ba9a26c9
Merge: 1405bd429099 d048c068d00d
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 18 18:25:07 2015 -0400

    Merge tag 'v3.18.16' into v3.18-rt

    Linux 3.18.16

commit 1405bd429099270cafee0f0e3931d2b9e57a7769
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 18 18:24:35 2015 -0400

    Linux 3.18.15-rt12

commit 6a2cdb623069a0965af7d2d695b6c6d036e7651f
Merge: f068d8c2b987 324d8201ba64
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 18 18:24:23 2015 -0400

    Merge tag 'v3.18.15' into v3.18-rt

    Linux 3.18.15

commit f068d8c2b9875c2e5fe15328aaa417578d15e60e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 18 18:23:55 2015 -0400

    Linux 3.18.14-rt11

commit 811fc7917f41b1aaee05e082501f08daebc6cada
Merge: 30d969c4966b 51af817611f2
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 18 18:23:42 2015 -0400

    Merge tag 'v3.18.14' into v3.18-rt

    Linux 3.18.14

commit 30d969c4966b97727b4f4ca9f9673cfba8d432ca
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 8 20:25:16 2011 +0200

    Linux 3.18.13-rt10

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-8vdw4bfcsds27cvox6rpb334@git.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit d7b9312ca453d6c26f3564ce71838681432d3c30
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jun 27 16:24:52 2014 +0200

    workqueue: Prevent deadlock/stall on RT

    Austin reported a XFS deadlock/stall on RT where scheduled work gets
    never exececuted and tasks are waiting for each other for ever.

    The underlying problem is the modification of the RT code to the
    handling of workers which are about to go to sleep. In mainline a
    worker thread which goes to sleep wakes an idle worker if there is
    more work to do. This happens from the guts of the schedule()
    function. On RT this must be outside and the accessed data structures
    are not protected against scheduling due to the spinlock to rtmutex
    conversion. So the naive solution to this was to move the code outside
    of the scheduler and protect the data structures by the pool
    lock. That approach turned out to be a little naive as we cannot call
    into that code when the thread blocks on a lock, as it is not allowed
    to block on two locks in parallel. So we dont call into the worker
    wakeup magic when the worker is blocked on a lock, which causes the
    deadlock/stall observed by Austin and Mike.

    Looking deeper into that worker code it turns out that the only
    relevant data structure which needs to be protected is the list of
    idle workers which can be woken up.

    So the solution is to protect the list manipulation operations with
    preempt_enable/disable pairs on RT and call unconditionally into the
    worker code even when the worker is blocked on a lock. The preemption
    protection is safe as there is nothing which can fiddle with the list
    outside of thread context.

    Reported-and_tested-by: Austin Schuh <austin@peloton-tech.com>
    Reported-and_tested-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://vger.kernel.org/r/alpine.DEB.2.10.1406271249510.5170@nanos
    Cc: Richard Weinberger <richard.weinberger@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 6153a86ee4a43b601b9f75b3ff4d38a61517cd69
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Jul 1 11:14:44 2014 -0400

    sched: Do not clear PF_NO_SETAFFINITY flag in select_fallback_rq()

    I talked with Peter Zijlstra about this, and he told me that the clearing
    of the PF_NO_SETAFFINITY flag was to deal with the optimization of
    migrate_disable/enable() that ignores tasks that have that flag set. But
    that optimization was removed when I did a rework of the cpu hotplug code.

    I found that ignoring tasks that had that flag set would cause those tasks
    to not sync with the hotplug code and cause the kernel to crash. Thus it
    needed to not treat them special and those tasks had to go though the same
    work as tasks without that flag set.

    Now that those tasks are not treated special, there's no reason to clear the
    flag.

    May still need to be tested as the migrate_me() code does not ignore those
    flags.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140701111444.0cfebaa1@gandalf.local.home
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 2c02051e1389a5e06a116aa357ac043003880a28
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Aug 29 11:48:57 2013 +0200

    md: disable bcache

    It uses anon semaphores
    |drivers/md/bcache/request.c: In function ‚Äòcached_dev_write_complete‚Äô:
    |drivers/md/bcache/request.c:1007:2: error: implicit declaration of function ‚Äòup_read_non_owner‚Äô [-Werror=implicit-function-declaration]
    |  up_read_non_owner(&dc->writeback_lock);
    |  ^
    |drivers/md/bcache/request.c: In function ‚Äòrequest_write‚Äô:
    |drivers/md/bcache/request.c:1033:2: error: implicit declaration of function ‚Äòdown_read_non_owner‚Äô [-Werror=implicit-function-declaration]
    |  down_read_non_owner(&dc->writeback_lock);
    |  ^

    either we get rid of those or we have to introduce them‚Ä¶

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ff29439279edc8adf71ce1a46f8d62a30c7622b2
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Jun 26 15:28:11 2013 -0400

    rt,ntp: Move call to schedule_delayed_work() to helper thread

    The ntp code for notify_cmos_timer() is called from a hard interrupt
    context. schedule_delayed_work() under PREEMPT_RT_FULL calls spinlocks
    that have been converted to mutexes, thus calling schedule_delayed_work()
    from interrupt is not safe.

    Add a helper thread that does the call to schedule_delayed_work and wake
    up that thread instead of calling schedule_delayed_work() directly.
    This is only for CONFIG_PREEMPT_RT_FULL, otherwise the code still calls
    schedule_delayed_work() directly in irq context.

    Note: There's a few places in the kernel that do this. Perhaps the RT
    code should have a dedicated thread that does the checks. Just register
    a notifier on boot up for your check and wake up the thread when
    needed. This will be a todo.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b9c4894f8bb2ec84d9f5ed9917e1b6db85f2e192
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Sat Jun 21 10:09:48 2014 +0200

    scheduling while atomic in cgroup code

    mm, memcg: make refill_stock() use get_cpu_light()

    Nikita reported the following memcg scheduling while atomic bug:

    Call Trace:
    [e22d5a90] [c0007ea8] show_stack+0x4c/0x168 (unreliable)
    [e22d5ad0] [c0618c04] __schedule_bug+0x94/0xb0
    [e22d5ae0] [c060b9ec] __schedule+0x530/0x550
    [e22d5bf0] [c060bacc] schedule+0x30/0xbc
    [e22d5c00] [c060ca24] rt_spin_lock_slowlock+0x180/0x27c
    [e22d5c70] [c00b39dc] res_counter_uncharge_until+0x40/0xc4
    [e22d5ca0] [c013ca88] drain_stock.isra.20+0x54/0x98
    [e22d5cc0] [c01402ac] __mem_cgroup_try_charge+0x2e8/0xbac
    [e22d5d70] [c01410d4] mem_cgroup_charge_common+0x3c/0x70
    [e22d5d90] [c0117284] __do_fault+0x38c/0x510
    [e22d5df0] [c011a5f4] handle_pte_fault+0x98/0x858
    [e22d5e50] [c060ed08] do_page_fault+0x42c/0x6fc
    [e22d5f40] [c000f5b4] handle_page_fault+0xc/0x80

    What happens:

       refill_stock()
          get_cpu_var()
          drain_stock()
             res_counter_uncharge()
                res_counter_uncharge_until()
                   spin_lock() <== boom

    Fix it by replacing get/put_cpu_var() with get/put_cpu_light().

    Cc: stable-rt@vger.kernel.org
    Reported-by: Nikita Yushchenko <nyushchenko@dev.rtsoft.ru>
    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 4e917116bd2c8f122302011a10f06fc21b4327b5
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Feb 13 15:52:24 2015 +0100

    cgroups: use simple wait in css_release()

    To avoid:
    |BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:914
    |in_atomic(): 1, irqs_disabled(): 0, pid: 92, name: rcuc/11
    |2 locks held by rcuc/11/92:
    | #0:  (rcu_callback){......}, at: [<ffffffff810e037e>] rcu_cpu_kthread+0x3de/0x940
    | #1:  (rcu_read_lock_sched){......}, at: [<ffffffff81328390>] percpu_ref_call_confirm_rcu+0x0/0xd0
    |Preemption disabled at:[<ffffffff813284e2>] percpu_ref_switch_to_atomic_rcu+0x82/0xc0
    |CPU: 11 PID: 92 Comm: rcuc/11 Not tainted 3.18.7-rt0+ #1
    | ffff8802398cdf80 ffff880235f0bc28 ffffffff815b3a12 0000000000000000
    | 0000000000000000 ffff880235f0bc48 ffffffff8109aa16 0000000000000000
    | ffff8802398cdf80 ffff880235f0bc78 ffffffff815b8dd4 000000000000df80
    |Call Trace:
    | [<ffffffff815b3a12>] dump_stack+0x4f/0x7c
    | [<ffffffff8109aa16>] __might_sleep+0x116/0x190
    | [<ffffffff815b8dd4>] rt_spin_lock+0x24/0x60
    | [<ffffffff8108d2cd>] queue_work_on+0x6d/0x1d0
    | [<ffffffff8110c881>] css_release+0x81/0x90
    | [<ffffffff8132844e>] percpu_ref_call_confirm_rcu+0xbe/0xd0
    | [<ffffffff813284e2>] percpu_ref_switch_to_atomic_rcu+0x82/0xc0
    | [<ffffffff810e03e5>] rcu_cpu_kthread+0x445/0x940
    | [<ffffffff81098a2d>] smpboot_thread_fn+0x18d/0x2d0
    | [<ffffffff810948d8>] kthread+0xe8/0x100
    | [<ffffffff815b9c3c>] ret_from_fork+0x7c/0xb0

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit cba17b120b8668423e5c76eb404ce2a9eeda1ed9
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Oct 28 11:50:06 2013 +0100

    a few open coded completions

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3806bf4645597de961a3b06df550e0a1aea25b4b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jan 11 11:23:51 2013 +0100

    completion: Use simple wait queues

    Completions have no long lasting callbacks and therefor do not need
    the complex waitqueue variant. Use simple waitqueues which reduces the
    contention on the waitqueue lock.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ba664706290b969e927e443cff7185dba5f7751b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 31 19:00:35 2013 +0200

    rcu-more-swait-conversions.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

    Merged Steven's

     static void rcu_nocb_gp_cleanup(struct rcu_state *rsp, struct rcu_node *rnp) {
    -       swait_wake(&rnp->nocb_gp_wq[rnp->completed & 0x1]);
    +       wake_up_all(&rnp->nocb_gp_wq[rnp->completed & 0x1]);
     }

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit ea1c90b872184b5ab10d20b45eebc3e6c4b1d278
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Apr 8 16:09:57 2013 +0200

    kernel/treercu: use a simple waitqueue

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b34c9115c114ff0aeec2b188d92b968370d04330
Author: Daniel Wagner <daniel.wagner@bmw-carit.de>
Date:   Fri Jul 11 15:26:11 2014 +0200

    work-simple: Simple work queue implemenation

    Provides a framework for enqueuing callbacks from irq context
    PREEMPT_RT_FULL safe. The callbacks are executed in kthread context.

    Bases on wait-simple.

    Signed-off-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit cbc71819cf4d50a8954d58b085a4b8f45b00f5bf
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Aug 27 14:20:26 2013 -0400

    simple-wait: rename and export the equivalent of waitqueue_active()

    The function "swait_head_has_waiters()" was internalized into
    wait-simple.c but it parallels the waitqueue_active of normal
    waitqueue support. Given that there are over 150 waitqueue_active
    users in drivers/ fs/ kernel/ and the like, lets make it globally
    visible, and rename it to parallel the waitqueue_active accordingly.
    We'll need to do this if we expect to expand its usage beyond RT.

    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9fe89722541a007333f085e454e12c32bc715862
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 10 11:47:35 2013 +0100

    wait-simple: Rework for use with completions

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 52ef6191c8dfe356f764936019f16fb88c44b2f6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 12 12:29:04 2011 +0100

    wait-simple: Simple waitqueue implementation

    wait_queue is a swiss army knife and in most of the cases the
    complexity is not needed. For RT waitqueues are a constant source of
    trouble as we can't convert the head lock to a raw spinlock due to
    fancy and long lasting callbacks.

    Provide a slim version, which allows RT to replace wait queues. This
    should go mainline as well, as it lowers memory consumption and
    runtime overhead.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

    smp_mb() added by Steven Rostedt to fix a race condition with swait
    wakeups vs adding items to the list.

commit 96f8ff8c3c40fdeba12f4ee578518669cb2e1a59
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Oct 28 12:19:57 2013 +0100

    wait.h: include atomic.h

    |  CC      init/main.o
    |In file included from include/linux/mmzone.h:9:0,
    |                 from include/linux/gfp.h:4,
    |                 from include/linux/kmod.h:22,
    |                 from include/linux/module.h:13,
    |                 from init/main.c:15:
    |include/linux/wait.h: In function ‚Äòwait_on_atomic_t‚Äô:
    |include/linux/wait.h:982:2: error: implicit declaration of function ‚Äòatomic_read‚Äô [-Werror=implicit-function-declaration]
    |  if (atomic_read(val) == 0)
    |  ^

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 794e1e7ee4a479b9f2b588334a15ba718e05157d
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Apr 25 18:12:52 2013 +0200

    drm/i915: drop trace_i915_gem_ring_dispatch on rt

    This tracepoint is responsible for:

    |[<814cc358>] __schedule_bug+0x4d/0x59
    |[<814d24cc>] __schedule+0x88c/0x930
    |[<814d3b90>] ? _raw_spin_unlock_irqrestore+0x40/0x50
    |[<814d3b95>] ? _raw_spin_unlock_irqrestore+0x45/0x50
    |[<810b57b5>] ? task_blocks_on_rt_mutex+0x1f5/0x250
    |[<814d27d9>] schedule+0x29/0x70
    |[<814d3423>] rt_spin_lock_slowlock+0x15b/0x278
    |[<814d3786>] rt_spin_lock+0x26/0x30
    |[<a00dced9>] gen6_gt_force_wake_get+0x29/0x60 [i915]
    |[<a00e183f>] gen6_ring_get_irq+0x5f/0x100 [i915]
    |[<a00b2a33>] ftrace_raw_event_i915_gem_ring_dispatch+0xe3/0x100 [i915]
    |[<a00ac1b3>] i915_gem_do_execbuffer.isra.13+0xbd3/0x1430 [i915]
    |[<810f8943>] ? trace_buffer_unlock_commit+0x43/0x60
    |[<8113e8d2>] ? ftrace_raw_event_kmem_alloc+0xd2/0x180
    |[<8101d063>] ? native_sched_clock+0x13/0x80
    |[<a00acf29>] i915_gem_execbuffer2+0x99/0x280 [i915]
    |[<a00114a3>] drm_ioctl+0x4c3/0x570 [drm]
    |[<8101d0d9>] ? sched_clock+0x9/0x10
    |[<a00ace90>] ? i915_gem_execbuffer+0x480/0x480 [i915]
    |[<810f1c18>] ? rb_commit+0x68/0xa0
    |[<810f1c6c>] ? ring_buffer_unlock_commit+0x1c/0xa0
    |[<81197467>] do_vfs_ioctl+0x97/0x540
    |[<81021318>] ? ftrace_raw_event_sys_enter+0xd8/0x130
    |[<811979a1>] sys_ioctl+0x91/0xb0
    |[<814db931>] tracesys+0xe1/0xe6

    Chris Wilson does not like to move i915_trace_irq_get() out of the macro

    |No. This enables the IRQ, as well as making a number of
    |very expensively serialised read, unconditionally.

    so it is gone now on RT.

    Cc: stable-rt@vger.kernel.org
    Reported-by: Joakim Hernberg <jbh@alchemy.lu>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit d72e8ca62b80c8054646c6ad5865c4521043216a
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jun 18 18:16:50 2015 -0400

    gpu/i915: don't open code these things

    The opencode part is gone in 1f83fee0 ("drm/i915: clear up wedged transitions")
    the owner check is still there.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit bd6759b25bdf225b62c96080a41efb175e45e594
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Apr 9 15:23:01 2015 +0200

    cpufreq: drop K8's driver from beeing selected

    Ralf posted a picture of a backtrace from

    | powernowk8_target_fn() -> transition_frequency_fidvid() and then at the
    | end:
    | 932         policy = cpufreq_cpu_get(smp_processor_id());
    | 933         cpufreq_cpu_put(policy);

    crashing the system on -RT. I assumed that policy was a NULL pointer but
    was rulled out. Since Ralf can't do any more investigations on this and
    I have no machine with this, I simply switch it off.

    Reported-by:  Ralf Mardorf <ralf.mardorf@alice-dsl.net>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 08ab2b9ddd033494c8b39faccf3fb40886d99951
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Feb 26 12:13:36 2015 +0100

    mmc: sdhci: don't provide hard irq handler

    the sdhci code provides both irq handlers: the primary and the thread
    handler. Initially it was meant for the primary handler to be very
    short.
    The result is not that on -RT we have the primrary handler grabing locks
    and this isn't really working. As a hack for now I just push both
    handler into the threaded mode.

    Cc: stable-rt@vger.kernel.org
    Reported-By: Michal ≈†mucr <msmucr@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 42a29d34198dd4ad56913a9dfa59588610533362
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 9 12:11:12 2013 +0100

    mmci: Remove bogus local_irq_save()

    On !RT interrupt runs with interrupts disabled. On RT it's in a
    thread, so no need to disable interrupts at all.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 90749b7cb0f15155a96038bd1720f3bd8a73d684
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Mar 21 11:35:49 2013 +0100

    i2c/omap: drop the lock hard irq context

    The lock is taken while reading two registers. On RT the first lock is
    taken in hard irq where it might sleep and in the threaded irq.
    The threaded irq runs in oneshot mode so the hard irq does not run until
    the thread the completes so there is no reason to grab the lock.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1d676eaf0238e7e2b5816e385a4c50483057977b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jan 23 14:45:59 2014 +0100

    leds: trigger: disable CPU trigger on -RT

    as it triggers:
    |CPU: 0 PID: 0 Comm: swapper Not tainted 3.12.8-rt10 #141
    |[<c0014aa4>] (unwind_backtrace+0x0/0xf8) from [<c0012788>] (show_stack+0x1c/0x20)
    |[<c0012788>] (show_stack+0x1c/0x20) from [<c043c8dc>] (dump_stack+0x20/0x2c)
    |[<c043c8dc>] (dump_stack+0x20/0x2c) from [<c004c5e8>] (__might_sleep+0x13c/0x170)
    |[<c004c5e8>] (__might_sleep+0x13c/0x170) from [<c043f270>] (__rt_spin_lock+0x28/0x38)
    |[<c043f270>] (__rt_spin_lock+0x28/0x38) from [<c043fa00>] (rt_read_lock+0x68/0x7c)
    |[<c043fa00>] (rt_read_lock+0x68/0x7c) from [<c036cf74>] (led_trigger_event+0x2c/0x5c)
    |[<c036cf74>] (led_trigger_event+0x2c/0x5c) from [<c036e0bc>] (ledtrig_cpu+0x54/0x5c)
    |[<c036e0bc>] (ledtrig_cpu+0x54/0x5c) from [<c000ffd8>] (arch_cpu_idle_exit+0x18/0x1c)
    |[<c000ffd8>] (arch_cpu_idle_exit+0x18/0x1c) from [<c00590b8>] (cpu_startup_entry+0xa8/0x234)
    |[<c00590b8>] (cpu_startup_entry+0xa8/0x234) from [<c043b2cc>] (rest_init+0xb8/0xe0)
    |[<c043b2cc>] (rest_init+0xb8/0xe0) from [<c061ebe0>] (start_kernel+0x2c4/0x380)

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit cb6a00ccb641dc66cb0161cdb8e77e37a9ad4e13
Author: Anders Roxell <anders.roxell@linaro.org>
Date:   Thu May 14 17:52:17 2015 +0200

    arch/arm64: Add lazy preempt support

    arm64 is missing support for PREEMPT_RT. The main feature which is
    lacking is support for lazy preemption. The arch-specific entry code,
    thread information structure definitions, and associated data tables
    have to be extended to provide this support. Then the Kconfig file has
    to be extended to indicate the support is available, and also to
    indicate that support for full RT preemption is now available.

    Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 25ceba931641f64443a57f1cc75e916960091615
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Nov 1 10:14:11 2012 +0100

    powerpc-preempt-lazy-support.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit bb781d199fd5e7a8e3a14d8b5b19564dd52022d5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 31 12:04:11 2012 +0100

    arm-preempt-lazy-support.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 351232c7685f740096966c3692454fafbc102d30
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Nov 1 11:03:47 2012 +0100

    x86-preempt-lazy.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ab8eca6042c89b4b7a58a3e4df419bb420ce3fe0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Oct 26 18:50:54 2012 +0100

    sched: Add support for lazy preemption

    It has become an obsession to mitigate the determinism vs. throughput
    loss of RT. Looking at the mainline semantics of preemption points
    gives a hint why RT sucks throughput wise for ordinary SCHED_OTHER
    tasks. One major issue is the wakeup of tasks which are right away
    preempting the waking task while the waking task holds a lock on which
    the woken task will block right after having preempted the wakee. In
    mainline this is prevented due to the implicit preemption disable of
    spin/rw_lock held regions. On RT this is not possible due to the fully
    preemptible nature of sleeping spinlocks.

    Though for a SCHED_OTHER task preempting another SCHED_OTHER task this
    is really not a correctness issue. RT folks are concerned about
    SCHED_FIFO/RR tasks preemption and not about the purely fairness
    driven SCHED_OTHER preemption latencies.

    So I introduced a lazy preemption mechanism which only applies to
    SCHED_OTHER tasks preempting another SCHED_OTHER task. Aside of the
    existing preempt_count each tasks sports now a preempt_lazy_count
    which is manipulated on lock acquiry and release. This is slightly
    incorrect as for lazyness reasons I coupled this on
    migrate_disable/enable so some other mechanisms get the same treatment
    (e.g. get_cpu_light).

    Now on the scheduler side instead of setting NEED_RESCHED this sets
    NEED_RESCHED_LAZY in case of a SCHED_OTHER/SCHED_OTHER preemption and
    therefor allows to exit the waking task the lock held region before
    the woken task preempts. That also works better for cross CPU wakeups
    as the other side can stay in the adaptive spinning loop.

    For RT class preemption there is no change. This simply sets
    NEED_RESCHED and forgoes the lazy preemption counter.

     Initial test do not expose any observable latency increasement, but
    history shows that I've been proven wrong before :)

    The lazy preemption mode is per default on, but with
    CONFIG_SCHED_DEBUG enabled it can be disabled via:

     # echo NO_PREEMPT_LAZY >/sys/kernel/debug/sched_features

    and reenabled via

     # echo PREEMPT_LAZY >/sys/kernel/debug/sched_features

    The test results so far are very machine and workload dependent, but
    there is a clear trend that it enhances the non RT workload
    performance.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e48332528e1a5f87e338e41711a5e5a896d9316b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Mar 21 20:19:05 2014 +0100

    rcu: make RCU_BOOST default on RT

    Since it is no longer invoked from the softirq people run into OOM more
    often if the priority of the RCU thread is too low. Making boosting
    default on RT should help in those case and it can be switched off if
    someone knows better.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit cc8351968493fff1f2215a11e29702e2ec3c5669
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Nov 4 13:21:10 2013 -0800

    rcu: Eliminate softirq processing from rcutree

    Running RCU out of softirq is a problem for some workloads that would
    like to manage RCU core processing independently of other softirq work,
    for example, setting kthread priority.  This commit therefore moves the
    RCU core work from softirq to a per-CPU/per-flavor SCHED_OTHER kthread
    named rcuc.  The SCHED_OTHER approach avoids the scalability problems
    that appeared with the earlier attempt to move RCU core processing to
    from softirq to kthreads.  That said, kernels built with RCU_BOOST=y
    will run the rcuc kthreads at the RCU-boosting priority.

    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Mike Galbraith <bitbucket@online.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e43f434c0b450492ec0b2229a40eaa5caec919ad
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Oct 28 13:26:09 2012 +0000

    rcu: Disable RCU_FAST_NO_HZ on RT

    This uses a timer_list timer from the irq disabled guts of the idle
    code. Disable it for now to prevent wreckage.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1a7bbec40c5541dd5ee1701c0af64d8c24ec0473
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Sat Apr 11 15:15:59 2015 +0200

    rt, nohz_full: fix nohz_full for PREEMPT_RT_FULL

    A task being ticked and trying to shut the tick down will fail due
    to having just awakened ksoftirqd, subtract it from nr_running.

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0ac3cb0cf2b8706f95b9c99d5e8c341cf1d04746
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Fri Dec 6 00:42:22 2013 +0100

    softirq: make migrate disable/enable conditioned on softirq_nestcnt transition

    This patch removes the recursive calls to migrate_disable/enable in
    local_bh_disable/enable

    the softirq-local-lock.patch introduces local_bh_disable/enable wich
    decrements/increments the current->softirq_nestcnt and disable/enables
    migration as well. as softirq_nestcnt (include/linux/sched.h conditioned
    on CONFIG_PREEMPT_RT_BASE) already is tracking the nesting level of the
    recursive calls to local_bh_disable/enable (all in kernel/softirq.c) - no
    need to do it twice.

    migrate_disable/enable thus can be conditionsed on softirq_nestcnt making
    a transition from 0-1 to disable migration and 1-0 to re-enable it.

    No change of functional behavior, this does noticably reduce the observed
    nesting level of migrate_disable/enable

    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 186f1b1557a1ac4710259fe2f6ae436edca846a5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Oct 28 13:46:16 2012 +0000

    softirq: Adapt NOHZ softirq pending check to new RT scheme

    We can't rely on ksoftirqd anymore and we need to check the tasks
    which run a particular softirq and if such a task is pi blocked ignore
    the other pending bits of that task as well.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ca3fcf7c4df81507726fff70b03fb3d135207b45
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Fri Jan 17 20:44:03 2014 +0100

    API cleanup - use local_lock not __local_lock for soft

    trivial API cleanup - kernel/softirq.c was mimiking local_lock.

    No change of functional behavior

    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit db6ad40861bba691e56283e9e15d09af9c64dc9f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 4 14:20:47 2012 +0100

    softirq: Split softirq locks

    The 3.x RT series removed the split softirq implementation in favour
    of pushing softirq processing into the context of the thread which
    raised it. Though this prevents us from handling the various softirqs
    at different priorities. Now instead of reintroducing the split
    softirq threads we split the locks which serialize the softirq
    processing.

    If a softirq is raised in context of a thread, then the softirq is
    noted on a per thread field, if the thread is in a bh disabled
    region. If the softirq is raised from hard interrupt context, then the
    bit is set in the flag field of ksoftirqd and ksoftirqd is invoked.
    When a thread leaves a bh disabled region, then it tries to execute
    the softirqs which have been raised in its own context. It acquires
    the per softirq / per cpu lock for the softirq and then checks,
    whether the softirq is still pending in the per cpu
    local_softirq_pending() field. If yes, it runs the softirq. If no,
    then some other task executed it already. This allows for zero config
    softirq elevation in the context of user space tasks or interrupt
    threads.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit cd86011493d596c494af30caa9251caf63f0cadf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 4 15:33:53 2012 +0100

    softirq: Split handling function

    Split out the inner handling function, so RT can reuse it.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9ed4dd6c24493b924a553e6ade204ada4787f5d5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 4 14:30:25 2012 +0100

    softirq: Make serving softirqs a task flag

    Avoid the percpu softirq_runner pointer magic by using a task flag.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2ba579e5cb101d6d619f0b3e2cbd93fca4bb86dc
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Oct 4 11:02:04 2012 -0400

    softirq: Init softirq local lock after per cpu section is set up

    I discovered this bug when booting 3.4-rt on my powerpc box. It crashed
    with the following report:

    ------------[ cut here ]------------
    kernel BUG at /work/rt/stable-rt.git/kernel/rtmutex_common.h:75!
    Oops: Exception in kernel mode, sig: 5 [#1]
    PREEMPT SMP NR_CPUS=64 NUMA PA Semi PWRficient
    Modules linked in:
    NIP: c0000000004aa03c LR: c0000000004aa01c CTR: c00000000009b2ac
    REGS: c00000003e8d7950 TRAP: 0700   Not tainted  (3.4.11-test-rt19)
    MSR: 9000000000029032 <SF,HV,EE,ME,IR,DR,RI>  CR: 24000082  XER: 20000000
    SOFTE: 0
    TASK = c00000003e8fdcd0[11] 'ksoftirqd/1' THREAD: c00000003e8d4000 CPU: 1
    GPR00: 0000000000000001 c00000003e8d7bd0 c000000000d6cbb0 0000000000000000
    GPR04: c00000003e8fdcd0 0000000000000000 0000000024004082 c000000000011454
    GPR08: 0000000000000000 0000000080000001 c00000003e8fdcd1 0000000000000000
    GPR12: 0000000024000084 c00000000fff0280 ffffffffffffffff 000000003ffffad8
    GPR16: ffffffffffffffff 000000000072c798 0000000000000060 0000000000000000
    GPR20: 0000000000642741 000000000072c858 000000003ffffaf0 0000000000000417
    GPR24: 000000000072dcd0 c00000003e7ff990 0000000000000000 0000000000000001
    GPR28: 0000000000000000 c000000000792340 c000000000ccec78 c000000001182338
    NIP [c0000000004aa03c] .wakeup_next_waiter+0x44/0xb8
    LR [c0000000004aa01c] .wakeup_next_waiter+0x24/0xb8
    Call Trace:
    [c00000003e8d7bd0] [c0000000004aa01c] .wakeup_next_waiter+0x24/0xb8 (unreliable)
    [c00000003e8d7c60] [c0000000004a0320] .rt_spin_lock_slowunlock+0x8c/0xe4
    [c00000003e8d7ce0] [c0000000004a07cc] .rt_spin_unlock+0x54/0x64
    [c00000003e8d7d60] [c0000000000636bc] .__thread_do_softirq+0x130/0x174
    [c00000003e8d7df0] [c00000000006379c] .run_ksoftirqd+0x9c/0x1a4
    [c00000003e8d7ea0] [c000000000080b68] .kthread+0xa8/0xb4
    [c00000003e8d7f90] [c00000000001c2f8] .kernel_thread+0x54/0x70
    Instruction dump:
    60000000 e86d01c8 38630730 4bff7061 60000000 ebbf0008 7c7c1b78 e81d0040
    7fe00278 7c000074 7800d182 68000001 <0b000000> e88d01c8 387d0010 38840738

    The rtmutex_common.h:75 is:

    rt_mutex_top_waiter(struct rt_mutex *lock)
    {
    	struct rt_mutex_waiter *w;

    	w = plist_first_entry(&lock->wait_list, struct rt_mutex_waiter,
    			       list_entry);
    	BUG_ON(w->lock != lock);

    	return w;
    }

    Where the waiter->lock is corrupted. I saw various other random bugs
    that all had to with the softirq lock and plist. As plist needs to be
    initialized before it is used I investigated how this lock is
    initialized. It's initialized with:

    void __init softirq_early_init(void)
    {
    	local_irq_lock_init(local_softirq_lock);
    }

    Where:

    #define local_irq_lock_init(lvar)					\
    	do {								\
    		int __cpu;						\
    		for_each_possible_cpu(__cpu)				\
    			spin_lock_init(&per_cpu(lvar, __cpu).lock);	\
    	} while (0)

    As the softirq lock is a local_irq_lock, which is a per_cpu lock, the
    initialization is done to all per_cpu versions of the lock. But lets
    look at where the softirq_early_init() is called from.

    In init/main.c: start_kernel()

    /*
     * Interrupts are still disabled. Do necessary setups, then
     * enable them
     */
    	softirq_early_init();
    	tick_init();
    	boot_cpu_init();
    	page_address_init();
    	printk(KERN_NOTICE "%s", linux_banner);
    	setup_arch(&command_line);
    	mm_init_owner(&init_mm, &init_task);
    	mm_init_cpumask(&init_mm);
    	setup_command_line(command_line);
    	setup_nr_cpu_ids();
    	setup_per_cpu_areas();
    	smp_prepare_boot_cpu();	/* arch-specific boot-cpu hooks */

    One of the first things that is called is the initialization of the
    softirq lock. But if you look further down, we see the per_cpu areas
    have not been set up yet. Thus initializing a local_irq_lock() before
    the per_cpu section is set up, may not work as it is initializing the
    per cpu locks before the per cpu exists.

    By moving the softirq_early_init() right after setup_per_cpu_areas(),
    the kernel boots fine.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Clark Williams <clark@redhat.com>
    Cc: John Kacur <jkacur@redhat.com>
    Cc: Carsten Emde <cbe@osadl.org>
    Cc: vomlehn@texas.net
    Link: http://lkml.kernel.org/r/1349362924.6755.18.camel@gandalf.local.home
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 762a936c108bd6136cb4b970bd7d5363d1985143
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Nov 13 17:17:09 2011 +0100

    softirq: Check preemption after reenabling interrupts

    raise_softirq_irqoff() disables interrupts and wakes the softirq
    daemon, but after reenabling interrupts there is no preemption check,
    so the execution of the softirq thread might be delayed arbitrarily.

    In principle we could add that check to local_irq_enable/restore, but
    that's overkill as the rasie_softirq_irqoff() sections are the only
    ones which show this behaviour.

    Reported-by: Carsten Emde <cbe@osadl.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit af7ca6d4adeb878b087e824e7e2d0789d7fe2180
Author: Yong Zhang <yong.zhang@windriver.com>
Date:   Wed Jul 11 22:05:21 2012 +0000

    perf: Make swevent hrtimer run in irq instead of softirq

    Otherwise we get a deadlock like below:

    [ 1044.042749] BUG: scheduling while atomic: ksoftirqd/21/141/0x00010003
    [ 1044.042752] INFO: lockdep is turned off.
    [ 1044.042754] Modules linked in:
    [ 1044.042757] Pid: 141, comm: ksoftirqd/21 Tainted: G        W    3.4.0-rc2-rt3-23676-ga723175-dirty #29
    [ 1044.042759] Call Trace:
    [ 1044.042761]  <IRQ>  [<ffffffff8107d8e5>] __schedule_bug+0x65/0x80
    [ 1044.042770]  [<ffffffff8168978c>] __schedule+0x83c/0xa70
    [ 1044.042775]  [<ffffffff8106bdd2>] ? prepare_to_wait+0x32/0xb0
    [ 1044.042779]  [<ffffffff81689a5e>] schedule+0x2e/0xa0
    [ 1044.042782]  [<ffffffff81071ebd>] hrtimer_wait_for_timer+0x6d/0xb0
    [ 1044.042786]  [<ffffffff8106bb30>] ? wake_up_bit+0x40/0x40
    [ 1044.042790]  [<ffffffff81071f20>] hrtimer_cancel+0x20/0x40
    [ 1044.042794]  [<ffffffff8111da0c>] perf_swevent_cancel_hrtimer+0x3c/0x50
    [ 1044.042798]  [<ffffffff8111da31>] task_clock_event_stop+0x11/0x40
    [ 1044.042802]  [<ffffffff8111da6e>] task_clock_event_del+0xe/0x10
    [ 1044.042805]  [<ffffffff8111c568>] event_sched_out+0x118/0x1d0
    [ 1044.042809]  [<ffffffff8111c649>] group_sched_out+0x29/0x90
    [ 1044.042813]  [<ffffffff8111ed7e>] __perf_event_disable+0x18e/0x200
    [ 1044.042817]  [<ffffffff8111c343>] remote_function+0x63/0x70
    [ 1044.042821]  [<ffffffff810b0aae>] generic_smp_call_function_single_interrupt+0xce/0x120
    [ 1044.042826]  [<ffffffff81022bc7>] smp_call_function_single_interrupt+0x27/0x40
    [ 1044.042831]  [<ffffffff8168d50c>] call_function_single_interrupt+0x6c/0x80
    [ 1044.042833]  <EOI>  [<ffffffff811275b0>] ? perf_event_overflow+0x20/0x20
    [ 1044.042840]  [<ffffffff8168b970>] ? _raw_spin_unlock_irq+0x30/0x70
    [ 1044.042844]  [<ffffffff8168b976>] ? _raw_spin_unlock_irq+0x36/0x70
    [ 1044.042848]  [<ffffffff810702e2>] run_hrtimer_softirq+0xc2/0x200
    [ 1044.042853]  [<ffffffff811275b0>] ? perf_event_overflow+0x20/0x20
    [ 1044.042857]  [<ffffffff81045265>] __do_softirq_common+0xf5/0x3a0
    [ 1044.042862]  [<ffffffff81045c3d>] __thread_do_softirq+0x15d/0x200
    [ 1044.042865]  [<ffffffff81045dda>] run_ksoftirqd+0xfa/0x210
    [ 1044.042869]  [<ffffffff81045ce0>] ? __thread_do_softirq+0x200/0x200
    [ 1044.042873]  [<ffffffff81045ce0>] ? __thread_do_softirq+0x200/0x200
    [ 1044.042877]  [<ffffffff8106b596>] kthread+0xb6/0xc0
    [ 1044.042881]  [<ffffffff8168b97b>] ? _raw_spin_unlock_irq+0x3b/0x70
    [ 1044.042886]  [<ffffffff8168d994>] kernel_thread_helper+0x4/0x10
    [ 1044.042889]  [<ffffffff8107d98c>] ? finish_task_switch+0x8c/0x110
    [ 1044.042894]  [<ffffffff8168b97b>] ? _raw_spin_unlock_irq+0x3b/0x70
    [ 1044.042897]  [<ffffffff8168bd5d>] ? retint_restore_args+0xe/0xe
    [ 1044.042900]  [<ffffffff8106b4e0>] ? kthreadd+0x1e0/0x1e0
    [ 1044.042902]  [<ffffffff8168d990>] ? gs_change+0xb/0xb

    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1341476476-5666-1-git-send-email-yong.zhang0@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 12192b5dd7deb4c03526db71aedc4d7cdb3beb3c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Sep 28 10:49:42 2012 +0100

    rt: rwsem/rwlock: lockdep annotations

    rwlocks and rwsems on RT do not allow multiple readers. Annotate the
    lockdep acquire functions accordingly.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 05ab85fca3303b3507da503136221927e56811fb
Author: Josh Cartwright <josh.cartwright@ni.com>
Date:   Wed Jan 28 13:08:45 2015 -0600

    lockdep: selftest: fix warnings due to missing PREEMPT_RT conditionals

    "lockdep: Selftest: Only do hardirq context test for raw spinlock"
    disabled the execution of certain tests with PREEMPT_RT_FULL, but did
    not prevent the tests from still being defined.  This leads to warnings
    like:

      ./linux/lib/locking-selftest.c:574:1: warning: 'irqsafe1_hard_rlock_12' defined but not used [-Wunused-function]
      ./linux/lib/locking-selftest.c:574:1: warning: 'irqsafe1_hard_rlock_21' defined but not used [-Wunused-function]
      ./linux/lib/locking-selftest.c:577:1: warning: 'irqsafe1_hard_wlock_12' defined but not used [-Wunused-function]
      ./linux/lib/locking-selftest.c:577:1: warning: 'irqsafe1_hard_wlock_21' defined but not used [-Wunused-function]
      ./linux/lib/locking-selftest.c:580:1: warning: 'irqsafe1_soft_spin_12' defined but not used [-Wunused-function]
      ...

    Fixed by wrapping the test definitions in #ifndef CONFIG_PREEMPT_RT_FULL
    conditionals.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Josh Cartwright <josh.cartwright@ni.com>
    Signed-off-by: Xander Huff <xander.huff@ni.com>
    Acked-by: Gratian Crisan <gratian.crisan@ni.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b11516c75d2c547c807bedfa66541e38bb2c9e34
Author: Yong Zhang <yong.zhang@windriver.com>
Date:   Mon Apr 16 15:01:56 2012 +0800

    lockdep: Selftest: Only do hardirq context test for raw spinlock

    On -rt there is no softirq context any more and rwlock is sleepable,
    disable softirq context test and rwlock+irq test.

    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Cc: Yong Zhang <yong.zhang@windriver.com>
    Link: http://lkml.kernel.org/r/1334559716-18447-3-git-send-email-yong.zhang0@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0b9fdeb0298c20b9ebabab513a1e025d10816986
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Oct 5 09:03:24 2012 +0100

    crypto: Convert crypto notifier chain to SRCU

    The crypto notifier deadlocks on RT. Though this can be a real deadlock
    on mainline as well due to fifo fair rwsems.

    The involved parties here are:

    [   82.172678] swapper/0       S 0000000000000001     0     1      0 0x00000000
    [   82.172682]  ffff88042f18fcf0 0000000000000046 ffff88042f18fc80 ffffffff81491238
    [   82.172685]  0000000000011cc0 0000000000011cc0 ffff88042f18c040 ffff88042f18ffd8
    [   82.172688]  0000000000011cc0 0000000000011cc0 ffff88042f18ffd8 0000000000011cc0
    [   82.172689] Call Trace:
    [   82.172697]  [<ffffffff81491238>] ? _raw_spin_unlock_irqrestore+0x6c/0x7a
    [   82.172701]  [<ffffffff8148fd3f>] schedule+0x64/0x66
    [   82.172704]  [<ffffffff8148ec6b>] schedule_timeout+0x27/0xd0
    [   82.172708]  [<ffffffff81043c0c>] ? unpin_current_cpu+0x1a/0x6c
    [   82.172713]  [<ffffffff8106e491>] ? migrate_enable+0x12f/0x141
    [   82.172716]  [<ffffffff8148fbbd>] wait_for_common+0xbb/0x11f
    [   82.172719]  [<ffffffff810709f2>] ? try_to_wake_up+0x182/0x182
    [   82.172722]  [<ffffffff8148fc96>] wait_for_completion_interruptible+0x1d/0x2e
    [   82.172726]  [<ffffffff811debfd>] crypto_wait_for_test+0x49/0x6b
    [   82.172728]  [<ffffffff811ded32>] crypto_register_alg+0x53/0x5a
    [   82.172730]  [<ffffffff811ded6c>] crypto_register_algs+0x33/0x72
    [   82.172734]  [<ffffffff81ad7686>] ? aes_init+0x12/0x12
    [   82.172737]  [<ffffffff81ad76ea>] aesni_init+0x64/0x66
    [   82.172741]  [<ffffffff81000318>] do_one_initcall+0x7f/0x13b
    [   82.172744]  [<ffffffff81ac4d34>] kernel_init+0x199/0x22c
    [   82.172747]  [<ffffffff81ac44ef>] ? loglevel+0x31/0x31
    [   82.172752]  [<ffffffff814987c4>] kernel_thread_helper+0x4/0x10
    [   82.172755]  [<ffffffff81491574>] ? retint_restore_args+0x13/0x13
    [   82.172759]  [<ffffffff81ac4b9b>] ? start_kernel+0x3ca/0x3ca
    [   82.172761]  [<ffffffff814987c0>] ? gs_change+0x13/0x13

    [   82.174186] cryptomgr_test  S 0000000000000001     0    41      2 0x00000000
    [   82.174189]  ffff88042c971980 0000000000000046 ffffffff81d74830 0000000000000292
    [   82.174192]  0000000000011cc0 0000000000011cc0 ffff88042c96eb80 ffff88042c971fd8
    [   82.174195]  0000000000011cc0 0000000000011cc0 ffff88042c971fd8 0000000000011cc0
    [   82.174195] Call Trace:
    [   82.174198]  [<ffffffff8148fd3f>] schedule+0x64/0x66
    [   82.174201]  [<ffffffff8148ec6b>] schedule_timeout+0x27/0xd0
    [   82.174204]  [<ffffffff81043c0c>] ? unpin_current_cpu+0x1a/0x6c
    [   82.174206]  [<ffffffff8106e491>] ? migrate_enable+0x12f/0x141
    [   82.174209]  [<ffffffff8148fbbd>] wait_for_common+0xbb/0x11f
    [   82.174212]  [<ffffffff810709f2>] ? try_to_wake_up+0x182/0x182
    [   82.174215]  [<ffffffff8148fc96>] wait_for_completion_interruptible+0x1d/0x2e
    [   82.174218]  [<ffffffff811e4883>] cryptomgr_notify+0x280/0x385
    [   82.174221]  [<ffffffff814943de>] notifier_call_chain+0x6b/0x98
    [   82.174224]  [<ffffffff8108a11c>] ? rt_down_read+0x10/0x12
    [   82.174227]  [<ffffffff810677cd>] __blocking_notifier_call_chain+0x70/0x8d
    [   82.174230]  [<ffffffff810677fe>] blocking_notifier_call_chain+0x14/0x16
    [   82.174234]  [<ffffffff811dd272>] crypto_probing_notify+0x24/0x50
    [   82.174236]  [<ffffffff811dd7a1>] crypto_alg_mod_lookup+0x3e/0x74
    [   82.174238]  [<ffffffff811dd949>] crypto_alloc_base+0x36/0x8f
    [   82.174241]  [<ffffffff811e9408>] cryptd_alloc_ablkcipher+0x6e/0xb5
    [   82.174243]  [<ffffffff811dd591>] ? kzalloc.clone.5+0xe/0x10
    [   82.174246]  [<ffffffff8103085d>] ablk_init_common+0x1d/0x38
    [   82.174249]  [<ffffffff8103852a>] ablk_ecb_init+0x15/0x17
    [   82.174251]  [<ffffffff811dd8c6>] __crypto_alloc_tfm+0xc7/0x114
    [   82.174254]  [<ffffffff811e0caa>] ? crypto_lookup_skcipher+0x1f/0xe4
    [   82.174256]  [<ffffffff811e0dcf>] crypto_alloc_ablkcipher+0x60/0xa5
    [   82.174258]  [<ffffffff811e5bde>] alg_test_skcipher+0x24/0x9b
    [   82.174261]  [<ffffffff8106d96d>] ? finish_task_switch+0x3f/0xfa
    [   82.174263]  [<ffffffff811e6b8e>] alg_test+0x16f/0x1d7
    [   82.174267]  [<ffffffff811e45ac>] ? cryptomgr_probe+0xac/0xac
    [   82.174269]  [<ffffffff811e45d8>] cryptomgr_test+0x2c/0x47
    [   82.174272]  [<ffffffff81061161>] kthread+0x7e/0x86
    [   82.174275]  [<ffffffff8106d9dd>] ? finish_task_switch+0xaf/0xfa
    [   82.174278]  [<ffffffff814987c4>] kernel_thread_helper+0x4/0x10
    [   82.174281]  [<ffffffff81491574>] ? retint_restore_args+0x13/0x13
    [   82.174284]  [<ffffffff810610e3>] ? __init_kthread_worker+0x8c/0x8c
    [   82.174287]  [<ffffffff814987c0>] ? gs_change+0x13/0x13

    [   82.174329] cryptomgr_probe D 0000000000000002     0    47      2 0x00000000
    [   82.174332]  ffff88042c991b70 0000000000000046 ffff88042c991bb0 0000000000000006
    [   82.174335]  0000000000011cc0 0000000000011cc0 ffff88042c98ed00 ffff88042c991fd8
    [   82.174338]  0000000000011cc0 0000000000011cc0 ffff88042c991fd8 0000000000011cc0
    [   82.174338] Call Trace:
    [   82.174342]  [<ffffffff8148fd3f>] schedule+0x64/0x66
    [   82.174344]  [<ffffffff814901ad>] __rt_mutex_slowlock+0x85/0xbe
    [   82.174347]  [<ffffffff814902d2>] rt_mutex_slowlock+0xec/0x159
    [   82.174351]  [<ffffffff81089c4d>] rt_mutex_fastlock.clone.8+0x29/0x2f
    [   82.174353]  [<ffffffff81490372>] rt_mutex_lock+0x33/0x37
    [   82.174356]  [<ffffffff8108a0f2>] __rt_down_read+0x50/0x5a
    [   82.174358]  [<ffffffff8108a11c>] ? rt_down_read+0x10/0x12
    [   82.174360]  [<ffffffff8108a11c>] rt_down_read+0x10/0x12
    [   82.174363]  [<ffffffff810677b5>] __blocking_notifier_call_chain+0x58/0x8d
    [   82.174366]  [<ffffffff810677fe>] blocking_notifier_call_chain+0x14/0x16
    [   82.174369]  [<ffffffff811dd272>] crypto_probing_notify+0x24/0x50
    [   82.174372]  [<ffffffff811debd6>] crypto_wait_for_test+0x22/0x6b
    [   82.174374]  [<ffffffff811decd3>] crypto_register_instance+0xb4/0xc0
    [   82.174377]  [<ffffffff811e9b76>] cryptd_create+0x378/0x3b6
    [   82.174379]  [<ffffffff811de512>] ? __crypto_lookup_template+0x5b/0x63
    [   82.174382]  [<ffffffff811e4545>] cryptomgr_probe+0x45/0xac
    [   82.174385]  [<ffffffff811e4500>] ? crypto_alloc_pcomp+0x1b/0x1b
    [   82.174388]  [<ffffffff81061161>] kthread+0x7e/0x86
    [   82.174391]  [<ffffffff8106d9dd>] ? finish_task_switch+0xaf/0xfa
    [   82.174394]  [<ffffffff814987c4>] kernel_thread_helper+0x4/0x10
    [   82.174398]  [<ffffffff81491574>] ? retint_restore_args+0x13/0x13
    [   82.174401]  [<ffffffff810610e3>] ? __init_kthread_worker+0x8c/0x8c
    [   82.174403]  [<ffffffff814987c0>] ? gs_change+0x13/0x13

    cryptomgr_test spawns the cryptomgr_probe thread from the notifier
    call. The probe thread fires the same notifier as the test thread and
    deadlocks on the rwsem on RT.

    Now this is a potential deadlock in mainline as well, because we have
    fifo fair rwsems. If another thread blocks with a down_write() on the
    notifier chain before the probe thread issues the down_read() it will
    block the probe thread and the whole party is dead locked.

    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9365ec788c61bfaefcc99b039fc19462c5499436
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Mar 20 18:06:20 2013 +0100

    net: Add a mutex around devnet_rename_seq

    On RT write_seqcount_begin() disables preemption and device_rename()
    allocates memory with GFP_KERNEL and grabs later the sysfs_mutex
    mutex. Serialize with a mutex and add use the non preemption disabling
    __write_seqcount_begin().

    To avoid writer starvation, let the reader grab the mutex and release
    it when it detects a writer in progress. This keeps the normal case
    (no reader on the fly) fast.

    [ tglx: Instead of replacing the seqcount by a mutex, add the mutex ]

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 943384ac3f8136fd70c3d588c32cd69573aac872
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Oct 28 15:12:49 2012 +0000

    net: Use local_bh_disable in netif_rx_ni()

    This code triggers the new WARN in __raise_softirq_irqsoff() though it
    actually looks at the softirq pending bit and calls into the softirq
    code, but that fits not well with the context related softirq model of
    RT. It's correct on mainline though, but going through
    local_bh_disable/enable here is not going to hurt badly.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 69789842941471243402b6f6b18ef8d054e23aec
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Oct 28 11:18:08 2012 +0100

    net: netfilter: Serialize xt_write_recseq sections on RT

    The netfilter code relies only on the implicit semantics of
    local_bh_disable() for serializing wt_write_recseq sections. RT breaks
    that and needs explicit serialization here.

    Reported-by: Peter LaDow <petela@gocougs.wsu.edu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7647044aa0519319d3862396d3586887c9f744a4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 26 16:21:08 2012 +0200

    net: Another local_irq_disable/kmalloc headache

    Replace it by a local lock. Though that's pretty inefficient :(

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ad59d5987f05fed2c85ef0714a86f22107291a70
Author: Priyanka Jain <Priyanka.Jain@freescale.com>
Date:   Thu May 17 09:35:11 2012 +0530

    net,RT:REmove preemption disabling in netif_rx()

    1)enqueue_to_backlog() (called from netif_rx) should be
      bind to a particluar CPU. This can be achieved by
      disabling migration. No need to disable preemption

    2)Fixes crash "BUG: scheduling while atomic: ksoftirqd"
      in case of RT.
      If preemption is disabled, enqueue_to_backog() is called
      in atomic context. And if backlog exceeds its count,
      kfree_skb() is called. But in RT, kfree_skb() might
      gets scheduled out, so it expects non atomic context.

    3)When CONFIG_PREEMPT_RT_FULL is not defined,
     migrate_enable(), migrate_disable() maps to
     preempt_enable() and preempt_disable(), so no
     change in functionality in case of non-RT.

    -Replace preempt_enable(), preempt_disable() with
     migrate_enable(), migrate_disable() respectively
    -Replace get_cpu(), put_cpu() with get_cpu_light(),
     put_cpu_light() respectively

    Signed-off-by: Priyanka Jain <Priyanka.Jain@freescale.com>
    Acked-by: Rajan Srivastava <Rajan.Srivastava@freescale.com>
    Cc: <rostedt@goodmis.orgn>
    Link: http://lkml.kernel.org/r/1337227511-2271-1-git-send-email-Priyanka.Jain@freescale.com
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7cf367eb301819101cfade1ff3690a71459c998e
Author: John Kacur <jkacur@redhat.com>
Date:   Fri Apr 27 12:48:46 2012 +0200

    scsi: qla2xxx: Use local_irq_save_nort() in qla2x00_poll

    RT triggers the following:

    [   11.307652]  [<ffffffff81077b27>] __might_sleep+0xe7/0x110
    [   11.307663]  [<ffffffff8150e524>] rt_spin_lock+0x24/0x60
    [   11.307670]  [<ffffffff8150da78>] ? rt_spin_lock_slowunlock+0x78/0x90
    [   11.307703]  [<ffffffffa0272d83>] qla24xx_intr_handler+0x63/0x2d0 [qla2xxx]
    [   11.307736]  [<ffffffffa0262307>] qla2x00_poll+0x67/0x90 [qla2xxx]

    Function qla2x00_poll does local_irq_save() before calling qla24xx_intr_handler
    which has a spinlock. Since spinlocks are sleepable on rt, it is not allowed
    to call them with interrupts disabled. Therefore we use local_irq_save_nort()
    instead which saves flags without disabling interrupts.

    This fix needs to be applied to v3.0-rt, v3.2-rt and v3.4-rt

    Suggested-by: Thomas Gleixner
    Signed-off-by: John Kacur <jkacur@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: David Sommerseth <davids@redhat.com>
    Link: http://lkml.kernel.org/r/1335523726-10024-1-git-send-email-jkacur@redhat.com
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 5bcb50b679467826f0b1f04fcc91cf0cfc25d062
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Tue Mar 24 08:14:49 2015 +0100

    hotplug: Use set_cpus_allowed_ptr() in sync_unplug_thread()

    do_set_cpus_allowed() is not safe vs ->sched_class change.

    crash> bt
    PID: 11676  TASK: ffff88026f979da0  CPU: 22  COMMAND: "sync_unplug/22"
     #0 [ffff880274d25bc8] machine_kexec at ffffffff8103b41c
     #1 [ffff880274d25c18] crash_kexec at ffffffff810d881a
     #2 [ffff880274d25cd8] oops_end at ffffffff81525818
     #3 [ffff880274d25cf8] do_invalid_op at ffffffff81003096
     #4 [ffff880274d25d90] invalid_op at ffffffff8152d3de
        [exception RIP: set_cpus_allowed_rt+18]
        RIP: ffffffff8109e012  RSP: ffff880274d25e48  RFLAGS: 00010202
        RAX: ffffffff8109e000  RBX: ffff88026f979da0  RCX: ffff8802770cb6e8
        RDX: 0000000000000000  RSI: ffffffff81add700  RDI: ffff88026f979da0
        RBP: ffff880274d25e78   R8: ffffffff816112e0   R9: 0000000000000001
        R10: 0000000000000001  R11: 0000000000011940  R12: ffff88026f979da0
        R13: ffff8802770cb6d0  R14: ffff880274d25fd8  R15: 0000000000000000
        ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018
     #5 [ffff880274d25e60] do_set_cpus_allowed at ffffffff8108e65f
     #6 [ffff880274d25e80] sync_unplug_thread at ffffffff81058c08
     #7 [ffff880274d25ed8] kthread at ffffffff8107cad6
     #8 [ffff880274d25f50] ret_from_fork at ffffffff8152bbbc
    crash> task_struct ffff88026f979da0 | grep class
      sched_class = 0xffffffff816111e0 <fair_sched_class+64>,

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 944e9b0583a496bdbe6c1fb1e606df0cac5d1c4b
Author: Tiejun Chen <tiejun.chen@windriver.com>
Date:   Thu Nov 7 10:06:07 2013 +0800

    cpu_down: move migrate_enable() back

    Commit 08c1ab68, "hotplug-use-migrate-disable.patch", intends to
    use migrate_enable()/migrate_disable() to replace that combination
    of preempt_enable() and preempt_disable(), but actually in
    !CONFIG_PREEMPT_RT_FULL case, migrate_enable()/migrate_disable()
    are still equal to preempt_enable()/preempt_disable(). So that
    followed cpu_hotplug_begin()/cpu_unplug_begin(cpu) would go schedule()
    to trigger schedule_debug() like this:

    _cpu_down()
    	|
    	+ migrate_disable() = preempt_disable()
    	|
    	+ cpu_hotplug_begin() or cpu_unplug_begin()
    		|
    		+ schedule()
    			|
    			+ __schedule()
    				|
    				+ preempt_disable();
    				|
    				+ __schedule_bug() is true!

    So we should move migrate_enable() as the original scheme.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 6a72d067f0bb9217f060a7fdc071252b6d744034
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Jun 14 17:16:35 2013 +0200

    kernel/hotplug: restore original cpu mask oncpu/down

    If a task which is allowed to run only on CPU X puts CPU Y down then it
    will be allowed on all CPUs but the on CPU Y after it comes back from
    kernel. This patch ensures that we don't lose the initial setting unless
    the CPU the task is running is going down.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1e549ba05162195bd193b27db22ba0d6d9eb91bc
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Jun 7 22:37:06 2013 +0200

    kernel/cpu: fix cpu down problem if kthread's cpu is going down

    If kthread is pinned to CPUx and CPUx is going down then we get into
    trouble:
    - first the unplug thread is created
    - it will set itself to hp->unplug. As a result, every task that is
      going to take a lock, has to leave the CPU.
    - the CPU_DOWN_PREPARE notifier are started. The worker thread will
      start a new process for the "high priority worker".
      Now kthread would like to take a lock but since it can't leave the CPU
      it will never complete its task.

    We could fire the unplug thread after the notifier but then the cpu is
    no longer marked "online" and the unplug thread will run on CPU0 which
    was fixed before :)

    So instead the unplug thread is started and kept waiting until the
    notfier complete their work.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 46b0f3390f0be32afceafacd4d7074741a21ea0c
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Dec 5 09:16:52 2013 -0500

    cpu hotplug: Document why PREEMPT_RT uses a spinlock

    The patch:

        cpu: Make hotplug.lock a "sleeping" spinlock on RT

        Tasks can block on hotplug.lock in pin_current_cpu(), but their
        state might be != RUNNING. So the mutex wakeup will set the state
        unconditionally to RUNNING. That might cause spurious unexpected
        wakeups. We could provide a state preserving mutex_lock() function,
        but this is semantically backwards. So instead we convert the
        hotplug.lock() to a spinlock for RT, which has the state preserving
        semantics already.

    Fixed a bug where the hotplug lock on PREEMPT_RT can be called after a
    task set its state to TASK_UNINTERRUPTIBLE and before it called
    schedule. If the hotplug_lock used a mutex, and there was contention,
    the current task's state would be turned to TASK_RUNNABLE and the
    schedule call will not sleep. This caused unexpected results.

    Although the patch had a description of the change, the code had no
    comments about it. This causes confusion to those that review the code,
    and as PREEMPT_RT is held in a quilt queue and not git, it's not as easy
    to see why a change was made. Even if it was in git, the code should
    still have a comment for something as subtle as this.

    Document the rational for using a spinlock on PREEMPT_RT in the hotplug
    lock code.

    Reported-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 1450f1ff59efca212416e72e1d687e3ad4b13b26
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Jul 16 08:07:43 2012 +0000

    cpu/rt: Rework cpu down for PREEMPT_RT

    Bringing a CPU down is a pain with the PREEMPT_RT kernel because
    tasks can be preempted in many more places than in non-RT. In
    order to handle per_cpu variables, tasks may be pinned to a CPU
    for a while, and even sleep. But these tasks need to be off the CPU
    if that CPU is going down.

    Several synchronization methods have been tried, but when stressed
    they failed. This is a new approach.

    A sync_tsk thread is still created and tasks may still block on a
    lock when the CPU is going down, but how that works is a bit different.
    When cpu_down() starts, it will create the sync_tsk and wait on it
    to inform that current tasks that are pinned on the CPU are no longer
    pinned. But new tasks that are about to be pinned will still be allowed
    to do so at this time.

    Then the notifiers are called. Several notifiers will bring down tasks
    that will enter these locations. Some of these tasks will take locks
    of other tasks that are on the CPU. If we don't let those other tasks
    continue, but make them block until CPU down is done, the tasks that
    the notifiers are waiting on will never complete as they are waiting
    for the locks held by the tasks that are blocked.

    Thus we still let the task pin the CPU until the notifiers are done.
    After the notifiers run, we then make new tasks entering the pinned
    CPU sections grab a mutex and wait. This mutex is now a per CPU mutex
    in the hotplug_pcp descriptor.

    To help things along, a new function in the scheduler code is created
    called migrate_me(). This function will try to migrate the current task
    off the CPU this is going down if possible. When the sync_tsk is created,
    all tasks will then try to migrate off the CPU going down. There are
    several cases that this wont work, but it helps in most cases.

    After the notifiers are called and if a task can't migrate off but enters
    the pin CPU sections, it will be forced to wait on the hotplug_pcp mutex
    until the CPU down is complete. Then the scheduler will force the migration
    anyway.

    Also, I found that THREAD_BOUND need to also be accounted for in the
    pinned CPU, and the migrate_disable no longer treats them special.
    This helps fix issues with ksoftirqd and workqueue that unbind on CPU down.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 0b85a5dd25a2cf944d4eebf7acb6507f7fc29e25
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Mar 2 10:36:57 2012 -0500

    cpu: Make hotplug.lock a "sleeping" spinlock on RT

    Tasks can block on hotplug.lock in pin_current_cpu(), but their state
    might be != RUNNING. So the mutex wakeup will set the state
    unconditionally to RUNNING. That might cause spurious unexpected
    wakeups. We could provide a state preserving mutex_lock() function,
    but this is semantically backwards. So instead we convert the
    hotplug.lock() to a spinlock for RT, which has the state preserving
    semantics already.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Carsten Emde <C.Emde@osadl.org>
    Cc: John Kacur <jkacur@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Clark Williams <clark.williams@gmail.com>
    Cc: stable-rt@vger.kernel.org
    Link: http://lkml.kernel.org/r/1330702617.25686.265.camel@gandalf.stny.rr.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 83e2f2d3ae1888cbddcd3ebc6ebb1f227684feba
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Sun Dec 1 23:03:52 2013 -0500

    seqlock: consolidate spin_lock/unlock waiting with spin_unlock_wait

    since c2f21ce ("locking: Implement new raw_spinlock")
    include/linux/spinlock.h includes spin_unlock_wait() to wait for a concurren
    holder of a lock. this patch just moves over to that API. spin_unlock_wait
    covers both raw_spinlock_t and spinlock_t so it should be safe here as well.
    the added rt-variant of read_seqbegin in include/linux/seqlock.h that is being
    modified, was introduced by patch:
      seqlock-prevent-rt-starvation.patch

    behavior should be unchanged.

    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ae9c60e19f78328e2acf1591261483d71b948d80
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 22 12:03:30 2012 +0100

    seqlock: Prevent rt starvation

    If a low prio writer gets preempted while holding the seqlock write
    locked, a high prio reader spins forever on RT.

    To prevent this let the reader grab the spinlock, so it blocks and
    eventually boosts the writer. This way the writer can proceed and
    endless spinning is prevented.

    For seqcount writers we disable preemption over the update code
    path. Thaanks to Al Viro for distangling some VFS code to make that
    possible.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit db54f9912a963b15dc97e85c48d87adcb397df86
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Aug 21 20:38:50 2012 +0200

    random: Make it work on rt

    Delegate the random insertion to the forced threaded interrupt
    handler. Store the return IP of the hard interrupt handler in the irq
    descriptor and feed it into the random generator as a source of
    entropy.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 031445c682ec3b82304dedb8edfce312966c66f8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 14 01:03:49 2011 +0100

    cpumask: Disable CONFIG_CPUMASK_OFFSTACK for RT

    We can't deal with the cpumask allocations which happen in atomic
    context (see arch/x86/kernel/apic/io_apic.c) on RT right now.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1021c9b61d1f627ee9d203d826c9ce8c79efaca0
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Feb 13 09:26:05 2013 -0500

    acpi/rt: Convert acpi_gbl_hardware lock back to a raw_spinlock_t

    We hit the following bug with 3.6-rt:

    [    5.898990] BUG: scheduling while atomic: swapper/3/0/0x00000002
    [    5.898991] no locks held by swapper/3/0.
    [    5.898993] Modules linked in:
    [    5.898996] Pid: 0, comm: swapper/3 Not tainted 3.6.11-rt28.19.el6rt.x86_64.debug #1
    [    5.898997] Call Trace:
    [    5.899011]  [<ffffffff810804e7>] __schedule_bug+0x67/0x90
    [    5.899028]  [<ffffffff81577923>] __schedule+0x793/0x7a0
    [    5.899032]  [<ffffffff810b4e40>] ? debug_rt_mutex_print_deadlock+0x50/0x200
    [    5.899034]  [<ffffffff81577b89>] schedule+0x29/0x70
    [    5.899036] BUG: scheduling while atomic: swapper/7/0/0x00000002
    [    5.899037] no locks held by swapper/7/0.
    [    5.899039]  [<ffffffff81578525>] rt_spin_lock_slowlock+0xe5/0x2f0
    [    5.899040] Modules linked in:
    [    5.899041]
    [    5.899045]  [<ffffffff81579a58>] ? _raw_spin_unlock_irqrestore+0x38/0x90
    [    5.899046] Pid: 0, comm: swapper/7 Not tainted 3.6.11-rt28.19.el6rt.x86_64.debug #1
    [    5.899047] Call Trace:
    [    5.899049]  [<ffffffff81578bc6>] rt_spin_lock+0x16/0x40
    [    5.899052]  [<ffffffff810804e7>] __schedule_bug+0x67/0x90
    [    5.899054]  [<ffffffff8157d3f0>] ? notifier_call_chain+0x80/0x80
    [    5.899056]  [<ffffffff81577923>] __schedule+0x793/0x7a0
    [    5.899059]  [<ffffffff812f2034>] acpi_os_acquire_lock+0x1f/0x23
    [    5.899062]  [<ffffffff810b4e40>] ? debug_rt_mutex_print_deadlock+0x50/0x200
    [    5.899068]  [<ffffffff8130be64>] acpi_write_bit_register+0x33/0xb0
    [    5.899071]  [<ffffffff81577b89>] schedule+0x29/0x70
    [    5.899072]  [<ffffffff8130be13>] ? acpi_read_bit_register+0x33/0x51
    [    5.899074]  [<ffffffff81578525>] rt_spin_lock_slowlock+0xe5/0x2f0
    [    5.899077]  [<ffffffff8131d1fc>] acpi_idle_enter_bm+0x8a/0x28e
    [    5.899079]  [<ffffffff81579a58>] ? _raw_spin_unlock_irqrestore+0x38/0x90
    [    5.899081]  [<ffffffff8107e5da>] ? this_cpu_load+0x1a/0x30
    [    5.899083]  [<ffffffff81578bc6>] rt_spin_lock+0x16/0x40
    [    5.899087]  [<ffffffff8144c759>] cpuidle_enter+0x19/0x20
    [    5.899088]  [<ffffffff8157d3f0>] ? notifier_call_chain+0x80/0x80
    [    5.899090]  [<ffffffff8144c777>] cpuidle_enter_state+0x17/0x50
    [    5.899092]  [<ffffffff812f2034>] acpi_os_acquire_lock+0x1f/0x23
    [    5.899094]  [<ffffffff8144d1a1>] cpuidle899101]  [<ffffffff8130be13>] ?

    As the acpi code disables interrupts in acpi_idle_enter_bm, and calls
    code that grabs the acpi lock, it causes issues as the lock is currently
    in RT a sleeping lock.

    The lock was converted from a raw to a sleeping lock due to some
    previous issues, and tests that showed it didn't seem to matter.
    Unfortunately, it did matter for one of our boxes.

    This patch converts the lock back to a raw lock. I've run this code on a
    few of my own machines, one being my laptop that uses the acpi quite
    extensively. I've been able to suspend and resume without issues.

    [ tglx: Made the change exclusive for acpi_gbl_hardware_lock ]

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: John Kacur <jkacur@gmail.com>
    Cc: Clark Williams <clark@redhat.com>
    Link: http://lkml.kernel.org/r/1360765565.23152.5.camel@gandalf.local.home
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit e94a9141aff904844a25abea41a294cd268edba5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 14 23:06:09 2011 +0100

    dm: Make rt aware

    Use the BUG_ON_NORT variant for the irq_disabled() checks. RT has
    interrupts legitimately enabled here as we cant deadlock against the
    irq thread due to the "sleeping spinlocks" conversion.

    Reported-by: Luis Claudio R. Goncalves <lclaudio@uudg.org>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ef5608962787fa2bed2f6f1089fe897177065c9c
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Feb 21 17:24:04 2014 +0100

    crypto: Reduce preempt disabled regions, more algos

    Don Estabrook reported
    | kernel: WARNING: CPU: 2 PID: 858 at kernel/sched/core.c:2428 migrate_disable+0xed/0x100()
    | kernel: WARNING: CPU: 2 PID: 858 at kernel/sched/core.c:2462 migrate_enable+0x17b/0x200()
    | kernel: WARNING: CPU: 3 PID: 865 at kernel/sched/core.c:2428 migrate_disable+0xed/0x100()

    and his backtrace showed some crypto functions which looked fine.

    The problem is the following sequence:

    glue_xts_crypt_128bit()
    {
    	blkcipher_walk_virt(); /* normal migrate_disable() */

    	glue_fpu_begin(); /* get atomic */

    	while (nbytes) {
    		__glue_xts_crypt_128bit();
    		blkcipher_walk_done(); /* with nbytes = 0, migrate_enable()
    					* while we are atomic */
    	};
    	glue_fpu_end() /* no longer atomic */
    }

    and this is why the counter get out of sync and the warning is printed.
    The other problem is that we are non-preemptible between
    glue_fpu_begin() and glue_fpu_end() and the latency grows. To fix this,
    I shorten the FPU off region and ensure blkcipher_walk_done() is called
    with preemption enabled. This might hurt the performance because we now
    enable/disable the FPU state more often but we gain lower latency and
    the bug is gone.

    Cc: stable-rt@vger.kernel.org
    Reported-by: Don Estabrook <don.estabrook@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f1354f176e1137fcc979c5a7f120bc8123dea802
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 14 18:19:27 2011 +0100

    x86: crypto: Reduce preempt disabled regions

    Restrict the preempt disabled regions to the actual floating point
    operations and enable preemption for the administrative actions.

    This is necessary on RT to avoid that kfree and other operations are
    called with preemption disabled.

    Reported-and-tested-by: Carsten Emde <cbe@osadl.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b87d1fa9fe0803e679533a6c6e24ab279f1e457d
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Sat Feb 14 11:01:16 2015 -0500

    sas-ata/isci: dont't disable interrupts in qc_issue handler

    On 3.14-rt we see the following trace on Canoe Pass for
    SCSI_ISCI "Intel(R) C600 Series Chipset SAS Controller"
    when the sas qc_issue handler is run:

     BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:905
     in_atomic(): 0, irqs_disabled(): 1, pid: 432, name: udevd
     CPU: 11 PID: 432 Comm: udevd Not tainted 3.14.28-rt22 #2
     Hardware name: Intel Corporation S2600CP/S2600CP, BIOS SE5C600.86B.02.01.0002.082220131453 08/22/2013
     ffff880fab500000 ffff880fa9f239c0 ffffffff81a2d273 0000000000000000
     ffff880fa9f239d8 ffffffff8107f023 ffff880faac23dc0 ffff880fa9f239f0
     ffffffff81a33cc0 ffff880faaeb1400 ffff880fa9f23a40 ffffffff815de891
     Call Trace:
     [<ffffffff81a2d273>] dump_stack+0x4e/0x7a
     [<ffffffff8107f023>] __might_sleep+0xe3/0x160
     [<ffffffff81a33cc0>] rt_spin_lock+0x20/0x50
     [<ffffffff815de891>] isci_task_execute_task+0x171/0x2f0  <-----
     [<ffffffff815cfecb>] sas_ata_qc_issue+0x25b/0x2a0
     [<ffffffff81606363>] ata_qc_issue+0x1f3/0x370
     [<ffffffff8160c600>] ? ata_scsi_invalid_field+0x40/0x40
     [<ffffffff8160c8f5>] ata_scsi_translate+0xa5/0x1b0
     [<ffffffff8160efc6>] ata_sas_queuecmd+0x86/0x280
     [<ffffffff815ce446>] sas_queuecommand+0x196/0x230
     [<ffffffff81081fad>] ? get_parent_ip+0xd/0x50
     [<ffffffff815b05a4>] scsi_dispatch_cmd+0xb4/0x210
     [<ffffffff815b7744>] scsi_request_fn+0x314/0x530

    and gdb shows:

    (gdb) list * isci_task_execute_task+0x171
    0xffffffff815ddfb1 is in isci_task_execute_task (drivers/scsi/isci/task.c:138).
    133             dev_dbg(&ihost->pdev->dev, "%s: num=%d\n", __func__, num);
    134
    135             for_each_sas_task(num, task) {
    136                     enum sci_status status = SCI_FAILURE;
    137
    138                     spin_lock_irqsave(&ihost->scic_lock, flags);    <-----
    139                     idev = isci_lookup_device(task->dev);
    140                     io_ready = isci_device_io_ready(idev, task);
    141                     tag = isci_alloc_tag(ihost);
    142                     spin_unlock_irqrestore(&ihost->scic_lock, flags);
    (gdb)

    In addition to the scic_lock, the function also contains locking of
    the task_state_lock -- which is clearly not a candidate for raw lock
    conversion.  As can be seen by the comment nearby, we really should
    be running the qc_issue code with interrupts enabled anyway.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit abb2583d2a08174782c20b980cf525b7b290544b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Nov 12 14:00:48 2011 +0100

    scsi-fcoe-rt-aware.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 76a1b971c11b014a48368b56276f791564fd0030
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Apr 8 20:33:24 2015 -0300

    KVM: use simple waitqueue for vcpu->wq

    The problem:

    On -RT, an emulated LAPIC timer instances has the following path:

    1) hard interrupt
    2) ksoftirqd is scheduled
    3) ksoftirqd wakes up vcpu thread
    4) vcpu thread is scheduled

    This extra context switch introduces unnecessary latency in the
    LAPIC path for a KVM guest.

    The solution:

    Allow waking up vcpu thread from hardirq context,
    thus avoiding the need for ksoftirqd to be scheduled.

    Normal waitqueues make use of spinlocks, which on -RT
    are sleepable locks. Therefore, waking up a waitqueue
    waiter involves locking a sleeping lock, which
    is not allowed from hard interrupt context.

    cyclictest command line:
    # cyclictest -m -n -q -p99 -l 1000000 -h60  -D 1m

    This patch reduces the average latency in my tests from 14us to 11us.

    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 439283235cc10165f2c42b210d6b029d4506e381
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Apr 8 20:33:25 2015 -0300

    KVM: lapic: mark LAPIC timer handler as irqsafe

    Since lapic timer handler only wakes up a simple waitqueue,
    it can be executed from hardirq context.

    Also handle the case where hrtimer_start_expires fails due to -ETIME,
    by injecting the interrupt to the guest immediately.

    Reduces average cyclictest latency by 3us.

    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b888d7d07478035c69df377f125bbf66fcb1a978
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Nov 6 12:26:18 2011 +0100

    x86-kvm-require-const-tsc-for-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 36e0b4a2296737ea6bf6339164203f9dab41a1ce
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 14 11:57:04 2011 +0200

    ipc/sem: Rework semaphore wakeups

    Current sysv sems have a weird ass wakeup scheme that involves keeping
    preemption disabled over a potential O(n^2) loop and busy waiting on
    that on other CPUs.

    Kill this and simply wake the task directly from under the sem_lock.

    This was discovered by a migrate_disable() debug feature that
    disallows:

      spin_lock();
      preempt_disable();
      spin_unlock()
      preempt_enable();

    Cc: Manfred Spraul <manfred@colorfullife.com>
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Link: http://lkml.kernel.org/r/1315994224.5040.1.camel@twins
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 5814b44e928d3562b7a5ee32d69cb212822a7e83
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 13 11:03:11 2013 +0100

    arm-enable-highmem-for-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3e922386d136c0272ed35acec4a35510025ef47f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Mar 11 21:37:27 2013 +0100

    arm/highmem: flush tlb on unmap

    The tlb should be flushed on unmap and thus make the mapping entry
    invalid. This is only done in the non-debug case which does not look
    right.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 44b5205c07f8b900ce2f552727e6bf62bf5a2b94
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Mar 11 17:09:55 2013 +0100

    x86/highmem: add a "already used pte" check

    This is a copy from kmap_atomic_prot().

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2af1a705b6cbb47456b1f5d901dbc3ffbed4639f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jul 28 10:43:51 2011 +0200

    mm, rt: kmap_atomic scheduling

    In fact, with migrate_disable() existing one could play games with
    kmap_atomic. You could save/restore the kmap_atomic slots on context
    switch (if there are any in use of course), this should be esp easy now
    that we have a kmap_atomic stack.

    Something like the below.. it wants replacing all the preempt_disable()
    stuff with pagefault_disable() && migrate_disable() of course, but then
    you can flip kmaps around like below.

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    [dvhart@linux.intel.com: build fix]
    Link: http://lkml.kernel.org/r/1311842631.5890.208.camel@twins

    [tglx@linutronix.de: Get rid of the per cpu variable and store the idx
    		     and the pte content right away in the task struct.
    		     Shortens the context switch code. ]

commit d887be88cc26133a4ed1f68ee04684123b983a56
Author: Clark Williams <williams@redhat.com>
Date:   Sat Jul 30 21:55:53 2011 -0500

    add /sys/kernel/realtime entry

    Add a /sys/kernel entry to indicate that the kernel is a
    realtime kernel.

    Clark says that he needs this for udev rules, udev needs to evaluate
    if its a PREEMPT_RT kernel a few thousand times and parsing uname
    output is too slow or so.

    Are there better solutions? Should it exist and return 0 on !-rt?

    Signed-off-by: Clark Williams <williams@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 502a1b8fff0448b1bbd2307065c760a6e25592d0
Author: Jason Wessel <jason.wessel@windriver.com>
Date:   Thu Jul 28 12:42:23 2011 -0500

    kgdb/serial: Short term workaround

    On 07/27/2011 04:37 PM, Thomas Gleixner wrote:
    >  - KGDB (not yet disabled) is reportedly unusable on -rt right now due
    >    to missing hacks in the console locking which I dropped on purpose.
    >

    To work around this in the short term you can use this patch, in
    addition to the clocksource watchdog patch that Thomas brewed up.

    Comments are welcome of course.  Ultimately the right solution is to
    change separation between the console and the HW to have a polled mode
    + work queue so as not to introduce any kind of latency.

    Thanks,
    Jason.

commit e30fdafb42ba0fe169f7c3831f789aff101769a0
Author: Carsten Emde <C.Emde@osadl.org>
Date:   Tue Jul 19 13:51:17 2011 +0100

    net: sysrq via icmp

    There are (probably rare) situations when a system crashed and the system
    console becomes unresponsive but the network icmp layer still is alive.
    Wouldn't it be wonderful, if we then could submit a sysreq command via ping?

    This patch provides this facility. Please consult the updated documentation
    Documentation/sysrq.txt for details.

    Signed-off-by: Carsten Emde <C.Emde@osadl.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 06a1bbf192e3fd4a5d143983c1f2f6c6f32d5e54
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Oct 6 10:48:39 2011 -0400

    net: Avoid livelock in net_tx_action() on RT

    qdisc_lock is taken w/o disabling interrupts or bottom halfs. So code
    holding a qdisc_lock() can be interrupted and softirqs can run on the
    return of interrupt in !RT.

    The spin_trylock() in net_tx_action() makes sure, that the softirq
    does not deadlock. When the lock can't be acquired q is requeued and
    the NET_TX softirq is raised. That causes the softirq to run over and
    over.

    That works in mainline as do_softirq() has a retry loop limit and
    leaves the softirq processing in the interrupt return path and
    schedules ksoftirqd. The task which holds qdisc_lock cannot be
    preempted, so the lock is released and either ksoftirqd or the next
    softirq in the return from interrupt path can proceed. Though it's a
    bit strange to actually run MAX_SOFTIRQ_RESTART (10) loops before it
    decides to bail out even if it's clear in the first iteration :)

    On RT all softirq processing is done in a FIFO thread and we don't
    have a loop limit, so ksoftirqd preempts the lock holder forever and
    unqueues and requeues until the reset button is hit.

    Due to the forced threading of ksoftirqd on RT we actually cannot
    deadlock on qdisc_lock because it's a "sleeping lock". So it's safe to
    replace the spin_trylock() with a spin_lock(). When contended,
    ksoftirqd is scheduled out and the lock holder can proceed.

    [ tglx: Massaged changelog and code comments ]

    Solved-by: Thomas Gleixner <tglx@linuxtronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Tested-by: Carsten Emde <cbe@osadl.org>
    Cc: Clark Williams <williams@redhat.com>
    Cc: John Kacur <jkacur@redhat.com>
    Cc: Luis Claudio R. Goncalves <lclaudio@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 0f13e4fd64fdcf997ee3b2533d0d7090b9acfe9b
Author: Yang Shi <yang.shi@windriver.com>
Date:   Tue Feb 17 14:18:24 2015 -0800

    mips: rt: Replace pagefault_* to raw version

    In k{un}map_coherent, pagefault_disable and pagefault_enable are called
    respectively, but k{un}map_coherent needs preempt disabled according to
    commit f8829caee311207afbc882794bdc5aa0db5caf33 ("[MIPS] Fix aliasing bug
    in copy_to_user_page / copy_from_user_page") to avoid dcache alias on COW.

    k{un}map_coherent are just called when cpu_has_dc_aliases == 1 with VIPT cache.
    However, actually, the most modern MIPS processors have PIPT dcache without
    dcache alias issue. In such case, k{un}map_atomic will be called with preempt
    enabled.

    To fix this, we replace pagefault_* to raw version in k{un}map_coherent, which
    disables preempt, otherwise the following kernel panic may be caught:

    CPU 0 Unable to handle kernel paging request at virtual address fffffffffffd5000, epc == ffffffff80122c00, ra == ffffffff8011fbcc
    Oops[#1]:
    CPU: 0 PID: 409 Comm: runltp Not tainted 3.14.17-rt5 #1
    task: 980000000fa936f0 ti: 980000000eed0000 task.ti: 980000000eed0000
    $ 0 : 0000000000000000 000000001400a4e1 fffffffffffd5000 0000000000000001
    $ 4 : 980000000cded000 fffffffffffd5000 980000000cdedf00 ffffffffffff00fe
    $ 8 : 0000000000000000 ffffffffffffff00 000000000000000d 0000000000000004
    $12 : 980000000eed3fe0 000000000000a400 ffffffffa00ae278 0000000000000000
    $16 : 980000000cded000 000000726eb855c8 98000000012ccfe8 ffffffff8095e0c0
    $20 : ffffffff80ad0000 ffffffff8095e0c0 98000000012d0bd8 980000000fb92000
    $24 : 0000000000000000 ffffffff80177fb0
    $28 : 980000000eed0000 980000000eed3b60 980000000fb92060 ffffffff8011fbcc
    Hi : 000000000002cb02
    Lo : 000000000000ee56
    epc : ffffffff80122c00 copy_page+0x38/0x548
        Not tainted
    ra : ffffffff8011fbcc copy_user_highpage+0x16c/0x180
    Status: 1400a4e3 KX SX UX KERNEL EXL IE
    Cause : 10800408
    BadVA : fffffffffffd5000
    PrId : 00010000 (MIPS64R2-generic)
    Modules linked in: i2c_piix4 i2c_core uhci_hcd
    Process runltp (pid: 409, threadinfo=980000000eed0000, task=980000000fa936f0, tls=000000fff7756700)
    Stack : 98000000012ccfe8 980000000eeb7ba8 980000000ecc7508 000000000666da5b
    000000726eb855c8 ffffffff802156e0 000000726ea4a000 98000000010007e0
    980000000fb92060 0000000000000000 0000000000000000 6db6db6db6db6db7
    0000000000000080 000000726eb855c8 980000000fb92000 980000000eeeec28
    980000000ecc7508 980000000fb92060 0000000000000001 00000000000000a9
    ffffffff80995e60 ffffffff80218910 000000001400a4e0 ffffffff804efd24
    980000000ee25b90 ffffffff8079cec4 ffffffff8079d49c ffffffff80979658
    000000000666da5b 980000000eeb7ba8 000000726eb855c8 00000000000000a9
    980000000fb92000 980000000fa936f0 980000000eed3eb0 0000000000000001
    980000000fb92088 0000000000030002 980000000ecc7508 ffffffff8011ecd0
    ...
    Call Trace:
    [<ffffffff80122c00>] copy_page+0x38/0x548
    [<ffffffff8011fbcc>] copy_user_highpage+0x16c/0x180
    [<ffffffff802156e0>] do_wp_page+0x658/0xcd8
    [<ffffffff80218910>] handle_mm_fault+0x7d8/0x1070
    [<ffffffff8011ecd0>] __do_page_fault+0x1a0/0x508
    [<ffffffff80104d84>] resume_userspace_check+0x0/0x10

    Or there may be random segmentation fault happened.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Yang Shi <yang.shi@windriver.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f36a550620174dcab99d5f9f2bf6e7338a6f65d1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 18 17:10:12 2011 +0200

    mips-disable-highmem-on-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 718b7876b9e756ab0dfaa669a548abb9cf1b11a3
Author: Yong Zhang <yong.zhang at windriver.com>
Date:   Thu Jan 29 12:56:18 2015 -0600

    ARM: cmpxchg: define __HAVE_ARCH_CMPXCHG for armv6 and later

    Both pi_stress and sigwaittest in rt-test show performance gain with
    __HAVE_ARCH_CMPXCHG. Testing result on coretile_express_a9x4:

    pi_stress -p 99 --duration=300 (on linux-3.4-rc5; bigger is better)
      vanilla:     Total inversion performed: 5493381
      patched:     Total inversion performed: 5621746

    sigwaittest -p 99 -l 100000 (on linux-3.4-rc5-rt6; less is better)
      3.4-rc5-rt6: Min   24, Cur   27, Avg   30, Max   98
      patched:     Min   19, Cur   21, Avg   23, Max   96

    Signed-off-by: Yong Zhang <yong.zhang0 at gmail.com>
    Cc: Russell King <rmk+kernel at arm.linux.org.uk>
    Cc: Nicolas Pitre <nico at linaro.org>
    Cc: Will Deacon <will.deacon at arm.com>
    Cc: Catalin Marinas <catalin.marinas at arm.com>
    Cc: Thomas Gleixner <tglx at linutronix.de>
    Cc: linux-arm-kernel at lists.infradead.org
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b09ff42869978eee595c8326e362a60f1c29d91b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 18 14:07:21 2015 +0100

    arm/futex: disable preemption during futex_atomic_cmpxchg_inatomic()

    The ARM UP implementation of futex_atomic_cmpxchg_inatomic() assumes that
    pagefault_disable() inherits a preempt disabled section. This assumtion
    is true for mainline but -RT reverts this and allows preemption in
    pagefault disabled regions.
    The code sequence of futex_atomic_cmpxchg_inatomic():

    |   x = *futex;
    |   if (x == oldval)
    |           *futex = newval;

    The problem occurs if the code is preempted after reading the futex value or
    after comparing it with x. While preempted, the futex owner has to be
    scheduled which then releases the lock (in userland because it has no waiter
    yet). Once the code is back on the CPU, it overwrites the futex value
    with with the old PID and the waiter bit set.

    The workaround is to explicit disable code preemption to avoid the
    described race window.

    Debugged-by:  Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ea0c18824e3651751e729b1a228a2151e98b345c
Author: Yadi.hu <yadi.hu@windriver.com>
Date:   Wed Dec 10 10:32:09 2014 +0800

    ARM: enable irq in translation/section permission fault handlers

    Probably happens on all ARM, with
    CONFIG_PREEMPT_RT_FULL
    CONFIG_DEBUG_ATOMIC_SLEEP

    This simple program....

    int main() {
       *((char*)0xc0001000) = 0;
    };

    [ 512.742724] BUG: sleeping function called from invalid context at kernel/rtmutex.c:658
    [ 512.743000] in_atomic(): 0, irqs_disabled(): 128, pid: 994, name: a
    [ 512.743217] INFO: lockdep is turned off.
    [ 512.743360] irq event stamp: 0
    [ 512.743482] hardirqs last enabled at (0): [< (null)>] (null)
    [ 512.743714] hardirqs last disabled at (0): [<c0426370>] copy_process+0x3b0/0x11c0
    [ 512.744013] softirqs last enabled at (0): [<c0426370>] copy_process+0x3b0/0x11c0
    [ 512.744303] softirqs last disabled at (0): [< (null)>] (null)
    [ 512.744631] [<c041872c>] (unwind_backtrace+0x0/0x104)
    [ 512.745001] [<c09af0c4>] (dump_stack+0x20/0x24)
    [ 512.745355] [<c0462490>] (__might_sleep+0x1dc/0x1e0)
    [ 512.745717] [<c09b6770>] (rt_spin_lock+0x34/0x6c)
    [ 512.746073] [<c0441bf0>] (do_force_sig_info+0x34/0xf0)
    [ 512.746457] [<c0442668>] (force_sig_info+0x18/0x1c)
    [ 512.746829] [<c041d880>] (__do_user_fault+0x9c/0xd8)
    [ 512.747185] [<c041d938>] (do_bad_area+0x7c/0x94)
    [ 512.747536] [<c041d990>] (do_sect_fault+0x40/0x48)
    [ 512.747898] [<c040841c>] (do_DataAbort+0x40/0xa0)
    [ 512.748181] Exception stack(0xecaa1fb0 to 0xecaa1ff8)

    Oxc0000000 belongs to kernel address space, user task can not be
    allowed to access it. For above condition, correct result is that
    test case should receive a ‚Äúsegment fault‚Äù and exits but not stacks.

    the root cause is commit 02fe2845d6a8 ("avoid enabling interrupts in
    prefetch/data abort handlers"),it deletes irq enable block in Data
    abort assemble code and move them into page/breakpiont/alignment fault
    handlers instead. But author does not enable irq in translation/section
    permission fault handlers. ARM disables irq when it enters exception/
    interrupt mode, if kernel doesn't enable irq, it would be still disabled
    during translation/section permission fault.

    We see the above splat because do_force_sig_info is still called with
    IRQs off, and that code eventually does a:

            spin_lock_irqsave(&t->sighand->siglock, flags);

    As this is architecture independent code, and we've not seen any other
    need for other arch to have the siglock converted to raw lock, we can
    conclude that we should enable irq for ARM translation/section
    permission exception.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Yadi.hu <yadi.hu@windriver.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 4d49acc457c08b62573a21cbfe7656932d6e2c8f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Sep 20 14:31:54 2013 +0200

    arm/unwind: use a raw_spin_lock

    Mostly unwind is done with irqs enabled however SLUB may call it with
    irqs disabled while creating a new SLUB cache.

    I had system freeze while loading a module which called
    kmem_cache_create() on init. That means SLUB's __slab_alloc() disabled
    interrupts and then

    ->new_slab_objects()
     ->new_slab()
      ->setup_object()
       ->setup_object_debug()
        ->init_tracking()
         ->set_track()
          ->save_stack_trace()
           ->save_stack_trace_tsk()
            ->walk_stackframe()
             ->unwind_frame()
              ->unwind_find_idx()
               =>spin_lock_irqsave(&unwind_lock);

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 878283f9a8f7831af39eb81a56e64074eadf0470
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 1 18:29:35 2010 +0200

    ARM: at91: tclib: Default to tclib timer for RT

    RT is not too happy about the shared timer interrupt in AT91
    devices. Default to tclib timer for RT.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit bfa2ddc7641af6fa203cba4c48e39f55a25d373f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 18 17:09:28 2011 +0200

    arm-disable-highmem-on-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 844670a1fb27482337b6295e485b8a59b75ed602
Author: Bogdan Purcareata <bogdan.purcareata@freescale.com>
Date:   Fri Apr 24 15:53:13 2015 +0000

    powerpc/kvm: Disable in-kernel MPIC emulation for PREEMPT_RT_FULL

    While converting the openpic emulation code to use a raw_spinlock_t enables
    guests to run on RT, there's still a performance issue. For interrupts sent in
    directed delivery mode with a multiple CPU mask, the emulated openpic will loop
    through all of the VCPUs, and for each VCPUs, it call IRQ_check, which will loop
    through all the pending interrupts for that VCPU. This is done while holding the
    raw_lock, meaning that in all this time the interrupts and preemption are
    disabled on the host Linux. A malicious user app can max both these number and
    cause a DoS.

    This temporary fix is sent for two reasons. First is so that users who want to
    use the in-kernel MPIC emulation are aware of the potential latencies, thus
    making sure that the hardware MPIC and their usage scenario does not involve
    interrupts sent in directed delivery mode, and the number of possible pending
    interrupts is kept small. Secondly, this should incentivize the development of a
    proper openpic emulation that would be better suited for RT.

    Acked-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Bogdan Purcareata <bogdan.purcareata@freescale.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 45fb8300317039db97f5ba4227da7a647aa9a588
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 18 17:08:34 2011 +0200

    power-disable-highmem-on-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 87d80586787140c3f0e379de2d6178f52a329646
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jun 18 18:16:39 2015 -0400

    Powerpc: Use generic rwsem on RT

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c5dd65c53132e14265c08467169067f8908797e4
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Mar 21 19:01:05 2013 +0100

    HACK: printk: drop the logbuf_lock more often

    The lock is hold with irgs off. The latency drops 500us+ on my arm bugs
    with a "full" buffer after executing "dmesg" on the shell.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit db5437f76b515e812ddb613eea4c7ca0dce5ccc5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 19 14:50:37 2012 +0200

    printk-rt-aware.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 791cb748d9d54429855391864dbfdc43a202e3fe
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Wed Feb 18 15:09:23 2015 +0100

    snd/pcm: fix snd_pcm_stream_lock*() irqs_disabled() splats

    Locking functions previously using read_lock_irq()/read_lock_irqsave() were
    changed to local_irq_disable/save(), leading to gripes.  Use nort variants.

    |BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:915
    |in_atomic(): 0, irqs_disabled(): 1, pid: 5947, name: alsa-sink-ALC88
    |CPU: 5 PID: 5947 Comm: alsa-sink-ALC88 Not tainted 3.18.7-rt1 #9
    |Hardware name: MEDION MS-7848/MS-7848, BIOS M7848W08.404 11/06/2014
    | ffff880409316240 ffff88040866fa38 ffffffff815bdeb5 0000000000000002
    | 0000000000000000 ffff88040866fa58 ffffffff81073c86 ffffffffa03b2640
    | ffff88040239ec00 ffff88040866fa78 ffffffff815c3d34 ffffffffa03b2640
    |Call Trace:
    | [<ffffffff815bdeb5>] dump_stack+0x4f/0x9e
    | [<ffffffff81073c86>] __might_sleep+0xe6/0x150
    | [<ffffffff815c3d34>] __rt_spin_lock+0x24/0x50
    | [<ffffffff815c4044>] rt_read_lock+0x34/0x40
    | [<ffffffffa03a2979>] snd_pcm_stream_lock+0x29/0x70 [snd_pcm]
    | [<ffffffffa03a355d>] snd_pcm_playback_poll+0x5d/0x120 [snd_pcm]
    | [<ffffffff811937a2>] do_sys_poll+0x322/0x5b0
    | [<ffffffff81193d48>] SyS_ppoll+0x1a8/0x1c0
    | [<ffffffff815c4556>] system_call_fastpath+0x16/0x1b

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b181371ee3b8217c6a5afd0c892c4a712f0aacd2
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Thu Jun 18 18:16:38 2015 -0400

    irq_work: Delegate non-immediate irq work to ksoftirqd

    Based on a patch from Jan Kiszka.

    Jan reported that ftrace queueing work from arbitrary contexts can
    and does lead to deadlock.  trace-cmd -e sched:* deadlocked in fact.

    Resolve the problem by delegating all non-immediate work to ksoftirqd.

    We need two lists to do this, one for hard irq, one for soft, so we
    can use the two existing lists, eliminating the -rt specific list and
    all of the ifdefery while we're at it.

    Strategy: Queue work tagged for hirq invocation to the raised_list,
    invoke via IPI as usual.  If a work item being queued to lazy_list,
    which becomes our all others list, is not a lazy work item, or the
    tick is stopped, fire an IPI to raise SOFTIRQ_TIMER immediately,
    otherwise let ksofirqd find it when the tick comes along.  Raising
    SOFTIRQ_TIMER via IPI even when queueing local ensures delegation.

    Cc: stable-rt@vger.kernel.org
    Acked-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit d00b03af0790973f7e4623590b92544fc760d75e
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Apr 10 11:50:22 2015 +0200

    kernel/irq_work: fix no_hz deadlock

    Invoking NO_HZ's irq_work callback from timer irq is not working very
    well if the callback decides to invoke hrtimer_cancel():

    |hrtimer_try_to_cancel+0x55/0x5f
    |hrtimer_cancel+0x16/0x28
    |tick_nohz_restart+0x17/0x72
    |__tick_nohz_full_check+0x8e/0x93
    |nohz_full_kick_work_func+0xe/0x10
    |irq_work_run_list+0x39/0x57
    |irq_work_tick+0x60/0x67
    |update_process_times+0x57/0x67
    |tick_sched_handle+0x4a/0x59
    |tick_sched_timer+0x3b/0x64
    |__run_hrtimer+0x7a/0x149
    |hrtimer_interrupt+0x1cc/0x2c5

    and here we deadlock while waiting for the lock which we are holding.
    To fix this I'm doing the same thing that upstream is doing: is the
    irq_work dedicated IRQ and use it only for what is marked as "hirq"
    which should only be the FULL_NO_HZ related work.

    Reported-by: Carsten Emde <C.Emde@osadl.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0b9ad001649bd72004a4c3d3a02b1eefddcbd0fb
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Mar 12 18:08:57 2015 -0400

    irq_work: Hide access to hirq_work_list in PREEMPT_RT_FULL

    The hirq_work_list is only defined when PREEMPT_RT_FULL is configured.
    Most access to it is within an #ifdef CONFIG_PREEMPT_RT_FULL, except
    for one. Encapsulate that location too.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit fded9ec3260f30b78a1b81298ef888efbed8aa83
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Jan 31 14:20:31 2014 +0100

    irq_work: allow certain work in hard irq context

    irq_work is processed in softirq context on -RT because we want to avoid
    long latencies which might arise from processing lots of perf events.
    The noHZ-full mode requires its callback to be called from real hardirq
    context (commit 76c24fb ("nohz: New APIs to re-evaluate the tick on full
    dynticks CPUs")). If it is called from a thread context we might get
    wrong results for checks like "is_idle_task(current)".
    This patch introduces a second list (hirq_work_list) which will be used
    if irq_work_run() has been invoked from hardirq context and process only
    work items marked with IRQ_WORK_HARD_IRQ.

    This patch also removes arch_irq_work_raise() from sparc & powerpc like
    it is already done for x86. Atleast for powerpc it is somehow
    superfluous because it is called from the timer interrupt which should
    invoke update_process_times().

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 68ba60027d46db2362afc323613c61674a106002
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 13 14:05:05 2011 +0200

    x86-no-perf-irq-work-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit d62d61139ea95934f2edcfd014f3636d53023bdd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 12 15:38:34 2011 +0200

    use skbufhead with raw lock

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 511d6eeaad031d2dcbfee7825089e3e0a8b5e858
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 13 11:03:16 2011 +0200

    jump-label-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 01ea4ab418f1f6f45f11652873a8f559cfba85c2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 21:41:35 2011 +0200

    debugobjects-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2ba9a0a6d6011fe90a83e7fd21c10914651593d6
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 9 11:58:17 2014 +0200

    percpu_ida: use locklocks

    the local_irq_save() + spin_lock() does not work that well on -RT

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b944b5a62931c4169208493c47d8469a7bb8766c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jun 18 18:16:37 2015 -0400

    idr: Use local lock instead of preempt enable/disable

    We need to protect the per cpu variable and prevent migration.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 88caac96e80920638017246b0d31c69ff2b0ec5d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 22 19:47:03 2011 +0200

    sched: Distangle worker accounting from rqlock

    The worker accounting for cpu bound workers is plugged into the core
    scheduler code and the wakeup code. This is not a hard requirement and
    can be avoided by keeping track of the state in the workqueue code
    itself.

    Keep track of the sleeping state in the worker itself and call the
    notifier before entering the core scheduler. There might be false
    positives when the task is woken between that call and actually
    scheduling, but that's not really different from scheduling and being
    woken immediately after switching away. There is also no harm from
    updating nr_running when the task returns from scheduling instead of
    accounting it in the wakeup code.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110622174919.135236139@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit bea28664bd8acf8365039df72747f7cbcb21c2e7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 1 11:02:42 2013 +0200

    workqueue vs ata-piix livelock fixup

    An Intel i7 system regularly detected rcu_preempt stalls after the kernel
    was upgraded from 3.6-rt to 3.8-rt. When the stall happened, disk I/O was no
    longer possible, unless the system was restarted.

    The kernel message was:
    INFO: rcu_preempt self-detected stall on CPU { 6}
    [..]
    NMI backtrace for cpu 6
    CPU 6
    Pid: 119, comm: irq/19-ata_piix Not tainted 3.8.13-rt13 #11 Shuttle Inc. SX58/SX58
    RIP: 0010:[<ffffffff8124ca60>]  [<ffffffff8124ca60>] ip_compute_csum+0x30/0x30
    RSP: 0018:ffff880333303cb0  EFLAGS: 00000002
    RAX: 0000000000000006 RBX: 00000000000003e9 RCX: 0000000000000034
    RDX: 0000000000000000 RSI: ffffffff81aa16d0 RDI: 0000000000000001
    RBP: ffff880333303ce8 R08: ffffffff81aa16d0 R09: ffffffff81c1b8cc
    R10: 0000000000000000 R11: 0000000000000000 R12: 000000000005161f
    R13: 0000000000000006 R14: ffffffff81aa16d0 R15: 0000000000000002
    FS:  0000000000000000(0000) GS:ffff880333300000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    CR2: 0000003c1b2bb420 CR3: 0000000001a0f000 CR4: 00000000000007e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    Process irq/19-ata_piix (pid: 119, threadinfo ffff88032d88a000, task ffff88032df80000)
    Stack:
    ffffffff8124cb32 000000000005161e 00000000000003e9 0000000000001000
    0000000000009022 ffffffff81aa16d0 0000000000000002 ffff880333303cf8
    ffffffff8124caa9 ffff880333303d08 ffffffff8124cad2 ffff880333303d28
    Call Trace:
    <IRQ>
    [<ffffffff8124cb32>] ? delay_tsc+0x33/0xe3
    [<ffffffff8124caa9>] __delay+0xf/0x11
    [<ffffffff8124cad2>] __const_udelay+0x27/0x29
    [<ffffffff8102d1fa>] native_safe_apic_wait_icr_idle+0x39/0x45
    [<ffffffff8102dc9b>] __default_send_IPI_dest_field.constprop.0+0x1e/0x58
    [<ffffffff8102dd1e>] default_send_IPI_mask_sequence_phys+0x49/0x7d
    [<ffffffff81030326>] physflat_send_IPI_all+0x17/0x19
    [<ffffffff8102de53>] arch_trigger_all_cpu_backtrace+0x50/0x79
    [<ffffffff810b21d0>] rcu_check_callbacks+0x1cb/0x568
    [<ffffffff81048c9c>] ? raise_softirq+0x2e/0x35
    [<ffffffff81086be0>] ? tick_sched_do_timer+0x38/0x38
    [<ffffffff8104f653>] update_process_times+0x44/0x55
    [<ffffffff81086866>] tick_sched_handle+0x4a/0x59
    [<ffffffff81086c1c>] tick_sched_timer+0x3c/0x5b
    [<ffffffff81062845>] __run_hrtimer+0x9b/0x158
    [<ffffffff810631d8>] hrtimer_interrupt+0x172/0x2aa
    [<ffffffff8102d498>] smp_apic_timer_interrupt+0x76/0x89
    [<ffffffff814d881d>] apic_timer_interrupt+0x6d/0x80
    <EOI>
    [<ffffffff81057cd2>] ? __local_lock_irqsave+0x17/0x4a
    [<ffffffff81059336>] try_to_grab_pending+0x42/0x17e
    [<ffffffff8105a699>] mod_delayed_work_on+0x32/0x88
    [<ffffffff8105a70b>] mod_delayed_work+0x1c/0x1e
    [<ffffffff8122ae84>] blk_run_queue_async+0x37/0x39
    [<ffffffff81230985>] flush_end_io+0xf1/0x107
    [<ffffffff8122e0da>] blk_finish_request+0x21e/0x264
    [<ffffffff8122e162>] blk_end_bidi_request+0x42/0x60
    [<ffffffff8122e1ba>] blk_end_request+0x10/0x12
    [<ffffffff8132de46>] scsi_io_completion+0x1bf/0x492
    [<ffffffff81335cec>] ? sd_done+0x298/0x2ef
    [<ffffffff81325a02>] scsi_finish_command+0xe9/0xf2
    [<ffffffff8132dbcb>] scsi_softirq_done+0x106/0x10f
    [<ffffffff812333d3>] blk_done_softirq+0x77/0x87
    [<ffffffff8104826f>] do_current_softirqs+0x172/0x2e1
    [<ffffffff810aa820>] ? irq_thread_fn+0x3a/0x3a
    [<ffffffff81048466>] local_bh_enable+0x43/0x72
    [<ffffffff810aa866>] irq_forced_thread_fn+0x46/0x52
    [<ffffffff810ab089>] irq_thread+0x8c/0x17c
    [<ffffffff810ab179>] ? irq_thread+0x17c/0x17c
    [<ffffffff810aaffd>] ? wake_threads_waitq+0x44/0x44
    [<ffffffff8105eb18>] kthread+0x8d/0x95
    [<ffffffff8105ea8b>] ? __kthread_parkme+0x65/0x65
    [<ffffffff814d7b7c>] ret_from_fork+0x7c/0xb0
    [<ffffffff8105ea8b>] ? __kthread_parkme+0x65/0x65

    The state of softirqd of this CPU at the time of the crash was:
    ksoftirqd/6     R  running task        0    53      2 0x00000000
    ffff88032fc39d18 0000000000000046 ffff88033330c4c0 ffff8803303f4710
    ffff88032fc39fd8 ffff88032fc39fd8 0000000000000000 0000000000062500
    ffff88032df88000 ffff8803303f4710 0000000000000000 ffff88032fc38000
    Call Trace:
    [<ffffffff8105a3ae>] ? __queue_work+0x27c/0x27c
    [<ffffffff814d178c>] preempt_schedule+0x61/0x76
    [<ffffffff8106cccf>] migrate_enable+0xe5/0x1df
    [<ffffffff8105a3ae>] ? __queue_work+0x27c/0x27c
    [<ffffffff8104ef52>] run_timer_softirq+0x161/0x1d6
    [<ffffffff8104826f>] do_current_softirqs+0x172/0x2e1
    [<ffffffff8104840b>] run_ksoftirqd+0x2d/0x45
    [<ffffffff8106658a>] smpboot_thread_fn+0x2ea/0x308
    [<ffffffff810662a0>] ? test_ti_thread_flag+0xc/0xc
    [<ffffffff810662a0>] ? test_ti_thread_flag+0xc/0xc
    [<ffffffff8105eb18>] kthread+0x8d/0x95
    [<ffffffff8105ea8b>] ? __kthread_parkme+0x65/0x65
    [<ffffffff814d7afc>] ret_from_fork+0x7c/0xb0
    [<ffffffff8105ea8b>] ? __kthread_parkme+0x65/0x65

    Apparently, the softirq demon and the ata_piix IRQ handler were waiting
    for each other to finish ending up in a livelock. After the below patch
    was applied, the system no longer crashes.

    Reported-by: Carsten Emde <C.Emde@osadl.org>
    Proposed-by: Thomas Gleixner <tglx@linutronix.de>
    Tested by: Carsten Emde <C.Emde@osadl.org>
    Signed-off-by: Carsten Emde <C.Emde@osadl.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3574d876e9f928e2c3bfd3c636e052a086cd7944
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 21:42:26 2011 +0200

    Use local irq lock instead of irq disable regions

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 77a7302617b21e0727cca4834dc70c2a91d71b4d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 24 15:26:54 2013 +0200

    workqueue: Use normal rcu

    There is no need for sched_rcu. The undocumented reason why sched_rcu
    is used is to avoid a few explicit rcu_read_lock()/unlock() pairs by
    abusing the fact that sched_rcu reader side critical sections are also
    protected by preempt or irq disabled regions.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0a88843d2e5064a3b068e19cee65671f4e9a0cac
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 7 21:10:04 2012 +0100

    net: Use cpu_chill() instead of cpu_relax()

    Retry loops on RT might loop forever when the modifying side was
    preempted. Use cpu_chill() instead of cpu_relax() to let the system
    make progress.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit beca6bc69b3a4e44dda49abae952c6c30e011a39
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 7 21:00:34 2012 +0100

    fs: dcache: Use cpu_chill() in trylock loops

    Retry loops on RT might loop forever when the modifying side was
    preempted. Use cpu_chill() instead of cpu_relax() to let the system
    make progress.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b2b10725dfd7f7291098463a8d6cc94926680ce2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 20 18:28:26 2012 +0100

    block: Use cpu_chill() for retry loops

    Retry loops on RT might loop forever when the modifying side was
    preempted. Steven also observed a live lock when there was a
    concurrent priority boosting going on.

    Use cpu_chill() instead of cpu_relax() to let the system
    make progress.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 04efb715543c4f464d654a591607170c53fb889c
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 18 18:37:26 2015 +0100

    block/mq: drop per ctx cpu_lock

    While converting the get_cpu() to get_cpu_light() I added a cpu lock to
    ensure the same code is not invoked twice on the same CPU. And now I run
    into this:

    | kernel BUG at kernel/locking/rtmutex.c:996!
    | invalid opcode: 0000 [#1] PREEMPT SMP
    | CPU0: 13 PID: 75 Comm: kworker/u258:0 Tainted: G          I    3.18.7-rt1.5+ #12
    | Workqueue: writeback bdi_writeback_workfn (flush-8:0)
    | task: ffff88023742a620 ti: ffff88023743c000 task.ti: ffff88023743c000
    | RIP: 0010:[<ffffffff81523cc0>]  [<ffffffff81523cc0>] rt_spin_lock_slowlock+0x280/0x2d0
    | Call Trace:
    |  [<ffffffff815254e7>] rt_spin_lock+0x27/0x60
    taking the same lock again
    |
    |  [<ffffffff8127c771>] blk_mq_insert_requests+0x51/0x130
    |  [<ffffffff8127d4a9>] blk_mq_flush_plug_list+0x129/0x140
    |  [<ffffffff81272461>] blk_flush_plug_list+0xd1/0x250
    |  [<ffffffff81522075>] schedule+0x75/0xa0
    |  [<ffffffff8152474d>] do_nanosleep+0xdd/0x180
    |  [<ffffffff810c8312>] __hrtimer_nanosleep+0xd2/0x1c0
    |  [<ffffffff810c8456>] cpu_chill+0x56/0x80
    |  [<ffffffff8107c13d>] try_to_grab_pending+0x1bd/0x390
    |  [<ffffffff8107c431>] cancel_delayed_work+0x21/0x170
    |  [<ffffffff81279a98>] blk_mq_stop_hw_queue+0x18/0x40
    |  [<ffffffffa000ac6f>] scsi_queue_rq+0x7f/0x830 [scsi_mod]
    |  [<ffffffff8127b0de>] __blk_mq_run_hw_queue+0x1ee/0x360
    |  [<ffffffff8127b528>] blk_mq_map_request+0x108/0x190
    take the lock  ^^^
    |
    |  [<ffffffff8127c8d2>] blk_sq_make_request+0x82/0x350
    |  [<ffffffff8126f6c0>] generic_make_request+0xd0/0x120
    |  [<ffffffff8126f788>] submit_bio+0x78/0x190
    |  [<ffffffff811bd537>] _submit_bh+0x117/0x180
    |  [<ffffffff811bf528>] __block_write_full_page.constprop.38+0x138/0x3f0
    |  [<ffffffff811bf880>] block_write_full_page+0xa0/0xe0
    |  [<ffffffff811c02b3>] blkdev_writepage+0x13/0x20
    |  [<ffffffff81127b25>] __writepage+0x15/0x40
    |  [<ffffffff8112873b>] write_cache_pages+0x1fb/0x440
    |  [<ffffffff811289be>] generic_writepages+0x3e/0x60
    |  [<ffffffff8112a17c>] do_writepages+0x1c/0x30
    |  [<ffffffff811b3603>] __writeback_single_inode+0x33/0x140
    |  [<ffffffff811b462d>] writeback_sb_inodes+0x2bd/0x490
    |  [<ffffffff811b4897>] __writeback_inodes_wb+0x97/0xd0
    |  [<ffffffff811b4a9b>] wb_writeback+0x1cb/0x210
    |  [<ffffffff811b505b>] bdi_writeback_workfn+0x25b/0x380
    |  [<ffffffff8107b50b>] process_one_work+0x1bb/0x490
    |  [<ffffffff8107c7ab>] worker_thread+0x6b/0x4f0
    |  [<ffffffff81081863>] kthread+0xe3/0x100
    |  [<ffffffff8152627c>] ret_from_fork+0x7c/0xb0

    After looking at this for a while it seems that it is save if blk_mq_ctx is
    used multiple times, the in struct lock protects the access.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9be74bb430b39b182a41eff47a7af8cd92e0524d
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Feb 13 11:01:26 2015 +0100

    block: blk-mq: use swait

    | BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:914
    | in_atomic(): 1, irqs_disabled(): 0, pid: 255, name: kworker/u257:6
    | 5 locks held by kworker/u257:6/255:
    |  #0:  ("events_unbound"){.+.+.+}, at: [<ffffffff8108edf1>] process_one_work+0x171/0x5e0
    |  #1:  ((&entry->work)){+.+.+.}, at: [<ffffffff8108edf1>] process_one_work+0x171/0x5e0
    |  #2:  (&shost->scan_mutex){+.+.+.}, at: [<ffffffffa000faa3>] __scsi_add_device+0xa3/0x130 [scsi_mod]
    |  #3:  (&set->tag_list_lock){+.+...}, at: [<ffffffff812f09fa>] blk_mq_init_queue+0x96a/0xa50
    |  #4:  (rcu_read_lock_sched){......}, at: [<ffffffff8132887d>] percpu_ref_kill_and_confirm+0x1d/0x120
    | Preemption disabled at:[<ffffffff812eff76>] blk_mq_freeze_queue_start+0x56/0x70
    |
    | CPU: 2 PID: 255 Comm: kworker/u257:6 Not tainted 3.18.7-rt0+ #1
    | Workqueue: events_unbound async_run_entry_fn
    |  0000000000000003 ffff8800bc29f998 ffffffff815b3a12 0000000000000000
    |  0000000000000000 ffff8800bc29f9b8 ffffffff8109aa16 ffff8800bc29fa28
    |  ffff8800bc5d1bc8 ffff8800bc29f9e8 ffffffff815b8dd4 ffff880000000000
    | Call Trace:
    |  [<ffffffff815b3a12>] dump_stack+0x4f/0x7c
    |  [<ffffffff8109aa16>] __might_sleep+0x116/0x190
    |  [<ffffffff815b8dd4>] rt_spin_lock+0x24/0x60
    |  [<ffffffff810b6089>] __wake_up+0x29/0x60
    |  [<ffffffff812ee06e>] blk_mq_usage_counter_release+0x1e/0x20
    |  [<ffffffff81328966>] percpu_ref_kill_and_confirm+0x106/0x120
    |  [<ffffffff812eff76>] blk_mq_freeze_queue_start+0x56/0x70
    |  [<ffffffff812f0000>] blk_mq_update_tag_set_depth+0x40/0xd0
    |  [<ffffffff812f0a1c>] blk_mq_init_queue+0x98c/0xa50
    |  [<ffffffffa000dcf0>] scsi_mq_alloc_queue+0x20/0x60 [scsi_mod]
    |  [<ffffffffa000ea35>] scsi_alloc_sdev+0x2f5/0x370 [scsi_mod]
    |  [<ffffffffa000f494>] scsi_probe_and_add_lun+0x9e4/0xdd0 [scsi_mod]
    |  [<ffffffffa000fb26>] __scsi_add_device+0x126/0x130 [scsi_mod]
    |  [<ffffffffa013033f>] ata_scsi_scan_host+0xaf/0x200 [libata]
    |  [<ffffffffa012b5b6>] async_port_probe+0x46/0x60 [libata]
    |  [<ffffffff810978fb>] async_run_entry_fn+0x3b/0xf0
    |  [<ffffffff8108ee81>] process_one_work+0x201/0x5e0

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e950dcae5993327d741a19fa63072246db985a7a
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sat May 3 11:00:29 2014 +0200

    blk-mq: revert raw locks, post pone notifier to POST_DEAD

    The blk_mq_cpu_notify_lock should be raw because some CPU down levels
    are called with interrupts off. The notifier itself calls currently one
    function that is blk_mq_hctx_notify().
    That function acquires the ctx->lock lock which is sleeping and I would
    prefer to keep it that way. That function only moves IO-requests from
    the CPU that is going offline to another CPU and it is currently the
    only one. Therefore I revert the list lock back to sleeping spinlocks
    and let the notifier run at POST_DEAD time.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e88b1b88379cc5390008de63e9eec9788b7c5aa3
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Mar 4 12:28:32 2014 -0500

    cpu_chill: Add a UNINTERRUPTIBLE hrtimer_nanosleep

    We hit another bug that was caused by switching cpu_chill() from
    msleep() to hrtimer_nanosleep().

    This time it is a livelock. The problem is that hrtimer_nanosleep()
    calls schedule with the state == TASK_INTERRUPTIBLE. But these means
    that if a signal is pending, the scheduler wont schedule, and will
    simply change the current task state back to TASK_RUNNING. This
    nullifies the whole point of cpu_chill() in the first place. That is,
    if a task is spinning on a try_lock() and it preempted the owner of the
    lock, if it has a signal pending, it will never give up the CPU to let
    the owner of the lock run.

    I made a static function __hrtimer_nanosleep() that takes a fifth
    parameter "state", which determines the task state of that the
    nanosleep() will be in. The normal hrtimer_nanosleep() will act the
    same, but cpu_chill() will call the __hrtimer_nanosleep() directly with
    the TASK_UNINTERRUPTIBLE state.

    cpu_chill() only cares that the first sleep happens, and does not care
    about the state of the restart schedule (in hrtimer_nanosleep_restart).

    Cc: stable-rt@vger.kernel.org
    Reported-by: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit aaf93301f217ae5db26c65ba87c2f534aa657ef3
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 19 11:56:06 2014 +0100

    kernel/hrtimer: be non-freezeable in cpu_chill()

    Since we replaced msleep() by hrtimer I see now and then (rarely) this:

    | [....] Waiting for /dev to be fully populated...
    | =====================================
    | [ BUG: udevd/229 still has locks held! ]
    | 3.12.11-rt17 #23 Not tainted
    | -------------------------------------
    | 1 lock held by udevd/229:
    |  #0:  (&type->i_mutex_dir_key#2){+.+.+.}, at: lookup_slow+0x28/0x98
    |
    | stack backtrace:
    | CPU: 0 PID: 229 Comm: udevd Not tainted 3.12.11-rt17 #23
    | (unwind_backtrace+0x0/0xf8) from (show_stack+0x10/0x14)
    | (show_stack+0x10/0x14) from (dump_stack+0x74/0xbc)
    | (dump_stack+0x74/0xbc) from (do_nanosleep+0x120/0x160)
    | (do_nanosleep+0x120/0x160) from (hrtimer_nanosleep+0x90/0x110)
    | (hrtimer_nanosleep+0x90/0x110) from (cpu_chill+0x30/0x38)
    | (cpu_chill+0x30/0x38) from (dentry_kill+0x158/0x1ec)
    | (dentry_kill+0x158/0x1ec) from (dput+0x74/0x15c)
    | (dput+0x74/0x15c) from (lookup_real+0x4c/0x50)
    | (lookup_real+0x4c/0x50) from (__lookup_hash+0x34/0x44)
    | (__lookup_hash+0x34/0x44) from (lookup_slow+0x38/0x98)
    | (lookup_slow+0x38/0x98) from (path_lookupat+0x208/0x7fc)
    | (path_lookupat+0x208/0x7fc) from (filename_lookup+0x20/0x60)
    | (filename_lookup+0x20/0x60) from (user_path_at_empty+0x50/0x7c)
    | (user_path_at_empty+0x50/0x7c) from (user_path_at+0x14/0x1c)
    | (user_path_at+0x14/0x1c) from (vfs_fstatat+0x48/0x94)
    | (vfs_fstatat+0x48/0x94) from (SyS_stat64+0x14/0x30)
    | (SyS_stat64+0x14/0x30) from (ret_fast_syscall+0x0/0x48)

    For now I see no better way but to disable the freezer the sleep the period.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit baf8afa0b784fee957a5e30d89f2c9d14f66e8bb
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Feb 5 11:51:25 2014 -0500

    rt: Make cpu_chill() use hrtimer instead of msleep()

    Ulrich Obergfell pointed out that cpu_chill() calls msleep() which is woken
    up by the ksoftirqd running the TIMER softirq. But as the cpu_chill() is
    called from softirq context, it may block the ksoftirqd() from running, in
    which case, it may never wake up the msleep() causing the deadlock.

    I checked the vmcore, and irq/74-qla2xxx is stuck in the msleep() call,
    running on CPU 8. The one ksoftirqd that is stuck, happens to be the one that
    runs on CPU 8, and it is blocked on a lock held by irq/74-qla2xxx. As that
    ksoftirqd is the one that will wake up irq/74-qla2xxx, and it happens to be
    blocked on a lock that irq/74-qla2xxx holds, we have our deadlock.

    The solution is not to convert the cpu_chill() back to a cpu_relax() as that
    will re-create a possible live lock that the cpu_chill() fixed earlier, and may
    also leave this bug open on other softirqs. The fix is to remove the
    dependency on ksoftirqd from cpu_chill(). That is, instead of calling
    msleep() that requires ksoftirqd to wake it up, use the
    hrtimer_nanosleep() code that does the wakeup from hard irq context.

    |Looks to be the lock of the block softirq. I don't have the core dump
    |anymore, but from what I could tell the ksoftirqd was blocked on the
    |block softirq lock, where the block softirq handler did a msleep
    |(called by the qla2xxx interrupt handler).
    |
    |Looking at trigger_softirq() in block/blk-softirq.c, it can do a
    |smp_callfunction() to another cpu to run the block softirq. If that
    |happens to be the cpu where the qla2xx irq handler is doing the block
    |softirq and is in a middle of a msleep(), I believe the ksoftirqd will
    |try to run the softirq. If it does that, then BOOM, it's deadlocked
    |because the ksoftirqd will never run the timer softirq either.

    |I should have also stated that it was only one lock that was involved.
    |But the lock owner was doing a msleep() that requires a wakeup by
    |ksoftirqd to continue. If ksoftirqd happens to be blocked on a lock
    |held by the msleep() caller, then you have your deadlock.
    |
    |It's best not to have any softirqs going to sleep requiring another
    |softirq to wake it up. Note, if we ever require a timer softirq to do a
    |cpu_chill() it will most definitely hit this deadlock.

    Cc: stable-rt@vger.kernel.org
    Found-by: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    [bigeasy: add the 4 | chapters from email]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit f758e66bd3d9c365b545b6eda1365099570b770c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 7 20:51:03 2012 +0100

    rt: Introduce cpu_chill()

    Retry loops on RT might loop forever when the modifying side was
    preempted. Add cpu_chill() to replace cpu_relax(). cpu_chill()
    defaults to cpu_relax() for non RT. On RT it puts the looping task to
    sleep for a tick so the preempted task can make progress.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0919713734439526cd22b47d5df46dee0ebeb235
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jan 29 15:10:08 2015 +0100

    block/mq: don't complete requests via IPI

    The IPI runs in hardirq context and there are sleeping locks. This patch
    moves the completion into a workqueue.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2227d67f765e493d98d9d299e06794f6ccde2175
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jun 18 18:16:34 2015 -0400

    block/mq: do not invoke preempt_disable()

    preempt_disable() and get_cpu() don't play well together with the sleeping
    locks it tries to allocate later.
    It seems to be enough to replace it with get_cpu_light() and migrate_disable().

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit bd3476d18d21af9c3f5ccaaa86bfe73e67a3a099
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 9 10:37:23 2014 +0200

    block: mq: use cpu_light()

    there is a might sleep splat because get_cpu() disables preemption and
    later we grab a lock. As a workaround for this we use get_cpu_light()
    and an additional lock to prevent taking the same ctx.

    There is a lock member in the ctx already but there some functions which do ++
    on the member and this works with irq off but on RT we would need the extra lock.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 8c89d898c98d40cbf64fb777105992c7ae8d784c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 12 11:39:36 2011 +0200

    mm-vmalloc.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 6225415cb9e2e66e1b55702845a5e8a7889eb1e2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 8 16:35:35 2011 +0200

    epoll.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 87df656d74fe10edb1bd4d8b0bd8ee32891728e9
Author: Daniel Wagner <wagi@monom.org>
Date:   Tue Feb 17 09:37:44 2015 +0100

    thermal: Defer thermal wakups to threads

    On RT the spin lock in pkg_temp_thermal_platfrom_thermal_notify will
    call schedule while we run in irq context.

    [<ffffffff816850ac>] dump_stack+0x4e/0x8f
    [<ffffffff81680f7d>] __schedule_bug+0xa6/0xb4
    [<ffffffff816896b4>] __schedule+0x5b4/0x700
    [<ffffffff8168982a>] schedule+0x2a/0x90
    [<ffffffff8168a8b5>] rt_spin_lock_slowlock+0xe5/0x2d0
    [<ffffffff8168afd5>] rt_spin_lock+0x25/0x30
    [<ffffffffa03a7b75>] pkg_temp_thermal_platform_thermal_notify+0x45/0x134 [x86_pkg_temp_thermal]
    [<ffffffff8103d4db>] ? therm_throt_process+0x1b/0x160
    [<ffffffff8103d831>] intel_thermal_interrupt+0x211/0x250
    [<ffffffff8103d8c1>] smp_thermal_interrupt+0x21/0x40
    [<ffffffff8169415d>] thermal_interrupt+0x6d/0x80

    Let's defer the work to a kthread.

    Signed-off-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    [bigeasy: reoder init/denit position. TODO: flush swork on exit]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 9142caeab0d55b99513ed8f5346a6a58b19d9ee0
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Sun Nov 2 08:31:37 2014 +0100

    x86: UV: raw_spinlock conversion

    Shrug.  Lots of hobbyists have a beast in their basement, right?

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Mike Galbraith <mgalbraith@suse.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a0e60e2354cae935c8fbcdc8dedbef0e9a812da8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 26 02:21:32 2009 +0200

    x86: Use generic rwsem_spinlocks on -rt

    Simplifies the separation of anon_rw_semaphores and rw_semaphores for
    -rt.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2d1febbf2ce2a89f82a28654f159509ca2aa814b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 16 14:25:18 2010 +0100

    x86: stackprotector: Avoid random pool on rt

    CPU bringup calls into the random pool to initialize the stack
    canary. During boot that works nicely even on RT as the might sleep
    checks are disabled. During CPU hotplug the might sleep checks
    trigger. Making the locks in random raw is a major PITA, so avoid the
    call on RT is the only sensible solution. This is basically the same
    randomness which we get during boot where the random pool has no
    entropy and we rely on the TSC randomnness.

    Reported-by: Carsten Emde <carsten.emde@osadl.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2eb64c6d926d8c360581c3f066e3335a8c30ac8a
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Feb 27 15:20:37 2015 +0100

    x86/mce: use swait queue for mce wakeups

    We had a customer report a lockup on a 3.0-rt kernel that had the
    following backtrace:

    [ffff88107fca3e80] rt_spin_lock_slowlock at ffffffff81499113
    [ffff88107fca3f40] rt_spin_lock at ffffffff81499a56
    [ffff88107fca3f50] __wake_up at ffffffff81043379
    [ffff88107fca3f80] mce_notify_irq at ffffffff81017328
    [ffff88107fca3f90] intel_threshold_interrupt at ffffffff81019508
    [ffff88107fca3fa0] smp_threshold_interrupt at ffffffff81019fc1
    [ffff88107fca3fb0] threshold_interrupt at ffffffff814a1853

    It actually bugged because the lock was taken by the same owner that
    already had that lock. What happened was the thread that was setting
    itself on a wait queue had the lock when an MCE triggered. The MCE
    interrupt does a wake up on its wait list and grabs the same lock.

    NOTE: THIS IS NOT A BUG ON MAINLINE

    Sorry for yelling, but as I Cc'd mainline maintainers I want them to
    know that this is an PREEMPT_RT bug only. I only Cc'd them for advice.

    On PREEMPT_RT the wait queue locks are converted from normal
    "spin_locks" into an rt_mutex (see the rt_spin_lock_slowlock above).
    These are not to be taken by hard interrupt context. This usually isn't
    a problem as most all interrupts in PREEMPT_RT are converted into
    schedulable threads. Unfortunately that's not the case with the MCE irq.

    As wait queue locks are notorious for long hold times, we can not
    convert them to raw_spin_locks without causing issues with -rt. But
    Thomas has created a "simple-wait" structure that uses raw spin locks
    which may have been a good fit.

    Unfortunately, wait queues are not the only issue, as the mce_notify_irq
    also does a schedule_work(), which grabs the workqueue spin locks that
    have the exact same issue.

    Thus, this patch I'm proposing is to move the actual work of the MCE
    interrupt into a helper thread that gets woken up on the MCE interrupt
    and does the work in a schedulable context.

    NOTE: THIS PATCH ONLY CHANGES THE BEHAVIOR WHEN PREEMPT_RT IS SET

    Oops, sorry for yelling again, but I want to stress that I keep the same
    behavior of mainline when PREEMPT_RT is not set. Thus, this only changes
    the MCE behavior when PREEMPT_RT is configured.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    [bigeasy@linutronix: make mce_notify_work() a proper prototype, use
    		     kthread_run()]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    [wagi: use work-simple framework to defer work to a kthread]
    Signed-off-by: Daniel Wagner <daniel.wagner@bmw-carit.de>

commit 7b72df262b92d023f46ba77afe94ae1759ae5be8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 13 16:33:39 2010 +0100

    x86: Convert mce timer to hrtimer

    mce_timer is started in atomic contexts of cpu bringup. This results
    in might_sleep() warnings on RT. Convert mce_timer to a hrtimer to
    avoid this.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    fold in:
    |From: Mike Galbraith <bitbucket@online.de>
    |Date: Wed, 29 May 2013 13:52:13 +0200
    |Subject: [PATCH] x86/mce: fix mce timer interval
    |
    |Seems mce timer fire at the wrong frequency in -rt kernels since roughly
    |forever due to 32 bit overflow.  3.8-rt is also missing a multiplier.
    |
    |Add missing us -> ns conversion and 32 bit overflow prevention.
    |
    |Signed-off-by: Mike Galbraith <bitbucket@online.de>
    |[bigeasy: use ULL instead of u64 cast]
    |Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 85a95534bbd85f94b841ff542c63c7cb8e64f865
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed May 13 11:36:32 2015 -0400

    xfs: Disable percpu SB on PREEMPT_RT_FULL

    Running a test on a large CPU count box with xfs, I hit a live lock
    with the following backtraces on several CPUs:

     Call Trace:
      [<ffffffff812c34f8>] __const_udelay+0x28/0x30
      [<ffffffffa033ab9a>] xfs_icsb_lock_cntr+0x2a/0x40 [xfs]
      [<ffffffffa033c871>] xfs_icsb_modify_counters+0x71/0x280 [xfs]
      [<ffffffffa03413e1>] xfs_trans_reserve+0x171/0x210 [xfs]
      [<ffffffffa0378cfd>] xfs_create+0x24d/0x6f0 [xfs]
      [<ffffffff8124c8eb>] ? avc_has_perm_flags+0xfb/0x1e0
      [<ffffffffa0336eeb>] xfs_vn_mknod+0xbb/0x1e0 [xfs]
      [<ffffffffa0337043>] xfs_vn_create+0x13/0x20 [xfs]
      [<ffffffff811b0edd>] vfs_create+0xcd/0x130
      [<ffffffff811b21ef>] do_last+0xb8f/0x1240
      [<ffffffff811b39b2>] path_openat+0xc2/0x490

    Looking at the code I see it was stuck at:

    STATIC void
    xfs_icsb_lock_cntr(
    	xfs_icsb_cnts_t	*icsbp)
    {
    	while (test_and_set_bit(XFS_ICSB_FLAG_LOCK, &icsbp->icsb_flags)) {
    		ndelay(1000);
    	}
    }

    In xfs_icsb_modify_counters() the code is fine. There's a
    preempt_disable() called when taking this bit spinlock and a
    preempt_enable() after it is released. The issue is that not all
    locations are protected by preempt_disable() when PREEMPT_RT is set.
    Namely the places that grab all CPU cntr locks.

    STATIC void
    xfs_icsb_lock_all_counters(
    	xfs_mount_t	*mp)
    {
    	xfs_icsb_cnts_t *cntp;
    	int		i;

    	for_each_online_cpu(i) {
    		cntp = (xfs_icsb_cnts_t *)per_cpu_ptr(mp->m_sb_cnts, i);
    		xfs_icsb_lock_cntr(cntp);
    	}
    }

    STATIC void
    xfs_icsb_disable_counter()
    {
    	[...]
    	xfs_icsb_lock_all_counters(mp);
    	[...]
    	xfs_icsb_unlock_all_counters(mp);
    }

    STATIC void
    xfs_icsb_balance_counter_locked()
    {
    	[...]
    	xfs_icsb_disable_counter();
    	[...]
    }

    STATIC void
    xfs_icsb_balance_counter(
    	xfs_mount_t	*mp,
    	xfs_sb_field_t  fields,
    	int		min_per_cpu)
    {
    	spin_lock(&mp->m_sb_lock);
    	xfs_icsb_balance_counter_locked(mp, fields, min_per_cpu);
    	spin_unlock(&mp->m_sb_lock);
    }

    Now, when PREEMPT_RT is not enabled, that spin_lock() disables
    preemption. But for PREEMPT_RT, it does not. Although with my test box I
    was not able to produce a task state of all tasks, but I'm assuming that
    some task called the xfs_icsb_lock_all_counters() and was preempted by
    an RT task and could not finish, causing all callers of that lock to
    block indefinitely.

    Dave Chinner has stated that the scalability of that code will probably
    be negated by PREEMPT_RT, and that it is probably best to just disable
    the code in question. Also, this code has been rewritten in newer kernels.

    Link: http://lkml.kernel.org/r/20150504004844.GA21261@dastard

    Suggested-by: Dave Chinner <david@fromorbit.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit be136578aa508c32a07b42b943b595cb00f6ede2
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Feb 16 18:49:10 2015 +0100

    fs/aio: simple simple work

    |BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:768
    |in_atomic(): 1, irqs_disabled(): 0, pid: 26, name: rcuos/2
    |2 locks held by rcuos/2/26:
    | #0:  (rcu_callback){.+.+..}, at: [<ffffffff810b1a12>] rcu_nocb_kthread+0x1e2/0x380
    | #1:  (rcu_read_lock_sched){.+.+..}, at: [<ffffffff812acd26>] percpu_ref_kill_rcu+0xa6/0x1c0
    |Preemption disabled at:[<ffffffff810b1a93>] rcu_nocb_kthread+0x263/0x380
    |Call Trace:
    | [<ffffffff81582e9e>] dump_stack+0x4e/0x9c
    | [<ffffffff81077aeb>] __might_sleep+0xfb/0x170
    | [<ffffffff81589304>] rt_spin_lock+0x24/0x70
    | [<ffffffff811c5790>] free_ioctx_users+0x30/0x130
    | [<ffffffff812ace34>] percpu_ref_kill_rcu+0x1b4/0x1c0
    | [<ffffffff810b1a93>] rcu_nocb_kthread+0x263/0x380
    | [<ffffffff8106e046>] kthread+0xd6/0xf0
    | [<ffffffff81591eec>] ret_from_fork+0x7c/0xb0

    replace this preempt_disable() friendly swork.

    Reported-By: Mike Galbraith <umgwanakikbuti@gmail.com>
    Suggested-by: Benjamin LaHaise <bcrl@kvack.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit dbfbb0fd50e5a2459643a0543b1e5907d72c52b8
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Feb 17 17:30:03 2014 +0100

    fs: jbd2: pull your plug when waiting for space

    Two cps in parallel managed to stall the the ext4 fs. It seems that
    journal code is either waiting for locks or sleeping waiting for
    something to happen. This seems similar to what Mike observed on ext3,
    here is his description:

    |With an -rt kernel, and a heavy sync IO load, tasks can jam
    |up on journal locks without unplugging, which can lead to
    |terminal IO starvation.  Unplug and schedule when waiting
    |for space.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3b5cf23e6b87a938522eb074baeb034e66dc9cb3
Author: Mike Galbraith <mgalbraith@suse.de>
Date:   Wed Jul 11 22:05:20 2012 +0000

    fs, jbd: pull your plug when waiting for space

    With an -rt kernel, and a heavy sync IO load, tasks can jam
    up on journal locks without unplugging, which can lead to
    terminal IO starvation.  Unplug and schedule when waiting for space.

    Signed-off-by: Mike Galbraith <mgalbraith@suse.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Theodore Tso <tytso@mit.edu>
    Link: http://lkml.kernel.org/r/1341812414.7370.73.camel@marge.simpson.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9385b2cb1c91cf7463904f020f2a4176a34e858f
Author: Mike Galbraith <efault@gmx.de>
Date:   Fri Jul 3 08:44:12 2009 -0500

    fs: ntfs: disable interrupt only on !RT

    On Sat, 2007-10-27 at 11:44 +0200, Ingo Molnar wrote:
    > * Nick Piggin <nickpiggin@yahoo.com.au> wrote:
    >
    > > > [10138.175796]  [<c0105de3>] show_trace+0x12/0x14
    > > > [10138.180291]  [<c0105dfb>] dump_stack+0x16/0x18
    > > > [10138.184769]  [<c011609f>] native_smp_call_function_mask+0x138/0x13d
    > > > [10138.191117]  [<c0117606>] smp_call_function+0x1e/0x24
    > > > [10138.196210]  [<c012f85c>] on_each_cpu+0x25/0x50
    > > > [10138.200807]  [<c0115c74>] flush_tlb_all+0x1e/0x20
    > > > [10138.205553]  [<c016caaf>] kmap_high+0x1b6/0x417
    > > > [10138.210118]  [<c011ec88>] kmap+0x4d/0x4f
    > > > [10138.214102]  [<c026a9d8>] ntfs_end_buffer_async_read+0x228/0x2f9
    > > > [10138.220163]  [<c01a0e9e>] end_bio_bh_io_sync+0x26/0x3f
    > > > [10138.225352]  [<c01a2b09>] bio_endio+0x42/0x6d
    > > > [10138.229769]  [<c02c2a08>] __end_that_request_first+0x115/0x4ac
    > > > [10138.235682]  [<c02c2da7>] end_that_request_chunk+0x8/0xa
    > > > [10138.241052]  [<c0365943>] ide_end_request+0x55/0x10a
    > > > [10138.246058]  [<c036dae3>] ide_dma_intr+0x6f/0xac
    > > > [10138.250727]  [<c0366d83>] ide_intr+0x93/0x1e0
    > > > [10138.255125]  [<c015afb4>] handle_IRQ_event+0x5c/0xc9
    > >
    > > Looks like ntfs is kmap()ing from interrupt context. Should be using
    > > kmap_atomic instead, I think.
    >
    > it's not atomic interrupt context but irq thread context - and -rt
    > remaps kmap_atomic() to kmap() internally.

    Hm.  Looking at the change to mm/bounce.c, perhaps I should do this
    instead?

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 193c864ae49b11cee6fba3f215fb6a4122daf71c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 14 17:05:09 2011 +0200

    fs-block-rt-support.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 234508f657569d2bd043efaa31733f2c4d959141
Author: Yong Zhang <yong.zhang0@gmail.com>
Date:   Tue May 15 13:53:56 2012 +0800

    mm: Protect activate_mm() by preempt_[disable&enable]_rt()

    User preempt_*_rt instead of local_irq_*_rt or otherwise there will be
    warning on ARM like below:

    WARNING: at build/linux/kernel/smp.c:459 smp_call_function_many+0x98/0x264()
    Modules linked in:
    [<c0013bb4>] (unwind_backtrace+0x0/0xe4) from [<c001be94>] (warn_slowpath_common+0x4c/0x64)
    [<c001be94>] (warn_slowpath_common+0x4c/0x64) from [<c001bec4>] (warn_slowpath_null+0x18/0x1c)
    [<c001bec4>] (warn_slowpath_null+0x18/0x1c) from [<c0053ff8>](smp_call_function_many+0x98/0x264)
    [<c0053ff8>] (smp_call_function_many+0x98/0x264) from [<c0054364>] (smp_call_function+0x44/0x6c)
    [<c0054364>] (smp_call_function+0x44/0x6c) from [<c0017d50>] (__new_context+0xbc/0x124)
    [<c0017d50>] (__new_context+0xbc/0x124) from [<c009e49c>] (flush_old_exec+0x460/0x5e4)
    [<c009e49c>] (flush_old_exec+0x460/0x5e4) from [<c00d61ac>] (load_elf_binary+0x2e0/0x11ac)
    [<c00d61ac>] (load_elf_binary+0x2e0/0x11ac) from [<c009d060>] (search_binary_handler+0x94/0x2a4)
    [<c009d060>] (search_binary_handler+0x94/0x2a4) from [<c009e8fc>] (do_execve+0x254/0x364)
    [<c009e8fc>] (do_execve+0x254/0x364) from [<c0010e84>] (sys_execve+0x34/0x54)
    [<c0010e84>] (sys_execve+0x34/0x54) from [<c000da00>] (ret_fast_syscall+0x0/0x30)
    ---[ end trace 0000000000000002 ]---

    The reason is that ARM need irq enabled when doing activate_mm().
    According to mm-protect-activate-switch-mm.patch, actually
    preempt_[disable|enable]_rt() is sufficient.

    Inspired-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1337061236-1766-1-git-send-email-yong.zhang0@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 64f5ddb2825e02b61450f7024fa17a45601dfb19
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 19 08:44:27 2009 -0500

    fs: namespace preemption fix

    On RT we cannot loop with preemption disabled here as
    mnt_make_readonly() might have been preempted. We can safely enable
    preemption while waiting for MNT_WRITE_HOLD to be cleared. Safe on !RT
    as well.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 14632781debe78c6268074f56b88275e7f548932
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 14 13:05:54 2011 +0100

    rt: Improve the serial console PASS_LIMIT

    Beyond the warning:

     drivers/tty/serial/8250/8250.c:1613:6: warning: unused variable ‚Äòpass_counter‚Äô [-Wunused-variable]

    the solution of just looping infinitely was ugly - up it to 1 million to
    give it a chance to continue in some really ugly situation.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e09f810843a2a4edb9ff4557b0d5e1d68a4eb7ee
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jan 8 21:36:51 2013 +0100

    drivers-tty-pl011-irq-disable-madness.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 57c8d2d5bafd7a14c2194fa2bcf06ea310bb0bf9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 28 13:32:57 2011 +0200

    drivers-tty-fix-omap-lock-crap.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1d9adb82b8245759db9fd8e7658098e56937c92e
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Fri May 2 13:13:34 2014 +0200

    stomp-machine: use lg_global_trylock_relax() to dead with stop_cpus_lock lglock

    If the stop machinery is called from inactive CPU we cannot use
    lg_global_lock(), because some other stomp machine invocation might be
    in progress and the lock can be contended.  We cannot schedule from this
    context, so use the lovely new lg_global_trylock_relax() primitive to
    do what we used to do via one mutex_trylock()/cpu_relax() loop.  We
    now do that trylock()/relax() across an entire herd of locks. Joy.

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c2f75f49bf720cacd1811e9a2774119e30c12ff1
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Fri May 2 13:13:22 2014 +0200

    stomp-machine: create lg_global_trylock_relax() primitive

    Create lg_global_trylock_relax() for use by stopper thread when it cannot
    schedule, to deal with stop_cpus_lock, which is now an lglock.

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 82a929edf081e08b8c8a6b8bf3fb477283a7f773
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 15 11:02:21 2011 +0200

    lglocks-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 14901b00902fab68a8671307d3decb9dc4ddda2f
Author: Tiejun Chen <tiejun.chen@windriver.com>
Date:   Wed Dec 18 17:51:49 2013 +0800

    rcutree/rcu_bh_qs: disable irq while calling rcu_preempt_qs()

    Any callers to the function rcu_preempt_qs() must disable irqs in
    order to protect the assignment to ->rcu_read_unlock_special. In
    RT case, rcu_bh_qs() as the wrapper of rcu_preempt_qs() is called
    in some scenarios where irq is enabled, like this path,

    do_single_softirq()
        |
        + local_irq_enable();
        + handle_softirq()
        |    |
        |    + rcu_bh_qs()
        |        |
        |        + rcu_preempt_qs()
        |
        + local_irq_disable()

    So here we'd better disable irq directly inside of rcu_bh_qs() to
    fix this, otherwise the kernel may be freezable sometimes as
    observed. And especially this way is also kind and safe for the
    potential rcu_bh_qs() usage elsewhere in the future.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Tiejun Chen <tiejun.chen@windriver.com>
    Signed-off-by: Bin Jiang <bin.jiang@windriver.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2de180d6ed51e5b475e395328be480c7db43fa66
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Oct 5 11:45:18 2011 -0700

    rcu: Make ksoftirqd do RCU quiescent states

    Implementing RCU-bh in terms of RCU-preempt makes the system vulnerable
    to network-based denial-of-service attacks.  This patch therefore
    makes __do_softirq() invoke rcu_bh_qs(), but only when __do_softirq()
    is running in ksoftirqd context.  A wrapper layer in interposed so that
    other calls to __do_softirq() avoid invoking rcu_bh_qs().  The underlying
    function __do_softirq_common() does the actual work.

    The reason that rcu_bh_qs() is bad in these non-ksoftirqd contexts is
    that there might be a local_bh_enable() inside an RCU-preempt read-side
    critical section.  This local_bh_enable() can invoke __do_softirq()
    directly, so if __do_softirq() were to invoke rcu_bh_qs() (which just
    calls rcu_preempt_qs() in the PREEMPT_RT_FULL case), there would be
    an illegal RCU-preempt quiescent state in the middle of an RCU-preempt
    read-side critical section.  Therefore, quiescent states can only happen
    in cases where __do_softirq() is invoked directly from ksoftirqd.

    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20111005184518.GA21601@linux.vnet.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ae125fe928db409bf1d5b0a2a3ed9e4458b00fbc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 14 10:57:54 2011 +0100

    rcu-more-fallout.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit abc72e520159b867bb447b9d6e6b21737a57f9a2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 5 11:59:38 2011 -0700

    rcu: Merge RCU-bh into RCU-preempt

    The Linux kernel has long RCU-bh read-side critical sections that
    intolerably increase scheduling latency under mainline's RCU-bh rules,
    which include RCU-bh read-side critical sections being non-preemptible.
    This patch therefore arranges for RCU-bh to be implemented in terms of
    RCU-preempt for CONFIG_PREEMPT_RT_FULL=y.

    This has the downside of defeating the purpose of RCU-bh, namely,
    handling the case where the system is subjected to a network-based
    denial-of-service attack that keeps at least one CPU doing full-time
    softirq processing.  This issue will be fixed by a later commit.

    The current commit will need some work to make it appropriate for
    mainline use, for example, it needs to be extended to cover Tiny RCU.

    [ paulmck: Added a useful changelog ]

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20111005185938.GA20403@linux.vnet.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 75eef8f3e3f3a4689703f03f5ad8bf2368805136
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Aug 13 00:23:17 2011 +0200

    rcu: Frob softirq test

    With RT_FULL we get the below wreckage:

    [  126.060484] =======================================================
    [  126.060486] [ INFO: possible circular locking dependency detected ]
    [  126.060489] 3.0.1-rt10+ #30
    [  126.060490] -------------------------------------------------------
    [  126.060492] irq/24-eth0/1235 is trying to acquire lock:
    [  126.060495]  (&(lock)->wait_lock#2){+.+...}, at: [<ffffffff81501c81>] rt_mutex_slowunlock+0x16/0x55
    [  126.060503]
    [  126.060504] but task is already holding lock:
    [  126.060506]  (&p->pi_lock){-...-.}, at: [<ffffffff81074fdc>] try_to_wake_up+0x35/0x429
    [  126.060511]
    [  126.060511] which lock already depends on the new lock.
    [  126.060513]
    [  126.060514]
    [  126.060514] the existing dependency chain (in reverse order) is:
    [  126.060516]
    [  126.060516] -> #1 (&p->pi_lock){-...-.}:
    [  126.060519]        [<ffffffff810afe9e>] lock_acquire+0x145/0x18a
    [  126.060524]        [<ffffffff8150291e>] _raw_spin_lock_irqsave+0x4b/0x85
    [  126.060527]        [<ffffffff810b5aa4>] task_blocks_on_rt_mutex+0x36/0x20f
    [  126.060531]        [<ffffffff815019bb>] rt_mutex_slowlock+0xd1/0x15a
    [  126.060534]        [<ffffffff81501ae3>] rt_mutex_lock+0x2d/0x2f
    [  126.060537]        [<ffffffff810d9020>] rcu_boost+0xad/0xde
    [  126.060541]        [<ffffffff810d90ce>] rcu_boost_kthread+0x7d/0x9b
    [  126.060544]        [<ffffffff8109a760>] kthread+0x99/0xa1
    [  126.060547]        [<ffffffff81509b14>] kernel_thread_helper+0x4/0x10
    [  126.060551]
    [  126.060552] -> #0 (&(lock)->wait_lock#2){+.+...}:
    [  126.060555]        [<ffffffff810af1b8>] __lock_acquire+0x1157/0x1816
    [  126.060558]        [<ffffffff810afe9e>] lock_acquire+0x145/0x18a
    [  126.060561]        [<ffffffff8150279e>] _raw_spin_lock+0x40/0x73
    [  126.060564]        [<ffffffff81501c81>] rt_mutex_slowunlock+0x16/0x55
    [  126.060566]        [<ffffffff81501ce7>] rt_mutex_unlock+0x27/0x29
    [  126.060569]        [<ffffffff810d9f86>] rcu_read_unlock_special+0x17e/0x1c4
    [  126.060573]        [<ffffffff810da014>] __rcu_read_unlock+0x48/0x89
    [  126.060576]        [<ffffffff8106847a>] select_task_rq_rt+0xc7/0xd5
    [  126.060580]        [<ffffffff8107511c>] try_to_wake_up+0x175/0x429
    [  126.060583]        [<ffffffff81075425>] wake_up_process+0x15/0x17
    [  126.060585]        [<ffffffff81080a51>] wakeup_softirqd+0x24/0x26
    [  126.060590]        [<ffffffff81081df9>] irq_exit+0x49/0x55
    [  126.060593]        [<ffffffff8150a3bd>] smp_apic_timer_interrupt+0x8a/0x98
    [  126.060597]        [<ffffffff81509793>] apic_timer_interrupt+0x13/0x20
    [  126.060600]        [<ffffffff810d5952>] irq_forced_thread_fn+0x1b/0x44
    [  126.060603]        [<ffffffff810d582c>] irq_thread+0xde/0x1af
    [  126.060606]        [<ffffffff8109a760>] kthread+0x99/0xa1
    [  126.060608]        [<ffffffff81509b14>] kernel_thread_helper+0x4/0x10
    [  126.060611]
    [  126.060612] other info that might help us debug this:
    [  126.060614]
    [  126.060615]  Possible unsafe locking scenario:
    [  126.060616]
    [  126.060617]        CPU0                    CPU1
    [  126.060619]        ----                    ----
    [  126.060620]   lock(&p->pi_lock);
    [  126.060623]                                lock(&(lock)->wait_lock);
    [  126.060625]                                lock(&p->pi_lock);
    [  126.060627]   lock(&(lock)->wait_lock);
    [  126.060629]
    [  126.060629]  *** DEADLOCK ***
    [  126.060630]
    [  126.060632] 1 lock held by irq/24-eth0/1235:
    [  126.060633]  #0:  (&p->pi_lock){-...-.}, at: [<ffffffff81074fdc>] try_to_wake_up+0x35/0x429
    [  126.060638]
    [  126.060638] stack backtrace:
    [  126.060641] Pid: 1235, comm: irq/24-eth0 Not tainted 3.0.1-rt10+ #30
    [  126.060643] Call Trace:
    [  126.060644]  <IRQ>  [<ffffffff810acbde>] print_circular_bug+0x289/0x29a
    [  126.060651]  [<ffffffff810af1b8>] __lock_acquire+0x1157/0x1816
    [  126.060655]  [<ffffffff810ab3aa>] ? trace_hardirqs_off_caller+0x1f/0x99
    [  126.060658]  [<ffffffff81501c81>] ? rt_mutex_slowunlock+0x16/0x55
    [  126.060661]  [<ffffffff810afe9e>] lock_acquire+0x145/0x18a
    [  126.060664]  [<ffffffff81501c81>] ? rt_mutex_slowunlock+0x16/0x55
    [  126.060668]  [<ffffffff8150279e>] _raw_spin_lock+0x40/0x73
    [  126.060671]  [<ffffffff81501c81>] ? rt_mutex_slowunlock+0x16/0x55
    [  126.060674]  [<ffffffff810d9655>] ? rcu_report_qs_rsp+0x87/0x8c
    [  126.060677]  [<ffffffff81501c81>] rt_mutex_slowunlock+0x16/0x55
    [  126.060680]  [<ffffffff810d9ea3>] ? rcu_read_unlock_special+0x9b/0x1c4
    [  126.060683]  [<ffffffff81501ce7>] rt_mutex_unlock+0x27/0x29
    [  126.060687]  [<ffffffff810d9f86>] rcu_read_unlock_special+0x17e/0x1c4
    [  126.060690]  [<ffffffff810da014>] __rcu_read_unlock+0x48/0x89
    [  126.060693]  [<ffffffff8106847a>] select_task_rq_rt+0xc7/0xd5
    [  126.060696]  [<ffffffff810683da>] ? select_task_rq_rt+0x27/0xd5
    [  126.060701]  [<ffffffff810a852a>] ? clockevents_program_event+0x8e/0x90
    [  126.060704]  [<ffffffff8107511c>] try_to_wake_up+0x175/0x429
    [  126.060708]  [<ffffffff810a95dc>] ? tick_program_event+0x1f/0x21
    [  126.060711]  [<ffffffff81075425>] wake_up_process+0x15/0x17
    [  126.060715]  [<ffffffff81080a51>] wakeup_softirqd+0x24/0x26
    [  126.060718]  [<ffffffff81081df9>] irq_exit+0x49/0x55
    [  126.060721]  [<ffffffff8150a3bd>] smp_apic_timer_interrupt+0x8a/0x98
    [  126.060724]  [<ffffffff81509793>] apic_timer_interrupt+0x13/0x20
    [  126.060726]  <EOI>  [<ffffffff81072855>] ? migrate_disable+0x75/0x12d
    [  126.060733]  [<ffffffff81080a61>] ? local_bh_disable+0xe/0x1f
    [  126.060736]  [<ffffffff81080a70>] ? local_bh_disable+0x1d/0x1f
    [  126.060739]  [<ffffffff810d5952>] irq_forced_thread_fn+0x1b/0x44
    [  126.060742]  [<ffffffff81502ac0>] ? _raw_spin_unlock_irq+0x3b/0x59
    [  126.060745]  [<ffffffff810d582c>] irq_thread+0xde/0x1af
    [  126.060748]  [<ffffffff810d5937>] ? irq_thread_fn+0x3a/0x3a
    [  126.060751]  [<ffffffff810d574e>] ? irq_finalize_oneshot+0xd1/0xd1
    [  126.060754]  [<ffffffff810d574e>] ? irq_finalize_oneshot+0xd1/0xd1
    [  126.060757]  [<ffffffff8109a760>] kthread+0x99/0xa1
    [  126.060761]  [<ffffffff81509b14>] kernel_thread_helper+0x4/0x10
    [  126.060764]  [<ffffffff81069ed7>] ? finish_task_switch+0x87/0x10a
    [  126.060768]  [<ffffffff81502ec4>] ? retint_restore_args+0xe/0xe
    [  126.060771]  [<ffffffff8109a6c7>] ? __init_kthread_worker+0x8c/0x8c
    [  126.060774]  [<ffffffff81509b10>] ? gs_change+0xb/0xb

    Because irq_exit() does:

    void irq_exit(void)
    {
    	account_system_vtime(current);
    	trace_hardirq_exit();
    	sub_preempt_count(IRQ_EXIT_OFFSET);
    	if (!in_interrupt() && local_softirq_pending())
    		invoke_softirq();

    	...
    }

    Which triggers a wakeup, which uses RCU, now if the interrupted task has
    t->rcu_read_unlock_special set, the rcu usage from the wakeup will end
    up in rcu_read_unlock_special(). rcu_read_unlock_special() will test
    for in_irq(), which will fail as we just decremented preempt_count
    with IRQ_EXIT_OFFSET, and in_sering_softirq(), which for
    PREEMPT_RT_FULL reads:

    int in_serving_softirq(void)
    {
    	int res;

    	preempt_disable();
    	res = __get_cpu_var(local_softirq_runner) == current;
    	preempt_enable();
    	return res;
    }

    Which will thus also fail, resulting in the above wreckage.

    The 'somewhat' ugly solution is to open-code the preempt_count() test
    in rcu_read_unlock_special().

    Also, we're not at all sure how ->rcu_read_unlock_special gets set
    here... so this is very likely a bandaid and more thought is required.

    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 28d4947cf915be6852b5d3480cb5ebb4a5b7a1b0
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jan 28 14:10:02 2015 +0100

    Revert "timers: do not raise softirq unconditionally"

    The patch I revert here triggers the HRtimer switch from hardirq instead
    of from softirq. As a result we get a periodic interrupt before the
    switch is complete (that is a hrtimer has been programmed) and so the
    tick still programms periodic mode. Since the timer has been shutdown,
    dev->next_event is set to max and the next increment makes it negative.
    And now we wait‚Ä¶

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2ce25f54d828e4c29aa113fc617a3be2d113dc3d
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri May 2 21:31:50 2014 +0200

    timer: do not spin_trylock() on UP

    This will void a warning comming from the spin-lock debugging code. The
    lock avoiding idea is from Steven Rostedt.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b9da5e90c68946a361e839276d6321e7f482f0a5
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Nov 15 15:46:50 2013 +0100

    rtmutex: use a trylock for waiter lock in trylock

    Mike Galbraith captered the following:
    | >#11 [ffff88017b243e90] _raw_spin_lock at ffffffff815d2596
    | >#12 [ffff88017b243e90] rt_mutex_trylock at ffffffff815d15be
    | >#13 [ffff88017b243eb0] get_next_timer_interrupt at ffffffff81063b42
    | >#14 [ffff88017b243f00] tick_nohz_stop_sched_tick at ffffffff810bd1fd
    | >#15 [ffff88017b243f70] tick_nohz_irq_exit at ffffffff810bd7d2
    | >#16 [ffff88017b243f90] irq_exit at ffffffff8105b02d
    | >#17 [ffff88017b243fb0] reschedule_interrupt at ffffffff815db3dd
    | >--- <IRQ stack> ---
    | >#18 [ffff88017a2a9bc8] reschedule_interrupt at ffffffff815db3dd
    | >    [exception RIP: task_blocks_on_rt_mutex+51]
    | >#19 [ffff88017a2a9ce0] rt_spin_lock_slowlock at ffffffff815d183c
    | >#20 [ffff88017a2a9da0] lock_timer_base.isra.35 at ffffffff81061cbf
    | >#21 [ffff88017a2a9dd0] schedule_timeout at ffffffff815cf1ce
    | >#22 [ffff88017a2a9e50] rcu_gp_kthread at ffffffff810f9bbb
    | >#23 [ffff88017a2a9ed0] kthread at ffffffff810796d5
    | >#24 [ffff88017a2a9f50] ret_from_fork at ffffffff815da04c

    lock_timer_base() does a try_lock() which deadlocks on the waiter lock
    not the lock itself.
    This patch takes the waiter_lock with trylock so it should work from interrupt
    context as well. If the fastpath doesn't work and the waiter_lock itself is
    taken then it seems that the lock itself taken.
    This patch also adds "rt_spin_unlock_after_trylock_in_irq" to keep lockdep
    happy. If we managed to take the wait_lock in the first place we should also
    be able to take it in the unlock path.

    Cc: stable-rt@vger.kernel.org
    Reported-by: Mike Galbraith <bitbucket@online.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7895a2f7957d6c86e73a6339ce009e3fdeb3d022
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Jan 31 12:07:57 2014 -0500

    timer/rt: Always raise the softirq if there's irq_work to be done

    It was previously discovered that some systems would hang on boot up
    with a previous version of 3.12-rt. This was due to RCU using irq_work,
    and RT defers the irq_work to a softirq. But if there's no active
    timers, the softirq will not be raised, and RCU work will not get done,
    causing the system to hang.  The fix was to check that if there was no
    active timers but irq_work to be done, then we should raise the softirq.

    But this fix was not 100% correct. It left out the case that there were
    active timers that were not expired yet. This would have the softirq
    not get raised even if there was irq work to be done.

    If there is irq_work to be done, then we must raise the timer softirq
    regardless of if there is active timers or whether they are expired or
    not. The softirq can handle those cases. But we can never ignore
    irq_work.

    As it is only PREEMPT_RT_FULL that requires irq_work to be done in the
    softirq, we can pull out the check in the active_timers condition, and
    make the code a bit cleaner by having the irq_work check separate, and
    put the code in with the other #ifdef PREEMPT_RT. If there is irq_work
    to be done, there's no need to check the active timers or if they are
    expired. Just raise the time softirq and be done with it. Otherwise, we
    can do the timer checks just like we do with non -rt.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit fc11e8e7d46a67071688a627503875a3e79e21c4
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Jan 24 15:09:33 2014 -0500

    timer: Raise softirq if there's irq_work

    [ Talking with Sebastian on IRC, it seems that doing the irq_work_run()
      from the interrupt in -rt is a bad thing. Here we simply raise the
      softirq if there's irq work to do. This too boots on my i7 ]

    After trying hard to figure out why my i7 box was locking up with the
    new active_timers code, that does not run the timer softirq if there
    are no active timers, I took an extra look at the softirq handler and
    noticed that it doesn't just run timer softirqs, it also runs irq work.

    This was the bug that was locking up the system. It wasn't missing a
    timer, it was missing irq work. By always doing the irq work callbacks,
    the system boots fine. The missing irq work callback was the RCU's
    sp_wakeup() function.

    No need to check for defined(CONFIG_IRQ_WORK). When that's not set the
    "irq_work_needs_cpu()" is a static inline that returns false.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 2300b85e986872c9da22967c6ffe945229c304b7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Nov 7 12:21:11 2013 +0100

    timers: do not raise softirq unconditionally

    Mike,

    On Thu, 7 Nov 2013, Mike Galbraith wrote:

    > On Thu, 2013-11-07 at 04:26 +0100, Mike Galbraith wrote:
    > > On Wed, 2013-11-06 at 18:49 +0100, Thomas Gleixner wrote:
    >
    > > > I bet you are trying to work around some of the side effects of the
    > > > occasional tick which is still necessary despite of full nohz, right?
    > >
    > > Nope, I wanted to check out cost of nohz_full for rt, and found that it
    > > doesn't work at all instead, looked, and found that the sole running
    > > task has just awakened ksoftirqd when it wants to shut the tick down, so
    > > that shutdown never happens.
    >
    > Like so in virgin 3.10-rt.  Box is x3550 M3 booted nowatchdog
    > rcu_nocbs=1-3 nohz_full=1-3, and CPUs1-3 are completely isolated via
    > cpusets as well.

    well, that very same problem is in mainline if you add "threadirqs" to
    the command line. But we can be smart about this. The untested patch
    below should address that issue. If that works on mainline we can
    adapt it for RT (needs a trylock(&base->lock) there).

    Though it's not a full solution. It needs some thought versus the
    softirq code of timers. Assume we have only one timer queued 1000
    ticks into the future. So this change will cause the timer softirq not
    to be called until that timer expires and then the timer softirq is
    going to do 1000 loops until it catches up with jiffies. That's
    anything but pretty ...

    What worries me more is this one:

      pert-5229  [003] d..h1..   684.482618: softirq_raise: vec=9 [action=RCU]

    The CPU has no callbacks as you shoved them over to cpu 0, so why is
    the RCU softirq raised?

    Thanks,

    	tglx
    ------------------
    Message-id: <alpine.DEB.2.02.1311071158350.23353@ionos.tec.linutronix.de>
    |CONFIG_NO_HZ_FULL + CONFIG_PREEMPT_RT_FULL = nogo
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7d4ecc3a384776c3d878ade08ca60a9e76cc6f52
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 22:08:38 2011 +0200

    timer-handle-idle-trylock-in-get-next-timer-irq.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7ee7753e580e6c06e63fee4d6562f88405c67312
Author: John Kacur <jkacur@redhat.com>
Date:   Mon Sep 19 11:09:27 2011 +0200

    rwlocks: Fix section mismatch

    This fixes the following build error for the preempt-rt kernel.

    make kernel/fork.o
      CC      kernel/fork.o
    kernel/fork.c:90: error: section of tasklist_lock conflicts with previous declaration
    make[2]: *** [kernel/fork.o] Error 1
    make[1]: *** [kernel/fork.o] Error 2

    The rt kernel cache aligns the RWLOCK in DEFINE_RWLOCK by default.
    The non-rt kernels explicitly cache align only the tasklist_lock in
    kernel/fork.c
    That can create a build conflict. This fixes the build problem by making the
    non-rt kernels cache align RWLOCKs by default. The side effect is that
    the other RWLOCKs are also cache aligned for non-rt.

    This is a short term solution for rt only.
    The longer term solution would be to push the cache aligned DEFINE_RWLOCK
    to mainline. If there are objections, then we could create a
    DEFINE_RWLOCK_CACHE_ALIGNED or something of that nature.

    Comments? Objections?

    Signed-off-by: John Kacur <jkacur@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.00.1109191104010.23118@localhost6.localdomain6
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7e9156d35eac29e389a5312ea5ef2e918e573126
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Thu Feb 26 09:02:05 2015 +0100

    locking: ww_mutex: fix ww_mutex vs self-deadlock

    If the caller already holds the mutex, task_blocks_on_rt_mutex()
    returns -EDEADLK, we proceed directly to rt_mutex_handle_deadlock()
    where it's instant game over.

    Let ww_mutexes return EDEADLK/EALREADY as they want to instead.

    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 888ea7b45b4d14eee047af26ceeb368a700a4946
Author: Gustavo Bittencourt <gbitten@gmail.com>
Date:   Tue Jan 20 18:02:29 2015 -0200

    rtmutex: enable deadlock detection in ww_mutex_lock functions

    The functions ww_mutex_lock_interruptible and ww_mutex_lock should return -EDEADLK when faced with
    a deadlock. To do so, the paramenter detect_deadlock in rt_mutex_slowlock must be TRUE.
    This patch corrects potential deadlocks when running PREEMPT_RT with nouveau driver.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Gustavo Bittencourt <gbitten@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit d7ae69437579f77e7d280260909f788b1c4cf804
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Mon Jun 2 15:12:44 2014 +0200

    rt,locking: fix __ww_mutex_lock_interruptible() lockdep annotation

    Using mutex_acquire_nest() as used in __ww_mutex_lock() fixes the
    splat below.  Remove superfluous line break in __ww_mutex_lock()
    as well.

    |=============================================
    |[ INFO: possible recursive locking detected ]
    |3.14.4-rt5 #26 Not tainted
    |---------------------------------------------
    |Xorg/4298 is trying to acquire lock:
    | (reservation_ww_class_mutex){+.+.+.}, at: [<ffffffffa02b4270>] nouveau_gem_ioctl_pushbuf+0x870/0x19f0 [nouveau]
    |but task is already holding lock:
    | (reservation_ww_class_mutex){+.+.+.}, at: [<ffffffffa02b4270>] nouveau_gem_ioctl_pushbuf+0x870/0x19f0 [nouveau]
    |other info that might help us debug this:
    | Possible unsafe locking scenario:
    |       CPU0
    |       ----
    |  lock(reservation_ww_class_mutex);
    |  lock(reservation_ww_class_mutex);
    |
    | *** DEADLOCK ***
    |
    | May be due to missing lock nesting notation
    |
    |3 locks held by Xorg/4298:
    | #0:  (&cli->mutex){+.+.+.}, at: [<ffffffffa02b597b>] nouveau_abi16_get+0x2b/0x100 [nouveau]
    | #1:  (reservation_ww_class_acquire){+.+...}, at: [<ffffffffa0160cd2>] drm_ioctl+0x4d2/0x610 [drm]
    | #2:  (reservation_ww_class_mutex){+.+.+.}, at: [<ffffffffa02b4270>] nouveau_gem_ioctl_pushbuf+0x870/0x19f0 [nouveau]

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 4cf1d2fea3c8b26e4d33a274d2381e8ffff27a02
Author: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
Date:   Mon Oct 28 09:36:37 2013 +0100

    rtmutex: add a first shot of ww_mutex

    lockdep says:
    | --------------------------------------------------------------------------
    | | Wound/wait tests |
    | ---------------------
    |                 ww api failures:  ok  |  ok  |  ok  |
    |              ww contexts mixing:  ok  |  ok  |
    |            finishing ww context:  ok  |  ok  |  ok  |  ok  |
    |              locking mismatches:  ok  |  ok  |  ok  |
    |                EDEADLK handling:  ok  |  ok  |  ok  |  ok  |  ok  |  ok  |  ok  |  ok  |  ok  |  ok  |
    |          spinlock nest unlocked:  ok  |
    | -----------------------------------------------------
    |                                |block | try  |context|
    | -----------------------------------------------------
    |                         context:  ok  |  ok  |  ok  |
    |                             try:  ok  |  ok  |  ok  |
    |                           block:  ok  |  ok  |  ok  |
    |                        spinlock:  ok  |  ok  |  ok  |

    Signed-off-by: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 26144ba6919cfea816540ac4d522402137d7c4cc
Author: Brad Mouring <bmouring@ni.com>
Date:   Wed Jan 14 15:11:38 2015 -0600

    rtmutex.c: Fix incorrect waiter check

    In task_blocks_on_lock, there's a null check on pi_blocked_on
    of the task_struct. This pointer can encode the fact that the
    task that contains the pointer is waking (preventing requeuing)
    and therefore is non-null. Use the inline function to avoid
    dereferencing an invalid "pointer"

    Signed-off-by: Brad Mouring <brad.mouring@ni.com>
    Reported-by: Ben Shelton <ben.shelton@ni.com>
    Reviewed-by: T Makphaibulchoke <tmac@hp.com>
    Tested-by: T Makphaibulchoke <tmac@hp.com>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 377d0df8e41646254fd94f63bafcd67035dfcef3
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Apr 8 16:08:46 2013 +0200

    percpu-rwsem: compile fix

    The shortcut on mainline skip lockdep. No idea why this is a good thing.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0bfa8b2c0ebc2d7374b9347b8987e47b11aaac07
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Sat Feb 8 12:39:20 2014 +0100

    rt: Cleanup of unnecessary do while 0 in read/write _lock()

    With the migration pushdonw a few of the do{ }while(0)
    loops became obsolete but got left over - this patch
    only removes this fallout.

    Patch applies on top of 3.12.9-rt13

    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b0d9a206f72f51a5e7cef219ddb4917e23aae879
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Apr 29 20:13:08 2014 -0400

    rwlock: disable migration before taking a lock

    If there's no complaints about it. I'm going to add this to the 3.12-rt
    stable tree. As without it, it fails horribly with the cpu hotplug
    stress test, and I wont release a stable kernel that does that.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ea4585b8dc18be63d9feccf8f3a8c00a0f98c1de
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Thu Jan 2 10:19:15 2014 +0100

    read_lock migrate_disable pushdown to rt_read_lock

    pushdown of migrate_disable/enable from read_*lock* to the rt_read_*lock*
    api level

    general mapping to mutexes:

    read_*lock*
      `-> rt_read_*lock*
              `-> __spin_lock (the sleeping spin locks)
                     `-> rt_mutex

    The real read_lock* mapping:

              read_lock_irqsave -.
    read_lock_irq                `-> rt_read_lock_irqsave()
           `->read_lock ---------.       \
              read_lock_bh ------+        \
                                 `--> rt_read_lock()
                                       if (rt_mutex_owner(lock) != current){
                                               `-> __rt_spin_lock()
                                                    rt_spin_lock_fastlock()
                                                           `->rt_mutex_cmpxchg()
                                        migrate_disable()
                                       }
                                       rwlock->read_depth++;
    read_trylock mapping:

    read_trylock
              `-> rt_read_trylock
                   if (rt_mutex_owner(lock) != current){
                                                  `-> rt_mutex_trylock()
                                                       rt_mutex_fasttrylock()
                                                        rt_mutex_cmpxchg()
                    migrate_disable()
                   }
                   rwlock->read_depth++;

    read_unlock* mapping:

    read_unlock_bh --------+
    read_unlock_irq -------+
    read_unlock_irqrestore +
    read_unlock -----------+
                           `-> rt_read_unlock()
                                if(--rwlock->read_depth==0){
                                          `-> __rt_spin_unlock()
                                               rt_spin_lock_fastunlock()
                                                            `-> rt_mutex_cmpxchg()
                                 migrate_disable()
                                }

    So calls to migrate_disable/enable() are better placed at the rt_read_*
    level of lock/trylock/unlock as all of the read_*lock* API has this as a
    common path. In the rt_read* API of lock/trylock/unlock the nesting level
    is already being recorded in rwlock->read_depth, so we can push down the
    migrate disable/enable to that level and condition it on the read_depth
    going from 0 to 1 -> migrate_disable and 1 to 0 -> migrate_enable. This
    eliminates the recursive calls that were needed when migrate_disable/enable
    was done at the read_*lock* level.

    The approach to read_*_bh also eliminates the concerns raised with the
    regards to api inbalances (read_lock_bh -> read_unlock+local_bh_enable)

    Tested-by: Carsten Emde <C.Emde@osadl.org>
    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c2120fcac9423786b0b0637f9bfd373f7797e6cd
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Thu Jan 2 10:18:42 2014 +0100

    write_lock migrate_disable pushdown to rt_write_lock

    pushdown of migrate_disable/enable from write_*lock* to the rt_write_*lock*
    api level

    general mapping of write_*lock* to mutexes:

    write_*lock*
      `-> rt_write_*lock*
              `-> __spin_lock (the sleeping __spin_lock)
                     `-> rt_mutex

    write_*lock*s are non-recursive so we have two lock chains to consider
     - write_trylock*/write_unlock
     - write_lock*/wirte_unlock
    for both paths the migration_disable/enable must be balanced.

    write_trylock* mapping:

    write_trylock_irqsave
                    `-> rt_write_trylock_irqsave
    write_trylock             \
              `-------->  rt_write_trylock
                           ret = rt_mutex_trylock
                                  rt_mutex_fasttrylock
                                   rt_mutex_cmpxchg
                           if (ret)
                                migrate_disable

    write_lock* mapping:

                      write_lock_irqsave
                                    `-> rt_write_lock_irqsave
    write_lock_irq -> write_lock ----.     \
                      write_lock_bh -+      \
                                     `-> rt_write_lock
                                          __rt_spin_lock()
                                           rt_spin_lock_fastlock()
                                            rt_mutex_cmpxchg()
                                         migrate_disable()

    write_unlock* mapping:

                        write_unlock_irqrestore.
                        write_unlock_bh -------+
    write_unlock_irq -> write_unlock ----------+
                                               `-> rt_write_unlock()
                                                    __rt_spin_unlock()
                                                     rt_spin_lock_fastunlock()
                                                      rt_mutex_cmpxchg()
                                                   migrate_enable()

    So calls to migrate_disable/enable() are better placed at the rt_write_*
    level of lock/trylock/unlock as all of the write_*lock* API has this as a
    common path.

    This approach to write_*_bh also eliminates the concerns raised with
    regards to api inbalances (write_lock_bh -> write_unlock+local_bh_enable)

    Tested-by: Carsten Emde <C.Emde@osadl.org>
    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 56d7a5dc67c41dcc5d1508e2e0ca4f556f8465a1
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Fri Nov 29 00:21:59 2013 -0500

    migrate_disable pushd down in rt_write_trylock_irqsave

    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 8c94a03d45fc2a8d14b4931c188094ec0a302422
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Fri Nov 29 00:17:27 2013 -0500

    migrate_disable pushd down in rt_spin_trylock_irqsave

    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f6e6b0ea66de93ec6098dbdbad670060b45f5dbb
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri May 2 17:32:30 2014 +0200

    Revert "migrate_disable pushd down in atomic_dec_and_spin_lock"

    This reverts commit ff9c870c3e27d58c9512fad122e91436681fee5a.
    Cc: stable-rt@vger.kernel.org

commit f65c0cc642f842001ad4d1b465fb9232f23bceb9
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Fri Nov 29 00:19:41 2013 -0500

    migrate_disable pushd down in atomic_dec_and_spin_lock

    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1285a83e11ef7966e137c4c2e8d8b677a158aac9
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Thu Nov 21 22:52:30 2013 -0500

    condition migration_disable on lock acquisition

    No need to unconditionally migrate_disable (what is it protecting ?) and
    re-enable on failure to acquire the lock.
    This patch moves the migrate_disable to be conditioned on sucessful lock
    acquisition only.

    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit de8135d5f7845537b18a555cf4dc60fce9bded63
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 25 12:16:43 2015 +0100

    Revert "rwsem-rt: Do not allow readers to nest"

    This behaviour is required by cpufreq and its logic is "okay": It does a
    read_lock followed by a try_read_lock.
    Lockdep warns if one try a read_lock twice in -RT and vanilla so it
    should be good. We still only allow multiple readers as long as it is in
    the same process.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a0a653a54044fa3219ac8370fb1694e5a83de9b5
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri May 2 10:53:30 2014 +0200

    rwsem-rt: Do not allow readers to nest

    The readers of mainline rwsems are not allowed to nest, the rwsems in the
    PREEMPT_RT kernel should not nest either.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 305ba8b59f1435b45794cce71a67722bf825159c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 26 19:39:56 2009 +0200

    rt: Add the preempt-rt lock replacement APIs

    Map spinlocks, rwlocks, rw_semaphores and semaphores to the rt_mutex
    based locking functions for preempt-rt.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a697d8343c311543e68242612e446f10995f3a49
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 29 21:02:53 2011 +0200

    rwsem-add-rt-variant.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 54d8b3dfd7f068afacce02b5e6f4974188112aa2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 29 20:56:22 2011 +0200

    rt-add-rt-to-mutex-headers.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 16520c03c153348ebca0f9b8e233beb593905a77
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 29 19:43:35 2011 +0200

    rt-add-rt-spinlocks.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a77156f3f62ac2beb877813f5b2a91163e3768c3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 29 20:06:39 2011 +0200

    rtmutex-avoid-include-hell.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 6812f564663299d3a11bb493083e6b6daa3d169e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 29 19:34:01 2011 +0200

    spinlock-types-separate-raw.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a8f869bb1c6b35cf1b9b6ee8b909bed3c90f27a8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jun 10 11:21:25 2011 +0200

    rt-mutex-add-sleeping-spinlocks-support.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7b21ce63432c178fd7152a113e65a075545b0bf7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jun 9 11:43:52 2011 +0200

    rtmutex-lock-killable.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 6cbc3ccf417114578045a7ff477d666d65e3df58
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 1 11:17:42 2013 +0100

    futex: Ensure lock/unlock symetry versus pi_lock and hash bucket lock

    In exit_pi_state_list() we have the following locking construct:

       spin_lock(&hb->lock);
       raw_spin_lock_irq(&curr->pi_lock);

       ...
       spin_unlock(&hb->lock);

    In !RT this works, but on RT the migrate_enable() function which is
    called from spin_unlock() sees atomic context due to the held pi_lock
    and just decrements the migrate_disable_atomic counter of the
    task. Now the next call to migrate_disable() sees the counter being
    negative and issues a warning. That check should be in
    migrate_enable() already.

    Fix this by dropping pi_lock before unlocking hb->lock and reaquire
    pi_lock after that again. This is safe as the loop code reevaluates
    head again under the pi_lock.

    Reported-by: Yong Zhang <yong.zhang@windriver.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e127e26db48b7a259e0bf12f2204f337197cbee9
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Jun 18 18:16:25 2015 -0400

    futex: Fix bug on when a requeued RT task times out

    Requeue with timeout causes a bug with PREEMPT_RT_FULL.

    The bug comes from a timed out condition.

    	TASK 1				TASK 2
    	------				------
        futex_wait_requeue_pi()
    	futex_wait_queue_me()
    	<timed out>

    					double_lock_hb();

    	raw_spin_lock(pi_lock);
    	if (current->pi_blocked_on) {
    	} else {
    	    current->pi_blocked_on = PI_WAKE_INPROGRESS;
    	    run_spin_unlock(pi_lock);
    	    spin_lock(hb->lock); <-- blocked!

    					plist_for_each_entry_safe(this) {
    					    rt_mutex_start_proxy_lock();
    						task_blocks_on_rt_mutex();
    						BUG_ON(task->pi_blocked_on)!!!!

    The BUG_ON() actually has a check for PI_WAKE_INPROGRESS, but the
    problem is that, after TASK 1 sets PI_WAKE_INPROGRESS, it then tries to
    grab the hb->lock, which it fails to do so. As the hb->lock is a mutex,
    it will block and set the "pi_blocked_on" to the hb->lock.

    When TASK 2 goes to requeue it, the check for PI_WAKE_INPROGESS fails
    because the task1's pi_blocked_on is no longer set to that, but instead,
    set to the hb->lock.

    The fix:

    When calling rt_mutex_start_proxy_lock() a check is made to see
    if the proxy tasks pi_blocked_on is set. If so, exit out early.
    Otherwise set it to a new flag PI_REQUEUE_INPROGRESS, which notifies
    the proxy task that it is being requeued, and will handle things
    appropriately.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 041c352dd551a13f136f55b7c1f7be7ad19c7503
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jun 10 11:04:15 2011 +0200

    rtmutex-futex-prepare-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9eedba5b8fc5f44fd6385f92ae49ea06a0d30215
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 6 16:51:31 2010 +0200

    md: raid5: Make raid5_percpu handling RT aware

    __raid_run_ops() disables preemption with get_cpu() around the access
    to the raid5_percpu variables. That causes scheduling while atomic
    spews on RT.

    Serialize the access to the percpu data with a lock and keep the code
    preemptible.

    Reported-by: Udo van den Heuvel <udovdh@xs4all.nl>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Udo van den Heuvel <udovdh@xs4all.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit eb634bf6d0984fd494494c9d93f9cb12f1242dde
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 28 20:42:16 2011 +0200

    local-vars-migrate-disable.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit cb211d97ce91ad909bb5aebc4c32d179ea72f545
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jan 31 13:01:27 2012 +0100

    genirq: Allow disabling of softirq processing in irq thread context

    The processing of softirqs in irq thread context is a performance gain
    for the non-rt workloads of a system, but it's counterproductive for
    interrupts which are explicitely related to the realtime
    workload. Allow such interrupts to prevent softirq processing in their
    thread context.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 03eb00662a8bffddc76c3f94b4ccdb11e2f19f92
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Nov 29 20:18:22 2011 -0500

    tasklet: Prevent tasklets from going into infinite spin in RT

    When CONFIG_PREEMPT_RT_FULL is enabled, tasklets run as threads,
    and spinlocks turn are mutexes. But this can cause issues with
    tasks disabling tasklets. A tasklet runs under ksoftirqd, and
    if a tasklets are disabled with tasklet_disable(), the tasklet
    count is increased. When a tasklet runs, it checks this counter
    and if it is set, it adds itself back on the softirq queue and
    returns.

    The problem arises in RT because ksoftirq will see that a softirq
    is ready to run (the tasklet softirq just re-armed itself), and will
    not sleep, but instead run the softirqs again. The tasklet softirq
    will still see that the count is non-zero and will not execute
    the tasklet and requeue itself on the softirq again, which will
    cause ksoftirqd to run it again and again and again.

    It gets worse because ksoftirqd runs as a real-time thread.
    If it preempted the task that disabled tasklets, and that task
    has migration disabled, or can't run for other reasons, the tasklet
    softirq will never run because the count will never be zero, and
    ksoftirqd will go into an infinite loop. As an RT task, it this
    becomes a big problem.

    This is a hack solution to have tasklet_disable stop tasklets, and
    when a tasklet runs, instead of requeueing the tasklet softirqd
    it delays it. When tasklet_enable() is called, and tasklets are
    waiting, then the tasklet_enable() will kick the tasklets to continue.
    This prevents the lock up from ksoftirq going into an infinite loop.

    [ rostedt@goodmis.org: ported to 3.0-rt ]

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 1bd1971588f3dfe28da96eb660e10d2f68c3e3f5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 21 21:06:43 2011 +0200

    softirq-make-fifo.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit eb6d2bb863ab9193a7addc0bf780496b28c61928
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 18 13:59:17 2011 +0200

    softirq-disable-softirq-stacks-for-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3c729d1a9e7e7552de474651f92189522292ea6f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 28 15:57:18 2011 +0200

    softirq-local-lock.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1716dde1349496239103a95d48c2abb5839d51b8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 21:51:45 2011 +0200

    mutex-no-spin-on-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ea318eede01dc0aeb9ea37b967efb9434f1cd968
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 18:51:23 2011 +0200

    lockdep-rt.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3387b912fa2cfec5df4ddf56fb8e6a71172524db
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 3 13:16:38 2009 -0500

    softirq: Sanitize softirq pending for NOHZ/RT

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 471373ce21ccf6e8d279423d616546be33ee2e5b
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Wed Feb 18 16:05:28 2015 +0100

    sunrpc: make svc_xprt_do_enqueue() use get_cpu_light()

    |BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:915
    |in_atomic(): 1, irqs_disabled(): 0, pid: 3194, name: rpc.nfsd
    |Preemption disabled at:[<ffffffffa06bf0bb>] svc_xprt_received+0x4b/0xc0 [sunrpc]
    |CPU: 6 PID: 3194 Comm: rpc.nfsd Not tainted 3.18.7-rt1 #9
    |Hardware name: MEDION MS-7848/MS-7848, BIOS M7848W08.404 11/06/2014
    | ffff880409630000 ffff8800d9a33c78 ffffffff815bdeb5 0000000000000002
    | 0000000000000000 ffff8800d9a33c98 ffffffff81073c86 ffff880408dd6008
    | ffff880408dd6000 ffff8800d9a33cb8 ffffffff815c3d84 ffff88040b3ac000
    |Call Trace:
    | [<ffffffff815bdeb5>] dump_stack+0x4f/0x9e
    | [<ffffffff81073c86>] __might_sleep+0xe6/0x150
    | [<ffffffff815c3d84>] rt_spin_lock+0x24/0x50
    | [<ffffffffa06beec0>] svc_xprt_do_enqueue+0x80/0x230 [sunrpc]
    | [<ffffffffa06bf0bb>] svc_xprt_received+0x4b/0xc0 [sunrpc]
    | [<ffffffffa06c03ed>] svc_add_new_perm_xprt+0x6d/0x80 [sunrpc]
    | [<ffffffffa06b2693>] svc_addsock+0x143/0x200 [sunrpc]
    | [<ffffffffa072e69c>] write_ports+0x28c/0x340 [nfsd]
    | [<ffffffffa072d2ac>] nfsctl_transaction_write+0x4c/0x80 [nfsd]
    | [<ffffffff8117ee83>] vfs_write+0xb3/0x1d0
    | [<ffffffff8117f889>] SyS_write+0x49/0xb0
    | [<ffffffff815c4556>] system_call_fastpath+0x16/0x1b

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e57d56df00c075856b7949047a938bb7561d383a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 16:29:27 2011 +0200

    net-netif_rx_ni-migrate-disable.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c04c69cb7fce0c16e19a669dfcdc3bad270124aa
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Nov 4 20:48:36 2011 +0100

    sched-clear-pf-thread-bound-on-fallback-rq.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 11cde089c2db6279b7a5ea21d16a1b277ed22e3a
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Mon Mar 24 13:18:48 2014 +0100

    sched: dont calculate hweight in update_migrate_disable()

    Proposal for a minor optimization in update_migrate_disable - its only a few
    instructions saved but those are in the hot path of locks so it might be worth
    it

    When being scheduled out while migrate_disable > 0 and migrate_disabled_updated
    is not yet set we end up here (kernel/sched/core.c):

    static inline void update_migrate_disable(struct task_struct *p)
    {
            ...

            mask = tsk_cpus_allowed(p);

            if (p->sched_class->set_cpus_allowed)
                    p->sched_class->set_cpus_allowed(p, mask);
            p->nr_cpus_allowed = cpumask_weight(mask);

    as we only can get here if migrate_disable > 0 there is no need to calculate
    the cpumask_weight(mask) as tsk_cpus_allowed in that case will return
    cpumask_of(task_cpu(p)) which only can have a hamming weight of 1 anyway.
    So we can simply do:

            p->nr_cpus_allowed = 1;

    without changing the behavior.

    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a702619afc5da958aa565c6ab41f5406aff747fc
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 27 08:40:25 2011 -0400

    sched: Have migrate_disable ignore bounded threads

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Clark Williams <williams@redhat.com>
    Link: http://lkml.kernel.org/r/20110927124423.567944215@goodmis.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit d1b5e4ff3305e6c2160f0c05f89c73aa170a720c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Sep 27 08:40:24 2011 -0400

    sched: Do not compare cpu masks in scheduler

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Clark Williams <williams@redhat.com>
    Link: http://lkml.kernel.org/r/20110927124423.128129033@goodmis.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 8eff52453592eadcd7b7a6343d345fb1401d1939
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Wed Nov 20 07:22:09 2013 +0800

    allow preemption in recursive migrate_disable call

    Minor cleanup in migrate_disable/migrate_enable. The recursive case
    does not need to disable preemption as it is "pinned" to the current
    cpu any way so it is safe to preempt it.

    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 37f7a09b9b4717d28ed2d604caccaab6857487fb
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Sep 27 08:40:23 2011 -0400

    sched: Postpone actual migration disalbe to schedule

    The migrate_disable() can cause a bit of a overhead to the RT kernel,
    as changing the affinity is expensive to do at every lock encountered.
    As a running task can not migrate, the actual disabling of migration
    does not need to occur until the task is about to schedule out.

    In most cases, a task that disables migration will enable it before
    it schedules making this change improve performance tremendously.

    [ Frank Rowand: UP compile fix ]

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Clark Williams <williams@redhat.com>
    Link: http://lkml.kernel.org/r/20110927124422.779693167@goodmis.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 35c5cf17e6e3e1d91163881f327a10ab788b9396
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Sep 2 14:29:27 2011 +0200

    sched: teach migrate_disable about atomic contexts

     <NMI>  [<ffffffff812dafd8>] spin_bug+0x94/0xa8
     [<ffffffff812db07f>] do_raw_spin_lock+0x43/0xea
     [<ffffffff814fa9be>] _raw_spin_lock_irqsave+0x6b/0x85
     [<ffffffff8106ff9e>] ? migrate_disable+0x75/0x12d
     [<ffffffff81078aaf>] ? pin_current_cpu+0x36/0xb0
     [<ffffffff8106ff9e>] migrate_disable+0x75/0x12d
     [<ffffffff81115b9d>] pagefault_disable+0xe/0x1f
     [<ffffffff81047027>] copy_from_user_nmi+0x74/0xe6
     [<ffffffff810489d7>] perf_callchain_user+0xf3/0x135

    Now clearly we can't go around taking locks from NMI context, cure
    this by short-circuiting migrate_disable() when we're in an atomic
    context already.

    Add some extra debugging to avoid things like:

      preempt_disable()
      migrate_disable();

      preempt_enable();
      migrate_enable();

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1314967297.1301.14.camel@twins
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/n/tip-wbot4vsmwhi8vmbf83hsclk6@git.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit eba6041d75e0c37942e9979dd8cc8744738d667b
Author: Mike Galbraith <efault@gmx.de>
Date:   Tue Aug 23 16:12:43 2011 +0200

    sched, rt: Fix migrate_enable() thinko

    Assigning mask = tsk_cpus_allowed(p) after p->migrate_disable = 0 ensures
    that we won't see a mask change.. no push/pull, we stack tasks on one CPU.

    Also add a couple fields to sched_debug for the next guy.

    [ Build fix from Stratos Psomadakis <psomas@gentoo.org> ]

    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@us.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1314108763.6689.4.camel@marge.simson.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 4298b970112654548b5b0db533a5001261e08351
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Aug 11 15:14:58 2011 +0200

    sched: Generic migrate_disable

    Make migrate_disable() be a preempt_disable() for !rt kernels. This
    allows generic code to use it but still enforces that these code
    sections stay relatively small.

    A preemptible migrate_disable() accessible for general use would allow
    people growing arbitrary per-cpu crap instead of clean these things
    up.

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-275i87sl8e1jcamtchmehonm@git.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit db4562d6c61a10550675d2a0973e0a03908af0fa
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Aug 11 15:03:35 2011 +0200

    sched: Optimize migrate_disable

    Change from task_rq_lock() to raw_spin_lock(&rq->lock) to avoid a few
    atomic ops. See comment on why it should be safe.

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-cbz6hkl5r5mvwtx5s3tor2y6@git.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 8e13c57f32e9b5084327465612d173d10a922eef
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 19:48:20 2011 +0200

    migrate-disable-rt-variant.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 478ce8a11eb5fa636b95c8909e019a55087cab13
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Nov 16 13:19:35 2011 -0500

    tracing: Show padding as unsigned short

    RT added two bytes to trace migrate disable counting to the trace events
    and used two bytes of the padding to make the change. The structures and
    all were updated correctly, but the display in the event formats was
    not:

    cat /debug/tracing/events/sched/sched_switch/format

    name: sched_switch
    ID: 51
    format:
    	field:unsigned short common_type;	offset:0;	size:2;	signed:0;
    	field:unsigned char common_flags;	offset:2;	size:1;	signed:0;
    	field:unsigned char common_preempt_count;	offset:3;	size:1;	signed:0;
    	field:int common_pid;	offset:4;	size:4;	signed:1;
    	field:unsigned short common_migrate_disable;	offset:8;	size:2;	signed:0;
    	field:int common_padding;	offset:10;	size:2;	signed:0;

    The field for common_padding has the correct size and offset, but the
    use of "int" might confuse some parsers (and people that are reading
    it). This needs to be changed to "unsigned short".

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1321467575.4181.36.camel@frodo
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 0e6dabdb01220ca05877d262f6b7d3b694eab3f6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 21:56:42 2011 +0200

    ftrace-migrate-disable-tracing.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0073ff485a214c5c8b6f26fda2f7419a6f7999a6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 19:35:29 2011 +0200

    hotplug-use-migrate-disable.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0a09ad42049b9a0c6daaac447ecf068d4d2469dd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jun 16 13:26:08 2011 +0200

    sched-migrate-disable.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit fd60ae14c22c6ef89bd776d6085e49908a7862c8
Author: Yong Zhang <yong.zhang0@gmail.com>
Date:   Thu Jul 28 11:16:00 2011 +0800

    hotplug: Reread hotplug_pcp on pin_current_cpu() retry

    When retry happens, it's likely that the task has been migrated to
    another cpu (except unplug failed), but it still derefernces the
    original hotplug_pcp per cpu data.

    Update the pointer to hotplug_pcp in the retry path, so it points to
    the current cpu.

    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20110728031600.GA338@windriver.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 5a106e8b6b006c79774527025a084a29e83766cb
Author: Yong Zhang <yong.zhang0@gmail.com>
Date:   Sun Oct 16 18:56:43 2011 +0800

    hotplug: sync_unplug: No "
    " in task name

    Otherwise the output will look a little odd.

    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Link: http://lkml.kernel.org/r/1318762607-2261-2-git-send-email-yong.zhang0@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1b91a5f6beef94bb02dbe3e5716c1d9aebf11579
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 15 12:36:06 2011 +0200

    hotplug: Lightweight get online cpus

    get_online_cpus() is a heavy weight function which involves a global
    mutex. migrate_disable() wants a simpler construct which prevents only
    a CPU from going doing while a task is in a migrate disabled section.

    Implement a per cpu lockless mechanism, which serializes only in the
    real unplug case on a global mutex. That serialization affects only
    tasks on the cpu which should be brought down.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ccc4421e7030e201193de8ea3979c8b4ceebc850
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 29 11:01:51 2011 +0200

    stomp-machine-raw-lock.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 417f4fbc834430b0855cd07c1c89f23bf9381618
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:30:27 2009 -0500

    stop_machine: convert stop_machine_run() to PREEMPT_RT

    Instead of playing with non-preemption, introduce explicit
    startup serialization. This is more robust and cleaner as
    well.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    [bigeasy: XXX: stopper_lock -> stop_cpus_lock]

commit b668d16924a21f0fddcb5ff61790b7314f6a783b
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Mar 18 15:12:49 2013 -0400

    sched/workqueue: Only wake up idle workers if not blocked on sleeping spin lock

    In -rt, most spin_locks() turn into mutexes. One of these spin_lock
    conversions is performed on the workqueue gcwq->lock. When the idle
    worker is worken, the first thing it will do is grab that same lock and
    it too will block, possibly jumping into the same code, but because
    nr_running would already be decremented it prevents an infinite loop.

    But this is still a waste of CPU cycles, and it doesn't follow the method
    of mainline, as new workers should only be woken when a worker thread is
    truly going to sleep, and not just blocked on a spin_lock().

    Check the saved_state too before waking up new workers.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 8d4323eb95a8191050d11ad3f0fd5bc7262573d1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 13 21:42:19 2011 +0100

    sched: ttwu: Return success when only changing the saved_state value

    When a task blocks on a rt lock, it saves the current state in
    p->saved_state, so a lock related wake up will not destroy the
    original state.

    When a real wakeup happens, while the task is running due to a lock
    wakeup already, we update p->saved_state to TASK_RUNNING, but we do
    not return success, which might cause another wakeup in the waitqueue
    code and the task remains in the waitqueue list. Return success in
    that case as well.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9e736e01ce9ccf745b323ab8733893e123537905
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 18 17:03:52 2011 +0200

    sched: Disable CONFIG_RT_GROUP_SCHED on RT

    Carsten reported problems when running:

    	taskset 01 chrt -f 1 sleep 1

    from within rc.local on a F15 machine. The task stays running and
    never gets on the run queue because some of the run queues have
    rt_throttled=1 which does not go away. Works nice from a ssh login
    shell. Disabling CONFIG_RT_GROUP_SCHED solves that as well.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit fd84d4f2bf05b5e2b7415359220618cfab36f829
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 13 16:42:35 2011 +0200

    sched-disable-ttwu-queue.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 6df7cffe1722cd7dbea13c75a4699d68f9ee77f1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 22:51:33 2011 +0200

    cond-resched-lock-rt-tweak.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 08f9e1bd9d9b118e20a03430cc5f2739ca2a907f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 14 09:56:44 2011 +0200

    cond-resched-softirq-fix.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b460b2bed2095acfefe4ae31d202c6b6d23756a1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 7 11:25:03 2011 +0200

    sched-cond-resched.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0094fe6880e4c31153c7563f4638edfdadba4d23
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 7 09:19:06 2011 +0200

    sched-might-sleep-do-not-account-rcu-depth.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1d784e0cb0204c5409287fa248ad8244bcf4b13f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jun 25 09:21:04 2011 +0200

    sched-rt-mutex-wakeup.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 4eddf3c4cfbdf56ea02f27832a5b480b961afb94
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 6 12:20:33 2011 +0200

    sched-mmdrop-delayed.patch

    Needs thread context (pgd_lock) -> ifdeffed. workqueues wont work with
    RT

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a7f161409b4134017cc5ef7aa813fc5a7365f961
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 6 12:12:51 2011 +0200

    sched-limit-nr-migrate.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 8cdcae82961712bf8d7e13b08260acaf3d33288c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 31 16:59:16 2011 +0200

    sched-delay-put-task.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0e80c9361cf8e4dfbb60db9a05980aa4ea7d8a4d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 3 08:44:44 2009 -0500

    posix-timers: Avoid wakeups when no timers are active

    Waking the thread even when no timers are scheduled is useless.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 43eeb469e17a14fbec1c0e29b34a2ad11a5170d3
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Jul 3 08:30:00 2009 -0500

    posix-timers: Shorten posix_cpu_timers/<CPU> kernel thread names

    Shorten the softirq kernel thread names because they always overflow the
    limited comm length, appearing as "posix_cpu_timer" CPU# times.

    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 70755ff18d73fd9ac12c2303cf2cf49d09a34206
Author: John Stultz <johnstul@us.ibm.com>
Date:   Fri Jul 3 08:29:58 2009 -0500

    posix-timers: thread posix-cpu-timers on -rt

    posix-cpu-timer code takes non -rt safe locks in hard irq
    context. Move it to a thread.

    [ 3.0 fixes from Peter Zijlstra <peterz@infradead.org> ]

    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit de9b9c9ae141e607463158394b4593b00c65c8db
Author: Yang Shi <yang.shi@windriver.com>
Date:   Mon Sep 16 14:09:19 2013 -0700

    hrtimer: Move schedule_work call to helper thread

    When run ltp leapsec_timer test, the following call trace is caught:

    BUG: sleeping function called from invalid context at kernel/rtmutex.c:659
    in_atomic(): 1, irqs_disabled(): 1, pid: 0, name: swapper/1
    Preemption disabled at:[<ffffffff810857f3>] cpu_startup_entry+0x133/0x310

    CPU: 1 PID: 0 Comm: swapper/1 Not tainted 3.10.10-rt3 #2
    Hardware name: Intel Corporation Calpella platform/MATXM-CORE-411-B, BIOS 4.6.3 08/18/2010
    ffffffff81c2f800 ffff880076843e40 ffffffff8169918d ffff880076843e58
    ffffffff8106db31 ffff88007684b4a0 ffff880076843e70 ffffffff8169d9c0
    ffff88007684b4a0 ffff880076843eb0 ffffffff81059da1 0000001876851200
    Call Trace:
    <IRQ>  [<ffffffff8169918d>] dump_stack+0x19/0x1b
    [<ffffffff8106db31>] __might_sleep+0xf1/0x170
    [<ffffffff8169d9c0>] rt_spin_lock+0x20/0x50
    [<ffffffff81059da1>] queue_work_on+0x61/0x100
    [<ffffffff81065aa1>] clock_was_set_delayed+0x21/0x30
    [<ffffffff810883be>] do_timer+0x40e/0x660
    [<ffffffff8108f487>] tick_do_update_jiffies64+0xf7/0x140
    [<ffffffff8108fe42>] tick_check_idle+0x92/0xc0
    [<ffffffff81044327>] irq_enter+0x57/0x70
    [<ffffffff816a040e>] smp_apic_timer_interrupt+0x3e/0x9b
    [<ffffffff8169f80a>] apic_timer_interrupt+0x6a/0x70
    <EOI>  [<ffffffff8155ea1c>] ? cpuidle_enter_state+0x4c/0xc0
    [<ffffffff8155eb68>] cpuidle_idle_call+0xd8/0x2d0
    [<ffffffff8100b59e>] arch_cpu_idle+0xe/0x30
    [<ffffffff8108585e>] cpu_startup_entry+0x19e/0x310
    [<ffffffff8168efa2>] start_secondary+0x1ad/0x1b0

    The clock_was_set_delayed is called in hard IRQ handler (timer interrupt), which
    calls schedule_work.

    Under PREEMPT_RT_FULL, schedule_work calls spinlocks which could sleep, so it's
    not safe to call schedule_work in interrupt context.

    Reference upstream commit b68d61c705ef02384c0538b8d9374545097899ca
    (rt,ntp: Move call to schedule_delayed_work() to helper thread)
    from git://git.kernel.org/pub/scm/linux/kernel/git/rt/linux-stable-rt.git, which
    makes a similar change.

    add a helper thread which does the call to schedule_work and wake up that
    thread instead of calling schedule_work directly.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Yang Shi <yang.shi@windriver.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit bf35702d23d7c2b9eff62d0576639515981d0a5b
Author: Watanabe <shunsuke.watanabe@tel.com>
Date:   Sun Oct 28 11:13:44 2012 +0100

    hrtimer: Raise softirq if hrtimer irq stalled

    When the hrtimer stall detection hits the softirq is not raised.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3290c0c59ae1d2d293df9f66f88ae6b0b45de734
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 25 11:08:40 2012 +0100

    timer-fd: Prevent live lock

    If hrtimer_try_to_cancel() requires a retry, then depending on the
    priority setting te retry loop might prevent timer callback completion
    on RT. Prevent that by waiting for completion on RT, no change for a
    non RT kernel.

    Reported-by: Sankara Muthukrishnan <sankara.m@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 307b64370e950af55691760d0842f495796e7d82
Author: Juri Lelli <juri.lelli@gmail.com>
Date:   Tue May 13 15:30:20 2014 +0200

    sched/deadline: dl_task_timer has to be irqsafe

    As for rt_period_timer, dl_task_timer has to be irqsafe.

    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f84a767bfaa44a62d1397427aaeef031072632e6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 3 08:44:31 2009 -0500

    hrtimer: fixup hrtimer callback changes for preempt-rt

    In preempt-rt we can not call the callbacks which take sleeping locks
    from the timer interrupt context.

    Bring back the softirq split for now, until we fixed the signal
    delivery problem for real.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit fd2f982f51b13f97f759784038e0c1d4dfab7453
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:29:34 2009 -0500

    hrtimers: prepare full preemption

    Make cancellation of a running callback in softirq context safe
    against preemption.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 38ebe45639c6a99a436858568372a508874010e8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 21 15:23:39 2011 +0200

    timers: Avoid the switch timers base set to NULL trick on RT

    On RT that code is preemptible, so we cannot assign NULL to timers
    base as a preempter would spin forever in lock_timer_base().

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9daf15cd2f8e8526f6b5cdcd60594812291d9d20
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Aug 21 11:56:45 2009 +0200

    timer: delay waking softirqs from the jiffy tick

    People were complaining about broken balancing with the recent -rt
    series.

    A look at /proc/sched_debug yielded:

    cpu#0, 2393.874 MHz
      .nr_running                    : 0
      .load                          : 0
      .cpu_load[0]                   : 177522
      .cpu_load[1]                   : 177522
      .cpu_load[2]                   : 177522
      .cpu_load[3]                   : 177522
      .cpu_load[4]                   : 177522
    cpu#1, 2393.874 MHz
      .nr_running                    : 4
      .load                          : 4096
      .cpu_load[0]                   : 181618
      .cpu_load[1]                   : 180850
      .cpu_load[2]                   : 180274
      .cpu_load[3]                   : 179938
      .cpu_load[4]                   : 179758

    Which indicated the cpu_load computation was hosed, the 177522 value
    indicates that there is one RT task runnable. Initially I thought the
    old problem of calculating the cpu_load from a softirq had re-surfaced,
    however looking at the code shows its being done from scheduler_tick().

    [ we really should fix this RT/cfs interaction some day... ]

    A few trace_printk()s later:

        sirq-timer/1-19    [001]   174.289744:     19: 50:S ==> [001]     0:140:R <idle>
              <idle>-0     [001]   174.290724: enqueue_task_rt: adding task: 19/sirq-timer/1 with load: 177522
              <idle>-0     [001]   174.290725:      0:140:R   + [001]    19: 50:S sirq-timer/1
              <idle>-0     [001]   174.290730: scheduler_tick: current load: 177522
              <idle>-0     [001]   174.290732: scheduler_tick: current: 0/swapper
              <idle>-0     [001]   174.290736:      0:140:R ==> [001]    19: 50:R sirq-timer/1
        sirq-timer/1-19    [001]   174.290741: dequeue_task_rt: removing task: 19/sirq-timer/1 with load: 177522
        sirq-timer/1-19    [001]   174.290743:     19: 50:S ==> [001]     0:140:R <idle>

    We see that we always raise the timer softirq before doing the load
    calculation. Avoid this by re-ordering the scheduler_tick() call in
    update_process_times() to occur before we deal with timers.

    This lowers the load back to sanity and restores regular load-balancing
    behaviour.

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 771957d022b53d1602e5c8879323f4151dfba42c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:30:20 2009 -0500

    timers: preempt-rt support

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ff0a427ee0b1e4ae66531073584fc2d6fd3bcfee
Author: Zhao Hongjiang <zhaohongjiang@huawei.com>
Date:   Wed Apr 17 17:44:16 2013 +0800

    timers: prepare for full preemption improve

    wake_up should do nothing on the nort, so we should use wakeup_timer_waiters,
    also fix a spell mistake.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Zhao Hongjiang <zhaohongjiang@huawei.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    [bigeasy: s/CONFIG_PREEMPT_RT_BASE/CONFIG_PREEMPT_RT_FULL/]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit fef25ab2000888494816a62e5c717021ce8609bc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:29:34 2009 -0500

    timers: prepare for full preemption

    When softirqs can be preempted we need to make sure that cancelling
    the timer from the active thread can not deadlock vs. a running timer
    callback. Add a waitqueue to resolve that.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit d58ddaad4e49025ee3ea56162b56371a85e70d7b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:44:07 2009 -0500

    relay: fix timer madness

    remove timer calls (!!!) from deep within the tracing infrastructure.
    This was totally bogus code that can cause lockups and worse.  Poll
    the buffer every 2 jiffies for now.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 6587dc38b3b35b2c65132f718d8cd27008d2add7
Author: KOBAYASHI Yoshitake <yoshitake.kobayashi@toshiba.co.jp>
Date:   Sat Jul 23 11:57:36 2011 +0900

    ipc/mqueue: Add a critical section to avoid a deadlock

    (Repost for v3.0-rt1 and changed the distination addreses)
    I have tested the following patch on v3.0-rt1 with PREEMPT_RT_FULL.
    In POSIX message queue, if a sender process uses SCHED_FIFO and
    has a higher priority than a receiver process, the sender will
    be stuck at ipc/mqueue.c:452

      452                 while (ewp->state == STATE_PENDING)
      453                         cpu_relax();

    Description of the problem
     (receiver process)
       1. receiver changes sender's state to STATE_PENDING (mqueue.c:846)
       2. wake up sender process and "switch to sender" (mqueue.c:847)
          Note: This context switch only happens in PREEMPT_RT_FULL kernel.
     (sender process)
       3. sender check the own state in above loop (mqueue.c:452-453)
       *. receiver will never wake up and cannot change sender's state to
          STATE_READY because sender has higher priority

    Signed-off-by: Yoshitake Kobayashi <yoshitake.kobayashi@toshiba.co.jp>
    Cc: viro@zeniv.linux.org.uk
    Cc: dchinner@redhat.com
    Cc: npiggin@kernel.dk
    Cc: hch@lst.de
    Cc: arnd@arndb.de
    Link: http://lkml.kernel.org/r/4E2A38A0.1090601@toshiba.co.jp
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f64fc1dec47dca22d6aa024acb1f522443a967c0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:30:12 2009 -0500

    ipc: Make the ipc code -rt aware

    RT serializes the code with the (rt)spinlock but keeps preemption
    enabled. Some parts of the code need to be atomic nevertheless.

    Protect it with preempt_disable/enable_rt pairts.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 10bc179a9ee3a4e5470c11eb494c23ac2e1069c6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jun 18 18:16:14 2015 -0400

    panic: skip get_random_bytes for RT_FULL in init_oops_id

commit c8ff608493e49812d0ac82cfdcac0f00eace65f7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 17 21:33:18 2011 +0200

    radix-tree-rt-aware.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 47cd30be59d2518e25510245d672bd0f898fca62
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jan 28 17:14:16 2015 +0100

    mm/memcontrol: do no disable interrupts

    There are a few local_irq_disable() which then take sleeping locks. This
    patch converts them local locks.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit dc7eb5c1a48b830dff2a7510ede9876f7c192ec1
Author: Yang Shi <yang.shi@windriver.com>
Date:   Wed Oct 30 11:48:33 2013 -0700

    mm/memcontrol: Don't call schedule_work_on in preemption disabled context

    The following trace is triggered when running ltp oom test cases:

    BUG: sleeping function called from invalid context at kernel/rtmutex.c:659
    in_atomic(): 1, irqs_disabled(): 0, pid: 17188, name: oom03
    Preemption disabled at:[<ffffffff8112ba70>] mem_cgroup_reclaim+0x90/0xe0

    CPU: 2 PID: 17188 Comm: oom03 Not tainted 3.10.10-rt3 #2
    Hardware name: Intel Corporation Calpella platform/MATXM-CORE-411-B, BIOS 4.6.3 08/18/2010
    ffff88007684d730 ffff880070df9b58 ffffffff8169918d ffff880070df9b70
    ffffffff8106db31 ffff88007688b4a0 ffff880070df9b88 ffffffff8169d9c0
    ffff88007688b4a0 ffff880070df9bc8 ffffffff81059da1 0000000170df9bb0
    Call Trace:
    [<ffffffff8169918d>] dump_stack+0x19/0x1b
    [<ffffffff8106db31>] __might_sleep+0xf1/0x170
    [<ffffffff8169d9c0>] rt_spin_lock+0x20/0x50
    [<ffffffff81059da1>] queue_work_on+0x61/0x100
    [<ffffffff8112b361>] drain_all_stock+0xe1/0x1c0
    [<ffffffff8112ba70>] mem_cgroup_reclaim+0x90/0xe0
    [<ffffffff8112beda>] __mem_cgroup_try_charge+0x41a/0xc40
    [<ffffffff810f1c91>] ? release_pages+0x1b1/0x1f0
    [<ffffffff8106f200>] ? sched_exec+0x40/0xb0
    [<ffffffff8112cc87>] mem_cgroup_charge_common+0x37/0x70
    [<ffffffff8112e2c6>] mem_cgroup_newpage_charge+0x26/0x30
    [<ffffffff8110af68>] handle_pte_fault+0x618/0x840
    [<ffffffff8103ecf6>] ? unpin_current_cpu+0x16/0x70
    [<ffffffff81070f94>] ? migrate_enable+0xd4/0x200
    [<ffffffff8110cde5>] handle_mm_fault+0x145/0x1e0
    [<ffffffff810301e1>] __do_page_fault+0x1a1/0x4c0
    [<ffffffff8169c9eb>] ? preempt_schedule_irq+0x4b/0x70
    [<ffffffff8169e3b7>] ? retint_kernel+0x37/0x40
    [<ffffffff8103053e>] do_page_fault+0xe/0x10
    [<ffffffff8169e4c2>] page_fault+0x22/0x30

    So, to prevent schedule_work_on from being called in preempt disabled context,
    replace the pair of get/put_cpu() to get/put_cpu_light().

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Yang Shi <yang.shi@windriver.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c9fe7bec582f2c45d44ddc8457a8eead8f5a1cf2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 27 11:11:46 2012 +0200

    mm: page_alloc: Use local_lock_on() instead of plain spinlock

    The plain spinlock while sufficient does not update the local_lock
    internals. Use a proper local_lock function instead to ease debugging.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f3d06f313ad95095259352d001f017b924a416ed
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jun 18 18:16:13 2015 -0400

    slub: delay ctor until the object is requested

    It seems that allocation of plenty objects causes latency on ARM since that
    code can not be preempted

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e718326ae200033775e1c58aa4bead1305dd6e2c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 9 12:08:15 2013 +0100

    slub: Enable irqs for __GFP_WAIT

    SYSTEM_RUNNING might be too late for enabling interrupts. Allocations
    with GFP_WAIT can happen before that. So use this as an indicator.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b60288957ddcbe79d324350c3195641fc48e7acd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 25 10:32:35 2012 +0100

    mm: Enable SLUB for RT

    Make SLUB RT aware and remove the restriction in Kconfig.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit cd733e22830e93ed12e9a9781c7f116c4d340a48
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:44:03 2009 -0500

    mm: Allow only slub on RT

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ec318a376bd9a4cc2c96012e719d1c91fe131f4a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 9 10:33:09 2013 +0100

    mm: bounce: Use local_irq_save_nort

    kmap_atomic() is preemptible on RT.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 98f0046b808df2574f3191747281d74b0caaf6c7
Author: Frank Rowand <frank.rowand@am.sony.com>
Date:   Sat Oct 1 18:58:13 2011 -0700

    ARM: Initialize ptl->lock for vector page

    Without this patch, ARM can not use SPLIT_PTLOCK_CPUS if
    PREEMPT_RT_FULL=y because vectors_user_mapping() creates a
    VM_ALWAYSDUMP mapping of the vector page (address 0xffff0000), but no
    ptl->lock has been allocated for the page.  An attempt to coredump
    that page will result in a kernel NULL pointer dereference when
    follow_page() attempts to lock the page.

    The call tree to the NULL pointer dereference is:

       do_notify_resume()
          get_signal_to_deliver()
             do_coredump()
                elf_core_dump()
                   get_dump_page()
                      __get_user_pages()
                         follow_page()
                            pte_offset_map_lock() <----- a #define
                               ...
                                  rt_spin_lock()

    The underlying problem is exposed by mm-shrink-the-page-frame-to-rt-size.patch.

    Signed-off-by: Frank Rowand <frank.rowand@am.sony.com>
    Cc: Frank <Frank_Rowand@sonyusa.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/4E87C535.2030907@am.sony.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e6c00d6689f90f97b1403eb11fdd780b8fbcd1d1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:30:13 2009 -0500

    mm: make vmstat -rt aware

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 222f418839f94f9e041ffb28e0a572ab4a94b4e1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:29:51 2009 -0500

    mm: convert swap to percpu locked

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 6cbf8c8e41e9a204af9103727bdf70a0bb499547
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 21 16:47:49 2011 +0200

    mm-page-alloc-fix.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 42f648a79851a090fab192bcd417c28c7b47ff63
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jul 3 08:44:37 2009 -0500

    mm: page_alloc reduce lock sections further

    Split out the pages which are to be freed into a separate list and
    call free_pages_bulk() outside of the percpu page allocator locks.

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 5c3912713edd07136f84b0562aa642afc52a08df
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:29:37 2009 -0500

    mm: page_alloc: rt-friendly per-cpu pages

    rt-friendly per-cpu pages: convert the irqs-off per-cpu locking
    method into a preemptible, explicit-per-cpu-locks method.

    Contains fixes from:
    	 Peter Zijlstra <a.p.zijlstra@chello.nl>
    	 Thomas Gleixner <tglx@linutronix.de>

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit de3601528e4a1914e7e5b187f93a043cd7d198c1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jun 17 15:42:38 2011 +0200

    cpu-rt-variants.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7d55b0d63bbc086b702564fbeb3617f71e411ae9
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Fri Jan 17 20:41:58 2014 +0100

    use local spin_locks in local_lock

    Drop recursive call to migrate_disabel/enable for local_*lock* api
    reported by Steven Rostedt.

    local_lock will call migrate_disable via get_local_var - call tree is

    get_locked_var
     `-> local_lock(lvar)
           `-> __local_lock(&get_local_var(lvar));
                              `--> # define get_local_var(var) (*({
                                        migrate_disable();
                                        &__get_cpu_var(var); }))       \

    thus there should be no need to call migrate_disable/enable recursively in
    spin_try/lock/unlock. This patch addes a spin_trylock_local and replaces
    the migration disabling calls by the local calls.

    This patch is incomplete as it does not yet cover the _irq/_irqsave variants
    by local locks. This patch requires the API cleanup in kernel/softirq.c or
    it would break softirq_lock/unlock with respect to migration.

    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b42ff5142588ef7da442222b82886cacda0ce3a5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 20 09:03:47 2011 +0200

    rt-local-irq-lock.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 58f89df925dad3eaf14f5156f4ed9a21be351a73
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jun 24 18:40:37 2011 +0200

    local-var.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 43fb4df6b20d12b7a4a47ab43699489ad7bc325f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Mar 25 18:34:20 2014 +0100

    net: gianfar: do not disable interrupts

    each per-queue lock is taken with spin_lock_irqsave() except in the case
    where all of them are taken for some kind of serialisation. As an
    optimisation local_irq_save() is used so that lock_tx_qs() and
    lock_rx_qs() can use just the spin_lock() variant instead.
    On RT local_irq_save() behaves differently so we use the nort()
    variant.
    Lockdep screems easily by "ethtool -K eth0 rx off tx off"

    What remains is missing lockdep annotation that makes lockdep think
    lock_tx_qs() may cause a dead lock.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e6411c05ab2a79c14263d3ad1e97bae76ad59f02
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Jul 3 08:30:00 2009 -0500

    drivers/net: vortex fix locking issues

    Argh, cut and paste wasn't enough...

    Use this patch instead.  It needs an irq disable.  But, believe it or not,
    on SMP this is actually better.  If the irq is shared (as it is in Mark's
    case), we don't stop the irq of other devices from being handled on
    another CPU (unfortunately for Mark, he pinned all interrupts to one CPU).

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

     drivers/net/ethernet/3com/3c59x.c |    8 ++++----
     1 file changed, 4 insertions(+), 4 deletions(-)

    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 99ff637e4a5e4229a4e43b4b7541879242946710
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jun 20 11:36:54 2009 +0200

    drivers/net: fix livelock issues

    Preempt-RT runs into a live lock issue with the NETDEV_TX_LOCKED micro
    optimization. The reason is that the softirq thread is rescheduling
    itself on that return value. Depending on priorities it starts to
    monoplize the CPU and livelock on UP systems.

    Remove it.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 403c0563d64aa6ec98ad3befeb35ad14727610fc
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Aug 21 17:48:46 2013 +0200

    genirq: do not invoke the affinity callback via a workqueue

    Joe Korty reported, that __irq_set_affinity_locked() schedules a
    workqueue while holding a rawlock which results in a might_sleep()
    warning.
    This patch moves the invokation into a process context so that we only
    wakeup() a process while holding the lock.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f667bb5e93d0c073ad0b3b8674550856e40f4745
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 3 11:57:29 2011 +0200

    genirq-force-threading.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 597c5f30d594a30c88e21b436fc0a9bd6d2db9a5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:29:57 2009 -0500

    genirq: disable irqpoll on -rt

    Creates long latencies for no value

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 50203b76bb54ea15966ccaa511938b70cb1c9b29
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 18 10:22:04 2011 +0100

    genirq: Disable DEBUG_SHIRQ for rt

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit fc717deb8498b617f7d1e5cf3e1280491876514d
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jun 21 15:07:25 2013 -0400

    list_bl.h: make list head locking RT safe

    As per changes in include/linux/jbd_common.h for avoiding the
    bit_spin_locks on RT ("fs: jbd/jbd2: Make state lock and journal
    head lock rt safe") we do the same thing here.

    We use the non atomic __set_bit and __clear_bit inside the scope of
    the lock to preserve the ability of the existing LIST_DEBUG code to
    use the zero'th bit in the sanity checks.

    As a bit spinlock, we had no lockdep visibility into the usage
    of the list head locking.  Now, if we were to implement it as a
    standard non-raw spinlock, we would see:

    BUG: sleeping function called from invalid context at kernel/rtmutex.c:658
    in_atomic(): 1, irqs_disabled(): 0, pid: 122, name: udevd
    5 locks held by udevd/122:
     #0:  (&sb->s_type->i_mutex_key#7/1){+.+.+.}, at: [<ffffffff811967e8>] lock_rename+0xe8/0xf0
     #1:  (rename_lock){+.+...}, at: [<ffffffff811a277c>] d_move+0x2c/0x60
     #2:  (&dentry->d_lock){+.+...}, at: [<ffffffff811a0763>] dentry_lock_for_move+0xf3/0x130
     #3:  (&dentry->d_lock/2){+.+...}, at: [<ffffffff811a0734>] dentry_lock_for_move+0xc4/0x130
     #4:  (&dentry->d_lock/3){+.+...}, at: [<ffffffff811a0747>] dentry_lock_for_move+0xd7/0x130
    Pid: 122, comm: udevd Not tainted 3.4.47-rt62 #7
    Call Trace:
     [<ffffffff810b9624>] __might_sleep+0x134/0x1f0
     [<ffffffff817a24d4>] rt_spin_lock+0x24/0x60
     [<ffffffff811a0c4c>] __d_shrink+0x5c/0xa0
     [<ffffffff811a1b2d>] __d_drop+0x1d/0x40
     [<ffffffff811a24be>] __d_move+0x8e/0x320
     [<ffffffff811a278e>] d_move+0x3e/0x60
     [<ffffffff81199598>] vfs_rename+0x198/0x4c0
     [<ffffffff8119b093>] sys_renameat+0x213/0x240
     [<ffffffff817a2de5>] ? _raw_spin_unlock+0x35/0x60
     [<ffffffff8107781c>] ? do_page_fault+0x1ec/0x4b0
     [<ffffffff817a32ca>] ? retint_swapgs+0xe/0x13
     [<ffffffff813eb0e6>] ? trace_hardirqs_on_thunk+0x3a/0x3f
     [<ffffffff8119b0db>] sys_rename+0x1b/0x20
     [<ffffffff817a3b96>] system_call_fastpath+0x1a/0x1f

    Since we are only taking the lock during short lived list operations,
    lets assume for now that it being raw won't be a significant latency
    concern.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 98bbd705bb6fdeb5cd176ef6064f52ae41c30de2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 18 10:11:25 2011 +0100

    fs: jbd/jbd2: Make state lock and journal head lock rt safe

    bit_spin_locks break under RT.

    Based on a previous patch from Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

    --

     include/linux/buffer_head.h |   10 ++++++++++
     include/linux/jbd_common.h  |   24 ++++++++++++++++++++++++
     2 files changed, 34 insertions(+)

commit 0f755a819fc9aa3236078001e59949d472855013
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 18 09:18:52 2011 +0100

    buffer_head: Replace bh_uptodate_lock for -rt

    Wrap the bit_spin_lock calls into a separate inline and add the RT
    replacements with a real spinlock.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 78a3a18287b640d7ba15d604628cc0d77c9fd2aa
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 21 21:05:33 2011 +0200

    net-wireless-warn-nort.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 31c5352bd1f1c38de9ad71157475f2b93f618340
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 22 08:07:08 2011 +0200

    signal-fix-up-rcu-wreckage.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 37e8661105f76eee410420630c56936c430a8c1e
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jan 29 17:19:44 2015 +0100

    mm/workingset: do not protect workingset_shadow_nodes with irq off

    workingset_shadow_nodes is protected by local_irq_disable(). Some users
    use spin_lock_irq().
    Replace the irq/on with a local_lock(). Rename workingset_shadow_nodes
    so I catch users of it which will be introduced later.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 8f6cd5b3b4db77edd0ed4d328cc529f8d68fd21d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 3 08:44:34 2009 -0500

    mm: scatterlist dont disable irqs on RT

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 14f48d9f1a5e3b6d6506cb48ffd40697a87616de
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Nov 8 17:34:54 2013 +0100

    usb: use _nort in giveback

    Since commit 94dfd7ed ("USB: HCD: support giveback of URB in tasklet
    context") I see

    |BUG: sleeping function called from invalid context at kernel/rtmutex.c:673
    |in_atomic(): 0, irqs_disabled(): 1, pid: 109, name: irq/11-uhci_hcd
    |no locks held by irq/11-uhci_hcd/109.
    |irq event stamp: 440
    |hardirqs last  enabled at (439): [<ffffffff816a7555>] _raw_spin_unlock_irqrestore+0x75/0x90
    |hardirqs last disabled at (440): [<ffffffff81514906>] __usb_hcd_giveback_urb+0x46/0xc0
    |softirqs last  enabled at (0): [<ffffffff81081821>] copy_process.part.52+0x511/0x1510
    |softirqs last disabled at (0): [<          (null)>]           (null)
    |CPU: 3 PID: 109 Comm: irq/11-uhci_hcd Not tainted 3.12.0-rt0-rc1+ #13
    |Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011
    | 0000000000000000 ffff8800db9ffbe0 ffffffff8169f064 0000000000000000
    | ffff8800db9ffbf8 ffffffff810b2122 ffff88020f03e888 ffff8800db9ffc18
    | ffffffff816a6944 ffffffff810b5748 ffff88020f03c000 ffff8800db9ffc50
    |Call Trace:
    | [<ffffffff8169f064>] dump_stack+0x4e/0x8f
    | [<ffffffff810b2122>] __might_sleep+0x112/0x190
    | [<ffffffff816a6944>] rt_spin_lock+0x24/0x60
    | [<ffffffff8158435b>] hid_ctrl+0x3b/0x190
    | [<ffffffff8151490f>] __usb_hcd_giveback_urb+0x4f/0xc0
    | [<ffffffff81514aaf>] usb_hcd_giveback_urb+0x3f/0x140
    | [<ffffffff815346af>] uhci_giveback_urb+0xaf/0x280
    | [<ffffffff8153666a>] uhci_scan_schedule+0x47a/0xb10
    | [<ffffffff81537336>] uhci_irq+0xa6/0x1a0
    | [<ffffffff81513c48>] usb_hcd_irq+0x28/0x40
    | [<ffffffff810c8ba3>] irq_forced_thread_fn+0x23/0x70
    | [<ffffffff810c918f>] irq_thread+0x10f/0x150
    | [<ffffffff810a6fad>] kthread+0xcd/0xe0
    | [<ffffffff816a842c>] ret_from_fork+0x7c/0xb0

    on -RT we run threaded so no need to disable interrupts.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c10d1d95d2ffe585845138de5a3fb4770b2e4ba3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:44:33 2009 -0500

    core: Do not disable interrupts on RT in res_counter.c

    Frederic Weisbecker reported this warning:

    [   45.228562] BUG: sleeping function called from invalid context at kernel/rtmutex.c:683
    [   45.228571] in_atomic(): 0, irqs_disabled(): 1, pid: 4290, name: ntpdate
    [   45.228576] INFO: lockdep is turned off.
    [   45.228580] irq event stamp: 0
    [   45.228583] hardirqs last  enabled at (0): [<(null)>] (null)
    [   45.228589] hardirqs last disabled at (0): [<ffffffff8025449d>] copy_process+0x68d/0x1500
    [   45.228602] softirqs last  enabled at (0): [<ffffffff8025449d>] copy_process+0x68d/0x1500
    [   45.228609] softirqs last disabled at (0): [<(null)>] (null)
    [   45.228617] Pid: 4290, comm: ntpdate Tainted: G        W  2.6.29-rc4-rt1-tip #1
    [   45.228622] Call Trace:
    [   45.228632]  [<ffffffff8027dfb0>] ? print_irqtrace_events+0xd0/0xe0
    [   45.228639]  [<ffffffff8024cd73>] __might_sleep+0x113/0x130
    [   45.228646]  [<ffffffff8077c811>] rt_spin_lock+0xa1/0xb0
    [   45.228653]  [<ffffffff80296a3d>] res_counter_charge+0x5d/0x130
    [   45.228660]  [<ffffffff802fb67f>] __mem_cgroup_try_charge+0x7f/0x180
    [   45.228667]  [<ffffffff802fc407>] mem_cgroup_charge_common+0x57/0x90
    [   45.228674]  [<ffffffff80212096>] ? ftrace_call+0x5/0x2b
    [   45.228680]  [<ffffffff802fc49d>] mem_cgroup_newpage_charge+0x5d/0x60
    [   45.228688]  [<ffffffff802d94ce>] __do_fault+0x29e/0x4c0
    [   45.228694]  [<ffffffff8077c843>] ? rt_spin_unlock+0x23/0x80
    [   45.228700]  [<ffffffff802db8b5>] handle_mm_fault+0x205/0x890
    [   45.228707]  [<ffffffff80212096>] ? ftrace_call+0x5/0x2b
    [   45.228714]  [<ffffffff8023495e>] do_page_fault+0x11e/0x2a0
    [   45.228720]  [<ffffffff8077e5a5>] page_fault+0x25/0x30
    [   45.228727]  [<ffffffff8043e1ed>] ? __clear_user+0x3d/0x70
    [   45.228733]  [<ffffffff8043e1d1>] ? __clear_user+0x21/0x70

    The reason is the raw IRQ flag use of kernel/res_counter.c.

    The irq flags tricks there seem a bit pointless: it cannot protect the
    c->parent linkage because local_irq_save() is only per CPU.

    So replace it with _nort(). This code needs a second look.

    Reported-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit db04f0c5516112d9e5f347905b57274811512682
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 21 23:06:05 2009 +0200

    core: Do not disable interrupts on RT in kernel/users.c

    Use the local_irq_*_nort variants to reduce latencies in RT. The code
    is serialized by the locks. No need to disable interrupts.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a25501eaa0c671243edf41e684aa9736191bd9c3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:30:16 2009 -0500

    input: gameport: Do not disable interrupts on PREEMPT_RT

    Use the _nort() primitives.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 5c926015aad841cb3f6036911966338d33c5cede
Author: Sven-Thorsten Dietrich <sdietrich@novell.com>
Date:   Fri Jul 3 08:30:35 2009 -0500

    infiniband: Mellanox IB driver patch use _nort() primitives

    Fixes in_atomic stack-dump, when Mellanox module is loaded into the RT
    Kernel.

    Michael S. Tsirkin <mst@dev.mellanox.co.il> sayeth:
    "Basically, if you just make spin_lock_irqsave (and spin_lock_irq) not disable
    interrupts for non-raw spinlocks, I think all of infiniband will be fine without
    changes."

    Signed-off-by: Sven-Thorsten Dietrich <sven@thebigcorporation.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 737467ddee5cf30146895fc15ef30ea86c009f05
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:30:16 2009 -0500

    ide: Do not disable interrupts for PREEMPT-RT

    Use the local_irq_*_nort variants.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3d8ebb7178dd0c54e59c16938b11b439e9cafdc8
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Jul 3 08:44:29 2009 -0500

    ata: Do not disable interrupts in ide code for preempt-rt

    Use the local_irq_*_nort variants.

    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 57628e45b8372c17e4c5d285b3377f03cf4c0b8e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 24 12:38:56 2009 +0200

    preempt: Provide preempt_*_(no)rt variants

    RT needs a few preempt_disable/enable points which are not necessary
    otherwise. Implement variants to avoid #ifdeffery.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a39859ded930181ebde4584709c6ed52ae70aa3a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 21 22:34:14 2009 +0200

    rt: local_irq_* variants depending on RT/!RT

    Add local_irq_*_(no)rt variant which are mainly used to break
    interrupt disabled sections on PREEMPT_RT or to explicitely disable
    interrupts on PREEMPT_RT.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3931e7ae598fc6c93ce5dbeba7363f35085bd8ea
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:29:58 2009 -0500

    bug: BUG_ON/WARN_ON variants dependend on RT/!RT

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 444a614b8d1964204c3e8b7236360e533192976a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 29 14:58:57 2011 +0200

    kconfig-preempt-rt-full.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c4731a36a2653fa76bdb4c916b5543d7d4beebf8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 24 12:11:43 2011 +0200

    kconfig-disable-a-few-options-rt.patch

    Disable stuff which is known to have issues on RT

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9b1697e8ea90daed8508475b36ee66f0e54815a5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jun 17 12:39:57 2011 +0200

    rt-preempt-base-config.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3cb624193998bf6cd78e0dd23d5a59a53d512ada
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Sep 2 14:29:33 2011 +0200

    printk: 'force_early_printk' boot param to help with debugging

    Gives me an option to screw printk and actually see what the machine
    says.

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1314967289.1301.11.camel@twins
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/n/tip-ykb97nsfmobq44xketrxs977@git.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3289e3eaf103600fbd9d12706b9f9aab99e119f0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 22 17:58:40 2011 +0200

    printk-kill.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 8006083dc41f8d51019b7ffa4ce7c3b1029daab7
Author: Mike Galbraith <bitbucket@online.de>
Date:   Fri Aug 30 07:57:25 2013 +0200

    hwlat-detector: Don't ignore threshold module parameter

    If the user specified a threshold at module load time, use it.

    Cc: stable-rt@vger.kernel.org
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Mike Galbraith <bitbucket@online.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 3b53ce09eae3ea471233216ded9a3b51200191a0
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Aug 19 17:33:27 2013 -0400

    hwlat-detector: Use thread instead of stop machine

    There's no reason to use stop machine to search for hardware latency.
    Simply disabling interrupts while running the loop will do enough to
    check if something comes in that wasn't disabled by interrupts being
    off, which is exactly what stop machine does.

    Instead of using stop machine, just have the thread disable interrupts
    while it checks for hardware latency.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 2408f5fc725bed11fcc0cd6b854267c57fb8b96e
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Aug 19 17:33:26 2013 -0400

    hwlat-detector: Use trace_clock_local if available

    As ktime_get() calls into the timing code which does a read_seq(), it
    may be affected by other CPUS that touch that lock. To remove this
    dependency, use the trace_clock_local() which is already exported
    for module use. If CONFIG_TRACING is enabled, use that as the clock,
    otherwise use ktime_get().

    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit dab54aa40225752280f6ffd8add08fe320749652
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Aug 19 17:33:25 2013 -0400

    hwlat-detector: Update hwlat_detector to add outer loop detection

    The hwlat_detector reads two timestamps in a row, then reports any
    gap between those calls. The problem is, it misses everything between
    the second reading of the time stamp to the first reading of the time stamp
    in the next loop. That's were most of the time is spent, which means,
    chances are likely that it will miss all hardware latencies. This
    defeats the purpose.

    By also testing the first time stamp from the previous loop second
    time stamp (the outer loop), we are more likely to find a latency.

    Setting the threshold to 1, here's what the report now looks like:

    1347415723.0232202770	0	2
    1347415725.0234202822	0	2
    1347415727.0236202875	0	2
    1347415729.0238202928	0	2
    1347415731.0240202980	0	2
    1347415734.0243203061	0	2
    1347415736.0245203113	0	2
    1347415738.0247203166	2	0
    1347415740.0249203219	0	3
    1347415742.0251203272	0	3
    1347415743.0252203299	0	3
    1347415745.0254203351	0	2
    1347415747.0256203404	0	2
    1347415749.0258203457	0	2
    1347415751.0260203510	0	2
    1347415754.0263203589	0	2
    1347415756.0265203642	0	2
    1347415758.0267203695	0	2
    1347415760.0269203748	0	2
    1347415762.0271203801	0	2
    1347415764.0273203853	2	0

    There's some hardware latency that takes 2 microseconds to run.

    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 4b314557bd4574fa35f62bb01e7b36e200078ef8
Author: Carsten Emde <C.Emde@osadl.org>
Date:   Tue Jul 19 13:53:12 2011 +0100

    hwlatdetect.patch

    Jon Masters developed this wonderful SMI detector. For details please
    consult Documentation/hwlat_detector.txt. It could be ported to Linux
    3.0 RT without any major change.

    Signed-off-by: Carsten Emde <C.Emde@osadl.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 4e0391bd7f49b3512bc3a1069b037267836f58a4
Author: Carsten Emde <C.Emde@osadl.org>
Date:   Tue Jul 19 14:03:41 2011 +0100

    latency-hist.patch

    This patch provides a recording mechanism to store data of potential
    sources of system latencies. The recordings separately determine the
    latency caused by a delayed timer expiration, by a delayed wakeup of the
    related user space program and by the sum of both. The histograms can be
    enabled and reset individually. The data are accessible via the debug
    filesystem. For details please consult Documentation/trace/histograms.txt.

    Signed-off-by: Carsten Emde <C.Emde@osadl.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 98ea0645b443656c93fb4ea4cfa4880f39001987
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 1 00:07:16 2011 +0100

    pci: Use __wake_up_all_locked pci_unblock_user_cfg_access()

    The waitqueue is protected by the pci_lock, so we can just avoid to
    lock the waitqueue lock itself. That prevents the
    might_sleep()/scheduling while atomic problem on RT

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1ff88560e5bef326a43c3dc2b5c6f5a45a793214
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Jun 18 18:16:05 2015 -0400

    x86: Do not disable preemption in int3 on 32bit

    Preemption must be disabled before enabling interrupts in do_trap
    on x86_64 because the stack in use for int3 and debug is a per CPU
    stack set by th IST. But 32bit does not have an IST and the stack
    still belongs to the current task and there is no problem in scheduling
    out the task.

    Keep preemption enabled on X86_32 when enabling interrupts for
    do_trap().

    The name of the function is changed from preempt_conditional_sti/cli()
    to conditional_sti/cli_ist(), to annotate that this function is used
    when the stack is on the IST.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 6bd0adca2c08be6af9f94ff0389a19aeb167b661
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:29:27 2009 -0500

    x86: Do not unmask io_apic when interrupt is in progress

    With threaded interrupts we might see an interrupt in progress on
    migration. Do not unmask it when this is the case.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b88020a80cddcb7a105f2e2ca1f9ea9cd7f56786
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 28 15:46:49 2011 +0200

    softirq-split-out-code.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7777b9116c010f094af99d1bff3a1de3352b213f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 28 15:44:15 2011 +0200

    softirq-thread-do-softirq.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ee256bfc1e6b4be14477f8363256fa61df425c07
Author: Marc Kleine-Budde <mkl@pengutronix.de>
Date:   Wed Mar 5 00:49:47 2014 +0100

    net: sched: dev_deactivate_many(): use msleep(1) instead of yield() to wait for outstanding qdisc_run calls

    On PREEMPT_RT enabled systems the interrupt handler run as threads at prio 50
    (by default). If a high priority userspace process tries to shut down a busy
    network interface it might spin in a yield loop waiting for the device to
    become idle. With the interrupt thread having a lower priority than the
    looping process it might never be scheduled and so result in a deadlock on UP
    systems.

    With Magic SysRq the following backtrace can be produced:

    > test_app R running      0   174    168 0x00000000
    > [<c02c7070>] (__schedule+0x220/0x3fc) from [<c02c7870>] (preempt_schedule_irq+0x48/0x80)
    > [<c02c7870>] (preempt_schedule_irq+0x48/0x80) from [<c0008fa8>] (svc_preempt+0x8/0x20)
    > [<c0008fa8>] (svc_preempt+0x8/0x20) from [<c001a984>] (local_bh_enable+0x18/0x88)
    > [<c001a984>] (local_bh_enable+0x18/0x88) from [<c025316c>] (dev_deactivate_many+0x220/0x264)
    > [<c025316c>] (dev_deactivate_many+0x220/0x264) from [<c023be04>] (__dev_close_many+0x64/0xd4)
    > [<c023be04>] (__dev_close_many+0x64/0xd4) from [<c023be9c>] (__dev_close+0x28/0x3c)
    > [<c023be9c>] (__dev_close+0x28/0x3c) from [<c023f7f0>] (__dev_change_flags+0x88/0x130)
    > [<c023f7f0>] (__dev_change_flags+0x88/0x130) from [<c023f904>] (dev_change_flags+0x10/0x48)
    > [<c023f904>] (dev_change_flags+0x10/0x48) from [<c024c140>] (do_setlink+0x370/0x7ec)
    > [<c024c140>] (do_setlink+0x370/0x7ec) from [<c024d2f0>] (rtnl_newlink+0x2b4/0x450)
    > [<c024d2f0>] (rtnl_newlink+0x2b4/0x450) from [<c024cfa0>] (rtnetlink_rcv_msg+0x158/0x1f4)
    > [<c024cfa0>] (rtnetlink_rcv_msg+0x158/0x1f4) from [<c0256740>] (netlink_rcv_skb+0xac/0xc0)
    > [<c0256740>] (netlink_rcv_skb+0xac/0xc0) from [<c024bbd8>] (rtnetlink_rcv+0x18/0x24)
    > [<c024bbd8>] (rtnetlink_rcv+0x18/0x24) from [<c02561b8>] (netlink_unicast+0x13c/0x198)
    > [<c02561b8>] (netlink_unicast+0x13c/0x198) from [<c025651c>] (netlink_sendmsg+0x264/0x2e0)
    > [<c025651c>] (netlink_sendmsg+0x264/0x2e0) from [<c022af98>] (sock_sendmsg+0x78/0x98)
    > [<c022af98>] (sock_sendmsg+0x78/0x98) from [<c022bb50>] (___sys_sendmsg.part.25+0x268/0x278)
    > [<c022bb50>] (___sys_sendmsg.part.25+0x268/0x278) from [<c022cf08>] (__sys_sendmsg+0x48/0x78)
    > [<c022cf08>] (__sys_sendmsg+0x48/0x78) from [<c0009320>] (ret_fast_syscall+0x0/0x2c)

    This patch works around the problem by replacing yield() by msleep(1), giving
    the interrupt thread time to finish, similar to other changes contained in the
    rt patch set. Using wait_for_completion() instead would probably be a better
    solution.

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Marc Kleine-Budde <mkl@pengutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 83a1acbc9e9e8c34169744c6d124c98ee28a41ae
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 28 10:59:58 2011 +0200

    net-flip-lock-dep-thingy.patch

    =======================================================
    [ INFO: possible circular locking dependency detected ]
    3.0.0-rc3+ #26
    -------------------------------------------------------
    ip/1104 is trying to acquire lock:
     (local_softirq_lock){+.+...}, at: [<ffffffff81056d12>] __local_lock+0x25/0x68

    but task is already holding lock:
     (sk_lock-AF_INET){+.+...}, at: [<ffffffff81433308>] lock_sock+0x10/0x12

    which lock already depends on the new lock.

    the existing dependency chain (in reverse order) is:

    -> #1 (sk_lock-AF_INET){+.+...}:
           [<ffffffff810836e5>] lock_acquire+0x103/0x12e
           [<ffffffff813e2781>] lock_sock_nested+0x82/0x92
           [<ffffffff81433308>] lock_sock+0x10/0x12
           [<ffffffff81433afa>] tcp_close+0x1b/0x355
           [<ffffffff81453c99>] inet_release+0xc3/0xcd
           [<ffffffff813dff3f>] sock_release+0x1f/0x74
           [<ffffffff813dffbb>] sock_close+0x27/0x2b
           [<ffffffff81129c63>] fput+0x11d/0x1e3
           [<ffffffff81126577>] filp_close+0x70/0x7b
           [<ffffffff8112667a>] sys_close+0xf8/0x13d
           [<ffffffff814ae882>] system_call_fastpath+0x16/0x1b

    -> #0 (local_softirq_lock){+.+...}:
           [<ffffffff81082ecc>] __lock_acquire+0xacc/0xdc8
           [<ffffffff810836e5>] lock_acquire+0x103/0x12e
           [<ffffffff814a7e40>] _raw_spin_lock+0x3b/0x4a
           [<ffffffff81056d12>] __local_lock+0x25/0x68
           [<ffffffff81056d8b>] local_bh_disable+0x36/0x3b
           [<ffffffff814a7fc4>] _raw_write_lock_bh+0x16/0x4f
           [<ffffffff81433c38>] tcp_close+0x159/0x355
           [<ffffffff81453c99>] inet_release+0xc3/0xcd
           [<ffffffff813dff3f>] sock_release+0x1f/0x74
           [<ffffffff813dffbb>] sock_close+0x27/0x2b
           [<ffffffff81129c63>] fput+0x11d/0x1e3
           [<ffffffff81126577>] filp_close+0x70/0x7b
           [<ffffffff8112667a>] sys_close+0xf8/0x13d
           [<ffffffff814ae882>] system_call_fastpath+0x16/0x1b

    other info that might help us debug this:

     Possible unsafe locking scenario:

           CPU0                    CPU1
           ----                    ----
      lock(sk_lock-AF_INET);
                                   lock(local_softirq_lock);
                                   lock(sk_lock-AF_INET);
      lock(local_softirq_lock);

     *** DEADLOCK ***

    1 lock held by ip/1104:
     #0:  (sk_lock-AF_INET){+.+...}, at: [<ffffffff81433308>] lock_sock+0x10/0x12

    stack backtrace:
    Pid: 1104, comm: ip Not tainted 3.0.0-rc3+ #26
    Call Trace:
     [<ffffffff81081649>] print_circular_bug+0x1f8/0x209
     [<ffffffff81082ecc>] __lock_acquire+0xacc/0xdc8
     [<ffffffff81056d12>] ? __local_lock+0x25/0x68
     [<ffffffff810836e5>] lock_acquire+0x103/0x12e
     [<ffffffff81056d12>] ? __local_lock+0x25/0x68
     [<ffffffff81046c75>] ? get_parent_ip+0x11/0x41
     [<ffffffff814a7e40>] _raw_spin_lock+0x3b/0x4a
     [<ffffffff81056d12>] ? __local_lock+0x25/0x68
     [<ffffffff81046c8c>] ? get_parent_ip+0x28/0x41
     [<ffffffff81056d12>] __local_lock+0x25/0x68
     [<ffffffff81056d8b>] local_bh_disable+0x36/0x3b
     [<ffffffff81433308>] ? lock_sock+0x10/0x12
     [<ffffffff814a7fc4>] _raw_write_lock_bh+0x16/0x4f
     [<ffffffff81433c38>] tcp_close+0x159/0x355
     [<ffffffff81453c99>] inet_release+0xc3/0xcd
     [<ffffffff813dff3f>] sock_release+0x1f/0x74
     [<ffffffff813dffbb>] sock_close+0x27/0x2b
     [<ffffffff81129c63>] fput+0x11d/0x1e3
     [<ffffffff81126577>] filp_close+0x70/0x7b
     [<ffffffff8112667a>] sys_close+0xf8/0x13d
     [<ffffffff814ae882>] system_call_fastpath+0x16/0x1b

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 589b1a123ce420658922ffc11c8f8ff17a051478
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 14 10:52:34 2011 +0100

    sysctl-include-fix.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1474240006a94fef5ea7054edb975c4063ff6b83
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 15 21:24:27 2011 +0200

    rwsem-inlcude-fix.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit c7fdd890802ca2332b0a8370d636991c1278db7e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 21 11:24:35 2011 +0200

    mm-page-alloc-use-list-last-entry.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 5fa3b565a6228980ad709b0745039474500a71a5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 15 10:29:00 2010 +0200

    suspend: Prevent might sleep splats

    timekeeping suspend/resume calls read_persistant_clock() which takes
    rtc_lock. That results in might sleep warnings because at that point
    we run with interrupts disabled.

    We cannot convert rtc_lock to a raw spinlock as that would trigger
    other might sleep warnings.

    As a temporary workaround we disable the might sleep warnings by
    setting system_state to SYSTEM_SUSPEND before calling sysdev_suspend()
    and restoring it to SYSTEM_RUNNING afer sysdev_resume().

    Needs to be revisited.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 0ee6f9a54a92fc47edb7f0696287e6ccb43d71d1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 25 22:06:27 2009 +0200

    mm: Remove preempt count from pagefault disable/enable

    Now that all users are cleaned up, we can remove the preemption count.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 38155f015e3b960ff5954743162d95fbc1d9b916
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Aug 5 17:16:58 2011 +0200

    mm: raw_pagefault_disable

    Adding migrate_disable() to pagefault_disable() to preserve the
    per-cpu thing for kmap_atomic might not have been the best of choices.
    But short of adding preempt_disable/migrate_disable foo all over the
    kmap code it still seems the best way.

    It does however yield the below borkage as well as wreck !-rt builds
    since !-rt does rely on pagefault_disable() not preempting. So fix all
    that up by adding raw_pagefault_disable().

     <NMI>  [<ffffffff81076d5c>] warn_slowpath_common+0x85/0x9d
     [<ffffffff81076e17>] warn_slowpath_fmt+0x46/0x48
     [<ffffffff814f7fca>] ? _raw_spin_lock+0x6c/0x73
     [<ffffffff810cac87>] ? watchdog_overflow_callback+0x9b/0xd0
     [<ffffffff810caca3>] watchdog_overflow_callback+0xb7/0xd0
     [<ffffffff810f51bb>] __perf_event_overflow+0x11c/0x1fe
     [<ffffffff810f298f>] ? perf_event_update_userpage+0x149/0x151
     [<ffffffff810f2846>] ? perf_event_task_disable+0x7c/0x7c
     [<ffffffff810f5b7c>] perf_event_overflow+0x14/0x16
     [<ffffffff81046e02>] x86_pmu_handle_irq+0xcb/0x108
     [<ffffffff814f9a6b>] perf_event_nmi_handler+0x46/0x91
     [<ffffffff814fb2ba>] notifier_call_chain+0x79/0xa6
     [<ffffffff814fb34d>] __atomic_notifier_call_chain+0x66/0x98
     [<ffffffff814fb2e7>] ? notifier_call_chain+0xa6/0xa6
     [<ffffffff814fb393>] atomic_notifier_call_chain+0x14/0x16
     [<ffffffff814fb3c3>] notify_die+0x2e/0x30
     [<ffffffff814f8f75>] do_nmi+0x7e/0x22b
     [<ffffffff814f8bca>] nmi+0x1a/0x2c
     [<ffffffff814fb130>] ? sub_preempt_count+0x4b/0xaa
     <<EOE>>  <IRQ>  [<ffffffff812d44cc>] delay_tsc+0xac/0xd1
     [<ffffffff812d4399>] __delay+0xf/0x11
     [<ffffffff812d95d9>] do_raw_spin_lock+0xd2/0x13c
     [<ffffffff814f813e>] _raw_spin_lock_irqsave+0x6b/0x85
     [<ffffffff8106772a>] ? task_rq_lock+0x35/0x8d
     [<ffffffff8106772a>] task_rq_lock+0x35/0x8d
     [<ffffffff8106fe2f>] migrate_disable+0x65/0x12c
     [<ffffffff81114e69>] pagefault_disable+0xe/0x1f
     [<ffffffff81039c73>] dump_trace+0x21f/0x2e2
     [<ffffffff8103ad79>] show_trace_log_lvl+0x54/0x5d
     [<ffffffff8103ad97>] show_trace+0x15/0x17
     [<ffffffff814f4f5f>] dump_stack+0x77/0x80
     [<ffffffff812d94b0>] spin_bug+0x9c/0xa3
     [<ffffffff81067745>] ? task_rq_lock+0x50/0x8d
     [<ffffffff812d954e>] do_raw_spin_lock+0x47/0x13c
     [<ffffffff814f7fbe>] _raw_spin_lock+0x60/0x73
     [<ffffffff81067745>] ? task_rq_lock+0x50/0x8d
     [<ffffffff81067745>] task_rq_lock+0x50/0x8d
     [<ffffffff8106fe2f>] migrate_disable+0x65/0x12c
     [<ffffffff81114e69>] pagefault_disable+0xe/0x1f
     [<ffffffff81039c73>] dump_trace+0x21f/0x2e2
     [<ffffffff8104369b>] save_stack_trace+0x2f/0x4c
     [<ffffffff810a7848>] save_trace+0x3f/0xaf
     [<ffffffff810aa2bd>] mark_lock+0x228/0x530
     [<ffffffff810aac27>] __lock_acquire+0x662/0x1812
     [<ffffffff8103dad4>] ? native_sched_clock+0x37/0x6d
     [<ffffffff810a790e>] ? trace_hardirqs_off_caller+0x1f/0x99
     [<ffffffff810693f6>] ? sched_rt_period_timer+0xbd/0x218
     [<ffffffff810ac403>] lock_acquire+0x145/0x18a
     [<ffffffff810693f6>] ? sched_rt_period_timer+0xbd/0x218
     [<ffffffff814f7f9e>] _raw_spin_lock+0x40/0x73
     [<ffffffff810693f6>] ? sched_rt_period_timer+0xbd/0x218
     [<ffffffff810693f6>] sched_rt_period_timer+0xbd/0x218
     [<ffffffff8109aa39>] __run_hrtimer+0x1e4/0x347
     [<ffffffff81069339>] ? can_migrate_task.clone.82+0x14a/0x14a
     [<ffffffff8109b97c>] hrtimer_interrupt+0xee/0x1d6
     [<ffffffff814fb23d>] ? add_preempt_count+0xae/0xb2
     [<ffffffff814ffb38>] smp_apic_timer_interrupt+0x85/0x98
     [<ffffffff814fef13>] apic_timer_interrupt+0x13/0x20

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-31keae8mkjiv8esq4rl76cib@git.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 7be67fb433a07e27820141e728d6d227c3a08861
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Aug 11 15:31:31 2011 +0200

    mm: pagefault_disabled()

    Wrap the test for pagefault_disabled() into a helper, this allows us
    to remove the need for current->pagefault_disabled on !-rt kernels.

    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-3yy517m8zsi9fpsf14xfaqkw@git.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2026d111f98d18a64cac1c8e2aba0c1c945b5d4a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 17 11:32:28 2011 +0100

    mm: Fixup all fault handlers to check current->pagefault_disable

    Necessary for decoupling pagefault disable from preempt count.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 22a8f4afd86db33d56831a68ccf3c912803a1459
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:30:37 2009 -0500

    mm: Prepare decoupling the page fault disabling logic

    Add a pagefault_disabled variable to task_struct to allow decoupling
    the pagefault-disabled logic from the preempt count.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f895dbd0f0fb00df4be46d66416e86e1f8035770
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:29:24 2009 -0500

    drivers/net: Use disable_irq_nosync() in 8139too

    Use disable_irq_nosync() instead of disable_irq() as this might be
    called in atomic context with netpoll.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f310542ac3a42dddf905bbfb09e0a99f8323ab45
Author: Benedikt Spranger <b.spranger@linutronix.de>
Date:   Mon Mar 8 18:57:04 2010 +0100

    clocksource: TCLIB: Allow higher clock rates for clock events

    As default the TCLIB uses the 32KiHz base clock rate for clock events.
    Add a compile time selection to allow higher clock resulution.

    (fixed up by Sami Pietik√§inen <Sami.Pietikainen@wapice.com>)

    Signed-off-by: Benedikt Spranger <b.spranger@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit ac6764b61b536e244f15a94e85f8f9260b716f85
Author: Benedikt Spranger <b.spranger@linutronix.de>
Date:   Sat Mar 6 17:47:10 2010 +0100

    ARM: AT91: PIT: Remove irq handler when clock event is unused

    Setup and remove the interrupt handler in clock event mode selection.
    This avoids calling the (shared) interrupt handler when the device is
    not used.

    Signed-off-by: Benedikt Spranger <b.spranger@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    [bigeasy: redo the patch with NR_IRQS_LEGACY which is probably required since
    commit 8fe82a55 ("ARM: at91: sparse irq support") which is included since v3.6.
    Patch based on what Sami Pietik√§inen <Sami.Pietikainen@wapice.com> suggested].
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit 8488db0f608509674e351dbc774821b785979152
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 3 08:29:30 2009 -0500

    drivers: random: Reduce preempt disabled region

    No need to keep preemption disabled across the whole function.

    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 878e2b9720a11be7c9bdca7245f803fce62c053f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jun 18 18:16:02 2015 -0400

    signal/x86: Delay calling signals in atomic

    On x86_64 we must disable preemption before we enable interrupts
    for stack faults, int3 and debugging, because the current task is using
    a per CPU debug stack defined by the IST. If we schedule out, another task
    can come in and use the same stack and cause the stack to be corrupted
    and crash the kernel on return.

    When CONFIG_PREEMPT_RT_FULL is enabled, spin_locks become mutexes, and
    one of these is the spin lock used in signal handling.

    Some of the debug code (int3) causes do_trap() to send a signal.
    This function calls a spin lock that has been converted to a mutex
    and has the possibility to sleep. If this happens, the above issues with
    the corrupted stack is possible.

    Instead of calling the signal right away, for PREEMPT_RT and x86_64,
    the signal information is stored on the stacks task_struct and
    TIF_NOTIFY_RESUME is set. Then on exit of the trap, the signal resume
    code will send the signal when preemption is enabled.

    [ rostedt: Switched from #ifdef CONFIG_PREEMPT_RT_FULL to
      ARCH_RT_DELAYS_SIGNAL_SEND and added comments to the code. ]

    Cc: stable-rt@vger.kernel.org
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 17f943f085048bdbeafa7fc853cd0924faaf9e21
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 3 08:44:56 2009 -0500

    signals: Allow rt tasks to cache one sigqueue struct

    To avoid allocation allow rt tasks to cache one sigqueue struct in
    task struct.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 9867af0a6edafdb4be20cb6b4c70dc179266d338
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 3 08:29:20 2009 -0500

    posix-timers: Prevent broadcast signals

    Posix timers should not send broadcast signals and kernel only
    signals. Prevent it.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 37d29af4abdfd53f0051df2cf1d54b39c334765b
Author: Frank Rowand <frank.rowand@am.sony.com>
Date:   Mon Sep 19 14:51:14 2011 -0700

    preempt-rt: Convert arm boot_lock to raw

    The arm boot_lock is used by the secondary processor startup code.  The locking
    task is the idle thread, which has idle->sched_class == &idle_sched_class.
    idle_sched_class->enqueue_task == NULL, so if the idle task blocks on the
    lock, the attempt to wake it when the lock becomes available will fail:

    try_to_wake_up()
       ...
          activate_task()
             enqueue_task()
                p->sched_class->enqueue_task(rq, p, flags)

    Fix by converting boot_lock to a raw spin lock.

    Signed-off-by: Frank Rowand <frank.rowand@am.sony.com>
    Link: http://lkml.kernel.org/r/4E77B952.3010606@am.sony.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f07c3e57be65eeb96aea96e9797566f51e651506
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Aug 29 18:21:04 2013 +0200

    ptrace: fix ptrace vs tasklist_lock race

    As explained by Alexander Fyodorov <halcy@yandex.ru>:

    |read_lock(&tasklist_lock) in ptrace_stop() is converted to mutex on RT kernel,
    |and it can remove __TASK_TRACED from task->state (by moving  it to
    |task->saved_state). If parent does wait() on child followed by a sys_ptrace
    |call, the following race can happen:
    |
    |- child sets __TASK_TRACED in ptrace_stop()
    |- parent does wait() which eventually calls wait_task_stopped() and returns
    |  child's pid
    |- child blocks on read_lock(&tasklist_lock) in ptrace_stop() and moves
    |  __TASK_TRACED flag to saved_state
    |- parent calls sys_ptrace, which calls ptrace_check_attach() and wait_task_inactive()

    The patch is based on his initial patch where an additional check is
    added in case the __TASK_TRACED moved to ->saved_state. The pi_lock is
    taken in case the caller is interrupted between looking into ->state and
    ->saved_state.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit f0305139607edc795a51080ead95ba37ddbe278d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 21 19:57:12 2011 +0200

    signal-revert-ptrace-preempt-magic.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit abb140db45faef203201e8d56d9bc8093432d1f8
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Sep 29 12:24:30 2011 -0500

    tracing: Account for preempt off in preempt_schedule()

    The preempt_schedule() uses the preempt_disable_notrace() version
    because it can cause infinite recursion by the function tracer as
    the function tracer uses preempt_enable_notrace() which may call
    back into the preempt_schedule() code as the NEED_RESCHED is still
    set and the PREEMPT_ACTIVE has not been set yet.

    See commit: d1f74e20b5b064a130cd0743a256c2d3cfe84010 that made this
    change.

    The preemptoff and preemptirqsoff latency tracers require the first
    and last preempt count modifiers to enable tracing. But this skips
    the checks. Since we can not convert them back to the non notrace
    version, we can use the idle() hooks for the latency tracers here.
    That is, the start/stop_critical_timings() works well to manually
    start and stop the latency tracer for preempt off timings.

    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Clark Williams <williams@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 878c892bbe2899add5af56bff2db205182b19d2e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 18 21:32:10 2011 +0200

    mips-enable-interrupts-in-signal.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2fb627cb2d731665cde7ea927265fb72f5a0f375
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 23 15:45:51 2013 +0200

    vtime-split-lock-and-seqcount.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit a3304e120757191ebc47d90c92ab2fd4715bf151
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 14 22:36:59 2013 +0100

    timekeeping-split-jiffies-lock.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2595f36acb8e8755cf26e0e4971d311af2c091b8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 22 19:47:02 2011 +0200

    block: Shorten interrupt disabled regions

    Moving the blk_sched_flush_plug() call out of the interrupt/preempt
    disabled region in the scheduler allows us to replace
    local_irq_save/restore(flags) by local_irq_disable/enable() in
    blk_flush_plug().

    Now instead of doing this we disable interrupts explicitely when we
    lock the request_queue and reenable them when we drop the lock. That
    allows interrupts to be handled when the plug list contains requests
    for more than one queue.

    Aside of that this change makes the scope of the irq disabled region
    more obvious. The current code confused the hell out of me when
    looking at:

     local_irq_save(flags);
       spin_lock(q->queue_lock);
       ...
       queue_unplugged(q...);
         scsi_request_fn();
           spin_unlock(q->queue_lock);
           spin_lock(shost->host_lock);
           spin_unlock_irq(shost->host_lock);

    -------------------^^^ ????

           spin_lock_irq(q->queue_lock);
           spin_unlock(q->lock);
     local_irq_restore(flags);

    Also add a comment to __blk_run_queue() documenting that
    q->request_fn() can drop q->queue_lock and reenable interrupts, but
    must return with q->queue_lock held and interrupts disabled.

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110622174919.025446432@linutronix.de
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit bdb2fead40b80c382d09eadfc52b9f148589918d
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Feb 12 16:01:13 2015 +0100

    gpio: omap: use raw locks for locking

    This patch converts gpio_bank.lock from a spin_lock into a
    raw_spin_lock. The call path is to access this lock is always under a
    raw_spin_lock, for instance
    - __setup_irq() holds &desc->lock with irq off
      + __irq_set_trigger()
       + omap_gpio_irq_type()

    - handle_level_irq() (runs with irqs off therefore raw locks)
      + mask_ack_irq()
       + omap_gpio_mask_irq()

    This fixes the obvious backtrace on -RT. However the locking vs context
    is not and this is not limited to -RT:
    - omap_gpio_irq_type() is called with IRQ off and has an conditional
      call to pm_runtime_get_sync() which may sleep. Either it may happen or
      it may not happen but pm_runtime_get_sync() should not be called with
      irqs off.

    - omap_gpio_debounce() is holding the lock with IRQs off.
      + omap2_set_gpio_debounce()
       + clk_prepare_enable()
        + clk_prepare() this one might sleep.
      The number of users of gpiod_set_debounce() / gpio_set_debounce()
      looks low but still this is not good.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e5b0df087de67eec5fd55aeffadb43443958ece4
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Mar 19 14:44:30 2013 +0100

    kernel/SRCU: provide a static initializer

    There are macros for static initializer for the three out of four
    possible notifier types, that are:
    	ATOMIC_NOTIFIER_HEAD()
    	BLOCKING_NOTIFIER_HEAD()
    	RAW_NOTIFIER_HEAD()

    This patch provides a static initilizer for the forth type to make it
    complete.

    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 609f63f27bbf61d6f1b1264fe161749da09a0878
Author: Allen Pais <allen.pais@oracle.com>
Date:   Fri Dec 13 09:44:41 2013 +0530

    sparc64: use generic rwsem spinlocks rt

    Signed-off-by: Allen Pais <allen.pais@oracle.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 64a909c25496307c3f37edb6271f347554359bf8
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Fri Aug 30 21:16:08 2013 +0400

    sparc: provide EARLY_PRINTK for SPARC

    sparc does not have CONFIG_EARLY_PRINTK option.

    So early-printk-consolidate.patch breaks compilation:

    arch/sparc/built-in.o: In function `setup_arch':
    (.init.text+0x15e4): undefined reference to `early_console'
    arch/sparc/built-in.o: In function `setup_arch':
    (.init.text+0x15ec): undefined reference to `early_console'

    The below addition fixes that.

    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 775b7bbadabb0b2060f5ddfb4df57a98e98afaa4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 23 11:04:08 2011 +0200

    early-printk-consolidate.patch

    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 40f6decb8178d7215ee45bb7faf0922a950c415a
Author: Anders Roxell <anders.roxell@linaro.org>
Date:   Mon Apr 27 22:53:09 2015 +0200

    arm64: Allow forced irq threading

    Now its safe to allow forced interrupt threading for arm64,
    all timer interrupts and the perf interrupt are marked NO_THREAD, as is
    the case with arch/arm: da0ec6f ARM: 7814/2: Allow forced irq threading

    Suggested-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 8f84ee9e86c65ce3b69bb2eac26ef30e40ccb4e8
Author: Anders Roxell <anders.roxell@linaro.org>
Date:   Mon Apr 27 22:53:08 2015 +0200

    arm64: Mark PMU interrupt IRQF_NO_THREAD

    Mark the PMU interrupts as non-threadable, as is the case with
    arch/arm: d9c3365 ARM: 7813/1: Mark pmu interupt IRQF_NO_THREAD

    Suggested-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 51bff1e70e134506c18d88eb185e3bfd6e53eaf4
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Nov 18 16:30:01 2014 +0800

    rcu: Revert "Allow post-unlock reference for rt_mutex" to avoid priority-inversion

    The patch dfeb9765ce3c ("Allow post-unlock reference for rt_mutex")
    ensured rcu-boost safe even the rt_mutex has post-unlock reference.

    But rt_mutex allowing post-unlock reference is definitely a bug and it was
    fixed by the commit 27e35715df54 ("rtmutex: Plug slow unlock race").
    This fix made the previous patch (dfeb9765ce3c) useless.

    And even worse, the priority-inversion introduced by the the previous
    patch still exists.

    rcu_read_unlock_special() {
    	rt_mutex_unlock(&rnp->boost_mtx);
    	/* Priority-Inversion:
    	 * the current task had been deboosted and preempted as a low
    	 * priority task immediately, it could wait long before reschedule in,
    	 * and the rcu-booster also waits on this low priority task and sleeps.
    	 * This priority-inversion makes rcu-booster can't work
    	 * as expected.
    	 */
    	complete(&rnp->boost_completion);
    }

    Just revert the patch to avoid it.

    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

Change-Id: I800c349d0fcb1245cf887dcefa35f7593b34b67a
---
 .../msm-3.18/Documentation/hwlat_detector.txt |   64 +
 kernel/msm-3.18/Documentation/sysrq.txt       |   11 +-
 .../Documentation/trace/histograms.txt        |  186 +
 kernel/msm-3.18/Makefile                      |    2 +-
 kernel/msm-3.18/arch/Kconfig                  |    2 +
 kernel/msm-3.18/arch/alpha/mm/fault.c         |    2 +-
 kernel/msm-3.18/arch/arm/Kconfig              |    1 +
 .../msm-3.18/arch/arm/include/asm/cmpxchg.h   |    2 +
 kernel/msm-3.18/arch/arm/include/asm/futex.h  |    5 +-
 .../msm-3.18/arch/arm/include/asm/switch_to.h |    8 +
 .../arch/arm/include/asm/thread_info.h        |    3 +
 kernel/msm-3.18/arch/arm/kernel/asm-offsets.c |    1 +
 kernel/msm-3.18/arch/arm/kernel/entry-armv.S  |   19 +-
 kernel/msm-3.18/arch/arm/kernel/process.c     |   24 +
 kernel/msm-3.18/arch/arm/kernel/signal.c      |    3 +-
 kernel/msm-3.18/arch/arm/kernel/smp.c         |    5 +-
 kernel/msm-3.18/arch/arm/kernel/unwind.c      |   14 +-
 kernel/msm-3.18/arch/arm/kvm/arm.c            |    4 +-
 kernel/msm-3.18/arch/arm/kvm/psci.c           |    4 +-
 .../arch/arm/mach-at91/at91rm9200_time.c      |    1 +
 .../msm-3.18/arch/arm/mach-exynos/platsmp.c   |   12 +-
 kernel/msm-3.18/arch/arm/mach-hisi/platmcpm.c |   26 +-
 .../msm-3.18/arch/arm/mach-omap2/omap-smp.c   |   10 +-
 .../msm-3.18/arch/arm/mach-prima2/platsmp.c   |   10 +-
 kernel/msm-3.18/arch/arm/mach-qcom/platsmp.c  |   10 +-
 kernel/msm-3.18/arch/arm/mach-spear/platsmp.c |   10 +-
 kernel/msm-3.18/arch/arm/mach-sti/platsmp.c   |   10 +-
 kernel/msm-3.18/arch/arm/mach-ux500/platsmp.c |   10 +-
 kernel/msm-3.18/arch/arm/mm/fault.c           |    8 +-
 kernel/msm-3.18/arch/arm/mm/highmem.c         |  101 +-
 .../arch/arm/plat-versatile/platsmp.c         |   10 +-
 kernel/msm-3.18/arch/arm64/Kconfig            |    2 +
 .../arch/arm64/include/asm/thread_info.h      |    3 +
 .../msm-3.18/arch/arm64/kernel/asm-offsets.c  |    1 +
 .../arch/arm64/kernel/debug-monitors.c        |   21 +-
 kernel/msm-3.18/arch/arm64/kernel/entry.S     |   13 +-
 .../msm-3.18/arch/arm64/kernel/perf_event.c   |  665 +---
 kernel/msm-3.18/arch/avr32/mm/fault.c         |    2 +-
 kernel/msm-3.18/arch/cris/mm/fault.c          |    2 +-
 kernel/msm-3.18/arch/frv/mm/fault.c           |    2 +-
 kernel/msm-3.18/arch/ia64/mm/fault.c          |    2 +-
 kernel/msm-3.18/arch/m32r/mm/fault.c          |    2 +-
 kernel/msm-3.18/arch/m68k/mm/fault.c          |    2 +-
 kernel/msm-3.18/arch/microblaze/mm/fault.c    |    2 +-
 kernel/msm-3.18/arch/mips/Kconfig             |    2 +-
 kernel/msm-3.18/arch/mips/kernel/signal.c     |    1 +
 kernel/msm-3.18/arch/mips/mm/fault.c          |    2 +-
 kernel/msm-3.18/arch/mips/mm/init.c           |    4 +-
 kernel/msm-3.18/arch/mn10300/mm/fault.c       |    2 +-
 kernel/msm-3.18/arch/parisc/mm/fault.c        |    2 +-
 kernel/msm-3.18/arch/powerpc/Kconfig          |    6 +-
 .../arch/powerpc/include/asm/kvm_host.h       |    4 +-
 .../arch/powerpc/include/asm/thread_info.h    |   15 +-
 .../arch/powerpc/kernel/asm-offsets.c         |    1 +
 .../msm-3.18/arch/powerpc/kernel/entry_32.S   |   17 +-
 .../msm-3.18/arch/powerpc/kernel/entry_64.S   |   14 +-
 kernel/msm-3.18/arch/powerpc/kernel/irq.c     |    2 +
 kernel/msm-3.18/arch/powerpc/kernel/misc_32.S |    2 +
 kernel/msm-3.18/arch/powerpc/kernel/misc_64.S |    2 +
 kernel/msm-3.18/arch/powerpc/kernel/time.c    |    2 +-
 kernel/msm-3.18/arch/powerpc/kvm/Kconfig      |    1 +
 kernel/msm-3.18/arch/powerpc/kvm/book3s_hv.c  |   20 +-
 kernel/msm-3.18/arch/powerpc/mm/fault.c       |    2 +-
 .../msm-3.18/arch/s390/include/asm/kvm_host.h |    2 +-
 kernel/msm-3.18/arch/s390/kvm/interrupt.c     |    8 +-
 kernel/msm-3.18/arch/s390/mm/fault.c          |    3 +-
 kernel/msm-3.18/arch/score/mm/fault.c         |    2 +-
 kernel/msm-3.18/arch/sh/kernel/irq.c          |    2 +
 kernel/msm-3.18/arch/sh/mm/fault.c            |    2 +-
 kernel/msm-3.18/arch/sparc/Kconfig            |   10 +-
 kernel/msm-3.18/arch/sparc/kernel/irq_64.c    |    2 +
 kernel/msm-3.18/arch/sparc/kernel/setup_32.c  |    1 +
 kernel/msm-3.18/arch/sparc/kernel/setup_64.c  |    8 +-
 kernel/msm-3.18/arch/sparc/mm/fault_32.c      |    2 +-
 kernel/msm-3.18/arch/sparc/mm/fault_64.c      |    2 +-
 kernel/msm-3.18/arch/tile/mm/fault.c          |    2 +-
 kernel/msm-3.18/arch/um/kernel/trap.c         |    2 +-
 kernel/msm-3.18/arch/x86/Kconfig              |    8 +-
 .../arch/x86/crypto/aesni-intel_glue.c        |   24 +-
 .../msm-3.18/arch/x86/crypto/cast5_avx_glue.c |   21 +-
 kernel/msm-3.18/arch/x86/crypto/glue_helper.c |   31 +-
 .../msm-3.18/arch/x86/include/asm/preempt.h   |   35 +-
 kernel/msm-3.18/arch/x86/include/asm/signal.h |   13 +
 .../arch/x86/include/asm/stackprotector.h     |   10 +-
 .../arch/x86/include/asm/thread_info.h        |    6 +
 .../msm-3.18/arch/x86/include/asm/uv/uv_bau.h |   14 +-
 .../msm-3.18/arch/x86/include/asm/uv/uv_hub.h |    2 +-
 .../msm-3.18/arch/x86/kernel/apic/io_apic.c   |    3 +-
 .../arch/x86/kernel/apic/x2apic_uv_x.c        |    2 +-
 kernel/msm-3.18/arch/x86/kernel/asm-offsets.c |    2 +
 .../msm-3.18/arch/x86/kernel/cpu/mcheck/mce.c |  126 +-
 .../x86/kernel/cpu/perf_event_intel_rapl.c    |   20 +-
 .../msm-3.18/arch/x86/kernel/dumpstack_32.c   |    4 +-
 .../msm-3.18/arch/x86/kernel/dumpstack_64.c   |    8 +-
 kernel/msm-3.18/arch/x86/kernel/entry_32.S    |   20 +-
 kernel/msm-3.18/arch/x86/kernel/entry_64.S    |   31 +-
 kernel/msm-3.18/arch/x86/kernel/irq_32.c      |    2 +
 kernel/msm-3.18/arch/x86/kernel/kvm.c         |   37 +-
 kernel/msm-3.18/arch/x86/kernel/process_32.c  |   32 +
 kernel/msm-3.18/arch/x86/kernel/signal.c      |    8 +
 kernel/msm-3.18/arch/x86/kernel/traps.c       |   28 +-
 kernel/msm-3.18/arch/x86/kvm/lapic.c          |   48 +-
 kernel/msm-3.18/arch/x86/kvm/x86.c            |    7 +
 kernel/msm-3.18/arch/x86/mm/fault.c           |    2 +-
 kernel/msm-3.18/arch/x86/mm/highmem_32.c      |    9 +-
 kernel/msm-3.18/arch/x86/mm/iomap_32.c        |   11 +-
 kernel/msm-3.18/arch/x86/mm/pageattr.c        |    8 +
 kernel/msm-3.18/arch/x86/platform/uv/tlb_uv.c |   26 +-
 .../msm-3.18/arch/x86/platform/uv/uv_time.c   |   21 +-
 kernel/msm-3.18/arch/xtensa/mm/fault.c        |    2 +-
 kernel/msm-3.18/block/blk-core.c              |   19 +-
 kernel/msm-3.18/block/blk-ioc.c               |    5 +-
 kernel/msm-3.18/block/blk-iopoll.c            |    3 +
 kernel/msm-3.18/block/blk-mq-cpu.c            |   17 +-
 kernel/msm-3.18/block/blk-mq.c                |   46 +-
 kernel/msm-3.18/block/blk-mq.h                |    9 +-
 kernel/msm-3.18/block/blk-softirq.c           |    3 +
 kernel/msm-3.18/block/bounce.c                |    4 +-
 kernel/msm-3.18/crypto/algapi.c               |    4 +-
 kernel/msm-3.18/crypto/api.c                  |    6 +-
 kernel/msm-3.18/crypto/internal.h             |    4 +-
 .../msm-3.18/drivers/acpi/acpica/acglobal.h   |    2 +-
 kernel/msm-3.18/drivers/acpi/acpica/hwregs.c  |    4 +-
 kernel/msm-3.18/drivers/acpi/acpica/hwxface.c |    4 +-
 kernel/msm-3.18/drivers/acpi/acpica/utmutex.c |    4 +-
 kernel/msm-3.18/drivers/ata/libata-sff.c      |   12 +-
 kernel/msm-3.18/drivers/block/zram/zram_drv.c |   30 +-
 kernel/msm-3.18/drivers/block/zram/zram_drv.h |   41 +
 kernel/msm-3.18/drivers/char/random.c         |   14 +-
 .../msm-3.18/drivers/clocksource/tcb_clksrc.c |   37 +-
 .../drivers/clocksource/timer-atmel-pit.c     |    4 +
 kernel/msm-3.18/drivers/cpufreq/Kconfig.x86   |    2 +-
 kernel/msm-3.18/drivers/cpufreq/cpufreq.c     |   36 +-
 kernel/msm-3.18/drivers/gpio/gpio-omap.c      |   74 +-
 .../msm-3.18/drivers/gpu/drm/i915/i915_gem.c  | 3451 +++++++++--------
 .../gpu/drm/i915/i915_gem_execbuffer.c        |  521 +--
 .../msm-3.18/drivers/gpu/drm/i915/i915_irq.c  |    2 +
 .../drivers/gpu/drm/i915/intel_sprite.c       | 1200 +++---
 .../drivers/gpu/drm/radeon/radeon_display.c   |    2 +
 kernel/msm-3.18/drivers/i2c/busses/i2c-omap.c |    5 +-
 kernel/msm-3.18/drivers/ide/alim15x3.c        |    4 +-
 kernel/msm-3.18/drivers/ide/hpt366.c          |    4 +-
 kernel/msm-3.18/drivers/ide/ide-io-std.c      |    8 +-
 kernel/msm-3.18/drivers/ide/ide-io.c          |    2 +-
 kernel/msm-3.18/drivers/ide/ide-iops.c        |    4 +-
 kernel/msm-3.18/drivers/ide/ide-probe.c       |    4 +-
 kernel/msm-3.18/drivers/ide/ide-taskfile.c    |    6 +-
 .../infiniband/ulp/ipoib/ipoib_multicast.c    |    4 +-
 .../drivers/input/gameport/gameport.c         |    8 +-
 kernel/msm-3.18/drivers/leds/trigger/Kconfig  |    2 +-
 kernel/msm-3.18/drivers/md/bcache/Kconfig     |    1 +
 kernel/msm-3.18/drivers/md/dm.c               |    4 +-
 kernel/msm-3.18/drivers/md/raid5.c            |    7 +-
 kernel/msm-3.18/drivers/md/raid5.h            |    1 +
 kernel/msm-3.18/drivers/misc/Kconfig          |   42 +-
 kernel/msm-3.18/drivers/misc/Makefile         |    1 +
 kernel/msm-3.18/drivers/misc/hwlat_detector.c | 1240 ++++++
 kernel/msm-3.18/drivers/mmc/host/mmci.c       |    5 -
 .../drivers/net/ethernet/3com/3c59x.c         |    8 +-
 .../net/ethernet/atheros/atl1c/atl1c_main.c   |    6 +-
 .../net/ethernet/atheros/atl1e/atl1e_main.c   |    3 +-
 .../drivers/net/ethernet/chelsio/cxgb/sge.c   |    3 +-
 .../drivers/net/ethernet/freescale/gianfar.c  |   12 +-
 .../drivers/net/ethernet/neterion/s2io.c      |    7 +-
 .../ethernet/oki-semi/pch_gbe/pch_gbe_main.c  |    6 +-
 .../drivers/net/ethernet/realtek/8139too.c    |    2 +-
 .../drivers/net/ethernet/tehuti/tehuti.c      |    9 +-
 kernel/msm-3.18/drivers/net/rionet.c          |    6 +-
 .../net/wireless/orinoco/orinoco_usb.c        |    2 +-
 kernel/msm-3.18/drivers/pci/access.c          |    2 +-
 .../drivers/pinctrl/qcom/pinctrl-msm.c        |   50 +-
 kernel/msm-3.18/drivers/scsi/fcoe/fcoe.c      |   20 +-
 kernel/msm-3.18/drivers/scsi/fcoe/fcoe_ctlr.c |    4 +-
 kernel/msm-3.18/drivers/scsi/libfc/fc_exch.c  |    4 +-
 kernel/msm-3.18/drivers/scsi/libsas/sas_ata.c |    4 +-
 .../drivers/scsi/qla2xxx/qla_inline.h         |    4 +-
 .../drivers/thermal/x86_pkg_temp_thermal.c    |   50 +-
 .../drivers/tty/serial/8250/8250_core.c       |   14 +-
 .../msm-3.18/drivers/tty/serial/amba-pl011.c  |   15 +-
 .../msm-3.18/drivers/tty/serial/omap-serial.c |   12 +-
 kernel/msm-3.18/drivers/usb/core/hcd.c        |    4 +-
 .../drivers/usb/gadget/function/f_fs.c        |    2 +-
 .../drivers/usb/gadget/legacy/inode.c         |    4 +-
 kernel/msm-3.18/fs/aio.c                      |   24 +-
 kernel/msm-3.18/fs/autofs4/autofs_i.h         |    1 +
 kernel/msm-3.18/fs/autofs4/expire.c           |    2 +-
 kernel/msm-3.18/fs/buffer.c                   |   21 +-
 kernel/msm-3.18/fs/dcache.c                   |   20 +-
 kernel/msm-3.18/fs/eventpoll.c                |    4 +-
 kernel/msm-3.18/fs/exec.c                     |    2 +
 kernel/msm-3.18/fs/f2fs/f2fs.h                | 2243 ++---------
 kernel/msm-3.18/fs/jbd/checkpoint.c           |    2 +
 kernel/msm-3.18/fs/jbd2/checkpoint.c          |    2 +
 kernel/msm-3.18/fs/namespace.c                |    8 +-
 kernel/msm-3.18/fs/ntfs/aops.c                |   14 +-
 kernel/msm-3.18/fs/timerfd.c                  |    5 +-
 kernel/msm-3.18/fs/xfs/xfs_linux.h            |    2 +-
 .../msm-3.18/include/acpi/platform/aclinux.h  |   15 +
 kernel/msm-3.18/include/asm-generic/bug.h     |   14 +
 kernel/msm-3.18/include/asm-generic/preempt.h |    4 +-
 kernel/msm-3.18/include/linux/blk-mq.h        |    1 +
 kernel/msm-3.18/include/linux/blkdev.h        |    3 +-
 kernel/msm-3.18/include/linux/bottom_half.h   |   12 +
 kernel/msm-3.18/include/linux/buffer_head.h   |   44 +
 kernel/msm-3.18/include/linux/cgroup.h        |    2 +
 kernel/msm-3.18/include/linux/completion.h    |    9 +-
 kernel/msm-3.18/include/linux/cpu.h           |    4 +
 kernel/msm-3.18/include/linux/delay.h         |    6 +
 kernel/msm-3.18/include/linux/ftrace.h        |   12 +
 kernel/msm-3.18/include/linux/ftrace_event.h  |    3 +
 kernel/msm-3.18/include/linux/highmem.h       |   28 +-
 kernel/msm-3.18/include/linux/hrtimer.h       |   16 +
 kernel/msm-3.18/include/linux/idr.h           |    4 +
 kernel/msm-3.18/include/linux/init_task.h     |   10 +-
 kernel/msm-3.18/include/linux/interrupt.h     |   73 +-
 kernel/msm-3.18/include/linux/irq.h           |    4 +-
 kernel/msm-3.18/include/linux/irq_work.h      |    7 +
 kernel/msm-3.18/include/linux/irqdesc.h       |    1 +
 kernel/msm-3.18/include/linux/irqflags.h      |   29 +-
 kernel/msm-3.18/include/linux/jbd_common.h    |   24 +
 kernel/msm-3.18/include/linux/jump_label.h    |    3 +-
 kernel/msm-3.18/include/linux/kdb.h           |    3 +-
 kernel/msm-3.18/include/linux/kernel.h        |    1 +
 kernel/msm-3.18/include/linux/kvm_host.h      |    4 +-
 kernel/msm-3.18/include/linux/lglock.h        |   27 +-
 kernel/msm-3.18/include/linux/list_bl.h       |   30 +-
 kernel/msm-3.18/include/linux/locallock.h     |  276 ++
 kernel/msm-3.18/include/linux/mm_types.h      |    5 +-
 kernel/msm-3.18/include/linux/module.h        |    6 +
 kernel/msm-3.18/include/linux/mutex.h         |   20 +-
 kernel/msm-3.18/include/linux/mutex_rt.h      |   89 +
 kernel/msm-3.18/include/linux/netdevice.h     |   10 +
 .../include/linux/netfilter/x_tables.h        |    7 +
 kernel/msm-3.18/include/linux/notifier.h      |   34 +-
 kernel/msm-3.18/include/linux/percpu.h        |   30 +
 kernel/msm-3.18/include/linux/pid.h           |    1 +
 kernel/msm-3.18/include/linux/preempt.h       |   74 +-
 kernel/msm-3.18/include/linux/printk.h        |    3 +-
 kernel/msm-3.18/include/linux/radix-tree.h    |    7 +-
 kernel/msm-3.18/include/linux/random.h        |    2 +-
 kernel/msm-3.18/include/linux/rcupdate.h      |   26 +
 kernel/msm-3.18/include/linux/rcutree.h       |   18 +-
 kernel/msm-3.18/include/linux/rtmutex.h       |   30 +-
 kernel/msm-3.18/include/linux/rwlock_rt.h     |   99 +
 kernel/msm-3.18/include/linux/rwlock_types.h  |    7 +-
 .../msm-3.18/include/linux/rwlock_types_rt.h  |   33 +
 kernel/msm-3.18/include/linux/rwsem.h         |    6 +
 kernel/msm-3.18/include/linux/rwsem_rt.h      |  134 +
 kernel/msm-3.18/include/linux/sched.h         |  249 +-
 kernel/msm-3.18/include/linux/seqlock.h       |   56 +-
 kernel/msm-3.18/include/linux/signal.h        |    1 +
 kernel/msm-3.18/include/linux/skbuff.h        |    7 +
 kernel/msm-3.18/include/linux/smp.h           |    3 +
 kernel/msm-3.18/include/linux/spinlock.h      |   12 +-
 .../msm-3.18/include/linux/spinlock_api_smp.h |    4 +-
 kernel/msm-3.18/include/linux/spinlock_rt.h   |  167 +
 .../msm-3.18/include/linux/spinlock_types.h   |   79 +-
 .../include/linux/spinlock_types_nort.h       |   33 +
 .../include/linux/spinlock_types_raw.h        |   56 +
 .../include/linux/spinlock_types_rt.h         |   51 +
 kernel/msm-3.18/include/linux/srcu.h          |    9 +-
 kernel/msm-3.18/include/linux/swap.h          |    4 +-
 kernel/msm-3.18/include/linux/sysctl.h        |    1 +
 kernel/msm-3.18/include/linux/thread_info.h   |   12 +-
 kernel/msm-3.18/include/linux/timer.h         |    2 +-
 kernel/msm-3.18/include/linux/uaccess.h       |   30 +-
 kernel/msm-3.18/include/linux/uprobes.h       |    1 +
 kernel/msm-3.18/include/linux/vmstat.h        |    4 +
 kernel/msm-3.18/include/linux/wait-simple.h   |  207 +
 kernel/msm-3.18/include/linux/wait.h          |    1 +
 kernel/msm-3.18/include/linux/work-simple.h   |   24 +
 kernel/msm-3.18/include/net/dst.h             |    2 +-
 kernel/msm-3.18/include/net/neighbour.h       |    4 +-
 kernel/msm-3.18/include/net/netns/ipv4.h      |    1 +
 kernel/msm-3.18/include/trace/events/hist.h   |   73 +
 .../include/trace/events/latency_hist.h       |   29 +
 kernel/msm-3.18/include/trace/events/sched.h  |   30 +-
 kernel/msm-3.18/init/Kconfig                  |    7 +-
 kernel/msm-3.18/init/Makefile                 |    2 +-
 kernel/msm-3.18/init/main.c                   |    1 +
 kernel/msm-3.18/ipc/mqueue.c                  |   71 +-
 kernel/msm-3.18/ipc/msg.c                     |   16 +-
 kernel/msm-3.18/ipc/sem.c                     |   10 +
 kernel/msm-3.18/kernel/Kconfig.locks          |    4 +-
 kernel/msm-3.18/kernel/Kconfig.preempt        |   33 +-
 kernel/msm-3.18/kernel/cgroup.c               |    9 +-
 kernel/msm-3.18/kernel/cpu.c                  |  344 +-
 kernel/msm-3.18/kernel/debug/kdb/kdb_io.c     |    6 +-
 kernel/msm-3.18/kernel/events/core.c          |    1 +
 kernel/msm-3.18/kernel/exit.c                 |    2 +-
 kernel/msm-3.18/kernel/fork.c                 |   40 +-
 kernel/msm-3.18/kernel/futex.c                |  117 +-
 kernel/msm-3.18/kernel/irq/handle.c           |    8 +-
 kernel/msm-3.18/kernel/irq/manage.c           |  260 +-
 kernel/msm-3.18/kernel/irq/settings.h         |   12 +
 kernel/msm-3.18/kernel/irq/spurious.c         |    8 +
 kernel/msm-3.18/kernel/irq_work.c             |   60 +-
 kernel/msm-3.18/kernel/ksysfs.c               |   12 +
 kernel/msm-3.18/kernel/locking/Makefile       |    9 +-
 kernel/msm-3.18/kernel/locking/lglock.c       |   79 +-
 kernel/msm-3.18/kernel/locking/lockdep.c      |   34 +-
 kernel/msm-3.18/kernel/locking/locktorture.c  |    1 -
 kernel/msm-3.18/kernel/locking/percpu-rwsem.c |    4 +
 kernel/msm-3.18/kernel/locking/rt.c           |  456 +++
 kernel/msm-3.18/kernel/locking/rtmutex.c      |  736 +++-
 .../msm-3.18/kernel/locking/rtmutex_common.h  |   14 +
 kernel/msm-3.18/kernel/locking/spinlock.c     |    7 +
 .../msm-3.18/kernel/locking/spinlock_debug.c  |    5 +
 kernel/msm-3.18/kernel/module.c               |   36 +-
 kernel/msm-3.18/kernel/panic.c                |    2 +
 kernel/msm-3.18/kernel/power/hibernate.c      |    7 +
 kernel/msm-3.18/kernel/power/suspend.c        |    4 +
 kernel/msm-3.18/kernel/printk/printk.c        |  150 +-
 kernel/msm-3.18/kernel/ptrace.c               |    9 +-
 kernel/msm-3.18/kernel/rcu/rcutorture.c       |    7 +
 kernel/msm-3.18/kernel/rcu/tiny.c             |    2 +
 kernel/msm-3.18/kernel/rcu/tree.c             |  149 +-
 kernel/msm-3.18/kernel/rcu/tree.h             |   10 +-
 kernel/msm-3.18/kernel/rcu/tree_plugin.h      |  164 +-
 kernel/msm-3.18/kernel/rcu/update.c           |    2 +
 kernel/msm-3.18/kernel/relay.c                |   14 +-
 kernel/msm-3.18/kernel/res_counter.c          |    8 +-
 kernel/msm-3.18/kernel/sched/Makefile         |    5 +-
 kernel/msm-3.18/kernel/sched/completion.c     |   34 +-
 kernel/msm-3.18/kernel/sched/core.c           |  561 ++-
 kernel/msm-3.18/kernel/sched/cputime.c        |   62 +-
 kernel/msm-3.18/kernel/sched/deadline.c       |    1 +
 kernel/msm-3.18/kernel/sched/debug.c          |    7 +
 kernel/msm-3.18/kernel/sched/fair.c           |   16 +-
 kernel/msm-3.18/kernel/sched/features.h       |   10 +-
 kernel/msm-3.18/kernel/sched/rt.c             |    1 +
 kernel/msm-3.18/kernel/sched/sched.h          |   10 +
 kernel/msm-3.18/kernel/sched/wait-simple.c    |  115 +
 kernel/msm-3.18/kernel/sched/work-simple.c    |  172 +
 kernel/msm-3.18/kernel/signal.c               |  135 +-
 kernel/msm-3.18/kernel/softirq.c              |  827 +++-
 kernel/msm-3.18/kernel/stop_machine.c         |   58 +-
 kernel/msm-3.18/kernel/time/hrtimer.c         |  346 +-
 kernel/msm-3.18/kernel/time/itimer.c          |    1 +
 kernel/msm-3.18/kernel/time/jiffies.c         |    7 +-
 kernel/msm-3.18/kernel/time/ntp.c             |   43 +
 .../msm-3.18/kernel/time/posix-cpu-timers.c   |  198 +-
 kernel/msm-3.18/kernel/time/posix-timers.c    |   37 +-
 .../kernel/time/tick-broadcast-hrtimer.c      |    1 +
 kernel/msm-3.18/kernel/time/tick-common.c     |   10 +-
 kernel/msm-3.18/kernel/time/tick-internal.h   |    3 +-
 kernel/msm-3.18/kernel/time/tick-sched.c      |   40 +-
 kernel/msm-3.18/kernel/time/timekeeping.c     |    6 +-
 kernel/msm-3.18/kernel/time/timer.c           |  112 +-
 kernel/msm-3.18/kernel/trace/Kconfig          |  104 +
 kernel/msm-3.18/kernel/trace/Makefile         |    4 +
 kernel/msm-3.18/kernel/trace/latency_hist.c   | 1178 ++++++
 kernel/msm-3.18/kernel/trace/trace.c          |   42 +-
 kernel/msm-3.18/kernel/trace/trace.h          |    2 +
 kernel/msm-3.18/kernel/trace/trace_events.c   |   10 +
 kernel/msm-3.18/kernel/trace/trace_irqsoff.c  |   11 +
 kernel/msm-3.18/kernel/trace/trace_output.c   |   18 +-
 .../kernel/trace/trace_sched_switch.c         |    2 +-
 .../kernel/trace/trace_sched_wakeup.c         |    2 +-
 kernel/msm-3.18/kernel/user.c                 |    4 +-
 kernel/msm-3.18/kernel/watchdog.c             |   15 +-
 kernel/msm-3.18/kernel/workqueue.c            |  225 +-
 kernel/msm-3.18/kernel/workqueue_internal.h   |    5 +-
 kernel/msm-3.18/lib/Kconfig                   |    1 +
 kernel/msm-3.18/lib/Kconfig.debug             |    2 +-
 kernel/msm-3.18/lib/debugobjects.c            |    5 +-
 kernel/msm-3.18/lib/dump_stack.c              |    6 +-
 kernel/msm-3.18/lib/idr.c                     |   36 +-
 kernel/msm-3.18/lib/locking-selftest.c        |   50 +
 kernel/msm-3.18/lib/percpu_ida.c              |   20 +-
 kernel/msm-3.18/lib/radix-tree.c              |   19 +-
 kernel/msm-3.18/lib/scatterlist.c             |    6 +-
 kernel/msm-3.18/lib/smp_processor_id.c        |    5 +-
 kernel/msm-3.18/localversion-rt               |    1 +
 kernel/msm-3.18/mm/Kconfig                    |    2 +-
 kernel/msm-3.18/mm/filemap.c                  |   11 +-
 kernel/msm-3.18/mm/highmem.c                  |    6 +-
 kernel/msm-3.18/mm/memcontrol.c               |   26 +-
 kernel/msm-3.18/mm/memory.c                   |   26 +
 kernel/msm-3.18/mm/mmu_context.c              |    2 +
 kernel/msm-3.18/mm/page_alloc.c               |  155 +-
 kernel/msm-3.18/mm/percpu.c                   |   37 +-
 kernel/msm-3.18/mm/slab.h                     |    4 +
 kernel/msm-3.18/mm/slub.c                     |  206 +-
 kernel/msm-3.18/mm/swap.c                     |   73 +-
 kernel/msm-3.18/mm/truncate.c                 |    7 +-
 kernel/msm-3.18/mm/vmalloc.c                  |   13 +-
 kernel/msm-3.18/mm/vmstat.c                   |    6 +
 kernel/msm-3.18/mm/workingset.c               |   23 +-
 kernel/msm-3.18/mm/zsmalloc.c                 |    6 +-
 kernel/msm-3.18/net/core/dev.c                |  163 +-
 kernel/msm-3.18/net/core/skbuff.c             |    6 +-
 kernel/msm-3.18/net/core/sock.c               |    3 +-
 kernel/msm-3.18/net/ipv4/icmp.c               |   38 +
 kernel/msm-3.18/net/ipv4/sysctl_net_ipv4.c    |    7 +
 kernel/msm-3.18/net/ipv4/tcp_ipv4.c           |    6 +
 kernel/msm-3.18/net/mac80211/rx.c             |    2 +-
 kernel/msm-3.18/net/netfilter/core.c          |    6 +
 kernel/msm-3.18/net/packet/af_packet.c        |    5 +-
 kernel/msm-3.18/net/rds/ib_rdma.c             |    3 +-
 kernel/msm-3.18/net/sched/sch_generic.c       |    2 +-
 kernel/msm-3.18/net/sunrpc/svc_xprt.c         |    4 +-
 kernel/msm-3.18/scripts/mkcompile_h           |    4 +-
 kernel/msm-3.18/sound/core/pcm_native.c       |    8 +-
 kernel/msm-3.18/virt/kvm/async_pf.c           |    4 +-
 kernel/msm-3.18/virt/kvm/kvm_main.c           |   16 +-
 406 files changed, 15382 insertions(+), 7065 deletions(-)
 create mode 100644 kernel/msm-3.18/Documentation/hwlat_detector.txt
 create mode 100644 kernel/msm-3.18/Documentation/trace/histograms.txt
 create mode 100644 kernel/msm-3.18/drivers/misc/hwlat_detector.c
 create mode 100644 kernel/msm-3.18/include/linux/locallock.h
 create mode 100644 kernel/msm-3.18/include/linux/mutex_rt.h
 create mode 100644 kernel/msm-3.18/include/linux/rwlock_rt.h
 create mode 100644 kernel/msm-3.18/include/linux/rwlock_types_rt.h
 create mode 100644 kernel/msm-3.18/include/linux/rwsem_rt.h
 create mode 100644 kernel/msm-3.18/include/linux/spinlock_rt.h
 create mode 100644 kernel/msm-3.18/include/linux/spinlock_types_nort.h
 create mode 100644 kernel/msm-3.18/include/linux/spinlock_types_raw.h
 create mode 100644 kernel/msm-3.18/include/linux/spinlock_types_rt.h
 create mode 100644 kernel/msm-3.18/include/linux/wait-simple.h
 create mode 100644 kernel/msm-3.18/include/linux/work-simple.h
 create mode 100644 kernel/msm-3.18/include/trace/events/hist.h
 create mode 100644 kernel/msm-3.18/include/trace/events/latency_hist.h
 create mode 100644 kernel/msm-3.18/kernel/locking/rt.c
 create mode 100644 kernel/msm-3.18/kernel/sched/wait-simple.c
 create mode 100644 kernel/msm-3.18/kernel/sched/work-simple.c
 create mode 100644 kernel/msm-3.18/kernel/trace/latency_hist.c
 create mode 100644 kernel/msm-3.18/localversion-rt

diff --git a/kernel/msm-3.18/Documentation/hwlat_detector.txt b/kernel/msm-3.18/Documentation/hwlat_detector.txt
new file mode 100644
index 000000000..cb6151648
--- /dev/null
+++ b/kernel/msm-3.18/Documentation/hwlat_detector.txt
@@ -0,0 +1,64 @@
+Introduction:
+-------------
+
+The module hwlat_detector is a special purpose kernel module that is used to
+detect large system latencies induced by the behavior of certain underlying
+hardware or firmware, independent of Linux itself. The code was developed
+originally to detect SMIs (System Management Interrupts) on x86 systems,
+however there is nothing x86 specific about this patchset. It was
+originally written for use by the "RT" patch since the Real Time
+kernel is highly latency sensitive.
+
+SMIs are usually not serviced by the Linux kernel, which typically does not
+even know that they are occuring. SMIs are instead are set up by BIOS code
+and are serviced by BIOS code, usually for "critical" events such as
+management of thermal sensors and fans. Sometimes though, SMIs are used for
+other tasks and those tasks can spend an inordinate amount of time in the
+handler (sometimes measured in milliseconds). Obviously this is a problem if
+you are trying to keep event service latencies down in the microsecond range.
+
+The hardware latency detector works by hogging all of the cpus for configurable
+amounts of time (by calling stop_machine()), polling the CPU Time Stamp Counter
+for some period, then looking for gaps in the TSC data. Any gap indicates a
+time when the polling was interrupted and since the machine is stopped and
+interrupts turned off the only thing that could do that would be an SMI.
+
+Note that the SMI detector should *NEVER* be used in a production environment.
+It is intended to be run manually to determine if the hardware platform has a
+problem with long system firmware service routines.
+
+Usage:
+------
+
+Loading the module hwlat_detector passing the parameter "enabled=1" (or by
+setting the "enable" entry in "hwlat_detector" debugfs toggled on) is the only
+step required to start the hwlat_detector. It is possible to redefine the
+threshold in microseconds (us) above which latency spikes will be taken
+into account (parameter "threshold=").
+
+Example:
+
+	# modprobe hwlat_detector enabled=1 threshold=100
+
+After the module is loaded, it creates a directory named "hwlat_detector" under
+the debugfs mountpoint, "/debug/hwlat_detector" for this text. It is necessary
+to have debugfs mounted, which might be on /sys/debug on your system.
+
+The /debug/hwlat_detector interface contains the following files:
+
+count			- number of latency spikes observed since last reset
+enable			- a global enable/disable toggle (0/1), resets count
+max			- maximum hardware latency actually observed (usecs)
+sample			- a pipe from which to read current raw sample data
+			  in the format <timestamp> <latency observed usecs>
+			  (can be opened O_NONBLOCK for a single sample)
+threshold		- minimum latency value to be considered (usecs)
+width			- time period to sample with CPUs held (usecs)
+			  must be less than the total window size (enforced)
+window			- total period of sampling, width being inside (usecs)
+
+By default we will set width to 500,000 and window to 1,000,000, meaning that
+we will sample every 1,000,000 usecs (1s) for 500,000 usecs (0.5s). If we
+observe any latencies that exceed the threshold (initially 100 usecs),
+then we write to a global sample ring buffer of 8K samples, which is
+consumed by reading from the "sample" (pipe) debugfs file interface.
diff --git a/kernel/msm-3.18/Documentation/sysrq.txt b/kernel/msm-3.18/Documentation/sysrq.txt
index 0e307c948..6964d0f80 100644
--- a/kernel/msm-3.18/Documentation/sysrq.txt
+++ b/kernel/msm-3.18/Documentation/sysrq.txt
@@ -59,10 +59,17 @@ On PowerPC - Press 'ALT - Print Screen (or F13) - <command key>,
 On other - If you know of the key combos for other architectures, please
            let me know so I can add them to this section.
 
-On all -  write a character to /proc/sysrq-trigger.  e.g.:
-
+On all -  write a character to /proc/sysrq-trigger, e.g.:
 		echo t > /proc/sysrq-trigger
 
+On all - Enable network SysRq by writing a cookie to icmp_echo_sysrq, e.g.
+		echo 0x01020304 >/proc/sys/net/ipv4/icmp_echo_sysrq
+	 Send an ICMP echo request with this pattern plus the particular
+	 SysRq command key. Example:
+		# ping -c1 -s57 -p0102030468
+	 will trigger the SysRq-H (help) command.
+
+
 *  What are the 'command' keys?
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 'b'     - Will immediately reboot the system without syncing or unmounting
diff --git a/kernel/msm-3.18/Documentation/trace/histograms.txt b/kernel/msm-3.18/Documentation/trace/histograms.txt
new file mode 100644
index 000000000..6f2aeabf7
--- /dev/null
+++ b/kernel/msm-3.18/Documentation/trace/histograms.txt
@@ -0,0 +1,186 @@
+		Using the Linux Kernel Latency Histograms
+
+
+This document gives a short explanation how to enable, configure and use
+latency histograms. Latency histograms are primarily relevant in the
+context of real-time enabled kernels (CONFIG_PREEMPT/CONFIG_PREEMPT_RT)
+and are used in the quality management of the Linux real-time
+capabilities.
+
+
+* Purpose of latency histograms
+
+A latency histogram continuously accumulates the frequencies of latency
+data. There are two types of histograms
+- potential sources of latencies
+- effective latencies
+
+
+* Potential sources of latencies
+
+Potential sources of latencies are code segments where interrupts,
+preemption or both are disabled (aka critical sections). To create
+histograms of potential sources of latency, the kernel stores the time
+stamp at the start of a critical section, determines the time elapsed
+when the end of the section is reached, and increments the frequency
+counter of that latency value - irrespective of whether any concurrently
+running process is affected by latency or not.
+- Configuration items (in the Kernel hacking/Tracers submenu)
+  CONFIG_INTERRUPT_OFF_LATENCY
+  CONFIG_PREEMPT_OFF_LATENCY
+
+
+* Effective latencies
+
+Effective latencies are actually occuring during wakeup of a process. To
+determine effective latencies, the kernel stores the time stamp when a
+process is scheduled to be woken up, and determines the duration of the
+wakeup time shortly before control is passed over to this process. Note
+that the apparent latency in user space may be somewhat longer, since the
+process may be interrupted after control is passed over to it but before
+the execution in user space takes place. Simply measuring the interval
+between enqueuing and wakeup may also not appropriate in cases when a
+process is scheduled as a result of a timer expiration. The timer may have
+missed its deadline, e.g. due to disabled interrupts, but this latency
+would not be registered. Therefore, the offsets of missed timers are
+recorded in a separate histogram. If both wakeup latency and missed timer
+offsets are configured and enabled, a third histogram may be enabled that
+records the overall latency as a sum of the timer latency, if any, and the
+wakeup latency. This histogram is called "timerandwakeup".
+- Configuration items (in the Kernel hacking/Tracers submenu)
+  CONFIG_WAKEUP_LATENCY
+  CONFIG_MISSED_TIMER_OFSETS
+
+
+* Usage
+
+The interface to the administration of the latency histograms is located
+in the debugfs file system. To mount it, either enter
+
+mount -t sysfs nodev /sys
+mount -t debugfs nodev /sys/kernel/debug
+
+from shell command line level, or add
+
+nodev	/sys			sysfs	defaults	0 0
+nodev	/sys/kernel/debug	debugfs	defaults	0 0
+
+to the file /etc/fstab. All latency histogram related files are then
+available in the directory /sys/kernel/debug/tracing/latency_hist. A
+particular histogram type is enabled by writing non-zero to the related
+variable in the /sys/kernel/debug/tracing/latency_hist/enable directory.
+Select "preemptirqsoff" for the histograms of potential sources of
+latencies and "wakeup" for histograms of effective latencies etc. The
+histogram data - one per CPU - are available in the files
+
+/sys/kernel/debug/tracing/latency_hist/preemptoff/CPUx
+/sys/kernel/debug/tracing/latency_hist/irqsoff/CPUx
+/sys/kernel/debug/tracing/latency_hist/preemptirqsoff/CPUx
+/sys/kernel/debug/tracing/latency_hist/wakeup/CPUx
+/sys/kernel/debug/tracing/latency_hist/wakeup/sharedprio/CPUx
+/sys/kernel/debug/tracing/latency_hist/missed_timer_offsets/CPUx
+/sys/kernel/debug/tracing/latency_hist/timerandwakeup/CPUx
+
+The histograms are reset by writing non-zero to the file "reset" in a
+particular latency directory. To reset all latency data, use
+
+#!/bin/sh
+
+TRACINGDIR=/sys/kernel/debug/tracing
+HISTDIR=$TRACINGDIR/latency_hist
+
+if test -d $HISTDIR
+then
+  cd $HISTDIR
+  for i in `find . | grep /reset$`
+  do
+    echo 1 >$i
+  done
+fi
+
+
+* Data format
+
+Latency data are stored with a resolution of one microsecond. The
+maximum latency is 10,240 microseconds. The data are only valid, if the
+overflow register is empty. Every output line contains the latency in
+microseconds in the first row and the number of samples in the second
+row. To display only lines with a positive latency count, use, for
+example,
+
+grep -v " 0$" /sys/kernel/debug/tracing/latency_hist/preemptoff/CPU0
+
+#Minimum latency: 0 microseconds.
+#Average latency: 0 microseconds.
+#Maximum latency: 25 microseconds.
+#Total samples: 3104770694
+#There are 0 samples greater or equal than 10240 microseconds
+#usecs	         samples
+    0	      2984486876
+    1	        49843506
+    2	        58219047
+    3	         5348126
+    4	         2187960
+    5	         3388262
+    6	          959289
+    7	          208294
+    8	           40420
+    9	            4485
+   10	           14918
+   11	           18340
+   12	           25052
+   13	           19455
+   14	            5602
+   15	             969
+   16	              47
+   17	              18
+   18	              14
+   19	               1
+   20	               3
+   21	               2
+   22	               5
+   23	               2
+   25	               1
+
+
+* Wakeup latency of a selected process
+
+To only collect wakeup latency data of a particular process, write the
+PID of the requested process to
+
+/sys/kernel/debug/tracing/latency_hist/wakeup/pid
+
+PIDs are not considered, if this variable is set to 0.
+
+
+* Details of the process with the highest wakeup latency so far
+
+Selected data of the process that suffered from the highest wakeup
+latency that occurred in a particular CPU are available in the file
+
+/sys/kernel/debug/tracing/latency_hist/wakeup/max_latency-CPUx.
+
+In addition, other relevant system data at the time when the
+latency occurred are given.
+
+The format of the data is (all in one line):
+<PID> <Priority> <Latency> (<Timeroffset>) <Command> \
+<- <PID> <Priority> <Command> <Timestamp>
+
+The value of <Timeroffset> is only relevant in the combined timer
+and wakeup latency recording. In the wakeup recording, it is
+always 0, in the missed_timer_offsets recording, it is the same
+as <Latency>.
+
+When retrospectively searching for the origin of a latency and
+tracing was not enabled, it may be helpful to know the name and
+some basic data of the task that (finally) was switching to the
+late real-tlme task. In addition to the victim's data, also the
+data of the possible culprit are therefore displayed after the
+"<-" symbol.
+
+Finally, the timestamp of the time when the latency occurred
+in <seconds>.<microseconds> after the most recent system boot
+is provided.
+
+These data are also reset when the wakeup histogram is reset.
diff --git a/kernel/msm-3.18/Makefile b/kernel/msm-3.18/Makefile
index 2ab322736..01275c101 100644
--- a/kernel/msm-3.18/Makefile
+++ b/kernel/msm-3.18/Makefile
@@ -406,7 +406,7 @@ KBUILD_CPPFLAGS := -D__KERNEL__
 KBUILD_CFLAGS   := -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs \
 		   -fno-strict-aliasing -fno-common \
 		   -Werror-implicit-function-declaration \
-		   -Wno-format-security \
+		   -Wno-format-security -fno-PIE \
 		   -std=gnu89
 
 KBUILD_AFLAGS_KERNEL :=
diff --git a/kernel/msm-3.18/arch/Kconfig b/kernel/msm-3.18/arch/Kconfig
index c484676a9..363c2c1a7 100644
--- a/kernel/msm-3.18/arch/Kconfig
+++ b/kernel/msm-3.18/arch/Kconfig
@@ -6,6 +6,7 @@ config OPROFILE
 	tristate "OProfile system profiling"
 	depends on PROFILING
 	depends on HAVE_OPROFILE
+	depends on !PREEMPT_RT_FULL
 	select RING_BUFFER
 	select RING_BUFFER_ALLOW_SWAP
 	help
@@ -49,6 +50,7 @@ config KPROBES
 config JUMP_LABEL
        bool "Optimize very unlikely/likely branches"
        depends on HAVE_ARCH_JUMP_LABEL
+       depends on (!INTERRUPT_OFF_HIST && !PREEMPT_OFF_HIST && !WAKEUP_LATENCY_HIST && !MISSED_TIMER_OFFSETS_HIST)
        help
          This option enables a transparent branch optimization that
 	 makes certain almost-always-true or almost-always-false branch
diff --git a/kernel/msm-3.18/arch/alpha/mm/fault.c b/kernel/msm-3.18/arch/alpha/mm/fault.c
index 9d0ac091a..870f62667 100644
--- a/kernel/msm-3.18/arch/alpha/mm/fault.c
+++ b/kernel/msm-3.18/arch/alpha/mm/fault.c
@@ -107,7 +107,7 @@ do_page_fault(unsigned long address, unsigned long mmcsr,
 
 	/* If we're in an interrupt context, or have no user context,
 	   we must not take the fault.  */
-	if (!mm || in_atomic())
+	if (!mm || pagefault_disabled())
 		goto no_context;
 
 #ifdef CONFIG_ALPHA_LARGE_VMALLOC
diff --git a/kernel/msm-3.18/arch/arm/Kconfig b/kernel/msm-3.18/arch/arm/Kconfig
index 90ae4a60f..a8e76f577 100644
--- a/kernel/msm-3.18/arch/arm/Kconfig
+++ b/kernel/msm-3.18/arch/arm/Kconfig
@@ -64,6 +64,7 @@ config ARM
 	select HAVE_PERF_EVENTS
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
+	select HAVE_PREEMPT_LAZY
 	select HAVE_RCU_TABLE_FREE if (SMP && ARM_LPAE)
 	select HAVE_REGS_AND_STACK_ACCESS_API
 	select HAVE_SYSCALL_TRACEPOINTS
diff --git a/kernel/msm-3.18/arch/arm/include/asm/cmpxchg.h b/kernel/msm-3.18/arch/arm/include/asm/cmpxchg.h
index abb2c3769..2386e9745 100644
--- a/kernel/msm-3.18/arch/arm/include/asm/cmpxchg.h
+++ b/kernel/msm-3.18/arch/arm/include/asm/cmpxchg.h
@@ -129,6 +129,8 @@ static inline unsigned long __xchg(unsigned long x, volatile void *ptr, int size
 
 #else	/* min ARCH >= ARMv6 */
 
+#define __HAVE_ARCH_CMPXCHG 1
+
 extern void __bad_cmpxchg(volatile void *ptr, int size);
 
 /*
diff --git a/kernel/msm-3.18/arch/arm/include/asm/futex.h b/kernel/msm-3.18/arch/arm/include/asm/futex.h
index 616db1c0b..d732d2e2b 100644
--- a/kernel/msm-3.18/arch/arm/include/asm/futex.h
+++ b/kernel/msm-3.18/arch/arm/include/asm/futex.h
@@ -106,8 +106,9 @@ futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,
 	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
 		return -EFAULT;
 
-	preempt_disable();
+	preempt_disable_rt();
 	__ua_flags = uaccess_save_and_enable();
+
 	__asm__ __volatile__("@futex_atomic_cmpxchg_inatomic\n"
 	"1:	" TUSER(ldr) "	%1, [%4]\n"
 	"	teq	%1, %2\n"
@@ -120,8 +121,8 @@ futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,
 	uaccess_restore(__ua_flags);
 
 	*uval = val;
-	preempt_enable();
 
+	preempt_enable_rt();
 	return ret;
 }
 
diff --git a/kernel/msm-3.18/arch/arm/include/asm/switch_to.h b/kernel/msm-3.18/arch/arm/include/asm/switch_to.h
index c99e25946..f3e3d800c 100644
--- a/kernel/msm-3.18/arch/arm/include/asm/switch_to.h
+++ b/kernel/msm-3.18/arch/arm/include/asm/switch_to.h
@@ -3,6 +3,13 @@
 
 #include <linux/thread_info.h>
 
+#if defined CONFIG_PREEMPT_RT_FULL && defined CONFIG_HIGHMEM
+void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p);
+#else
+static inline void
+switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p) { }
+#endif
+
 /*
  * For v7 SMP cores running a preemptible kernel we may be pre-empted
  * during a TLB maintenance operation, so execute an inner-shareable dsb
@@ -22,6 +29,7 @@ extern struct task_struct *__switch_to(struct task_struct *, struct thread_info
 
 #define switch_to(prev,next,last)					\
 do {									\
+	switch_kmaps(prev, next);					\
 	last = __switch_to(prev,task_thread_info(prev), task_thread_info(next));	\
 } while (0)
 
diff --git a/kernel/msm-3.18/arch/arm/include/asm/thread_info.h b/kernel/msm-3.18/arch/arm/include/asm/thread_info.h
index dfc237fd3..d662e5366 100644
--- a/kernel/msm-3.18/arch/arm/include/asm/thread_info.h
+++ b/kernel/msm-3.18/arch/arm/include/asm/thread_info.h
@@ -50,6 +50,7 @@ struct cpu_context_save {
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
+	int			preempt_lazy_count;	/* 0 => preemptable, <0 => bug */
 	mm_segment_t		addr_limit;	/* address limit */
 	struct task_struct	*task;		/* main task structure */
 	struct exec_domain	*exec_domain;	/* execution domain */
@@ -146,6 +147,7 @@ extern int vfp_restore_user_hwstate(struct user_vfp __user *,
 #define TIF_SIGPENDING		0
 #define TIF_NEED_RESCHED	1
 #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
+#define TIF_NEED_RESCHED_LAZY	3
 #define TIF_UPROBE		7
 #define TIF_SYSCALL_TRACE	8
 #define TIF_SYSCALL_AUDIT	9
@@ -160,6 +162,7 @@ extern int vfp_restore_user_hwstate(struct user_vfp __user *,
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
diff --git a/kernel/msm-3.18/arch/arm/kernel/asm-offsets.c b/kernel/msm-3.18/arch/arm/kernel/asm-offsets.c
index 72b4c3873..8d30457dd 100644
--- a/kernel/msm-3.18/arch/arm/kernel/asm-offsets.c
+++ b/kernel/msm-3.18/arch/arm/kernel/asm-offsets.c
@@ -55,6 +55,7 @@ int main(void)
   BLANK();
   DEFINE(TI_FLAGS,		offsetof(struct thread_info, flags));
   DEFINE(TI_PREEMPT,		offsetof(struct thread_info, preempt_count));
+  DEFINE(TI_PREEMPT_LAZY,	offsetof(struct thread_info, preempt_lazy_count));
   DEFINE(TI_ADDR_LIMIT,		offsetof(struct thread_info, addr_limit));
   DEFINE(TI_TASK,		offsetof(struct thread_info, task));
   DEFINE(TI_EXEC_DOMAIN,	offsetof(struct thread_info, exec_domain));
diff --git a/kernel/msm-3.18/arch/arm/kernel/entry-armv.S b/kernel/msm-3.18/arch/arm/kernel/entry-armv.S
index 40d7c6e23..aa64b3f6f 100644
--- a/kernel/msm-3.18/arch/arm/kernel/entry-armv.S
+++ b/kernel/msm-3.18/arch/arm/kernel/entry-armv.S
@@ -212,11 +212,18 @@ __irq_svc:
 #ifdef CONFIG_PREEMPT
 	get_thread_info tsk
 	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
-	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
 	teq	r8, #0				@ if preempt count != 0
+	bne	1f				@ return from exeption
+	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
+	tst	r0, #_TIF_NEED_RESCHED		@ if NEED_RESCHED is set
+	blne	svc_preempt			@ preempt!
+
+	ldr	r8, [tsk, #TI_PREEMPT_LAZY]	@ get preempt lazy count
+	teq	r8, #0				@ if preempt lazy count != 0
 	movne	r0, #0				@ force flags to 0
-	tst	r0, #_TIF_NEED_RESCHED
+	tst	r0, #_TIF_NEED_RESCHED_LAZY
 	blne	svc_preempt
+1:
 #endif
 
 	svc_exit r5, irq = 1			@ return from exception
@@ -231,8 +238,14 @@ svc_preempt:
 1:	bl	preempt_schedule_irq		@ irq en/disable is done inside
 	ldr	r0, [tsk, #TI_FLAGS]		@ get new tasks TI_FLAGS
 	tst	r0, #_TIF_NEED_RESCHED
+	bne	1b
+	tst	r0, #_TIF_NEED_RESCHED_LAZY
 	reteq	r8				@ go again
-	b	1b
+	ldr	r0, [tsk, #TI_PREEMPT_LAZY]	@ get preempt lazy count
+	teq	r0, #0				@ if preempt lazy count != 0
+	beq	1b
+	ret	r8				@ go again
+
 #endif
 
 __und_fault:
diff --git a/kernel/msm-3.18/arch/arm/kernel/process.c b/kernel/msm-3.18/arch/arm/kernel/process.c
index de57f006b..6b028fa50 100644
--- a/kernel/msm-3.18/arch/arm/kernel/process.c
+++ b/kernel/msm-3.18/arch/arm/kernel/process.c
@@ -599,6 +599,30 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 }
 
 #ifdef CONFIG_MMU
+/*
+ * CONFIG_SPLIT_PTLOCK_CPUS results in a page->ptl lock.  If the lock is not
+ * initialized by pgtable_page_ctor() then a coredump of the vector page will
+ * fail.
+ */
+static int __init vectors_user_mapping_init_page(void)
+{
+	struct page *page;
+	unsigned long addr = 0xffff0000;
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+
+	pgd = pgd_offset_k(addr);
+	pud = pud_offset(pgd, addr);
+	pmd = pmd_offset(pud, addr);
+	page = pmd_page(*(pmd));
+
+	pgtable_page_ctor(page);
+
+	return 0;
+}
+late_initcall(vectors_user_mapping_init_page);
+
 #ifdef CONFIG_KUSER_HELPERS
 /*
  * The vectors page is always readable from user space for the
diff --git a/kernel/msm-3.18/arch/arm/kernel/signal.c b/kernel/msm-3.18/arch/arm/kernel/signal.c
index 81d104cd5..7e5c5768b 100644
--- a/kernel/msm-3.18/arch/arm/kernel/signal.c
+++ b/kernel/msm-3.18/arch/arm/kernel/signal.c
@@ -579,7 +579,8 @@ asmlinkage int
 do_work_pending(struct pt_regs *regs, unsigned int thread_flags, int syscall)
 {
 	do {
-		if (likely(thread_flags & _TIF_NEED_RESCHED)) {
+		if (likely(thread_flags & (_TIF_NEED_RESCHED |
+					   _TIF_NEED_RESCHED_LAZY))) {
 			schedule();
 		} else {
 			if (unlikely(!user_mode(regs)))
diff --git a/kernel/msm-3.18/arch/arm/kernel/smp.c b/kernel/msm-3.18/arch/arm/kernel/smp.c
index 227aaebbc..cbc3c0225 100644
--- a/kernel/msm-3.18/arch/arm/kernel/smp.c
+++ b/kernel/msm-3.18/arch/arm/kernel/smp.c
@@ -211,8 +211,6 @@ int __cpu_disable(void)
 	flush_cache_louis();
 	local_flush_tlb_all();
 
-	clear_tasks_mm_cpumask(cpu);
-
 	return 0;
 }
 
@@ -228,6 +226,9 @@ void __cpu_die(unsigned int cpu)
 		pr_err("CPU%u: cpu didn't die\n", cpu);
 		return;
 	}
+
+	clear_tasks_mm_cpumask(cpu);
+
 	printk(KERN_NOTICE "CPU%u: shutdown\n", cpu);
 
 	/*
diff --git a/kernel/msm-3.18/arch/arm/kernel/unwind.c b/kernel/msm-3.18/arch/arm/kernel/unwind.c
index cbb85c5fa..5184fe85d 100644
--- a/kernel/msm-3.18/arch/arm/kernel/unwind.c
+++ b/kernel/msm-3.18/arch/arm/kernel/unwind.c
@@ -93,7 +93,7 @@ extern const struct unwind_idx __start_unwind_idx[];
 static const struct unwind_idx *__origin_unwind_idx;
 extern const struct unwind_idx __stop_unwind_idx[];
 
-static DEFINE_SPINLOCK(unwind_lock);
+static DEFINE_RAW_SPINLOCK(unwind_lock);
 static LIST_HEAD(unwind_tables);
 
 /* Convert a prel31 symbol to an absolute address */
@@ -201,7 +201,7 @@ static const struct unwind_idx *unwind_find_idx(unsigned long addr)
 		/* module unwind tables */
 		struct unwind_table *table;
 
-		spin_lock_irqsave(&unwind_lock, flags);
+		raw_spin_lock_irqsave(&unwind_lock, flags);
 		list_for_each_entry(table, &unwind_tables, list) {
 			if (addr >= table->begin_addr &&
 			    addr < table->end_addr) {
@@ -213,7 +213,7 @@ static const struct unwind_idx *unwind_find_idx(unsigned long addr)
 				break;
 			}
 		}
-		spin_unlock_irqrestore(&unwind_lock, flags);
+		raw_spin_unlock_irqrestore(&unwind_lock, flags);
 	}
 
 	pr_debug("%s: idx = %p\n", __func__, idx);
@@ -530,9 +530,9 @@ struct unwind_table *unwind_table_add(unsigned long start, unsigned long size,
 	tab->begin_addr = text_addr;
 	tab->end_addr = text_addr + text_size;
 
-	spin_lock_irqsave(&unwind_lock, flags);
+	raw_spin_lock_irqsave(&unwind_lock, flags);
 	list_add_tail(&tab->list, &unwind_tables);
-	spin_unlock_irqrestore(&unwind_lock, flags);
+	raw_spin_unlock_irqrestore(&unwind_lock, flags);
 
 	return tab;
 }
@@ -544,9 +544,9 @@ void unwind_table_del(struct unwind_table *tab)
 	if (!tab)
 		return;
 
-	spin_lock_irqsave(&unwind_lock, flags);
+	raw_spin_lock_irqsave(&unwind_lock, flags);
 	list_del(&tab->list);
-	spin_unlock_irqrestore(&unwind_lock, flags);
+	raw_spin_unlock_irqrestore(&unwind_lock, flags);
 
 	kfree(tab);
 }
diff --git a/kernel/msm-3.18/arch/arm/kvm/arm.c b/kernel/msm-3.18/arch/arm/kvm/arm.c
index 575c3e327..15e2fa4f4 100644
--- a/kernel/msm-3.18/arch/arm/kvm/arm.c
+++ b/kernel/msm-3.18/arch/arm/kvm/arm.c
@@ -445,9 +445,9 @@ static int kvm_vcpu_first_run_init(struct kvm_vcpu *vcpu)
 
 static void vcpu_pause(struct kvm_vcpu *vcpu)
 {
-	wait_queue_head_t *wq = kvm_arch_vcpu_wq(vcpu);
+	struct swait_head *wq = kvm_arch_vcpu_wq(vcpu);
 
-	wait_event_interruptible(*wq, !vcpu->arch.pause);
+	swait_event_interruptible(*wq, !vcpu->arch.pause);
 }
 
 static int kvm_vcpu_initialized(struct kvm_vcpu *vcpu)
diff --git a/kernel/msm-3.18/arch/arm/kvm/psci.c b/kernel/msm-3.18/arch/arm/kvm/psci.c
index 4d0d89e34..e4f4a7067 100644
--- a/kernel/msm-3.18/arch/arm/kvm/psci.c
+++ b/kernel/msm-3.18/arch/arm/kvm/psci.c
@@ -67,7 +67,7 @@ static unsigned long kvm_psci_vcpu_on(struct kvm_vcpu *source_vcpu)
 {
 	struct kvm *kvm = source_vcpu->kvm;
 	struct kvm_vcpu *vcpu = NULL, *tmp;
-	wait_queue_head_t *wq;
+	struct swait_head *wq;
 	unsigned long cpu_id;
 	unsigned long context_id;
 	unsigned long mpidr;
@@ -124,7 +124,7 @@ static unsigned long kvm_psci_vcpu_on(struct kvm_vcpu *source_vcpu)
 	smp_mb();		/* Make sure the above is visible */
 
 	wq = kvm_arch_vcpu_wq(vcpu);
-	wake_up_interruptible(wq);
+	swait_wake_interruptible(wq);
 
 	return PSCI_RET_SUCCESS;
 }
diff --git a/kernel/msm-3.18/arch/arm/mach-at91/at91rm9200_time.c b/kernel/msm-3.18/arch/arm/mach-at91/at91rm9200_time.c
index 7fd13aef9..93491b119 100644
--- a/kernel/msm-3.18/arch/arm/mach-at91/at91rm9200_time.c
+++ b/kernel/msm-3.18/arch/arm/mach-at91/at91rm9200_time.c
@@ -135,6 +135,7 @@ clkevt32k_mode(enum clock_event_mode mode, struct clock_event_device *dev)
 		break;
 	case CLOCK_EVT_MODE_SHUTDOWN:
 	case CLOCK_EVT_MODE_UNUSED:
+		remove_irq(NR_IRQS_LEGACY + AT91_ID_SYS, &at91rm9200_timer_irq);
 	case CLOCK_EVT_MODE_RESUME:
 		irqmask = 0;
 		break;
diff --git a/kernel/msm-3.18/arch/arm/mach-exynos/platsmp.c b/kernel/msm-3.18/arch/arm/mach-exynos/platsmp.c
index 41ae28d69..9b96bc41b 100644
--- a/kernel/msm-3.18/arch/arm/mach-exynos/platsmp.c
+++ b/kernel/msm-3.18/arch/arm/mach-exynos/platsmp.c
@@ -137,7 +137,7 @@ static void __iomem *scu_base_addr(void)
 	return (void __iomem *)(S5P_VA_SCU);
 }
 
-static DEFINE_SPINLOCK(boot_lock);
+static DEFINE_RAW_SPINLOCK(boot_lock);
 
 static void exynos_secondary_init(unsigned int cpu)
 {
@@ -150,8 +150,8 @@ static void exynos_secondary_init(unsigned int cpu)
 	/*
 	 * Synchronise with the boot thread.
 	 */
-	spin_lock(&boot_lock);
-	spin_unlock(&boot_lock);
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 }
 
 static int exynos_boot_secondary(unsigned int cpu, struct task_struct *idle)
@@ -165,7 +165,7 @@ static int exynos_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * Set synchronisation state between this boot processor
 	 * and the secondary one
 	 */
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 
 	/*
 	 * The secondary processor is waiting to be released from
@@ -192,7 +192,7 @@ static int exynos_boot_secondary(unsigned int cpu, struct task_struct *idle)
 
 		if (timeout == 0) {
 			printk(KERN_ERR "cpu1 power enable failed");
-			spin_unlock(&boot_lock);
+			raw_spin_unlock(&boot_lock);
 			return -ETIMEDOUT;
 		}
 	}
@@ -242,7 +242,7 @@ static int exynos_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * calibrations, then wait for it to finish
 	 */
 fail:
-	spin_unlock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 
 	return pen_release != -1 ? ret : 0;
 }
diff --git a/kernel/msm-3.18/arch/arm/mach-hisi/platmcpm.c b/kernel/msm-3.18/arch/arm/mach-hisi/platmcpm.c
index 280f3f14f..bc2ed95c0 100644
--- a/kernel/msm-3.18/arch/arm/mach-hisi/platmcpm.c
+++ b/kernel/msm-3.18/arch/arm/mach-hisi/platmcpm.c
@@ -57,7 +57,7 @@
 
 static void __iomem *sysctrl, *fabric;
 static int hip04_cpu_table[HIP04_MAX_CLUSTERS][HIP04_MAX_CPUS_PER_CLUSTER];
-static DEFINE_SPINLOCK(boot_lock);
+static DEFINE_RAW_SPINLOCK(boot_lock);
 static u32 fabric_phys_addr;
 /*
  * [0]: bootwrapper physical address
@@ -104,7 +104,7 @@ static int hip04_mcpm_power_up(unsigned int cpu, unsigned int cluster)
 	if (cluster >= HIP04_MAX_CLUSTERS || cpu >= HIP04_MAX_CPUS_PER_CLUSTER)
 		return -EINVAL;
 
-	spin_lock_irq(&boot_lock);
+	raw_spin_lock_irq(&boot_lock);
 
 	if (hip04_cpu_table[cluster][cpu])
 		goto out;
@@ -133,7 +133,7 @@ static int hip04_mcpm_power_up(unsigned int cpu, unsigned int cluster)
 	udelay(20);
 out:
 	hip04_cpu_table[cluster][cpu]++;
-	spin_unlock_irq(&boot_lock);
+	raw_spin_unlock_irq(&boot_lock);
 
 	return 0;
 }
@@ -149,7 +149,7 @@ static void hip04_mcpm_power_down(void)
 
 	__mcpm_cpu_going_down(cpu, cluster);
 
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 	BUG_ON(__mcpm_cluster_state(cluster) != CLUSTER_UP);
 	hip04_cpu_table[cluster][cpu]--;
 	if (hip04_cpu_table[cluster][cpu] == 1) {
@@ -162,7 +162,7 @@ static void hip04_mcpm_power_down(void)
 
 	last_man = hip04_cluster_is_down(cluster);
 	if (last_man && __mcpm_outbound_enter_critical(cpu, cluster)) {
-		spin_unlock(&boot_lock);
+		raw_spin_unlock(&boot_lock);
 		/* Since it's Cortex A15, disable L2 prefetching. */
 		asm volatile(
 		"mcr	p15, 1, %0, c15, c0, 3 \n\t"
@@ -173,7 +173,7 @@ static void hip04_mcpm_power_down(void)
 		hip04_set_snoop_filter(cluster, 0);
 		__mcpm_outbound_leave_critical(cluster, CLUSTER_DOWN);
 	} else {
-		spin_unlock(&boot_lock);
+		raw_spin_unlock(&boot_lock);
 		v7_exit_coherency_flush(louis);
 	}
 
@@ -192,7 +192,7 @@ static int hip04_mcpm_wait_for_powerdown(unsigned int cpu, unsigned int cluster)
 	       cpu >= HIP04_MAX_CPUS_PER_CLUSTER);
 
 	count = TIMEOUT_MSEC / POLL_MSEC;
-	spin_lock_irq(&boot_lock);
+	raw_spin_lock_irq(&boot_lock);
 	for (tries = 0; tries < count; tries++) {
 		if (hip04_cpu_table[cluster][cpu]) {
 			ret = -EBUSY;
@@ -202,10 +202,10 @@ static int hip04_mcpm_wait_for_powerdown(unsigned int cpu, unsigned int cluster)
 		data = readl_relaxed(sysctrl + SC_CPU_RESET_STATUS(cluster));
 		if (data & CORE_WFI_STATUS(cpu))
 			break;
-		spin_unlock_irq(&boot_lock);
+		raw_spin_unlock_irq(&boot_lock);
 		/* Wait for clean L2 when the whole cluster is down. */
 		msleep(POLL_MSEC);
-		spin_lock_irq(&boot_lock);
+		raw_spin_lock_irq(&boot_lock);
 	}
 	if (tries >= count)
 		goto err;
@@ -220,10 +220,10 @@ static int hip04_mcpm_wait_for_powerdown(unsigned int cpu, unsigned int cluster)
 	}
 	if (tries >= count)
 		goto err;
-	spin_unlock_irq(&boot_lock);
+	raw_spin_unlock_irq(&boot_lock);
 	return 0;
 err:
-	spin_unlock_irq(&boot_lock);
+	raw_spin_unlock_irq(&boot_lock);
 	return ret;
 }
 
@@ -235,10 +235,10 @@ static void hip04_mcpm_powered_up(void)
 	cpu = MPIDR_AFFINITY_LEVEL(mpidr, 0);
 	cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);
 
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 	if (!hip04_cpu_table[cluster][cpu])
 		hip04_cpu_table[cluster][cpu] = 1;
-	spin_unlock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 }
 
 static void __naked hip04_mcpm_power_up_setup(unsigned int affinity_level)
diff --git a/kernel/msm-3.18/arch/arm/mach-omap2/omap-smp.c b/kernel/msm-3.18/arch/arm/mach-omap2/omap-smp.c
index 5305ec734..19732b560 100644
--- a/kernel/msm-3.18/arch/arm/mach-omap2/omap-smp.c
+++ b/kernel/msm-3.18/arch/arm/mach-omap2/omap-smp.c
@@ -43,7 +43,7 @@
 /* SCU base address */
 static void __iomem *scu_base;
 
-static DEFINE_SPINLOCK(boot_lock);
+static DEFINE_RAW_SPINLOCK(boot_lock);
 
 void __iomem *omap4_get_scu_base(void)
 {
@@ -74,8 +74,8 @@ static void omap4_secondary_init(unsigned int cpu)
 	/*
 	 * Synchronise with the boot thread.
 	 */
-	spin_lock(&boot_lock);
-	spin_unlock(&boot_lock);
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 }
 
 static int omap4_boot_secondary(unsigned int cpu, struct task_struct *idle)
@@ -89,7 +89,7 @@ static int omap4_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * Set synchronisation state between this boot processor
 	 * and the secondary one
 	 */
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 
 	/*
 	 * Update the AuxCoreBoot0 with boot state for secondary core.
@@ -166,7 +166,7 @@ static int omap4_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * Now the secondary core is starting up let it run its
 	 * calibrations, then wait for it to finish
 	 */
-	spin_unlock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 
 	return 0;
 }
diff --git a/kernel/msm-3.18/arch/arm/mach-prima2/platsmp.c b/kernel/msm-3.18/arch/arm/mach-prima2/platsmp.c
index 335c12e92..2bb579679 100644
--- a/kernel/msm-3.18/arch/arm/mach-prima2/platsmp.c
+++ b/kernel/msm-3.18/arch/arm/mach-prima2/platsmp.c
@@ -23,7 +23,7 @@
 static void __iomem *scu_base;
 static void __iomem *rsc_base;
 
-static DEFINE_SPINLOCK(boot_lock);
+static DEFINE_RAW_SPINLOCK(boot_lock);
 
 static struct map_desc scu_io_desc __initdata = {
 	.length		= SZ_4K,
@@ -56,8 +56,8 @@ static void sirfsoc_secondary_init(unsigned int cpu)
 	/*
 	 * Synchronise with the boot thread.
 	 */
-	spin_lock(&boot_lock);
-	spin_unlock(&boot_lock);
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 }
 
 static struct of_device_id rsc_ids[]  = {
@@ -95,7 +95,7 @@ static int sirfsoc_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	/* make sure write buffer is drained */
 	mb();
 
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 
 	/*
 	 * The secondary processor is waiting to be released from
@@ -127,7 +127,7 @@ static int sirfsoc_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * now the secondary core is starting up let it run its
 	 * calibrations, then wait for it to finish
 	 */
-	spin_unlock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 
 	return pen_release != -1 ? -ENOSYS : 0;
 }
diff --git a/kernel/msm-3.18/arch/arm/mach-qcom/platsmp.c b/kernel/msm-3.18/arch/arm/mach-qcom/platsmp.c
index d6908569e..80c5e4fc5 100644
--- a/kernel/msm-3.18/arch/arm/mach-qcom/platsmp.c
+++ b/kernel/msm-3.18/arch/arm/mach-qcom/platsmp.c
@@ -46,7 +46,7 @@
 
 extern void secondary_startup(void);
 
-static DEFINE_SPINLOCK(boot_lock);
+static DEFINE_RAW_SPINLOCK(boot_lock);
 
 #ifdef CONFIG_HOTPLUG_CPU
 static void __ref qcom_cpu_die(unsigned int cpu)
@@ -60,8 +60,8 @@ static void qcom_secondary_init(unsigned int cpu)
 	/*
 	 * Synchronise with the boot thread.
 	 */
-	spin_lock(&boot_lock);
-	spin_unlock(&boot_lock);
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 }
 
 static int scss_release_secondary(unsigned int cpu)
@@ -284,7 +284,7 @@ static int qcom_boot_secondary(unsigned int cpu, int (*func)(unsigned int))
 	 * set synchronisation state between this boot processor
 	 * and the secondary one
 	 */
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 
 	/*
 	 * Send the secondary CPU a soft interrupt, thereby causing
@@ -297,7 +297,7 @@ static int qcom_boot_secondary(unsigned int cpu, int (*func)(unsigned int))
 	 * now the secondary core is starting up let it run its
 	 * calibrations, then wait for it to finish
 	 */
-	spin_unlock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 
 	return ret;
 }
diff --git a/kernel/msm-3.18/arch/arm/mach-spear/platsmp.c b/kernel/msm-3.18/arch/arm/mach-spear/platsmp.c
index fd4297713..b0553b2c2 100644
--- a/kernel/msm-3.18/arch/arm/mach-spear/platsmp.c
+++ b/kernel/msm-3.18/arch/arm/mach-spear/platsmp.c
@@ -32,7 +32,7 @@ static void write_pen_release(int val)
 	sync_cache_w(&pen_release);
 }
 
-static DEFINE_SPINLOCK(boot_lock);
+static DEFINE_RAW_SPINLOCK(boot_lock);
 
 static void __iomem *scu_base = IOMEM(VA_SCU_BASE);
 
@@ -47,8 +47,8 @@ static void spear13xx_secondary_init(unsigned int cpu)
 	/*
 	 * Synchronise with the boot thread.
 	 */
-	spin_lock(&boot_lock);
-	spin_unlock(&boot_lock);
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 }
 
 static int spear13xx_boot_secondary(unsigned int cpu, struct task_struct *idle)
@@ -59,7 +59,7 @@ static int spear13xx_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * set synchronisation state between this boot processor
 	 * and the secondary one
 	 */
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 
 	/*
 	 * The secondary processor is waiting to be released from
@@ -84,7 +84,7 @@ static int spear13xx_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * now the secondary core is starting up let it run its
 	 * calibrations, then wait for it to finish
 	 */
-	spin_unlock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 
 	return pen_release != -1 ? -ENOSYS : 0;
 }
diff --git a/kernel/msm-3.18/arch/arm/mach-sti/platsmp.c b/kernel/msm-3.18/arch/arm/mach-sti/platsmp.c
index d4b624f8d..56d402812 100644
--- a/kernel/msm-3.18/arch/arm/mach-sti/platsmp.c
+++ b/kernel/msm-3.18/arch/arm/mach-sti/platsmp.c
@@ -34,7 +34,7 @@ static void write_pen_release(int val)
 	sync_cache_w(&pen_release);
 }
 
-static DEFINE_SPINLOCK(boot_lock);
+static DEFINE_RAW_SPINLOCK(boot_lock);
 
 static void sti_secondary_init(unsigned int cpu)
 {
@@ -49,8 +49,8 @@ static void sti_secondary_init(unsigned int cpu)
 	/*
 	 * Synchronise with the boot thread.
 	 */
-	spin_lock(&boot_lock);
-	spin_unlock(&boot_lock);
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 }
 
 static int sti_boot_secondary(unsigned int cpu, struct task_struct *idle)
@@ -61,7 +61,7 @@ static int sti_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * set synchronisation state between this boot processor
 	 * and the secondary one
 	 */
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 
 	/*
 	 * The secondary processor is waiting to be released from
@@ -92,7 +92,7 @@ static int sti_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * now the secondary core is starting up let it run its
 	 * calibrations, then wait for it to finish
 	 */
-	spin_unlock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 
 	return pen_release != -1 ? -ENOSYS : 0;
 }
diff --git a/kernel/msm-3.18/arch/arm/mach-ux500/platsmp.c b/kernel/msm-3.18/arch/arm/mach-ux500/platsmp.c
index a44967f31..3af22a483 100644
--- a/kernel/msm-3.18/arch/arm/mach-ux500/platsmp.c
+++ b/kernel/msm-3.18/arch/arm/mach-ux500/platsmp.c
@@ -51,7 +51,7 @@ static void __iomem *scu_base_addr(void)
 	return NULL;
 }
 
-static DEFINE_SPINLOCK(boot_lock);
+static DEFINE_RAW_SPINLOCK(boot_lock);
 
 static void ux500_secondary_init(unsigned int cpu)
 {
@@ -64,8 +64,8 @@ static void ux500_secondary_init(unsigned int cpu)
 	/*
 	 * Synchronise with the boot thread.
 	 */
-	spin_lock(&boot_lock);
-	spin_unlock(&boot_lock);
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 }
 
 static int ux500_boot_secondary(unsigned int cpu, struct task_struct *idle)
@@ -76,7 +76,7 @@ static int ux500_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * set synchronisation state between this boot processor
 	 * and the secondary one
 	 */
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 
 	/*
 	 * The secondary processor is waiting to be released from
@@ -97,7 +97,7 @@ static int ux500_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * now the secondary core is starting up let it run its
 	 * calibrations, then wait for it to finish
 	 */
-	spin_unlock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 
 	return pen_release != -1 ? -ENOSYS : 0;
 }
diff --git a/kernel/msm-3.18/arch/arm/mm/fault.c b/kernel/msm-3.18/arch/arm/mm/fault.c
index c914e319d..1f9c85490 100644
--- a/kernel/msm-3.18/arch/arm/mm/fault.c
+++ b/kernel/msm-3.18/arch/arm/mm/fault.c
@@ -281,7 +281,7 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 	 * If we're in an interrupt, or have no irqs, or have no user
 	 * context, we must not take the fault..
 	 */
-	if (in_atomic() || irqs_disabled() || !mm)
+    if (irqs_disabled() || !mm || pagefault_disabled())
 		goto no_context;
 
 	if (user_mode(regs))
@@ -435,6 +435,9 @@ do_translation_fault(unsigned long addr, unsigned int fsr,
 	if (addr < TASK_SIZE)
 		return do_page_fault(addr, fsr, regs);
 
+	if (interrupts_enabled(regs))
+		local_irq_enable();
+
 	if (user_mode(regs))
 		goto bad_area;
 
@@ -502,6 +505,9 @@ do_translation_fault(unsigned long addr, unsigned int fsr,
 static int
 do_sect_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 {
+	if (interrupts_enabled(regs))
+		local_irq_enable();
+
 	do_bad_area(addr, fsr, regs);
 	return 0;
 }
diff --git a/kernel/msm-3.18/arch/arm/mm/highmem.c b/kernel/msm-3.18/arch/arm/mm/highmem.c
index 26343fde1..f8b4c3bd3 100644
--- a/kernel/msm-3.18/arch/arm/mm/highmem.c
+++ b/kernel/msm-3.18/arch/arm/mm/highmem.c
@@ -10,7 +10,6 @@
  * published by the Free Software Foundation.
  */
 
-#include <linux/cpu.h>
 #include <linux/module.h>
 #include <linux/highmem.h>
 #include <linux/interrupt.h>
@@ -19,20 +18,19 @@
 #include <asm/tlbflush.h>
 #include "mm.h"
 
+pte_t *fixmap_page_table;
+
 static inline void set_fixmap_pte(int idx, pte_t pte)
 {
 	unsigned long vaddr = __fix_to_virt(idx);
-	pte_t *ptep = pte_offset_kernel(pmd_off_k(vaddr), vaddr);
-
-	set_pte_ext(ptep, pte, 0);
+	set_pte_ext(fixmap_page_table + idx, pte, 0);
 	local_flush_tlb_kernel_page(vaddr);
 }
 
 static inline pte_t get_fixmap_pte(unsigned long vaddr)
 {
-	pte_t *ptep = pte_offset_kernel(pmd_off_k(vaddr), vaddr);
-
-	return *ptep;
+	unsigned long idx = __virt_to_fix(vaddr);
+	return *(fixmap_page_table + idx);
 }
 
 void *kmap(struct page *page)
@@ -55,6 +53,7 @@ EXPORT_SYMBOL(kunmap);
 
 void *kmap_atomic(struct page *page)
 {
+	pte_t pte = mk_pte(page, kmap_prot);
 	unsigned int idx;
 	unsigned long vaddr;
 	void *kmap;
@@ -79,21 +78,24 @@ void *kmap_atomic(struct page *page)
 
 	type = kmap_atomic_idx_push();
 
-	idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+	idx = type + KM_TYPE_NR * smp_processor_id();
 	vaddr = __fix_to_virt(idx);
 #ifdef CONFIG_DEBUG_HIGHMEM
 	/*
 	 * With debugging enabled, kunmap_atomic forces that entry to 0.
 	 * Make sure it was indeed properly unmapped.
 	 */
-	BUG_ON(!pte_none(get_fixmap_pte(vaddr)));
+	BUG_ON(!pte_none(*(fixmap_page_table + idx)));
 #endif
 	/*
 	 * When debugging is off, kunmap_atomic leaves the previous mapping
 	 * in place, so the contained TLB flush ensures the TLB is updated
 	 * with the new mapping.
 	 */
-	set_fixmap_pte(idx, mk_pte(page, kmap_prot));
+#ifdef CONFIG_PREEMPT_RT_FULL
+	current->kmap_pte[type] = pte;
+#endif
+	set_fixmap_pte(idx, pte);
 
 	return (void *)vaddr;
 }
@@ -106,16 +108,19 @@ void __kunmap_atomic(void *kvaddr)
 
 	if (kvaddr >= (void *)FIXADDR_START) {
 		type = kmap_atomic_idx();
-		idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+		idx = type + KM_TYPE_NR * smp_processor_id();
 
 		if (cache_is_vivt())
 			__cpuc_flush_dcache_area((void *)vaddr, PAGE_SIZE);
+#ifdef CONFIG_PREEMPT_RT_FULL
+		current->kmap_pte[type] = __pte(0);
+#endif
 #ifdef CONFIG_DEBUG_HIGHMEM
 		BUG_ON(vaddr != __fix_to_virt(idx));
-		set_fixmap_pte(idx, __pte(0));
 #else
 		(void) idx;  /* to kill a warning */
 #endif
+		set_fixmap_pte(idx, __pte(0));
 		kmap_atomic_idx_pop();
 	} else if (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {
 		/* this address was obtained through kmap_high_get() */
@@ -127,6 +132,7 @@ EXPORT_SYMBOL(__kunmap_atomic);
 
 void *kmap_atomic_pfn(unsigned long pfn)
 {
+	pte_t pte = pfn_pte(pfn, kmap_prot);
 	unsigned long vaddr;
 	int idx, type;
 	struct page *page = pfn_to_page(pfn);
@@ -136,12 +142,15 @@ void *kmap_atomic_pfn(unsigned long pfn)
 		return page_address(page);
 
 	type = kmap_atomic_idx_push();
-	idx = FIX_KMAP_BEGIN + type + KM_TYPE_NR * smp_processor_id();
+	idx = type + KM_TYPE_NR * smp_processor_id();
 	vaddr = __fix_to_virt(idx);
 #ifdef CONFIG_DEBUG_HIGHMEM
-	BUG_ON(!pte_none(get_fixmap_pte(vaddr)));
+	BUG_ON(!pte_none(*(fixmap_page_table + idx)));
+#endif
+#ifdef CONFIG_PREEMPT_RT_FULL
+	current->kmap_pte[type] = pte;
 #endif
-	set_fixmap_pte(idx, pfn_pte(pfn, kmap_prot));
+	set_fixmap_pte(idx, pte);
 
 	return (void *)vaddr;
 }
@@ -156,57 +165,27 @@ struct page *kmap_atomic_to_page(const void *ptr)
 	return pte_page(get_fixmap_pte(vaddr));
 }
 
-#ifdef CONFIG_ARCH_WANT_KMAP_ATOMIC_FLUSH
-static void kmap_remove_unused_cpu(int cpu)
+#if defined CONFIG_PREEMPT_RT_FULL
+void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p)
 {
-	int start_idx, idx, type;
+	int i;
 
-	pagefault_disable();
-	type = kmap_atomic_idx();
-	start_idx = FIX_KMAP_BEGIN + type + 1 + KM_TYPE_NR * cpu;
-
-	for (idx = start_idx; idx < KM_TYPE_NR + KM_TYPE_NR * cpu; idx++) {
-		unsigned long vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-		pte_t ptep;
+	/*
+	 * Clear @prev's kmap_atomic mappings
+	 */
+	for (i = 0; i < prev_p->kmap_idx; i++) {
+		int idx = i + KM_TYPE_NR * smp_processor_id();
 
-		ptep = get_top_pte(vaddr);
-		if (ptep)
-			set_top_pte(vaddr, __pte(0));
+		set_fixmap_pte(idx, __pte(0));
 	}
-	pagefault_enable();
-}
-
-static void kmap_remove_unused(void *unused)
-{
-	kmap_remove_unused_cpu(smp_processor_id());
-}
-
-void kmap_atomic_flush_unused(void)
-{
-	on_each_cpu(kmap_remove_unused, NULL, 1);
-}
+	/*
+	 * Restore @next_p's kmap_atomic mappings
+	 */
+	for (i = 0; i < next_p->kmap_idx; i++) {
+		int idx = i + KM_TYPE_NR * smp_processor_id();
 
-static int hotplug_kmap_atomic_callback(struct notifier_block *nfb,
-					unsigned long action, void *hcpu)
-{
-	switch (action & (~CPU_TASKS_FROZEN)) {
-	case CPU_DYING:
-		kmap_remove_unused_cpu((int)hcpu);
-		break;
-	default:
-		break;
+		if (!pte_none(next_p->kmap_pte[i]))
+			set_fixmap_pte(idx, next_p->kmap_pte[i]);
 	}
-
-	return NOTIFY_OK;
-}
-
-static struct notifier_block hotplug_kmap_atomic_notifier = {
-	.notifier_call = hotplug_kmap_atomic_callback,
-};
-
-static int __init init_kmap_atomic(void)
-{
-	return register_hotcpu_notifier(&hotplug_kmap_atomic_notifier);
 }
-early_initcall(init_kmap_atomic);
 #endif
diff --git a/kernel/msm-3.18/arch/arm/plat-versatile/platsmp.c b/kernel/msm-3.18/arch/arm/plat-versatile/platsmp.c
index 53feb90c8..b4a8d54fc 100644
--- a/kernel/msm-3.18/arch/arm/plat-versatile/platsmp.c
+++ b/kernel/msm-3.18/arch/arm/plat-versatile/platsmp.c
@@ -30,7 +30,7 @@ static void write_pen_release(int val)
 	sync_cache_w(&pen_release);
 }
 
-static DEFINE_SPINLOCK(boot_lock);
+static DEFINE_RAW_SPINLOCK(boot_lock);
 
 void versatile_secondary_init(unsigned int cpu)
 {
@@ -43,8 +43,8 @@ void versatile_secondary_init(unsigned int cpu)
 	/*
 	 * Synchronise with the boot thread.
 	 */
-	spin_lock(&boot_lock);
-	spin_unlock(&boot_lock);
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 }
 
 int versatile_boot_secondary(unsigned int cpu, struct task_struct *idle)
@@ -55,7 +55,7 @@ int versatile_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * Set synchronisation state between this boot processor
 	 * and the secondary one
 	 */
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 
 	/*
 	 * This is really belt and braces; we hold unintended secondary
@@ -85,7 +85,7 @@ int versatile_boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 * now the secondary core is starting up let it run its
 	 * calibrations, then wait for it to finish
 	 */
-	spin_unlock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 
 	return pen_release != -1 ? -ENOSYS : 0;
 }
diff --git a/kernel/msm-3.18/arch/arm64/Kconfig b/kernel/msm-3.18/arch/arm64/Kconfig
index 0444366b2..78a57c15c 100644
--- a/kernel/msm-3.18/arch/arm64/Kconfig
+++ b/kernel/msm-3.18/arch/arm64/Kconfig
@@ -71,8 +71,10 @@ config ARM64
 	select HAVE_PERF_REGS
 	select HAVE_PERF_USER_STACK_DUMP
 	select HAVE_RCU_TABLE_FREE
+	select HAVE_PREEMPT_LAZY
 	select HAVE_SYSCALL_TRACEPOINTS
 	select IRQ_DOMAIN
+	select IRQ_FORCED_THREADING
 	select MODULES_USE_ELF_RELA
 	select NO_BOOTMEM
 	select OF
diff --git a/kernel/msm-3.18/arch/arm64/include/asm/thread_info.h b/kernel/msm-3.18/arch/arm64/include/asm/thread_info.h
index 7193434e1..65e6b9725 100644
--- a/kernel/msm-3.18/arch/arm64/include/asm/thread_info.h
+++ b/kernel/msm-3.18/arch/arm64/include/asm/thread_info.h
@@ -52,6 +52,7 @@ struct thread_info {
 	u64			ttbr0;		/* saved TTBR0_EL1 */
 #endif
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
+	int			preempt_lazy_count;	/* 0 => preemptable, <0 => bug */
 	int			cpu;		/* cpu */
 };
 
@@ -113,6 +114,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_NEED_RESCHED	1
 #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
 #define TIF_FOREIGN_FPSTATE	3	/* CPU's FP state is not current's */
+#define TIF_NEED_RESCHED_LAZY	4
 #define TIF_NOHZ		7
 #define TIF_SYSCALL_TRACE	8
 #define TIF_SYSCALL_AUDIT	9
@@ -130,6 +132,7 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
 #define _TIF_FOREIGN_FPSTATE	(1 << TIF_FOREIGN_FPSTATE)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_NOHZ		(1 << TIF_NOHZ)
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
diff --git a/kernel/msm-3.18/arch/arm64/kernel/asm-offsets.c b/kernel/msm-3.18/arch/arm64/kernel/asm-offsets.c
index 92a5d25bf..b51653677 100644
--- a/kernel/msm-3.18/arch/arm64/kernel/asm-offsets.c
+++ b/kernel/msm-3.18/arch/arm64/kernel/asm-offsets.c
@@ -35,6 +35,7 @@ int main(void)
   BLANK();
   DEFINE(TI_FLAGS,		offsetof(struct thread_info, flags));
   DEFINE(TI_PREEMPT,		offsetof(struct thread_info, preempt_count));
+  DEFINE(TI_PREEMPT_LAZY,	offsetof(struct thread_info, preempt_lazy_count));
   DEFINE(TI_ADDR_LIMIT,		offsetof(struct thread_info, addr_limit));
   DEFINE(TI_TASK,		offsetof(struct thread_info, task));
   DEFINE(TI_EXEC_DOMAIN,	offsetof(struct thread_info, exec_domain));
diff --git a/kernel/msm-3.18/arch/arm64/kernel/debug-monitors.c b/kernel/msm-3.18/arch/arm64/kernel/debug-monitors.c
index d35057c09..cfd9d88cb 100644
--- a/kernel/msm-3.18/arch/arm64/kernel/debug-monitors.c
+++ b/kernel/msm-3.18/arch/arm64/kernel/debug-monitors.c
@@ -185,20 +185,21 @@ static void clear_regs_spsr_ss(struct pt_regs *regs)
 
 /* EL1 Single Step Handler hooks */
 static LIST_HEAD(step_hook);
-static DEFINE_RWLOCK(step_hook_lock);
+static DEFINE_SPINLOCK(step_hook_lock);
 
 void register_step_hook(struct step_hook *hook)
 {
-	write_lock(&step_hook_lock);
-	list_add(&hook->node, &step_hook);
-	write_unlock(&step_hook_lock);
+	spin_lock(&step_hook_lock);
+	list_add_rcu(&hook->node, &step_hook);
+	spin_unlock(&step_hook_lock);
 }
 
 void unregister_step_hook(struct step_hook *hook)
 {
-	write_lock(&step_hook_lock);
-	list_del(&hook->node);
-	write_unlock(&step_hook_lock);
+	spin_lock(&step_hook_lock);
+	list_del_rcu(&hook->node);
+	spin_unlock(&step_hook_lock);
+	synchronize_rcu();
 }
 
 /*
@@ -212,15 +213,15 @@ static int call_step_hook(struct pt_regs *regs, unsigned int esr)
 	struct step_hook *hook;
 	int retval = DBG_HOOK_ERROR;
 
-	read_lock(&step_hook_lock);
+	rcu_read_lock();
 
-	list_for_each_entry(hook, &step_hook, node)	{
+	list_for_each_entry_rcu(hook, &step_hook, node)	{
 		retval = hook->fn(regs, esr);
 		if (retval == DBG_HOOK_HANDLED)
 			break;
 	}
 
-	read_unlock(&step_hook_lock);
+	rcu_read_unlock();
 
 	return retval;
 }
diff --git a/kernel/msm-3.18/arch/arm64/kernel/entry.S b/kernel/msm-3.18/arch/arm64/kernel/entry.S
index 9adad63e4..f40178b72 100644
--- a/kernel/msm-3.18/arch/arm64/kernel/entry.S
+++ b/kernel/msm-3.18/arch/arm64/kernel/entry.S
@@ -444,11 +444,16 @@ el1_irq:
 #ifdef CONFIG_PREEMPT
 	get_thread_info tsk
 	ldr	w24, [tsk, #TI_PREEMPT]		// get preempt count
-	cbnz	w24, 1f				// preempt count != 0
+	cbnz	w24, 2f				// preempt count != 0
 	ldr	x0, [tsk, #TI_FLAGS]		// get flags
-	tbz	x0, #TIF_NEED_RESCHED, 1f	// needs rescheduling?
-	bl	el1_preempt
+	tbnz	x0, #TIF_NEED_RESCHED, 1f	// needs rescheduling?
+
+	ldr	w24, [tsk, #TI_PREEMPT_LAZY]	// get preempt lazy count
+	cbnz	w24, 2f				// preempt lazy count != 0
+	tbz	x0, #TIF_NEED_RESCHED_LAZY, 2f	// needs rescheduling?
 1:
+	bl	el1_preempt
+2:
 #endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	bl	trace_hardirqs_on
@@ -462,6 +467,7 @@ el1_preempt:
 1:	bl	preempt_schedule_irq		// irq en/disable is done inside
 	ldr	x0, [tsk, #TI_FLAGS]		// get new tasks TI_FLAGS
 	tbnz	x0, #TIF_NEED_RESCHED, 1b	// needs rescheduling?
+	tbnz	x0, #TIF_NEED_RESCHED_LAZY, 1b	// needs rescheduling?
 	ret	x24
 #endif
 
@@ -743,6 +749,7 @@ ret_fast_syscall_trace:
  */
 work_pending:
 	tbnz	x1, #TIF_NEED_RESCHED, work_resched
+	tbnz	x1, #TIF_NEED_RESCHED_LAZY, work_resched
 	/* TIF_SIGPENDING, TIF_NOTIFY_RESUME or TIF_FOREIGN_FPSTATE case */
 	ldr	x2, [sp, #S_PSTATE]
 	mov	x0, sp				// 'regs'
diff --git a/kernel/msm-3.18/arch/arm64/kernel/perf_event.c b/kernel/msm-3.18/arch/arm64/kernel/perf_event.c
index f70223151..ea465b973 100644
--- a/kernel/msm-3.18/arch/arm64/kernel/perf_event.c
+++ b/kernel/msm-3.18/arch/arm64/kernel/perf_event.c
@@ -29,9 +29,6 @@
 #include <linux/platform_device.h>
 #include <linux/spinlock.h>
 #include <linux/uaccess.h>
-#include <linux/cpu_pm.h>
-#include <linux/debugfs.h>
-#include <linux/of.h>
 
 #include <asm/cputype.h>
 #include <asm/irq.h>
@@ -48,9 +45,6 @@
 static DEFINE_PER_CPU(struct perf_event * [ARMPMU_MAX_HWEVENTS], hw_events);
 static DEFINE_PER_CPU(unsigned long [BITS_TO_LONGS(ARMPMU_MAX_HWEVENTS)], used_mask);
 static DEFINE_PER_CPU(struct pmu_hw_events, cpu_hw_events);
-static DEFINE_PER_CPU(u32, from_idle);
-static DEFINE_PER_CPU(u32, armv8_pm_pmuserenr);
-static DEFINE_PER_CPU(u32, hotplug_down);
 
 #define to_arm_pmu(p) (container_of(p, struct arm_pmu, pmu))
 
@@ -129,13 +123,13 @@ armpmu_map_raw_event(u32 raw_event_mask, u64 config)
 	return (int)(config & raw_event_mask);
 }
 
-int map_cpu_event(struct perf_event *event,
-		  const unsigned (*event_map)[PERF_COUNT_HW_MAX],
-		  const unsigned (*cache_map)
+static int map_cpu_event(struct perf_event *event,
+			 const unsigned (*event_map)[PERF_COUNT_HW_MAX],
+			 const unsigned (*cache_map)
 					[PERF_COUNT_HW_CACHE_MAX]
 					[PERF_COUNT_HW_CACHE_OP_MAX]
 					[PERF_COUNT_HW_CACHE_RESULT_MAX],
-		  u32 raw_event_mask)
+			 u32 raw_event_mask)
 {
 	u64 config = event->attr.config;
 
@@ -294,14 +288,6 @@ armpmu_add(struct perf_event *event, int flags)
 
 	perf_pmu_disable(event->pmu);
 
-	if (armpmu->check_event) {
-		if (armpmu->check_event(armpmu, hwc)) {
-			event->state = PERF_EVENT_STATE_OFF;
-			hwc->idx = -1;
-			goto out;
-		}
-	}
-
 	/* If we don't have a space for the counter then finish early. */
 	idx = armpmu->get_event_idx(hw_events, hwc);
 	if (idx < 0) {
@@ -396,7 +382,30 @@ armpmu_disable_percpu_irq(void *data)
 static void
 armpmu_release_hardware(struct arm_pmu *armpmu)
 {
-	armpmu->free_irq(armpmu);
+	int irq;
+	unsigned int i, irqs;
+	struct platform_device *pmu_device = armpmu->plat_device;
+
+	irqs = min(pmu_device->num_resources, num_possible_cpus());
+	if (!irqs)
+		return;
+
+	irq = platform_get_irq(pmu_device, 0);
+	if (irq <= 0)
+		return;
+
+	if (irq_is_percpu(irq)) {
+		on_each_cpu(armpmu_disable_percpu_irq, &irq, 1);
+		free_percpu_irq(irq, &cpu_hw_events);
+	} else {
+		for (i = 0; i < irqs; ++i) {
+			if (!cpumask_test_and_clear_cpu(i, &armpmu->active_irqs))
+				continue;
+			irq = platform_get_irq(pmu_device, i);
+			if (irq > 0)
+				free_irq(irq, armpmu);
+		}
+	}
 }
 
 static void
@@ -409,7 +418,8 @@ armpmu_enable_percpu_irq(void *data)
 static int
 armpmu_reserve_hardware(struct arm_pmu *armpmu)
 {
-	int err;
+	int err, irq;
+	unsigned int i, irqs;
 	struct platform_device *pmu_device = armpmu->plat_device;
 
 	if (!pmu_device) {
@@ -417,13 +427,61 @@ armpmu_reserve_hardware(struct arm_pmu *armpmu)
 		return -ENODEV;
 	}
 
-	err = armpmu->request_irq(armpmu, armpmu->handle_irq);
-	if (err) {
-		armpmu_release_hardware(armpmu);
-		return err;
+	irqs = min(pmu_device->num_resources, num_possible_cpus());
+	if (!irqs) {
+		pr_err("no irqs for PMUs defined\n");
+		return -ENODEV;
+	}
+
+	irq = platform_get_irq(pmu_device, 0);
+	if (irq <= 0) {
+		pr_err("failed to get valid irq for PMU device\n");
+		return -ENODEV;
 	}
 
-	armpmu->pmu_state = ARM_PMU_STATE_RUNNING;
+	if (irq_is_percpu(irq)) {
+		err = request_percpu_irq(irq, armpmu->handle_irq,
+				"arm-pmu", &cpu_hw_events);
+
+		if (err) {
+			pr_err("unable to request percpu IRQ%d for ARM PMU counters\n",
+					irq);
+			armpmu_release_hardware(armpmu);
+			return err;
+		}
+
+		on_each_cpu(armpmu_enable_percpu_irq, &irq, 1);
+	} else {
+		for (i = 0; i < irqs; ++i) {
+			err = 0;
+			irq = platform_get_irq(pmu_device, i);
+			if (irq <= 0)
+				continue;
+
+			/*
+			 * If we have a single PMU interrupt that we can't shift,
+			 * assume that we're running on a uniprocessor machine and
+			 * continue. Otherwise, continue without this interrupt.
+			 */
+			if (irq_set_affinity(irq, cpumask_of(i)) && irqs > 1) {
+				pr_warning("unable to set irq affinity (irq=%d, cpu=%u)\n",
+						irq, i);
+				continue;
+			}
+
+			err = request_irq(irq, armpmu->handle_irq,
+					IRQF_NOBALANCING | IRQF_NO_THREAD,
+					"arm-pmu", armpmu);
+			if (err) {
+				pr_err("unable to request IRQ%d for ARM PMU counters\n",
+						irq);
+				armpmu_release_hardware(armpmu);
+				return err;
+			}
+
+			cpumask_set_cpu(i, &armpmu->active_irqs);
+		}
+	}
 
 	return 0;
 }
@@ -547,24 +605,6 @@ static void armpmu_enable(struct pmu *pmu)
 	struct arm_pmu *armpmu = to_arm_pmu(pmu);
 	struct pmu_hw_events *hw_events = armpmu->get_hw_events();
 	int enabled = bitmap_weight(hw_events->used_mask, armpmu->num_events);
-	int idx;
-
-	if (*hw_events->from_idle) {
-		for (idx = 0; idx <= armpmu->num_events; ++idx) {
-			struct perf_event *event = hw_events->events[idx];
-			struct hw_perf_event *hwc = &event->hw;
-
-			if (!event)
-				continue;
-
-			armpmu->enable(hwc, hwc->idx);
-		}
-
-		/* Reset bit so we don't needlessly re-enable counters.*/
-		*hw_events->from_idle = 0;
-		/* Don't start the PMU before enabling counters after idle. */
-		isb();
-	}
 
 	if (enabled)
 		armpmu->start();
@@ -644,7 +684,7 @@ enum armv8_pmuv3_perf_types {
 };
 
 /* PMUv3 HW events mapping. */
-const unsigned armv8_pmuv3_perf_map[PERF_COUNT_HW_MAX] = {
+static const unsigned armv8_pmuv3_perf_map[PERF_COUNT_HW_MAX] = {
 	[PERF_COUNT_HW_CPU_CYCLES]		= ARMV8_PMUV3_PERFCTR_CLOCK_CYCLES,
 	[PERF_COUNT_HW_INSTRUCTIONS]		= ARMV8_PMUV3_PERFCTR_INSTR_EXECUTED,
 	[PERF_COUNT_HW_CACHE_REFERENCES]	= ARMV8_PMUV3_PERFCTR_L1_DCACHE_ACCESS,
@@ -656,9 +696,9 @@ const unsigned armv8_pmuv3_perf_map[PERF_COUNT_HW_MAX] = {
 	[PERF_COUNT_HW_STALLED_CYCLES_BACKEND]	= HW_OP_UNSUPPORTED,
 };
 
-const unsigned armv8_pmuv3_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]
-					 [PERF_COUNT_HW_CACHE_OP_MAX]
-					 [PERF_COUNT_HW_CACHE_RESULT_MAX] = {
+static const unsigned armv8_pmuv3_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]
+						[PERF_COUNT_HW_CACHE_OP_MAX]
+						[PERF_COUNT_HW_CACHE_RESULT_MAX] = {
 	[C(L1D)] = {
 		[C(OP_READ)] = {
 			[C(RESULT_ACCESS)]	= ARMV8_PMUV3_PERFCTR_L1_DCACHE_ACCESS,
@@ -818,7 +858,7 @@ static inline u32 armv8pmu_pmcr_read(void)
 	return val;
 }
 
-inline void armv8pmu_pmcr_write(u32 val)
+static inline void armv8pmu_pmcr_write(u32 val)
 {
 	val &= ARMV8_PMCR_MASK;
 	isb();
@@ -894,7 +934,7 @@ static inline void armv8pmu_write_counter(int idx, u32 value)
 		asm volatile("msr pmxevcntr_el0, %0" :: "r" (value));
 }
 
-inline void armv8pmu_write_evtype(int idx, u32 val)
+static inline void armv8pmu_write_evtype(int idx, u32 val)
 {
 	if (armv8pmu_select_counter(idx) == idx) {
 		val &= ARMV8_EVTYPE_MASK;
@@ -902,7 +942,7 @@ inline void armv8pmu_write_evtype(int idx, u32 val)
 	}
 }
 
-inline int armv8pmu_enable_counter(int idx)
+static inline int armv8pmu_enable_counter(int idx)
 {
 	u32 counter;
 
@@ -917,7 +957,7 @@ inline int armv8pmu_enable_counter(int idx)
 	return idx;
 }
 
-inline int armv8pmu_disable_counter(int idx)
+static inline int armv8pmu_disable_counter(int idx)
 {
 	u32 counter;
 
@@ -932,7 +972,7 @@ inline int armv8pmu_disable_counter(int idx)
 	return idx;
 }
 
-inline int armv8pmu_enable_intens(int idx)
+static inline int armv8pmu_enable_intens(int idx)
 {
 	u32 counter;
 
@@ -947,7 +987,7 @@ inline int armv8pmu_enable_intens(int idx)
 	return idx;
 }
 
-inline int armv8pmu_disable_intens(int idx)
+static inline int armv8pmu_disable_intens(int idx)
 {
 	u32 counter;
 
@@ -966,7 +1006,7 @@ inline int armv8pmu_disable_intens(int idx)
 	return idx;
 }
 
-inline u32 armv8pmu_getreset_flags(void)
+static inline u32 armv8pmu_getreset_flags(void)
 {
 	u32 value;
 
@@ -984,7 +1024,6 @@ static void armv8pmu_enable_event(struct hw_perf_event *hwc, int idx)
 {
 	unsigned long flags;
 	struct pmu_hw_events *events = cpu_pmu->get_hw_events();
-	u64 prev_count = local64_read(&hwc->prev_count);
 
 	/*
 	 * Enable counter and interrupt, and set the counter to count
@@ -1007,11 +1046,6 @@ static void armv8pmu_enable_event(struct hw_perf_event *hwc, int idx)
 	 */
 	armv8pmu_enable_intens(idx);
 
-	/*
-	 * Restore previous value
-	 */
-	armv8pmu_write_counter(idx, prev_count & 0xffffffff);
-
 	/*
 	 * Enable counter
 	 */
@@ -1043,105 +1077,6 @@ static void armv8pmu_disable_event(struct hw_perf_event *hwc, int idx)
 	raw_spin_unlock_irqrestore(&events->pmu_lock, flags);
 }
 
-static int armv8pmu_request_irq(struct arm_pmu *cpu_pmu, irq_handler_t handler)
-{
-	int err, irq;
-	unsigned int i, irqs;
-	struct platform_device *pmu_device = cpu_pmu->plat_device;
-
-	irqs = min(pmu_device->num_resources, num_possible_cpus());
-	if (!irqs) {
-		pr_err("no irqs for PMUs defined\n");
-		return -ENODEV;
-	}
-
-	irq = platform_get_irq(pmu_device, 0);
-	if (irq <= 0) {
-		pr_err("failed to get valid irq for PMU device\n");
-		return -ENODEV;
-	}
-
-	if (irq_is_percpu(irq)) {
-		err = request_percpu_irq(irq, handler,
-				"arm-pmu", &cpu_hw_events);
-
-		if (err) {
-			pr_err("unable to request percpu IRQ%d for ARM PMU counters\n",
-					irq);
-			return err;
-		}
-
-		on_each_cpu(armpmu_enable_percpu_irq, &irq, 1);
-	} else {
-		for (i = 0; i < irqs; ++i) {
-			err = 0;
-			irq = platform_get_irq(pmu_device, i);
-			if (irq <= 0)
-				continue;
-
-			/*
-			 * If we have a single PMU interrupt that we can't
-			 * shift, assume that we're running on a uniprocessor
-			 * machine and continue.
-			 * Otherwise, continue without this interrupt.
-			 */
-			if (irq_set_affinity(irq, cpumask_of(i)) && irqs > 1) {
-				pr_warn("unable to set irq affinity (irq=%d, cpu=%u)\n",
-						irq, i);
-				continue;
-			}
-
-			err = request_irq(irq, handler, IRQF_NOBALANCING,
-					"arm-pmu", cpu_pmu);
-			if (err) {
-				pr_err("unable to request IRQ%d for ARM PMU counters\n",
-						irq);
-				return err;
-			}
-
-			cpumask_set_cpu(i, &cpu_pmu->active_irqs);
-		}
-	}
-
-	return 0;
-}
-
-static void armv8pmu_free_irq(struct arm_pmu *cpu_pmu)
-{
-	int irq;
-	unsigned int i, irqs;
-	struct platform_device *pmu_device = cpu_pmu->plat_device;
-
-	irqs = min(pmu_device->num_resources, num_possible_cpus());
-	if (!irqs)
-		return;
-
-	irq = platform_get_irq(pmu_device, 0);
-	if (irq <= 0)
-		return;
-
-	/*
-	 * If a cpu comes online during this function, do not enable its irq.
-	 * If a cpu goes offline, it should disable its irq.
-	 */
-	cpu_pmu->pmu_state = ARM_PMU_STATE_GOING_DOWN;
-
-	if (irq_is_percpu(irq)) {
-		on_each_cpu(armpmu_disable_percpu_irq, &irq, 1);
-		free_percpu_irq(irq, &cpu_hw_events);
-	} else {
-		for (i = 0; i < irqs; ++i) {
-			if (!cpumask_test_and_clear_cpu(i,
-							&cpu_pmu->active_irqs))
-				continue;
-			irq = platform_get_irq(pmu_device, i);
-			if (irq > 0)
-				free_irq(irq, cpu_pmu);
-		}
-	}
-	cpu_pmu->pmu_state = ARM_PMU_STATE_OFF;
-}
-
 static irqreturn_t armv8pmu_handle_irq(int irq_num, void *dev)
 {
 	u32 pmovsr;
@@ -1232,10 +1167,12 @@ static int armv8pmu_get_event_idx(struct pmu_hw_events *cpuc,
 	int idx;
 	unsigned long evtype = event->config_base & ARMV8_EVTYPE_EVENT;
 
-	/* Place the first cycle counter request into the cycle counter. */
+	/* Always place a cycle counter into the cycle counter. */
 	if (evtype == ARMV8_PMUV3_PERFCTR_CLOCK_CYCLES) {
-		if (!test_and_set_bit(ARMV8_IDX_CYCLE_COUNTER, cpuc->used_mask))
-			return ARMV8_IDX_CYCLE_COUNTER;
+		if (test_and_set_bit(ARMV8_IDX_CYCLE_COUNTER, cpuc->used_mask))
+			return -EAGAIN;
+
+		return ARMV8_IDX_CYCLE_COUNTER;
 	}
 
 	/*
@@ -1259,6 +1196,8 @@ static int armv8pmu_set_event_filter(struct hw_perf_event *event,
 {
 	unsigned long config_base = 0;
 
+	if (attr->exclude_idle)
+		return -EPERM;
 	if (attr->exclude_user)
 		config_base |= ARMV8_EXCLUDE_EL0;
 	if (attr->exclude_kernel)
@@ -1275,22 +1214,6 @@ static int armv8pmu_set_event_filter(struct hw_perf_event *event,
 	return 0;
 }
 
-#ifdef CONFIG_PERF_EVENTS_USERMODE
-static void armv8pmu_init_usermode(void)
-{
-	/* Enable access from userspace. */
-	asm volatile("msr pmuserenr_el0, %0" :: "r" (0xF));
-
-}
-#else
-static inline void armv8pmu_init_usermode(void)
-{
-	/* Disable access from userspace. */
-	asm volatile("msr pmuserenr_el0, %0" :: "r" (0));
-
-}
-#endif
-
 static void armv8pmu_reset(void *info)
 {
 	u32 idx, nb_cnt = cpu_pmu->num_events;
@@ -1300,9 +1223,10 @@ static void armv8pmu_reset(void *info)
 		armv8pmu_disable_event(NULL, idx);
 
 	/* Initialize & Reset PMNC: C and P bits. */
-	armv8pmu_pmcr_write(armv8pmu_pmcr_read() | ARMV8_PMCR_P | ARMV8_PMCR_C);
+	armv8pmu_pmcr_write(ARMV8_PMCR_P | ARMV8_PMCR_C);
 
-	armv8pmu_init_usermode();
+	/* Disable access from userspace. */
+	asm volatile("msr pmuserenr_el0, %0" :: "r" (0));
 }
 
 static int armv8_pmuv3_map_event(struct perf_event *event)
@@ -1312,27 +1236,6 @@ static int armv8_pmuv3_map_event(struct perf_event *event)
 				ARMV8_EVTYPE_EVENT);
 }
 
-static void armv8pmu_save_pm_registers(void *hcpu)
-{
-	u32 val;
-	u64 lcpu = (u64)hcpu;
-	int cpu = (int)lcpu;
-
-	asm volatile("mrs %0, pmuserenr_el0" : "=r" (val));
-	per_cpu(armv8_pm_pmuserenr, cpu) = val;
-}
-
-static void armv8pmu_restore_pm_registers(void *hcpu)
-{
-	u32 val;
-	u64 lcpu = (u64)hcpu;
-	int cpu = (int)lcpu;
-
-	val = per_cpu(armv8_pm_pmuserenr, cpu);
-	if (val != 0)
-		asm volatile("msr pmuserenr_el0, %0" :: "r" (val));
-}
-
 static struct arm_pmu armv8pmu = {
 	.handle_irq		= armv8pmu_handle_irq,
 	.enable			= armv8pmu_enable_event,
@@ -1343,10 +1246,6 @@ static struct arm_pmu armv8pmu = {
 	.start			= armv8pmu_start,
 	.stop			= armv8pmu_stop,
 	.reset			= armv8pmu_reset,
-	.request_irq		= armv8pmu_request_irq,
-	.free_irq		= armv8pmu_free_irq,
-	.save_pm_registers	= armv8pmu_save_pm_registers,
-	.restore_pm_registers	= armv8pmu_restore_pm_registers,
 	.max_period		= (1LLU << 32) - 1,
 };
 
@@ -1383,272 +1282,20 @@ cpu_pmu_reset(void)
 }
 arch_initcall(cpu_pmu_reset);
 
-static int cpu_has_active_perf(int cpu)
-{
-	struct pmu_hw_events *hw_events;
-	int enabled;
-
-	if (!cpu_pmu)
-		return 0;
-	hw_events = &per_cpu(cpu_hw_events, cpu);
-	enabled = bitmap_weight(hw_events->used_mask, cpu_pmu->num_events);
-
-	if (enabled)
-		/*Even one event's existence is good enough.*/
-		return 1;
-
-	return 0;
-}
-
-static void armpmu_update_counters(void *x)
-{
-	struct pmu_hw_events *hw_events;
-	int idx;
-
-	if (!cpu_pmu)
-		return;
-
-	hw_events = cpu_pmu->get_hw_events();
-
-	for (idx = 0; idx <= cpu_pmu->num_events; ++idx) {
-		struct perf_event *event = hw_events->events[idx];
-
-		if (!event)
-			continue;
-
-		cpu_pmu->pmu.read(event);
-	}
-}
-
-static void armpmu_idle_update(void)
-{
-	struct pmu_hw_events *hw_events;
-	int idx;
-
-	if (!cpu_pmu)
-		return;
-
-	hw_events = cpu_pmu->get_hw_events();
-
-	for (idx = 0; idx <= cpu_pmu->num_events; ++idx) {
-		struct perf_event *event = hw_events->events[idx];
-
-		if (!event || !event->attr.exclude_idle)
-			continue;
-
-		cpu_pmu->pmu.read(event);
-	}
-}
-
-static void armpmu_hotplug_enable(void *parm_pmu)
-{
-	struct arm_pmu *armpmu = parm_pmu;
-	struct pmu *pmu = &(armpmu->pmu);
-	struct pmu_hw_events *hw_events = armpmu->get_hw_events();
-	int idx;
-
-	for (idx = 0; idx <= armpmu->num_events; ++idx) {
-		struct perf_event *event = hw_events->events[idx];
-		if (!event)
-			continue;
-
-		event->state = event->hotplug_save_state;
-		pmu->start(event, 0);
-	}
-	per_cpu(hotplug_down, smp_processor_id()) = 0;
-}
-
-static void armpmu_hotplug_disable(void *parm_pmu)
-{
-	struct arm_pmu *armpmu = parm_pmu;
-	struct pmu *pmu = &(armpmu->pmu);
-	struct pmu_hw_events *hw_events = armpmu->get_hw_events();
-	int idx;
-
-	for (idx = 0; idx <= armpmu->num_events; ++idx) {
-		struct perf_event *event = hw_events->events[idx];
-		if (!event)
-			continue;
-
-		event->hotplug_save_state = event->state;
-		/*
-		 * Prevent timer tick handler perf callback from enabling
-		 * this event and potentially generating an interrupt
-		 * before the CPU goes down.
-		 */
-		event->state = PERF_EVENT_STATE_OFF;
-		pmu->stop(event, 0);
-	}
-	per_cpu(hotplug_down, smp_processor_id()) = 1;
-}
-
-/*
- * PMU hardware loses all context when a CPU goes offline.
- * When a CPU is hotplugged back in, since some hardware registers are
- * UNKNOWN at reset, the PMU must be explicitly reset to avoid reading
- * junk values out of them.
- */
-static int __cpuinit cpu_pmu_notify(struct notifier_block *b,
-				    unsigned long action, void *hcpu)
-{
-	int irq;
-	struct pmu *pmu;
-	u64 lcpu = (u64)hcpu;
-	int cpu = (int)lcpu;
-	unsigned long masked_action = action & ~CPU_TASKS_FROZEN;
-	int ret = NOTIFY_DONE;
-
-	if ((masked_action != CPU_DOWN_PREPARE) &&
-	    (masked_action != CPU_DOWN_FAILED) &&
-	    (masked_action != CPU_STARTING))
-		return NOTIFY_DONE;
-
-	if (masked_action == CPU_STARTING)
-		ret = NOTIFY_OK;
-
-	if (!cpu_pmu)
-		return ret;
-
-	switch (masked_action) {
-	case CPU_DOWN_PREPARE:
-		if (cpu_pmu->save_pm_registers)
-			smp_call_function_single(cpu,
-				cpu_pmu->save_pm_registers, hcpu, 1);
-		if (cpu_pmu->pmu_state != ARM_PMU_STATE_OFF) {
-			if (cpu_has_active_perf(cpu))
-				smp_call_function_single(cpu,
-					armpmu_hotplug_disable, cpu_pmu, 1);
-			/* Disarm the PMU IRQ before disappearing. */
-			if (cpu_pmu->plat_device) {
-				irq = cpu_pmu->percpu_irq;
-				smp_call_function_single(cpu,
-					    armpmu_disable_percpu_irq, &irq, 1);
-			}
-		}
-		break;
-
-	case CPU_STARTING:
-	case CPU_DOWN_FAILED:
-		/* Reset PMU to clear counters for ftrace buffer */
-		if (cpu_pmu->reset)
-			cpu_pmu->reset(NULL);
-		if (cpu_pmu->restore_pm_registers)
-			cpu_pmu->restore_pm_registers(hcpu);
-		if (cpu_pmu->pmu_state == ARM_PMU_STATE_RUNNING) {
-			/* Arm the PMU IRQ before appearing. */
-			if (cpu_pmu->plat_device) {
-				irq = cpu_pmu->percpu_irq;
-				armpmu_enable_percpu_irq(&irq);
-			}
-			if (cpu_has_active_perf(cpu)) {
-				armpmu_hotplug_enable(cpu_pmu);
-				pmu = &cpu_pmu->pmu;
-				pmu->pmu_enable(pmu);
-			}
-		}
-		break;
-	}
-	return ret;
-}
-
-static struct notifier_block cpu_pmu_hotplug_notifier __cpuinitdata = {
-	.notifier_call = cpu_pmu_notify,
-};
-
-static int perf_cpu_pm_notifier(struct notifier_block *self, unsigned long cmd,
-		void *v)
-{
-	struct pmu *pmu;
-	u64 lcpu = smp_processor_id();
-	int cpu = (int)lcpu;
-
-	if (!cpu_pmu)
-		return NOTIFY_OK;
-
-	/* If the cpu is going down, don't do anything here */
-	if (per_cpu(hotplug_down, cpu))
-		return NOTIFY_OK;
-
-	switch (cmd) {
-	case CPU_PM_ENTER:
-		if (cpu_pmu->save_pm_registers)
-			cpu_pmu->save_pm_registers((void *)lcpu);
-		if (cpu_has_active_perf(cpu)) {
-			armpmu_update_counters(NULL);
-			pmu = &cpu_pmu->pmu;
-			pmu->pmu_disable(pmu);
-		}
-		break;
-
-	case CPU_PM_ENTER_FAILED:
-	case CPU_PM_EXIT:
-		if (cpu_has_active_perf(cpu) && cpu_pmu->reset)
-			cpu_pmu->reset(NULL);
-		if (cpu_pmu->restore_pm_registers)
-			cpu_pmu->restore_pm_registers((void *)lcpu);
-		if (cpu_has_active_perf(cpu)) {
-			/*
-			 * Flip this bit so armpmu_enable knows it needs
-			 * to re-enable active counters.
-			 */
-			__get_cpu_var(from_idle) = 1;
-			pmu = &cpu_pmu->pmu;
-			pmu->pmu_enable(pmu);
-		}
-		break;
-	}
-
-	return NOTIFY_OK;
-}
-
-static struct notifier_block perf_cpu_pm_notifier_block = {
-	.notifier_call = perf_cpu_pm_notifier,
-};
-
-static int perf_cpu_idle_notifier(struct notifier_block *nb,
-				unsigned long action, void *data)
-{
-	if (action == IDLE_START)
-		armpmu_idle_update();
-
-	return NOTIFY_OK;
-}
-
-static struct notifier_block perf_cpu_idle_nb = {
-	.notifier_call = perf_cpu_idle_notifier,
-};
-
 /*
  * PMU platform driver and devicetree bindings.
  */
 static const struct of_device_id armpmu_of_device_ids[] = {
 	{.compatible = "arm,armv8-pmuv3"},
-#ifdef CONFIG_ARCH_MSM8996
-	{.compatible = "qcom,kryo-pmuv3", .data = kryo_pmu_init},
-#endif
 	{},
 };
 
 static int armpmu_device_probe(struct platform_device *pdev)
 {
-	struct device_node *node = pdev->dev.of_node;
-	const struct of_device_id *of_id;
-	int (*init_fn)(struct arm_pmu *);
-
 	if (!cpu_pmu)
 		return -ENODEV;
 
-	if (node) {
-		of_id = of_match_node(armpmu_of_device_ids, node);
-		if (of_id) {
-			init_fn = of_id->data;
-			if (init_fn)
-				init_fn(cpu_pmu);
-		}
-	}
-
 	cpu_pmu->plat_device = pdev;
-	cpu_pmu->percpu_irq = platform_get_irq(cpu_pmu->plat_device, 0);
 	return 0;
 }
 
@@ -1662,29 +1309,7 @@ static struct platform_driver armpmu_driver = {
 
 static int __init register_pmu_driver(void)
 {
-	int err;
-
-	err = register_cpu_notifier(&cpu_pmu_hotplug_notifier);
-	if (err)
-		return err;
-
-	err = cpu_pm_register_notifier(&perf_cpu_pm_notifier_block);
-	if (err)
-		goto err_cpu_pm;
-
-	idle_notifier_register(&perf_cpu_idle_nb);
-
-	err = platform_driver_register(&armpmu_driver);
-	if (err)
-		goto err_driver;
-	return 0;
-
-err_driver:
-	cpu_pm_unregister_notifier(&perf_cpu_pm_notifier_block);
-	idle_notifier_unregister(&perf_cpu_idle_nb);
-err_cpu_pm:
-	unregister_cpu_notifier(&cpu_pmu_hotplug_notifier);
-	return err;
+	return platform_driver_register(&armpmu_driver);
 }
 device_initcall(register_pmu_driver);
 
@@ -1700,7 +1325,6 @@ static void __init cpu_pmu_init(struct arm_pmu *armpmu)
 		struct pmu_hw_events *events = &per_cpu(cpu_hw_events, cpu);
 		events->events = per_cpu(hw_events, cpu);
 		events->used_mask = per_cpu(used_mask, cpu);
-		events->from_idle = &per_cpu(from_idle, cpu);
 		raw_spin_lock_init(&events->pmu_lock);
 	}
 	armpmu->get_hw_events = armpmu_get_cpu_events;
@@ -1708,9 +1332,9 @@ static void __init cpu_pmu_init(struct arm_pmu *armpmu)
 
 static int __init init_hw_perf_events(void)
 {
-	u64 dfr = read_system_reg(SYS_ID_AA64DFR0_EL1);
+	u64 dfr = read_cpuid(ID_AA64DFR0_EL1);
 
-	switch (cpuid_feature_extract_field(dfr, ID_AA64DFR0_PMUVER_SHIFT)) {
+	switch ((dfr >> 8) & 0xf) {
 	case 0x1:	/* PMUv3 */
 		cpu_pmu = armv8_pmuv3_pmu_init();
 		break;
@@ -1906,78 +1530,3 @@ unsigned long perf_misc_flags(struct pt_regs *regs)
 
 	return misc;
 }
-
-static struct dentry *perf_debug_dir;
-
-struct dentry *perf_create_debug_dir(void)
-{
-	if (!perf_debug_dir)
-		perf_debug_dir = debugfs_create_dir("msm_perf", NULL);
-	return perf_debug_dir;
-}
-
-#ifdef CONFIG_PERF_EVENTS_RESET_PMU_DEBUGFS
-static __ref void reset_pmu_force(void)
-{
-	int cpu, ret;
-	u32 save_online_mask = 0;
-
-	for_each_possible_cpu(cpu) {
-		if (!cpu_online(cpu)) {
-			save_online_mask |= BIT(cpu);
-			lock_device_hotplug();
-			ret = device_online(get_cpu_device(cpu));
-			unlock_device_hotplug();
-			if (ret)
-				pr_err("Failed to bring up CPU: %d, ret: %d\n",
-				       cpu, ret);
-		}
-	}
-	if (cpu_pmu && cpu_pmu->reset)
-		on_each_cpu(cpu_pmu->reset, NULL, 1);
-	if (cpu_pmu && cpu_pmu->plat_device)
-		armpmu_release_hardware(cpu_pmu);
-	for_each_possible_cpu(cpu) {
-		if ((save_online_mask & BIT(cpu)) && cpu_online(cpu)) {
-			lock_device_hotplug();
-			ret = device_offline(get_cpu_device(cpu));
-			unlock_device_hotplug();
-			if (ret)
-				pr_err("Failed to bring down CPU: %d, ret: %d\n",
-						cpu, ret);
-		}
-	}
-}
-
-static int write_enabled_perfpmu_action(void *data, u64 val)
-{
-	if (val != 0)
-		reset_pmu_force();
-	return 0;
-}
-
-DEFINE_SIMPLE_ATTRIBUTE(fops_pmuaction,
-		NULL, write_enabled_perfpmu_action, "%llu\n");
-
-int __init init_pmu_actions(void)
-{
-	struct dentry *dir;
-	struct dentry *file;
-	unsigned int value = 1;
-
-	dir = perf_create_debug_dir();
-	if (!dir)
-		return -ENOMEM;
-	file = debugfs_create_file("resetpmu", 0220, dir,
-		&value, &fops_pmuaction);
-	if (!file)
-		return -ENOMEM;
-	return 0;
-}
-#else
-int __init init_pmu_actions(void)
-{
-	return 0;
-}
-#endif
-late_initcall(init_pmu_actions);
diff --git a/kernel/msm-3.18/arch/avr32/mm/fault.c b/kernel/msm-3.18/arch/avr32/mm/fault.c
index d223a8b57..8a7fb9343 100644
--- a/kernel/msm-3.18/arch/avr32/mm/fault.c
+++ b/kernel/msm-3.18/arch/avr32/mm/fault.c
@@ -81,7 +81,7 @@ asmlinkage void do_page_fault(unsigned long ecr, struct pt_regs *regs)
 	 * If we're in an interrupt or have no user context, we must
 	 * not take the fault...
 	 */
-	if (in_atomic() || !mm || regs->sr & SYSREG_BIT(GM))
+	if (!mm || regs->sr & SYSREG_BIT(GM) || pagefault_disabled())
 		goto no_context;
 
 	local_irq_enable();
diff --git a/kernel/msm-3.18/arch/cris/mm/fault.c b/kernel/msm-3.18/arch/cris/mm/fault.c
index 2686a7aa8..b1a5631d9 100644
--- a/kernel/msm-3.18/arch/cris/mm/fault.c
+++ b/kernel/msm-3.18/arch/cris/mm/fault.c
@@ -113,7 +113,7 @@ do_page_fault(unsigned long address, struct pt_regs *regs,
 	 * user context, we must not take the fault.
 	 */
 
-	if (in_atomic() || !mm)
+	if (!mm || pagefault_disabled())
 		goto no_context;
 
 	if (user_mode(regs))
diff --git a/kernel/msm-3.18/arch/frv/mm/fault.c b/kernel/msm-3.18/arch/frv/mm/fault.c
index ec4917ddf..c90d23f50 100644
--- a/kernel/msm-3.18/arch/frv/mm/fault.c
+++ b/kernel/msm-3.18/arch/frv/mm/fault.c
@@ -78,7 +78,7 @@ asmlinkage void do_page_fault(int datammu, unsigned long esr0, unsigned long ear
 	 * If we're in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
-	if (in_atomic() || !mm)
+	if (!mm || pagefault_disabled())
 		goto no_context;
 
 	if (user_mode(__frame))
diff --git a/kernel/msm-3.18/arch/ia64/mm/fault.c b/kernel/msm-3.18/arch/ia64/mm/fault.c
index ba5ba7acc..f9f7d083c 100644
--- a/kernel/msm-3.18/arch/ia64/mm/fault.c
+++ b/kernel/msm-3.18/arch/ia64/mm/fault.c
@@ -96,7 +96,7 @@ ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *re
 	/*
 	 * If we're in an interrupt or have no user context, we must not take the fault..
 	 */
-	if (in_atomic() || !mm)
+	if (!mm || pagefault_disabled())
 		goto no_context;
 
 #ifdef CONFIG_VIRTUAL_MEM_MAP
diff --git a/kernel/msm-3.18/arch/m32r/mm/fault.c b/kernel/msm-3.18/arch/m32r/mm/fault.c
index e3d4d4890..7069ad371 100644
--- a/kernel/msm-3.18/arch/m32r/mm/fault.c
+++ b/kernel/msm-3.18/arch/m32r/mm/fault.c
@@ -114,7 +114,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code,
 	 * If we're in an interrupt or have no user context or are running in an
 	 * atomic region then we must not take the fault..
 	 */
-	if (in_atomic() || !mm)
+	if (!mm || pagefault_disabled())
 		goto bad_area_nosemaphore;
 
 	if (error_code & ACE_USERMODE)
diff --git a/kernel/msm-3.18/arch/m68k/mm/fault.c b/kernel/msm-3.18/arch/m68k/mm/fault.c
index b2f04aee4..5de2621aa 100644
--- a/kernel/msm-3.18/arch/m68k/mm/fault.c
+++ b/kernel/msm-3.18/arch/m68k/mm/fault.c
@@ -81,7 +81,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * If we're in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
-	if (in_atomic() || !mm)
+	if (!mm || pagefault_disabled())
 		goto no_context;
 
 	if (user_mode(regs))
diff --git a/kernel/msm-3.18/arch/microblaze/mm/fault.c b/kernel/msm-3.18/arch/microblaze/mm/fault.c
index d46a5ebb7..914ae86b8 100644
--- a/kernel/msm-3.18/arch/microblaze/mm/fault.c
+++ b/kernel/msm-3.18/arch/microblaze/mm/fault.c
@@ -107,7 +107,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long address,
 	if ((error_code & 0x13) == 0x13 || (error_code & 0x11) == 0x11)
 		is_write = 0;
 
-	if (unlikely(in_atomic() || !mm)) {
+	if (unlikely(!mm || pagefault_disabled())) {
 		if (kernel_mode(regs))
 			goto bad_area_nosemaphore;
 
diff --git a/kernel/msm-3.18/arch/mips/Kconfig b/kernel/msm-3.18/arch/mips/Kconfig
index 32aff6337..b21f29091 100644
--- a/kernel/msm-3.18/arch/mips/Kconfig
+++ b/kernel/msm-3.18/arch/mips/Kconfig
@@ -2196,7 +2196,7 @@ config CPU_R4400_WORKAROUNDS
 #
 config HIGHMEM
 	bool "High Memory Support"
-	depends on 32BIT && CPU_SUPPORTS_HIGHMEM && SYS_SUPPORTS_HIGHMEM && !CPU_MIPS32_3_5_EVA
+	depends on 32BIT && CPU_SUPPORTS_HIGHMEM && SYS_SUPPORTS_HIGHMEM && !CPU_MIPS32_3_5_EVA && !PREEMPT_RT_FULL
 
 config CPU_SUPPORTS_HIGHMEM
 	bool
diff --git a/kernel/msm-3.18/arch/mips/kernel/signal.c b/kernel/msm-3.18/arch/mips/kernel/signal.c
index 27a1646a7..cf2ec9898 100644
--- a/kernel/msm-3.18/arch/mips/kernel/signal.c
+++ b/kernel/msm-3.18/arch/mips/kernel/signal.c
@@ -613,6 +613,7 @@ asmlinkage void do_notify_resume(struct pt_regs *regs, void *unused,
 	__u32 thread_info_flags)
 {
 	local_irq_enable();
+	preempt_check_resched();
 
 	user_exit();
 
diff --git a/kernel/msm-3.18/arch/mips/mm/fault.c b/kernel/msm-3.18/arch/mips/mm/fault.c
index 70ab5d664..5b41b84d9 100644
--- a/kernel/msm-3.18/arch/mips/mm/fault.c
+++ b/kernel/msm-3.18/arch/mips/mm/fault.c
@@ -89,7 +89,7 @@ static void __kprobes __do_page_fault(struct pt_regs *regs, unsigned long write,
 	 * If we're in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
-	if (in_atomic() || !mm)
+	if (!mm || pagefault_disabled())
 		goto bad_area_nosemaphore;
 
 	if (user_mode(regs))
diff --git a/kernel/msm-3.18/arch/mips/mm/init.c b/kernel/msm-3.18/arch/mips/mm/init.c
index f42e35e42..e09dae6ce 100644
--- a/kernel/msm-3.18/arch/mips/mm/init.c
+++ b/kernel/msm-3.18/arch/mips/mm/init.c
@@ -90,7 +90,7 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 
 	BUG_ON(Page_dcache_dirty(page));
 
-	pagefault_disable();
+	raw_pagefault_disable();
 	idx = (addr >> PAGE_SHIFT) & (FIX_N_COLOURS - 1);
 	idx += in_interrupt() ? FIX_N_COLOURS : 0;
 	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
@@ -146,7 +146,7 @@ void kunmap_coherent(void)
 	tlbw_use_hazard();
 	write_c0_entryhi(old_ctx);
 	local_irq_restore(flags);
-	pagefault_enable();
+	raw_pagefault_enable();
 }
 
 void copy_user_highpage(struct page *to, struct page *from,
diff --git a/kernel/msm-3.18/arch/mn10300/mm/fault.c b/kernel/msm-3.18/arch/mn10300/mm/fault.c
index 0c2cc5d39..54305d255 100644
--- a/kernel/msm-3.18/arch/mn10300/mm/fault.c
+++ b/kernel/msm-3.18/arch/mn10300/mm/fault.c
@@ -168,7 +168,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long fault_code,
 	 * If we're in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
-	if (in_atomic() || !mm)
+	if (!mm || pagefault_disabled())
 		goto no_context;
 
 	if ((fault_code & MMUFCR_xFC_ACCESS) == MMUFCR_xFC_ACCESS_USR)
diff --git a/kernel/msm-3.18/arch/parisc/mm/fault.c b/kernel/msm-3.18/arch/parisc/mm/fault.c
index 3b7c02f9b..95475ce74 100644
--- a/kernel/msm-3.18/arch/parisc/mm/fault.c
+++ b/kernel/msm-3.18/arch/parisc/mm/fault.c
@@ -208,7 +208,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long code,
 	int fault;
 	unsigned int flags;
 
-	if (in_atomic())
+	if (pagefault_disabled())
 		goto no_context;
 
 	tsk = current;
diff --git a/kernel/msm-3.18/arch/powerpc/Kconfig b/kernel/msm-3.18/arch/powerpc/Kconfig
index 33c84664a..2b3133925 100644
--- a/kernel/msm-3.18/arch/powerpc/Kconfig
+++ b/kernel/msm-3.18/arch/powerpc/Kconfig
@@ -60,10 +60,11 @@ config LOCKDEP_SUPPORT
 
 config RWSEM_GENERIC_SPINLOCK
 	bool
+	default y if PREEMPT_RT_FULL
 
 config RWSEM_XCHGADD_ALGORITHM
 	bool
-	default y
+	default y if !PREEMPT_RT_FULL
 
 config GENERIC_LOCKBREAK
 	bool
@@ -137,6 +138,7 @@ config PPC
 	select ARCH_HAS_TICK_BROADCAST if GENERIC_CLOCKEVENTS_BROADCAST
 	select GENERIC_STRNCPY_FROM_USER
 	select GENERIC_STRNLEN_USER
+	select HAVE_PREEMPT_LAZY
 	select HAVE_MOD_ARCH_SPECIFIC
 	select MODULES_USE_ELF_RELA
 	select CLONE_BACKWARDS
@@ -304,7 +306,7 @@ menu "Kernel options"
 
 config HIGHMEM
 	bool "High memory support"
-	depends on PPC32
+	depends on PPC32 && !PREEMPT_RT_FULL
 
 source kernel/Kconfig.hz
 source kernel/Kconfig.preempt
diff --git a/kernel/msm-3.18/arch/powerpc/include/asm/kvm_host.h b/kernel/msm-3.18/arch/powerpc/include/asm/kvm_host.h
index 9bb51b9e0..8f6428c12 100644
--- a/kernel/msm-3.18/arch/powerpc/include/asm/kvm_host.h
+++ b/kernel/msm-3.18/arch/powerpc/include/asm/kvm_host.h
@@ -296,7 +296,7 @@ struct kvmppc_vcore {
 	u8 in_guest;
 	struct list_head runnable_threads;
 	spinlock_t lock;
-	wait_queue_head_t wq;
+	struct swait_head wq;
 	u64 stolen_tb;
 	u64 preempt_tb;
 	struct kvm_vcpu *runner;
@@ -619,7 +619,7 @@ struct kvm_vcpu_arch {
 	u8 prodded;
 	u32 last_inst;
 
-	wait_queue_head_t *wqp;
+	struct swait_head *wqp;
 	struct kvmppc_vcore *vcore;
 	int ret;
 	int trap;
diff --git a/kernel/msm-3.18/arch/powerpc/include/asm/thread_info.h b/kernel/msm-3.18/arch/powerpc/include/asm/thread_info.h
index 9344114b1..16ff509e2 100644
--- a/kernel/msm-3.18/arch/powerpc/include/asm/thread_info.h
+++ b/kernel/msm-3.18/arch/powerpc/include/asm/thread_info.h
@@ -43,6 +43,9 @@ struct thread_info {
 	int		cpu;			/* cpu we're on */
 	int		preempt_count;		/* 0 => preemptable,
 						   <0 => BUG */
+	int		preempt_lazy_count;	/* 0 => preemptable,
+						   <0 => BUG */
+	struct restart_block restart_block;
 	unsigned long	local_flags;		/* private flags for thread */
 
 	/* low level flags - has atomic operations done on it */
@@ -58,6 +61,9 @@ struct thread_info {
 	.exec_domain =	&default_exec_domain,	\
 	.cpu =		0,			\
 	.preempt_count = INIT_PREEMPT_COUNT,	\
+	.restart_block = {			\
+		.fn = do_no_restart_syscall,	\
+	},					\
 	.flags =	0,			\
 }
 
@@ -84,8 +90,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_SYSCALL_TRACE	0	/* syscall trace active */
 #define TIF_SIGPENDING		1	/* signal pending */
 #define TIF_NEED_RESCHED	2	/* rescheduling necessary */
-#define TIF_POLLING_NRFLAG	3	/* true if poll_idle() is polling
-					   TIF_NEED_RESCHED */
+#define TIF_NEED_RESCHED_LAZY	3	/* lazy rescheduling necessary */
 #define TIF_32BIT		4	/* 32 bit binary */
 #define TIF_RESTORE_TM		5	/* need to restore TM FP/VEC/VSX */
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
@@ -103,6 +108,8 @@ static inline struct thread_info *current_thread_info(void)
 #if defined(CONFIG_PPC64)
 #define TIF_ELF2ABI		18	/* function descriptors must die! */
 #endif
+#define TIF_POLLING_NRFLAG	19	/* true if poll_idle() is polling
+					   TIF_NEED_RESCHED */
 
 /* as above, but as bit values */
 #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
@@ -121,14 +128,16 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_SYSCALL_TRACEPOINT	(1<<TIF_SYSCALL_TRACEPOINT)
 #define _TIF_EMULATE_STACK_STORE	(1<<TIF_EMULATE_STACK_STORE)
 #define _TIF_NOHZ		(1<<TIF_NOHZ)
+#define _TIF_NEED_RESCHED_LAZY	(1<<TIF_NEED_RESCHED_LAZY)
 #define _TIF_SYSCALL_T_OR_A	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
 				 _TIF_SECCOMP | _TIF_SYSCALL_TRACEPOINT | \
 				 _TIF_NOHZ)
 
 #define _TIF_USER_WORK_MASK	(_TIF_SIGPENDING | _TIF_NEED_RESCHED | \
 				 _TIF_NOTIFY_RESUME | _TIF_UPROBE | \
-				 _TIF_RESTORE_TM)
+				 _TIF_RESTORE_TM | _TIF_NEED_RESCHED_LAZY)
 #define _TIF_PERSYSCALL_MASK	(_TIF_RESTOREALL|_TIF_NOERROR)
+#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
 
 /* Bits in local_flags */
 /* Don't move TLF_NAPPING without adjusting the code in entry_32.S */
diff --git a/kernel/msm-3.18/arch/powerpc/kernel/asm-offsets.c b/kernel/msm-3.18/arch/powerpc/kernel/asm-offsets.c
index a892c651a..5184b38f5 100644
--- a/kernel/msm-3.18/arch/powerpc/kernel/asm-offsets.c
+++ b/kernel/msm-3.18/arch/powerpc/kernel/asm-offsets.c
@@ -159,6 +159,7 @@ int main(void)
 	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
 	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
 	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
+	DEFINE(TI_PREEMPT_LAZY, offsetof(struct thread_info, preempt_lazy_count));
 	DEFINE(TI_TASK, offsetof(struct thread_info, task));
 	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
 
diff --git a/kernel/msm-3.18/arch/powerpc/kernel/entry_32.S b/kernel/msm-3.18/arch/powerpc/kernel/entry_32.S
index 22b45a495..081f92619 100644
--- a/kernel/msm-3.18/arch/powerpc/kernel/entry_32.S
+++ b/kernel/msm-3.18/arch/powerpc/kernel/entry_32.S
@@ -890,7 +890,14 @@ resume_kernel:
 	cmpwi	0,r0,0		/* if non-zero, just restore regs and return */
 	bne	restore
 	andi.	r8,r8,_TIF_NEED_RESCHED
+	bne+	1f
+	lwz	r0,TI_PREEMPT_LAZY(r9)
+	cmpwi	0,r0,0		/* if non-zero, just restore regs and return */
+	bne	restore
+	lwz	r0,TI_FLAGS(r9)
+	andi.	r0,r0,_TIF_NEED_RESCHED_LAZY
 	beq+	restore
+1:
 	lwz	r3,_MSR(r1)
 	andi.	r0,r3,MSR_EE	/* interrupts off? */
 	beq	restore		/* don't schedule if so */
@@ -901,11 +908,11 @@ resume_kernel:
 	 */
 	bl	trace_hardirqs_off
 #endif
-1:	bl	preempt_schedule_irq
+2:	bl	preempt_schedule_irq
 	CURRENT_THREAD_INFO(r9, r1)
 	lwz	r3,TI_FLAGS(r9)
-	andi.	r0,r3,_TIF_NEED_RESCHED
-	bne-	1b
+	andi.	r0,r3,_TIF_NEED_RESCHED_MASK
+	bne-	2b
 #ifdef CONFIG_TRACE_IRQFLAGS
 	/* And now, to properly rebalance the above, we tell lockdep they
 	 * are being turned back on, which will happen when we return
@@ -1226,7 +1233,7 @@ global_dbcr0:
 #endif /* !(CONFIG_4xx || CONFIG_BOOKE) */
 
 do_work:			/* r10 contains MSR_KERNEL here */
-	andi.	r0,r9,_TIF_NEED_RESCHED
+	andi.	r0,r9,_TIF_NEED_RESCHED_MASK
 	beq	do_user_signal
 
 do_resched:			/* r10 contains MSR_KERNEL here */
@@ -1247,7 +1254,7 @@ recheck:
 	MTMSRD(r10)		/* disable interrupts */
 	CURRENT_THREAD_INFO(r9, r1)
 	lwz	r9,TI_FLAGS(r9)
-	andi.	r0,r9,_TIF_NEED_RESCHED
+	andi.	r0,r9,_TIF_NEED_RESCHED_MASK
 	bne-	do_resched
 	andi.	r0,r9,_TIF_USER_WORK_MASK
 	beq	restore_user
diff --git a/kernel/msm-3.18/arch/powerpc/kernel/entry_64.S b/kernel/msm-3.18/arch/powerpc/kernel/entry_64.S
index e233c0fec..b7b673a57 100644
--- a/kernel/msm-3.18/arch/powerpc/kernel/entry_64.S
+++ b/kernel/msm-3.18/arch/powerpc/kernel/entry_64.S
@@ -644,7 +644,7 @@ _GLOBAL(ret_from_except_lite)
 #else
 	beq	restore
 #endif
-1:	andi.	r0,r4,_TIF_NEED_RESCHED
+1:	andi.	r0,r4,_TIF_NEED_RESCHED_MASK
 	beq	2f
 	bl	restore_interrupts
 	SCHEDULE_USER
@@ -706,10 +706,18 @@ resume_kernel:
 
 #ifdef CONFIG_PREEMPT
 	/* Check if we need to preempt */
+	lwz	r8,TI_PREEMPT(r9)
+	cmpwi	0,r8,0		/* if non-zero, just restore regs and return */
+	bne	restore
 	andi.	r0,r4,_TIF_NEED_RESCHED
+	bne+	check_count
+
+	andi.	r0,r4,_TIF_NEED_RESCHED_LAZY
 	beq+	restore
+	lwz	r8,TI_PREEMPT_LAZY(r9)
+
 	/* Check that preempt_count() == 0 and interrupts are enabled */
-	lwz	r8,TI_PREEMPT(r9)
+check_count:
 	cmpwi	cr1,r8,0
 	ld	r0,SOFTE(r1)
 	cmpdi	r0,0
@@ -726,7 +734,7 @@ resume_kernel:
 	/* Re-test flags and eventually loop */
 	CURRENT_THREAD_INFO(r9, r1)
 	ld	r4,TI_FLAGS(r9)
-	andi.	r0,r4,_TIF_NEED_RESCHED
+	andi.	r0,r4,_TIF_NEED_RESCHED_MASK
 	bne	1b
 
 	/*
diff --git a/kernel/msm-3.18/arch/powerpc/kernel/irq.c b/kernel/msm-3.18/arch/powerpc/kernel/irq.c
index c14383575..ec023fd7b 100644
--- a/kernel/msm-3.18/arch/powerpc/kernel/irq.c
+++ b/kernel/msm-3.18/arch/powerpc/kernel/irq.c
@@ -615,6 +615,7 @@ void irq_ctx_init(void)
 	}
 }
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 void do_softirq_own_stack(void)
 {
 	struct thread_info *curtp, *irqtp;
@@ -632,6 +633,7 @@ void do_softirq_own_stack(void)
 	if (irqtp->flags)
 		set_bits(irqtp->flags, &curtp->flags);
 }
+#endif
 
 irq_hw_number_t virq_to_hw(unsigned int virq)
 {
diff --git a/kernel/msm-3.18/arch/powerpc/kernel/misc_32.S b/kernel/msm-3.18/arch/powerpc/kernel/misc_32.S
index 7c6bb4b17..e9dfe2270 100644
--- a/kernel/msm-3.18/arch/powerpc/kernel/misc_32.S
+++ b/kernel/msm-3.18/arch/powerpc/kernel/misc_32.S
@@ -40,6 +40,7 @@
  * We store the saved ksp_limit in the unused part
  * of the STACK_FRAME_OVERHEAD
  */
+#ifndef CONFIG_PREEMPT_RT_FULL
 _GLOBAL(call_do_softirq)
 	mflr	r0
 	stw	r0,4(r1)
@@ -56,6 +57,7 @@ _GLOBAL(call_do_softirq)
 	stw	r10,THREAD+KSP_LIMIT(r2)
 	mtlr	r0
 	blr
+#endif
 
 /*
  * void call_do_irq(struct pt_regs *regs, struct thread_info *irqtp);
diff --git a/kernel/msm-3.18/arch/powerpc/kernel/misc_64.S b/kernel/msm-3.18/arch/powerpc/kernel/misc_64.S
index 4e314b90c..8a7238dd2 100644
--- a/kernel/msm-3.18/arch/powerpc/kernel/misc_64.S
+++ b/kernel/msm-3.18/arch/powerpc/kernel/misc_64.S
@@ -29,6 +29,7 @@
 
 	.text
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 _GLOBAL(call_do_softirq)
 	mflr	r0
 	std	r0,16(r1)
@@ -39,6 +40,7 @@ _GLOBAL(call_do_softirq)
 	ld	r0,16(r1)
 	mtlr	r0
 	blr
+#endif
 
 _GLOBAL(call_do_irq)
 	mflr	r0
diff --git a/kernel/msm-3.18/arch/powerpc/kernel/time.c b/kernel/msm-3.18/arch/powerpc/kernel/time.c
index 7505599c2..f4de02181 100644
--- a/kernel/msm-3.18/arch/powerpc/kernel/time.c
+++ b/kernel/msm-3.18/arch/powerpc/kernel/time.c
@@ -424,7 +424,7 @@ unsigned long profile_pc(struct pt_regs *regs)
 EXPORT_SYMBOL(profile_pc);
 #endif
 
-#ifdef CONFIG_IRQ_WORK
+#if defined(CONFIG_IRQ_WORK)
 
 /*
  * 64-bit uses a byte in the PACA, 32-bit uses a per-cpu variable...
diff --git a/kernel/msm-3.18/arch/powerpc/kvm/Kconfig b/kernel/msm-3.18/arch/powerpc/kvm/Kconfig
index 602eb51d2..60fc1adab 100644
--- a/kernel/msm-3.18/arch/powerpc/kvm/Kconfig
+++ b/kernel/msm-3.18/arch/powerpc/kvm/Kconfig
@@ -157,6 +157,7 @@ config KVM_E500MC
 config KVM_MPIC
 	bool "KVM in-kernel MPIC emulation"
 	depends on KVM && E500
+	depends on !PREEMPT_RT_FULL
 	select HAVE_KVM_IRQCHIP
 	select HAVE_KVM_IRQFD
 	select HAVE_KVM_IRQ_ROUTING
diff --git a/kernel/msm-3.18/arch/powerpc/kvm/book3s_hv.c b/kernel/msm-3.18/arch/powerpc/kvm/book3s_hv.c
index 060880ac7..f114c926f 100644
--- a/kernel/msm-3.18/arch/powerpc/kvm/book3s_hv.c
+++ b/kernel/msm-3.18/arch/powerpc/kvm/book3s_hv.c
@@ -84,11 +84,11 @@ static void kvmppc_fast_vcpu_kick_hv(struct kvm_vcpu *vcpu)
 {
 	int me;
 	int cpu = vcpu->cpu;
-	wait_queue_head_t *wqp;
+	struct swait_head *wqp;
 
 	wqp = kvm_arch_vcpu_wq(vcpu);
-	if (waitqueue_active(wqp)) {
-		wake_up_interruptible(wqp);
+	if (swaitqueue_active(wqp)) {
+		swait_wake_interruptible(wqp);
 		++vcpu->stat.halt_wakeup;
 	}
 
@@ -639,8 +639,8 @@ int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu)
 		tvcpu->arch.prodded = 1;
 		smp_mb();
 		if (vcpu->arch.ceded) {
-			if (waitqueue_active(&vcpu->wq)) {
-				wake_up_interruptible(&vcpu->wq);
+			if (swaitqueue_active(&vcpu->wq)) {
+				swait_wake_interruptible(&vcpu->wq);
 				vcpu->stat.halt_wakeup++;
 			}
 		}
@@ -1363,7 +1363,7 @@ static struct kvmppc_vcore *kvmppc_vcore_create(struct kvm *kvm, int core)
 
 	INIT_LIST_HEAD(&vcore->runnable_threads);
 	spin_lock_init(&vcore->lock);
-	init_waitqueue_head(&vcore->wq);
+	init_swait_head(&vcore->wq);
 	vcore->preempt_tb = TB_NIL;
 	vcore->lpcr = kvm->arch.lpcr;
 	vcore->first_vcpuid = core * threads_per_subcore;
@@ -1832,13 +1832,13 @@ static void kvmppc_wait_for_exec(struct kvm_vcpu *vcpu, int wait_state)
  */
 static void kvmppc_vcore_blocked(struct kvmppc_vcore *vc)
 {
-	DEFINE_WAIT(wait);
+	DEFINE_SWAITER(wait);
 
-	prepare_to_wait(&vc->wq, &wait, TASK_INTERRUPTIBLE);
+	swait_prepare(&vc->wq, &wait, TASK_INTERRUPTIBLE);
 	vc->vcore_state = VCORE_SLEEPING;
 	spin_unlock(&vc->lock);
 	schedule();
-	finish_wait(&vc->wq, &wait);
+	swait_finish(&vc->wq, &wait);
 	spin_lock(&vc->lock);
 	vc->vcore_state = VCORE_INACTIVE;
 }
@@ -1879,7 +1879,7 @@ static int kvmppc_run_vcpu(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu)
 			kvmppc_create_dtl_entry(vcpu, vc);
 			kvmppc_start_thread(vcpu);
 		} else if (vc->vcore_state == VCORE_SLEEPING) {
-			wake_up(&vc->wq);
+			swait_wake(&vc->wq);
 		}
 
 	}
diff --git a/kernel/msm-3.18/arch/powerpc/mm/fault.c b/kernel/msm-3.18/arch/powerpc/mm/fault.c
index f06b56baf..1e949bd22 100644
--- a/kernel/msm-3.18/arch/powerpc/mm/fault.c
+++ b/kernel/msm-3.18/arch/powerpc/mm/fault.c
@@ -273,7 +273,7 @@ int __kprobes do_page_fault(struct pt_regs *regs, unsigned long address,
 	if (!arch_irq_disabled_regs(regs))
 		local_irq_enable();
 
-	if (in_atomic() || mm == NULL) {
+	if (in_atomic() || mm == NULL || pagefault_disabled()) {
 		if (!user_mode(regs)) {
 			rc = SIGSEGV;
 			goto bail;
diff --git a/kernel/msm-3.18/arch/s390/include/asm/kvm_host.h b/kernel/msm-3.18/arch/s390/include/asm/kvm_host.h
index 2175f911a..884ec9b27 100644
--- a/kernel/msm-3.18/arch/s390/include/asm/kvm_host.h
+++ b/kernel/msm-3.18/arch/s390/include/asm/kvm_host.h
@@ -311,7 +311,7 @@ struct kvm_s390_local_interrupt {
 	struct list_head list;
 	atomic_t active;
 	struct kvm_s390_float_interrupt *float_int;
-	wait_queue_head_t *wq;
+	struct swait_head *wq;
 	atomic_t *cpuflags;
 	unsigned int action_bits;
 };
diff --git a/kernel/msm-3.18/arch/s390/kvm/interrupt.c b/kernel/msm-3.18/arch/s390/kvm/interrupt.c
index cd6344d33..ca578e5db 100644
--- a/kernel/msm-3.18/arch/s390/kvm/interrupt.c
+++ b/kernel/msm-3.18/arch/s390/kvm/interrupt.c
@@ -620,13 +620,13 @@ no_timer:
 
 void kvm_s390_vcpu_wakeup(struct kvm_vcpu *vcpu)
 {
-	if (waitqueue_active(&vcpu->wq)) {
+	if (swaitqueue_active(&vcpu->wq)) {
 		/*
 		 * The vcpu gave up the cpu voluntarily, mark it as a good
 		 * yield-candidate.
 		 */
 		vcpu->preempted = true;
-		wake_up_interruptible(&vcpu->wq);
+		swait_wake_interruptible(&vcpu->wq);
 		vcpu->stat.halt_wakeup++;
 	}
 }
@@ -747,7 +747,7 @@ int kvm_s390_inject_program_int(struct kvm_vcpu *vcpu, u16 code)
 	spin_lock(&li->lock);
 	list_add(&inti->list, &li->list);
 	atomic_set(&li->active, 1);
-	BUG_ON(waitqueue_active(li->wq));
+	BUG_ON(swaitqueue_active(li->wq));
 	spin_unlock(&li->lock);
 	return 0;
 }
@@ -772,7 +772,7 @@ int kvm_s390_inject_prog_irq(struct kvm_vcpu *vcpu,
 	spin_lock(&li->lock);
 	list_add(&inti->list, &li->list);
 	atomic_set(&li->active, 1);
-	BUG_ON(waitqueue_active(li->wq));
+	BUG_ON(swaitqueue_active(li->wq));
 	spin_unlock(&li->lock);
 	return 0;
 }
diff --git a/kernel/msm-3.18/arch/s390/mm/fault.c b/kernel/msm-3.18/arch/s390/mm/fault.c
index fbe8f2cf9..43ec237a1 100644
--- a/kernel/msm-3.18/arch/s390/mm/fault.c
+++ b/kernel/msm-3.18/arch/s390/mm/fault.c
@@ -435,7 +435,8 @@ static inline int do_exception(struct pt_regs *regs, int access)
 	 * user context.
 	 */
 	fault = VM_FAULT_BADCONTEXT;
-	if (unlikely(!user_space_fault(regs) || in_atomic() || !mm))
+	if (unlikely(!user_space_fault(regs) || !mm ||
+		     tsk->pagefault_disabled))
 		goto out;
 
 	address = trans_exc_code & __FAIL_ADDR_MASK;
diff --git a/kernel/msm-3.18/arch/score/mm/fault.c b/kernel/msm-3.18/arch/score/mm/fault.c
index 6860beb2a..b946291e6 100644
--- a/kernel/msm-3.18/arch/score/mm/fault.c
+++ b/kernel/msm-3.18/arch/score/mm/fault.c
@@ -73,7 +73,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 	* If we're in an interrupt or have no user
 	* context, we must not take the fault..
 	*/
-	if (in_atomic() || !mm)
+	if (!mm || pagefault_disabled())
 		goto bad_area_nosemaphore;
 
 	if (user_mode(regs))
diff --git a/kernel/msm-3.18/arch/sh/kernel/irq.c b/kernel/msm-3.18/arch/sh/kernel/irq.c
index 65a1ecd77..d5dd95e66 100644
--- a/kernel/msm-3.18/arch/sh/kernel/irq.c
+++ b/kernel/msm-3.18/arch/sh/kernel/irq.c
@@ -149,6 +149,7 @@ void irq_ctx_exit(int cpu)
 	hardirq_ctx[cpu] = NULL;
 }
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 void do_softirq_own_stack(void)
 {
 	struct thread_info *curctx;
@@ -176,6 +177,7 @@ void do_softirq_own_stack(void)
 		  "r5", "r6", "r7", "r8", "r9", "r15", "t", "pr"
 	);
 }
+#endif
 #else
 static inline void handle_one_irq(unsigned int irq)
 {
diff --git a/kernel/msm-3.18/arch/sh/mm/fault.c b/kernel/msm-3.18/arch/sh/mm/fault.c
index a58fec9b5..364cae580 100644
--- a/kernel/msm-3.18/arch/sh/mm/fault.c
+++ b/kernel/msm-3.18/arch/sh/mm/fault.c
@@ -440,7 +440,7 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,
 	 * If we're in an interrupt, have no user context or are running
 	 * in an atomic region then we must not take the fault:
 	 */
-	if (unlikely(in_atomic() || !mm)) {
+	if (unlikely(!mm || pagefault_disabled())) {
 		bad_area_nosemaphore(regs, error_code, address);
 		return;
 	}
diff --git a/kernel/msm-3.18/arch/sparc/Kconfig b/kernel/msm-3.18/arch/sparc/Kconfig
index 96ac69c5e..950b5733f 100644
--- a/kernel/msm-3.18/arch/sparc/Kconfig
+++ b/kernel/msm-3.18/arch/sparc/Kconfig
@@ -182,12 +182,10 @@ config NR_CPUS
 source kernel/Kconfig.hz
 
 config RWSEM_GENERIC_SPINLOCK
-	bool
-	default y if SPARC32
+	def_bool PREEMPT_RT_FULL
 
 config RWSEM_XCHGADD_ALGORITHM
-	bool
-	default y if SPARC64
+	def_bool !RWSEM_GENERIC_SPINLOCK && !PREEMPT_RT_FULL
 
 config GENERIC_HWEIGHT
 	bool
@@ -528,6 +526,10 @@ menu "Executable file formats"
 
 source "fs/Kconfig.binfmt"
 
+config EARLY_PRINTK
+	bool
+	default y
+
 config COMPAT
 	bool
 	depends on SPARC64
diff --git a/kernel/msm-3.18/arch/sparc/kernel/irq_64.c b/kernel/msm-3.18/arch/sparc/kernel/irq_64.c
index 4033c23bd..763cd88b4 100644
--- a/kernel/msm-3.18/arch/sparc/kernel/irq_64.c
+++ b/kernel/msm-3.18/arch/sparc/kernel/irq_64.c
@@ -849,6 +849,7 @@ void __irq_entry handler_irq(int pil, struct pt_regs *regs)
 	set_irq_regs(old_regs);
 }
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 void do_softirq_own_stack(void)
 {
 	void *orig_sp, *sp = softirq_stack[smp_processor_id()];
@@ -863,6 +864,7 @@ void do_softirq_own_stack(void)
 	__asm__ __volatile__("mov %0, %%sp"
 			     : : "r" (orig_sp));
 }
+#endif
 
 #ifdef CONFIG_HOTPLUG_CPU
 void fixup_irqs(void)
diff --git a/kernel/msm-3.18/arch/sparc/kernel/setup_32.c b/kernel/msm-3.18/arch/sparc/kernel/setup_32.c
index baef495c0..3548e8591 100644
--- a/kernel/msm-3.18/arch/sparc/kernel/setup_32.c
+++ b/kernel/msm-3.18/arch/sparc/kernel/setup_32.c
@@ -309,6 +309,7 @@ void __init setup_arch(char **cmdline_p)
 
 	boot_flags_init(*cmdline_p);
 
+	early_console = &prom_early_console;
 	register_console(&prom_early_console);
 
 	printk("ARCH: ");
diff --git a/kernel/msm-3.18/arch/sparc/kernel/setup_64.c b/kernel/msm-3.18/arch/sparc/kernel/setup_64.c
index c38d19fc2..5e78950e2 100644
--- a/kernel/msm-3.18/arch/sparc/kernel/setup_64.c
+++ b/kernel/msm-3.18/arch/sparc/kernel/setup_64.c
@@ -563,6 +563,12 @@ static void __init init_sparc64_elf_hwcap(void)
 		pause_patch();
 }
 
+static inline void register_prom_console(void)
+{
+	early_console = &prom_early_console;
+	register_console(&prom_early_console);
+}
+
 void __init setup_arch(char **cmdline_p)
 {
 	/* Initialize PROM console and command line. */
@@ -574,7 +580,7 @@ void __init setup_arch(char **cmdline_p)
 #ifdef CONFIG_EARLYFB
 	if (btext_find_display())
 #endif
-		register_console(&prom_early_console);
+		register_prom_console();
 
 	if (tlb_type == hypervisor)
 		printk("ARCH: SUN4V\n");
diff --git a/kernel/msm-3.18/arch/sparc/mm/fault_32.c b/kernel/msm-3.18/arch/sparc/mm/fault_32.c
index 70d817154..7c04a4947 100644
--- a/kernel/msm-3.18/arch/sparc/mm/fault_32.c
+++ b/kernel/msm-3.18/arch/sparc/mm/fault_32.c
@@ -196,7 +196,7 @@ asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,
 	 * If we're in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
-	if (in_atomic() || !mm)
+	if (!mm || pagefault_disabled())
 		goto no_context;
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
diff --git a/kernel/msm-3.18/arch/sparc/mm/fault_64.c b/kernel/msm-3.18/arch/sparc/mm/fault_64.c
index 479823249..28afbdf06 100644
--- a/kernel/msm-3.18/arch/sparc/mm/fault_64.c
+++ b/kernel/msm-3.18/arch/sparc/mm/fault_64.c
@@ -330,7 +330,7 @@ asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)
 	 * If we're in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
-	if (in_atomic() || !mm)
+	if (!mm || pagefault_disabled())
 		goto intr_or_no_mm;
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
diff --git a/kernel/msm-3.18/arch/tile/mm/fault.c b/kernel/msm-3.18/arch/tile/mm/fault.c
index c6d2a76d9..4cdab266b 100644
--- a/kernel/msm-3.18/arch/tile/mm/fault.c
+++ b/kernel/msm-3.18/arch/tile/mm/fault.c
@@ -357,7 +357,7 @@ static int handle_page_fault(struct pt_regs *regs,
 	 * If we're in an interrupt, have no user context or are running in an
 	 * atomic region then we must not take the fault.
 	 */
-	if (in_atomic() || !mm) {
+	if (!mm || pagefault_disabled()) {
 		vma = NULL;  /* happy compiler */
 		goto bad_area_nosemaphore;
 	}
diff --git a/kernel/msm-3.18/arch/um/kernel/trap.c b/kernel/msm-3.18/arch/um/kernel/trap.c
index 209617302..6126d3a23 100644
--- a/kernel/msm-3.18/arch/um/kernel/trap.c
+++ b/kernel/msm-3.18/arch/um/kernel/trap.c
@@ -38,7 +38,7 @@ int handle_page_fault(unsigned long address, unsigned long ip,
 	 * If the fault was during atomic operation, don't take the fault, just
 	 * fail.
 	 */
-	if (in_atomic())
+	if (pagefault_disabled())
 		goto out_nosemaphore;
 
 	if (is_user)
diff --git a/kernel/msm-3.18/arch/x86/Kconfig b/kernel/msm-3.18/arch/x86/Kconfig
index 3de18af37..24a11158b 100644
--- a/kernel/msm-3.18/arch/x86/Kconfig
+++ b/kernel/msm-3.18/arch/x86/Kconfig
@@ -21,6 +21,7 @@ config X86_64
 ### Arch settings
 config X86
 	def_bool y
+	select HAVE_PREEMPT_LAZY
 	select ARCH_MIGHT_HAVE_ACPI_PDC if ACPI
 	select ARCH_HAS_DEBUG_STRICT_USER_COPY_CHECKS
 	select ARCH_HAS_FAST_MULTIPLIER
@@ -222,8 +223,11 @@ config ARCH_MAY_HAVE_PC_FDC
 	def_bool y
 	depends on ISA_DMA_API
 
+config RWSEM_GENERIC_SPINLOCK
+	def_bool PREEMPT_RT_FULL
+
 config RWSEM_XCHGADD_ALGORITHM
-	def_bool y
+	def_bool !RWSEM_GENERIC_SPINLOCK && !PREEMPT_RT_FULL
 
 config GENERIC_CALIBRATE_DELAY
 	def_bool y
@@ -833,7 +837,7 @@ config IOMMU_HELPER
 config MAXSMP
 	bool "Enable Maximum number of SMP Processors and NUMA Nodes"
 	depends on X86_64 && SMP && DEBUG_KERNEL
-	select CPUMASK_OFFSTACK
+	select CPUMASK_OFFSTACK if !PREEMPT_RT_FULL
 	---help---
 	  Enable maximum number of CPUS and NUMA Nodes for this architecture.
 	  If unsure, say N.
diff --git a/kernel/msm-3.18/arch/x86/crypto/aesni-intel_glue.c b/kernel/msm-3.18/arch/x86/crypto/aesni-intel_glue.c
index 5a93783a8..d828fd40d 100644
--- a/kernel/msm-3.18/arch/x86/crypto/aesni-intel_glue.c
+++ b/kernel/msm-3.18/arch/x86/crypto/aesni-intel_glue.c
@@ -381,14 +381,14 @@ static int ecb_encrypt(struct blkcipher_desc *desc,
 	err = blkcipher_walk_virt(desc, &walk);
 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_ecb_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,
-			      nbytes & AES_BLOCK_MASK);
+				nbytes & AES_BLOCK_MASK);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = blkcipher_walk_done(desc, &walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -405,14 +405,14 @@ static int ecb_decrypt(struct blkcipher_desc *desc,
 	err = blkcipher_walk_virt(desc, &walk);
 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_ecb_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = blkcipher_walk_done(desc, &walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -429,14 +429,14 @@ static int cbc_encrypt(struct blkcipher_desc *desc,
 	err = blkcipher_walk_virt(desc, &walk);
 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_cbc_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = blkcipher_walk_done(desc, &walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -453,14 +453,14 @@ static int cbc_decrypt(struct blkcipher_desc *desc,
 	err = blkcipher_walk_virt(desc, &walk);
 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes)) {
+		kernel_fpu_begin();
 		aesni_cbc_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 			      nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = blkcipher_walk_done(desc, &walk, nbytes);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
@@ -512,18 +512,20 @@ static int ctr_crypt(struct blkcipher_desc *desc,
 	err = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);
 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
 
-	kernel_fpu_begin();
 	while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
+		kernel_fpu_begin();
 		aesni_ctr_enc_tfm(ctx, walk.dst.virt.addr, walk.src.virt.addr,
 				  nbytes & AES_BLOCK_MASK, walk.iv);
+		kernel_fpu_end();
 		nbytes &= AES_BLOCK_SIZE - 1;
 		err = blkcipher_walk_done(desc, &walk, nbytes);
 	}
 	if (walk.nbytes) {
+		kernel_fpu_begin();
 		ctr_crypt_final(ctx, &walk);
+		kernel_fpu_end();
 		err = blkcipher_walk_done(desc, &walk, 0);
 	}
-	kernel_fpu_end();
 
 	return err;
 }
diff --git a/kernel/msm-3.18/arch/x86/crypto/cast5_avx_glue.c b/kernel/msm-3.18/arch/x86/crypto/cast5_avx_glue.c
index 60ada677a..7344a356e 100644
--- a/kernel/msm-3.18/arch/x86/crypto/cast5_avx_glue.c
+++ b/kernel/msm-3.18/arch/x86/crypto/cast5_avx_glue.c
@@ -60,7 +60,7 @@ static inline void cast5_fpu_end(bool fpu_enabled)
 static int ecb_crypt(struct blkcipher_desc *desc, struct blkcipher_walk *walk,
 		     bool enc)
 {
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct cast5_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);
 	const unsigned int bsize = CAST5_BLOCK_SIZE;
 	unsigned int nbytes;
@@ -76,7 +76,7 @@ static int ecb_crypt(struct blkcipher_desc *desc, struct blkcipher_walk *walk,
 		u8 *wsrc = walk->src.virt.addr;
 		u8 *wdst = walk->dst.virt.addr;
 
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, nbytes);
 
 		/* Process multi-block batch */
 		if (nbytes >= bsize * CAST5_PARALLEL_BLOCKS) {
@@ -104,10 +104,9 @@ static int ecb_crypt(struct blkcipher_desc *desc, struct blkcipher_walk *walk,
 		} while (nbytes >= bsize);
 
 done:
+		cast5_fpu_end(fpu_enabled);
 		err = blkcipher_walk_done(desc, walk, nbytes);
 	}
-
-	cast5_fpu_end(fpu_enabled);
 	return err;
 }
 
@@ -228,7 +227,7 @@ done:
 static int cbc_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,
 		       struct scatterlist *src, unsigned int nbytes)
 {
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct blkcipher_walk walk;
 	int err;
 
@@ -237,12 +236,11 @@ static int cbc_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,
 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
 
 	while ((nbytes = walk.nbytes)) {
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, nbytes);
 		nbytes = __cbc_decrypt(desc, &walk);
+		cast5_fpu_end(fpu_enabled);
 		err = blkcipher_walk_done(desc, &walk, nbytes);
 	}
-
-	cast5_fpu_end(fpu_enabled);
 	return err;
 }
 
@@ -312,7 +310,7 @@ done:
 static int ctr_crypt(struct blkcipher_desc *desc, struct scatterlist *dst,
 		     struct scatterlist *src, unsigned int nbytes)
 {
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct blkcipher_walk walk;
 	int err;
 
@@ -321,13 +319,12 @@ static int ctr_crypt(struct blkcipher_desc *desc, struct scatterlist *dst,
 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
 
 	while ((nbytes = walk.nbytes) >= CAST5_BLOCK_SIZE) {
-		fpu_enabled = cast5_fpu_begin(fpu_enabled, nbytes);
+		fpu_enabled = cast5_fpu_begin(false, nbytes);
 		nbytes = __ctr_crypt(desc, &walk);
+		cast5_fpu_end(fpu_enabled);
 		err = blkcipher_walk_done(desc, &walk, nbytes);
 	}
 
-	cast5_fpu_end(fpu_enabled);
-
 	if (walk.nbytes) {
 		ctr_crypt_final(desc, &walk);
 		err = blkcipher_walk_done(desc, &walk, 0);
diff --git a/kernel/msm-3.18/arch/x86/crypto/glue_helper.c b/kernel/msm-3.18/arch/x86/crypto/glue_helper.c
index 432f1d76c..4a2bd21c2 100644
--- a/kernel/msm-3.18/arch/x86/crypto/glue_helper.c
+++ b/kernel/msm-3.18/arch/x86/crypto/glue_helper.c
@@ -39,7 +39,7 @@ static int __glue_ecb_crypt_128bit(const struct common_glue_ctx *gctx,
 	void *ctx = crypto_blkcipher_ctx(desc->tfm);
 	const unsigned int bsize = 128 / 8;
 	unsigned int nbytes, i, func_bytes;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	int err;
 
 	err = blkcipher_walk_virt(desc, walk);
@@ -49,7 +49,7 @@ static int __glue_ecb_crypt_128bit(const struct common_glue_ctx *gctx,
 		u8 *wdst = walk->dst.virt.addr;
 
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     desc, fpu_enabled, nbytes);
+					     desc, false, nbytes);
 
 		for (i = 0; i < gctx->num_funcs; i++) {
 			func_bytes = bsize * gctx->funcs[i].num_blocks;
@@ -71,10 +71,10 @@ static int __glue_ecb_crypt_128bit(const struct common_glue_ctx *gctx,
 		}
 
 done:
+		glue_fpu_end(fpu_enabled);
 		err = blkcipher_walk_done(desc, walk, nbytes);
 	}
 
-	glue_fpu_end(fpu_enabled);
 	return err;
 }
 
@@ -194,7 +194,7 @@ int glue_cbc_decrypt_128bit(const struct common_glue_ctx *gctx,
 			    struct scatterlist *src, unsigned int nbytes)
 {
 	const unsigned int bsize = 128 / 8;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct blkcipher_walk walk;
 	int err;
 
@@ -203,12 +203,12 @@ int glue_cbc_decrypt_128bit(const struct common_glue_ctx *gctx,
 
 	while ((nbytes = walk.nbytes)) {
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     desc, fpu_enabled, nbytes);
+					     desc, false, nbytes);
 		nbytes = __glue_cbc_decrypt_128bit(gctx, desc, &walk);
+		glue_fpu_end(fpu_enabled);
 		err = blkcipher_walk_done(desc, &walk, nbytes);
 	}
 
-	glue_fpu_end(fpu_enabled);
 	return err;
 }
 EXPORT_SYMBOL_GPL(glue_cbc_decrypt_128bit);
@@ -278,7 +278,7 @@ int glue_ctr_crypt_128bit(const struct common_glue_ctx *gctx,
 			  struct scatterlist *src, unsigned int nbytes)
 {
 	const unsigned int bsize = 128 / 8;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct blkcipher_walk walk;
 	int err;
 
@@ -287,13 +287,12 @@ int glue_ctr_crypt_128bit(const struct common_glue_ctx *gctx,
 
 	while ((nbytes = walk.nbytes) >= bsize) {
 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-					     desc, fpu_enabled, nbytes);
+					     desc, false, nbytes);
 		nbytes = __glue_ctr_crypt_128bit(gctx, desc, &walk);
+		glue_fpu_end(fpu_enabled);
 		err = blkcipher_walk_done(desc, &walk, nbytes);
 	}
 
-	glue_fpu_end(fpu_enabled);
-
 	if (walk.nbytes) {
 		glue_ctr_crypt_final_128bit(
 			gctx->funcs[gctx->num_funcs - 1].fn_u.ctr, desc, &walk);
@@ -348,7 +347,7 @@ int glue_xts_crypt_128bit(const struct common_glue_ctx *gctx,
 			  void *tweak_ctx, void *crypt_ctx)
 {
 	const unsigned int bsize = 128 / 8;
-	bool fpu_enabled = false;
+	bool fpu_enabled;
 	struct blkcipher_walk walk;
 	int err;
 
@@ -361,21 +360,21 @@ int glue_xts_crypt_128bit(const struct common_glue_ctx *gctx,
 
 	/* set minimum length to bsize, for tweak_fn */
 	fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
-				     desc, fpu_enabled,
+				     desc, false,
 				     nbytes < bsize ? bsize : nbytes);
-
 	/* calculate first value of T */
 	tweak_fn(tweak_ctx, walk.iv, walk.iv);
+	glue_fpu_end(fpu_enabled);
 
 	while (nbytes) {
+		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
+				desc, false, nbytes);
 		nbytes = __glue_xts_crypt_128bit(gctx, crypt_ctx, desc, &walk);
 
+		glue_fpu_end(fpu_enabled);
 		err = blkcipher_walk_done(desc, &walk, nbytes);
 		nbytes = walk.nbytes;
 	}
-
-	glue_fpu_end(fpu_enabled);
-
 	return err;
 }
 EXPORT_SYMBOL_GPL(glue_xts_crypt_128bit);
diff --git a/kernel/msm-3.18/arch/x86/include/asm/preempt.h b/kernel/msm-3.18/arch/x86/include/asm/preempt.h
index 906e52246..ba4976828 100644
--- a/kernel/msm-3.18/arch/x86/include/asm/preempt.h
+++ b/kernel/msm-3.18/arch/x86/include/asm/preempt.h
@@ -85,17 +85,46 @@ static __always_inline void __preempt_count_sub(int val)
  * a decrement which hits zero means we have no preempt_count and should
  * reschedule.
  */
-static __always_inline bool __preempt_count_dec_and_test(void)
+static __always_inline bool ____preempt_count_dec_and_test(void)
 {
 	GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), "e");
 }
 
+static __always_inline bool __preempt_count_dec_and_test(void)
+{
+	if (____preempt_count_dec_and_test())
+		return true;
+#ifdef CONFIG_PREEMPT_LAZY
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
+	return false;
+#endif
+}
+
 /*
  * Returns true when we need to resched and can (barring IRQ state).
  */
-static __always_inline bool should_resched(int preempt_offset)
+static __always_inline bool should_resched(void)
 {
-	return unlikely(raw_cpu_read_4(__preempt_count) == preempt_offset);
+#ifdef CONFIG_PREEMPT_LAZY
+	u32 tmp;
+
+	tmp = raw_cpu_read_4(__preempt_count);
+	if (!tmp)
+		return true;
+
+	/* preempt count == 0 ? */
+	tmp &= ~PREEMPT_NEED_RESCHED;
+	if (tmp)
+		return false;
+	if (current_thread_info()->preempt_lazy_count)
+		return false;
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+#else
+	return unlikely(!raw_cpu_read_4(__preempt_count));
+#endif
 }
 
 #ifdef CONFIG_PREEMPT
diff --git a/kernel/msm-3.18/arch/x86/include/asm/signal.h b/kernel/msm-3.18/arch/x86/include/asm/signal.h
index 31eab867e..b1b08a28c 100644
--- a/kernel/msm-3.18/arch/x86/include/asm/signal.h
+++ b/kernel/msm-3.18/arch/x86/include/asm/signal.h
@@ -23,6 +23,19 @@ typedef struct {
 	unsigned long sig[_NSIG_WORDS];
 } sigset_t;
 
+/*
+ * Because some traps use the IST stack, we must keep preemption
+ * disabled while calling do_trap(), but do_trap() may call
+ * force_sig_info() which will grab the signal spin_locks for the
+ * task, which in PREEMPT_RT_FULL are mutexes.  By defining
+ * ARCH_RT_DELAYS_SIGNAL_SEND the force_sig_info() will set
+ * TIF_NOTIFY_RESUME and set up the signal to be sent on exit of the
+ * trap.
+ */
+#if defined(CONFIG_PREEMPT_RT_FULL) && defined(CONFIG_X86_64)
+#define ARCH_RT_DELAYS_SIGNAL_SEND
+#endif
+
 #ifndef CONFIG_COMPAT
 typedef sigset_t compat_sigset_t;
 #endif
diff --git a/kernel/msm-3.18/arch/x86/include/asm/stackprotector.h b/kernel/msm-3.18/arch/x86/include/asm/stackprotector.h
index 6a998598f..64fb5cbe5 100644
--- a/kernel/msm-3.18/arch/x86/include/asm/stackprotector.h
+++ b/kernel/msm-3.18/arch/x86/include/asm/stackprotector.h
@@ -57,7 +57,7 @@
  */
 static __always_inline void boot_init_stack_canary(void)
 {
-	u64 canary;
+	u64 uninitialized_var(canary);
 	u64 tsc;
 
 #ifdef CONFIG_X86_64
@@ -68,8 +68,16 @@ static __always_inline void boot_init_stack_canary(void)
 	 * of randomness. The TSC only matters for very early init,
 	 * there it already has some randomness on most systems. Later
 	 * on during the bootup the random pool has true entropy too.
+	 *
+	 * For preempt-rt we need to weaken the randomness a bit, as
+	 * we can't call into the random generator from atomic context
+	 * due to locking constraints. We just leave canary
+	 * uninitialized and use the TSC based randomness on top of
+	 * it.
 	 */
+#ifndef CONFIG_PREEMPT_RT_FULL
 	get_random_bytes(&canary, sizeof(canary));
+#endif
 	tsc = __native_read_tsc();
 	canary += tsc + (tsc << 32UL);
 
diff --git a/kernel/msm-3.18/arch/x86/include/asm/thread_info.h b/kernel/msm-3.18/arch/x86/include/asm/thread_info.h
index 391a8711f..d7fddb6dc 100644
--- a/kernel/msm-3.18/arch/x86/include/asm/thread_info.h
+++ b/kernel/msm-3.18/arch/x86/include/asm/thread_info.h
@@ -57,6 +57,8 @@ struct thread_info {
 	__u32			status;		/* thread synchronous flags */
 	__u32			cpu;		/* current CPU */
 	int			saved_preempt_count;
+	int			preempt_lazy_count;	/* 0 => lazy preemptable
+							   <0 => BUG */
 	mm_segment_t		addr_limit;
 	void __user		*sysenter_return;
 	unsigned int		sig_on_uaccess_error:1;
@@ -98,6 +100,7 @@ struct thread_info {
 #define TIF_SYSCALL_EMU		6	/* syscall emulation active */
 #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
 #define TIF_SECCOMP		8	/* secure computing */
+#define TIF_NEED_RESCHED_LAZY	9	/* lazy rescheduling necessary */
 #define TIF_MCE_NOTIFY		10	/* notify userspace of an MCE */
 #define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
 #define TIF_UPROBE		12	/* breakpointed or singlestepping */
@@ -123,6 +126,7 @@ struct thread_info {
 #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
+#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
 #define _TIF_MCE_NOTIFY		(1 << TIF_MCE_NOTIFY)
 #define _TIF_USER_RETURN_NOTIFY	(1 << TIF_USER_RETURN_NOTIFY)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
@@ -173,6 +177,8 @@ struct thread_info {
 #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
 #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)
 
+#define _TIF_NEED_RESCHED_MASK (_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
+
 #define STACK_WARN		(THREAD_SIZE/8)
 #define KERNEL_STACK_OFFSET	(5*(BITS_PER_LONG/8))
 
diff --git a/kernel/msm-3.18/arch/x86/include/asm/uv/uv_bau.h b/kernel/msm-3.18/arch/x86/include/asm/uv/uv_bau.h
index 2d60a7813..ddc4c9e3d 100644
--- a/kernel/msm-3.18/arch/x86/include/asm/uv/uv_bau.h
+++ b/kernel/msm-3.18/arch/x86/include/asm/uv/uv_bau.h
@@ -615,9 +615,9 @@ struct bau_control {
 	cycles_t		send_message;
 	cycles_t		period_end;
 	cycles_t		period_time;
-	spinlock_t		uvhub_lock;
-	spinlock_t		queue_lock;
-	spinlock_t		disable_lock;
+	raw_spinlock_t		uvhub_lock;
+	raw_spinlock_t		queue_lock;
+	raw_spinlock_t		disable_lock;
 	/* tunables */
 	int			max_concurr;
 	int			max_concurr_const;
@@ -776,15 +776,15 @@ static inline int atom_asr(short i, struct atomic_short *v)
  * to be lowered below the current 'v'.  atomic_add_unless can only stop
  * on equal.
  */
-static inline int atomic_inc_unless_ge(spinlock_t *lock, atomic_t *v, int u)
+static inline int atomic_inc_unless_ge(raw_spinlock_t *lock, atomic_t *v, int u)
 {
-	spin_lock(lock);
+	raw_spin_lock(lock);
 	if (atomic_read(v) >= u) {
-		spin_unlock(lock);
+		raw_spin_unlock(lock);
 		return 0;
 	}
 	atomic_inc(v);
-	spin_unlock(lock);
+	raw_spin_unlock(lock);
 	return 1;
 }
 
diff --git a/kernel/msm-3.18/arch/x86/include/asm/uv/uv_hub.h b/kernel/msm-3.18/arch/x86/include/asm/uv/uv_hub.h
index a00ad8f2a..c2729abe0 100644
--- a/kernel/msm-3.18/arch/x86/include/asm/uv/uv_hub.h
+++ b/kernel/msm-3.18/arch/x86/include/asm/uv/uv_hub.h
@@ -492,7 +492,7 @@ struct uv_blade_info {
 	unsigned short	nr_online_cpus;
 	unsigned short	pnode;
 	short		memory_nid;
-	spinlock_t	nmi_lock;	/* obsolete, see uv_hub_nmi */
+	raw_spinlock_t	nmi_lock;	/* obsolete, see uv_hub_nmi */
 	unsigned long	nmi_count;	/* obsolete, see uv_hub_nmi */
 };
 extern struct uv_blade_info *uv_blade_info;
diff --git a/kernel/msm-3.18/arch/x86/kernel/apic/io_apic.c b/kernel/msm-3.18/arch/x86/kernel/apic/io_apic.c
index 7ffe0a2b8..12706a1a4 100644
--- a/kernel/msm-3.18/arch/x86/kernel/apic/io_apic.c
+++ b/kernel/msm-3.18/arch/x86/kernel/apic/io_apic.c
@@ -2494,7 +2494,8 @@ static bool io_apic_level_ack_pending(struct irq_cfg *cfg)
 static inline bool ioapic_irqd_mask(struct irq_data *data, struct irq_cfg *cfg)
 {
 	/* If we are moving the irq we need to mask it */
-	if (unlikely(irqd_is_setaffinity_pending(data))) {
+	if (unlikely(irqd_is_setaffinity_pending(data) &&
+		     !irqd_irq_inprogress(data))) {
 		mask_ioapic(cfg);
 		return true;
 	}
diff --git a/kernel/msm-3.18/arch/x86/kernel/apic/x2apic_uv_x.c b/kernel/msm-3.18/arch/x86/kernel/apic/x2apic_uv_x.c
index 8e9dcfd63..2cfedcae3 100644
--- a/kernel/msm-3.18/arch/x86/kernel/apic/x2apic_uv_x.c
+++ b/kernel/msm-3.18/arch/x86/kernel/apic/x2apic_uv_x.c
@@ -918,7 +918,7 @@ void __init uv_system_init(void)
 			uv_blade_info[blade].pnode = pnode;
 			uv_blade_info[blade].nr_possible_cpus = 0;
 			uv_blade_info[blade].nr_online_cpus = 0;
-			spin_lock_init(&uv_blade_info[blade].nmi_lock);
+			raw_spin_lock_init(&uv_blade_info[blade].nmi_lock);
 			min_pnode = min(pnode, min_pnode);
 			max_pnode = max(pnode, max_pnode);
 			blade++;
diff --git a/kernel/msm-3.18/arch/x86/kernel/asm-offsets.c b/kernel/msm-3.18/arch/x86/kernel/asm-offsets.c
index 9f6b93419..5701b5075 100644
--- a/kernel/msm-3.18/arch/x86/kernel/asm-offsets.c
+++ b/kernel/msm-3.18/arch/x86/kernel/asm-offsets.c
@@ -32,6 +32,7 @@ void common(void) {
 	OFFSET(TI_flags, thread_info, flags);
 	OFFSET(TI_status, thread_info, status);
 	OFFSET(TI_addr_limit, thread_info, addr_limit);
+	OFFSET(TI_preempt_lazy_count, thread_info, preempt_lazy_count);
 
 	BLANK();
 	OFFSET(crypto_tfm_ctx_offset, crypto_tfm, __crt_ctx);
@@ -71,4 +72,5 @@ void common(void) {
 
 	BLANK();
 	DEFINE(PTREGS_SIZE, sizeof(struct pt_regs));
+	DEFINE(_PREEMPT_ENABLED, PREEMPT_ENABLED);
 }
diff --git a/kernel/msm-3.18/arch/x86/kernel/cpu/mcheck/mce.c b/kernel/msm-3.18/arch/x86/kernel/cpu/mcheck/mce.c
index bf44e45a2..1492d3adf 100644
--- a/kernel/msm-3.18/arch/x86/kernel/cpu/mcheck/mce.c
+++ b/kernel/msm-3.18/arch/x86/kernel/cpu/mcheck/mce.c
@@ -41,6 +41,8 @@
 #include <linux/debugfs.h>
 #include <linux/irq_work.h>
 #include <linux/export.h>
+#include <linux/jiffies.h>
+#include <linux/work-simple.h>
 
 #include <asm/processor.h>
 #include <asm/tlbflush.h>
@@ -1272,7 +1274,7 @@ void mce_log_therm_throt_event(__u64 status)
 static unsigned long check_interval = 5 * 60; /* 5 minutes */
 
 static DEFINE_PER_CPU(unsigned long, mce_next_interval); /* in jiffies */
-static DEFINE_PER_CPU(struct timer_list, mce_timer);
+static DEFINE_PER_CPU(struct hrtimer, mce_timer);
 
 static unsigned long mce_adjust_timer_default(unsigned long interval)
 {
@@ -1289,14 +1291,11 @@ static int cmc_error_seen(void)
 	return test_and_clear_bit(0, v);
 }
 
-static void mce_timer_fn(unsigned long data)
+static enum hrtimer_restart mce_timer_fn(struct hrtimer *timer)
 {
-	struct timer_list *t = this_cpu_ptr(&mce_timer);
 	unsigned long iv;
 	int notify;
 
-	WARN_ON(smp_processor_id() != data);
-
 	if (mce_available(this_cpu_ptr(&cpu_info))) {
 		machine_check_poll(MCP_TIMESTAMP,
 				this_cpu_ptr(&mce_poll_banks));
@@ -1319,9 +1318,11 @@ static void mce_timer_fn(unsigned long data)
 	__this_cpu_write(mce_next_interval, iv);
 	/* Might have become 0 after CMCI storm subsided */
 	if (iv) {
-		t->expires = jiffies + iv;
-		add_timer_on(t, smp_processor_id());
+		hrtimer_forward_now(timer, ns_to_ktime(
+					jiffies_to_usecs(iv) * 1000ULL));
+		return HRTIMER_RESTART;
 	}
+	return HRTIMER_NORESTART;
 }
 
 /*
@@ -1329,28 +1330,37 @@ static void mce_timer_fn(unsigned long data)
  */
 void mce_timer_kick(unsigned long interval)
 {
-	struct timer_list *t = this_cpu_ptr(&mce_timer);
-	unsigned long when = jiffies + interval;
+	struct hrtimer *t = this_cpu_ptr(&mce_timer);
 	unsigned long iv = __this_cpu_read(mce_next_interval);
 
-	if (timer_pending(t)) {
-		if (time_before(when, t->expires))
-			mod_timer_pinned(t, when);
+	if (hrtimer_active(t)) {
+		s64 exp;
+		s64 intv_us;
+
+		intv_us = jiffies_to_usecs(interval);
+		exp = ktime_to_us(hrtimer_expires_remaining(t));
+		if (intv_us < exp) {
+			hrtimer_cancel(t);
+			hrtimer_start_range_ns(t,
+					ns_to_ktime(intv_us * 1000),
+					0, HRTIMER_MODE_REL_PINNED);
+		}
 	} else {
-		t->expires = round_jiffies(when);
-		add_timer_on(t, smp_processor_id());
+		hrtimer_start_range_ns(t,
+			ns_to_ktime(jiffies_to_usecs(interval) * 1000ULL),
+				0, HRTIMER_MODE_REL_PINNED);
 	}
 	if (interval < iv)
 		__this_cpu_write(mce_next_interval, interval);
 }
 
-/* Must not be called in IRQ context where del_timer_sync() can deadlock */
+/* Must not be called in IRQ context where hrtimer_cancel() can deadlock */
 static void mce_timer_delete_all(void)
 {
 	int cpu;
 
 	for_each_online_cpu(cpu)
-		del_timer_sync(&per_cpu(mce_timer, cpu));
+		hrtimer_cancel(&per_cpu(mce_timer, cpu));
 }
 
 static void mce_do_trigger(struct work_struct *work)
@@ -1360,6 +1370,56 @@ static void mce_do_trigger(struct work_struct *work)
 
 static DECLARE_WORK(mce_trigger_work, mce_do_trigger);
 
+static void __mce_notify_work(struct swork_event *event)
+{
+	/* Not more than two messages every minute */
+	static DEFINE_RATELIMIT_STATE(ratelimit, 60*HZ, 2);
+
+	/* wake processes polling /dev/mcelog */
+	wake_up_interruptible(&mce_chrdev_wait);
+
+	/*
+	 * There is no risk of missing notifications because
+	 * work_pending is always cleared before the function is
+	 * executed.
+	 */
+	if (mce_helper[0] && !work_pending(&mce_trigger_work))
+		schedule_work(&mce_trigger_work);
+
+	if (__ratelimit(&ratelimit))
+		pr_info(HW_ERR "Machine check events logged\n");
+}
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+static bool notify_work_ready __read_mostly;
+static struct swork_event notify_work;
+
+static int mce_notify_work_init(void)
+{
+	int err;
+
+	err = swork_get();
+	if (err)
+		return err;
+
+	INIT_SWORK(&notify_work, __mce_notify_work);
+	notify_work_ready = true;
+	return 0;
+}
+
+static void mce_notify_work(void)
+{
+	if (notify_work_ready)
+		swork_queue(&notify_work);
+}
+#else
+static void mce_notify_work(void)
+{
+	__mce_notify_work(NULL);
+}
+static inline int mce_notify_work_init(void) { return 0; }
+#endif
+
 /*
  * Notify the user(s) about new machine check events.
  * Can be called from interrupt context, but not from machine check/NMI
@@ -1367,19 +1427,8 @@ static DECLARE_WORK(mce_trigger_work, mce_do_trigger);
  */
 int mce_notify_irq(void)
 {
-	/* Not more than two messages every minute */
-	static DEFINE_RATELIMIT_STATE(ratelimit, 60*HZ, 2);
-
 	if (test_and_clear_bit(0, &mce_need_notify)) {
-		/* wake processes polling /dev/mcelog */
-		wake_up_interruptible(&mce_chrdev_wait);
-
-		if (mce_helper[0])
-			schedule_work(&mce_trigger_work);
-
-		if (__ratelimit(&ratelimit))
-			pr_info(HW_ERR "Machine check events logged\n");
-
+		mce_notify_work();
 		return 1;
 	}
 	return 0;
@@ -1650,7 +1699,7 @@ static void __mcheck_cpu_init_vendor(struct cpuinfo_x86 *c)
 	}
 }
 
-static void mce_start_timer(unsigned int cpu, struct timer_list *t)
+static void mce_start_timer(unsigned int cpu, struct hrtimer *t)
 {
 	unsigned long iv = check_interval * HZ;
 
@@ -1659,16 +1708,17 @@ static void mce_start_timer(unsigned int cpu, struct timer_list *t)
 
 	per_cpu(mce_next_interval, cpu) = iv;
 
-	t->expires = round_jiffies(jiffies + iv);
-	add_timer_on(t, cpu);
+	hrtimer_start_range_ns(t, ns_to_ktime(jiffies_to_usecs(iv) * 1000ULL),
+			0, HRTIMER_MODE_REL_PINNED);
 }
 
 static void __mcheck_cpu_init_timer(void)
 {
-	struct timer_list *t = this_cpu_ptr(&mce_timer);
+	struct hrtimer *t = this_cpu_ptr(&mce_timer);
 	unsigned int cpu = smp_processor_id();
 
-	setup_timer(t, mce_timer_fn, cpu);
+	hrtimer_init(t, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	t->function = mce_timer_fn;
 	mce_start_timer(cpu, t);
 }
 
@@ -2345,6 +2395,8 @@ static void mce_disable_cpu(void *h)
 	if (!mce_available(raw_cpu_ptr(&cpu_info)))
 		return;
 
+	hrtimer_cancel(this_cpu_ptr(&mce_timer));
+
 	if (!(action & CPU_TASKS_FROZEN))
 		cmci_clear();
 	for (i = 0; i < mca_cfg.banks; i++) {
@@ -2371,6 +2423,7 @@ static void mce_reenable_cpu(void *h)
 		if (b->init)
 			wrmsrl(MSR_IA32_MCx_CTL(i), b->ctl);
 	}
+	__mcheck_cpu_init_timer();
 }
 
 /* Get notified when a cpu comes on/off. Be hotplug friendly. */
@@ -2378,7 +2431,6 @@ static int
 mce_cpu_callback(struct notifier_block *nfb, unsigned long action, void *hcpu)
 {
 	unsigned int cpu = (unsigned long)hcpu;
-	struct timer_list *t = &per_cpu(mce_timer, cpu);
 
 	switch (action & ~CPU_TASKS_FROZEN) {
 	case CPU_ONLINE:
@@ -2398,11 +2450,9 @@ mce_cpu_callback(struct notifier_block *nfb, unsigned long action, void *hcpu)
 		break;
 	case CPU_DOWN_PREPARE:
 		smp_call_function_single(cpu, mce_disable_cpu, &action, 1);
-		del_timer_sync(t);
 		break;
 	case CPU_DOWN_FAILED:
 		smp_call_function_single(cpu, mce_reenable_cpu, &action, 1);
-		mce_start_timer(cpu, t);
 		break;
 	}
 
@@ -2441,6 +2491,10 @@ static __init int mcheck_init_device(void)
 		goto err_out;
 	}
 
+	err = mce_notify_work_init();
+	if (err)
+		goto err_out;
+
 	if (!zalloc_cpumask_var(&mce_device_initialized, GFP_KERNEL)) {
 		err = -ENOMEM;
 		goto err_out;
diff --git a/kernel/msm-3.18/arch/x86/kernel/cpu/perf_event_intel_rapl.c b/kernel/msm-3.18/arch/x86/kernel/cpu/perf_event_intel_rapl.c
index 611d821ea..f35cf3f09 100644
--- a/kernel/msm-3.18/arch/x86/kernel/cpu/perf_event_intel_rapl.c
+++ b/kernel/msm-3.18/arch/x86/kernel/cpu/perf_event_intel_rapl.c
@@ -104,7 +104,7 @@ static struct kobj_attribute format_attr_##_var =		\
 #define RAPL_CNTR_WIDTH 32 /* 32-bit rapl counters */
 
 struct rapl_pmu {
-	spinlock_t	 lock;
+	raw_spinlock_t	 lock;
 	int		 hw_unit;  /* 1/2^hw_unit Joule */
 	int		 n_active; /* number of active events */
 	struct list_head active_list;
@@ -194,13 +194,13 @@ static enum hrtimer_restart rapl_hrtimer_handle(struct hrtimer *hrtimer)
 	if (!pmu->n_active)
 		return HRTIMER_NORESTART;
 
-	spin_lock_irqsave(&pmu->lock, flags);
+	raw_spin_lock_irqsave(&pmu->lock, flags);
 
 	list_for_each_entry(event, &pmu->active_list, active_entry) {
 		rapl_event_update(event);
 	}
 
-	spin_unlock_irqrestore(&pmu->lock, flags);
+	raw_spin_unlock_irqrestore(&pmu->lock, flags);
 
 	hrtimer_forward_now(hrtimer, pmu->timer_interval);
 
@@ -237,9 +237,9 @@ static void rapl_pmu_event_start(struct perf_event *event, int mode)
 	struct rapl_pmu *pmu = __this_cpu_read(rapl_pmu);
 	unsigned long flags;
 
-	spin_lock_irqsave(&pmu->lock, flags);
+	raw_spin_lock_irqsave(&pmu->lock, flags);
 	__rapl_pmu_event_start(pmu, event);
-	spin_unlock_irqrestore(&pmu->lock, flags);
+	raw_spin_unlock_irqrestore(&pmu->lock, flags);
 }
 
 static void rapl_pmu_event_stop(struct perf_event *event, int mode)
@@ -248,7 +248,7 @@ static void rapl_pmu_event_stop(struct perf_event *event, int mode)
 	struct hw_perf_event *hwc = &event->hw;
 	unsigned long flags;
 
-	spin_lock_irqsave(&pmu->lock, flags);
+	raw_spin_lock_irqsave(&pmu->lock, flags);
 
 	/* mark event as deactivated and stopped */
 	if (!(hwc->state & PERF_HES_STOPPED)) {
@@ -273,7 +273,7 @@ static void rapl_pmu_event_stop(struct perf_event *event, int mode)
 		hwc->state |= PERF_HES_UPTODATE;
 	}
 
-	spin_unlock_irqrestore(&pmu->lock, flags);
+	raw_spin_unlock_irqrestore(&pmu->lock, flags);
 }
 
 static int rapl_pmu_event_add(struct perf_event *event, int mode)
@@ -282,14 +282,14 @@ static int rapl_pmu_event_add(struct perf_event *event, int mode)
 	struct hw_perf_event *hwc = &event->hw;
 	unsigned long flags;
 
-	spin_lock_irqsave(&pmu->lock, flags);
+	raw_spin_lock_irqsave(&pmu->lock, flags);
 
 	hwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;
 
 	if (mode & PERF_EF_START)
 		__rapl_pmu_event_start(pmu, event);
 
-	spin_unlock_irqrestore(&pmu->lock, flags);
+	raw_spin_unlock_irqrestore(&pmu->lock, flags);
 
 	return 0;
 }
@@ -551,7 +551,7 @@ static int rapl_cpu_prepare(int cpu)
 	if (!pmu)
 		return -1;
 
-	spin_lock_init(&pmu->lock);
+	raw_spin_lock_init(&pmu->lock);
 
 	INIT_LIST_HEAD(&pmu->active_list);
 
diff --git a/kernel/msm-3.18/arch/x86/kernel/dumpstack_32.c b/kernel/msm-3.18/arch/x86/kernel/dumpstack_32.c
index 5abd4cd42..1282817bb 100644
--- a/kernel/msm-3.18/arch/x86/kernel/dumpstack_32.c
+++ b/kernel/msm-3.18/arch/x86/kernel/dumpstack_32.c
@@ -42,7 +42,7 @@ void dump_trace(struct task_struct *task, struct pt_regs *regs,
 		unsigned long *stack, unsigned long bp,
 		const struct stacktrace_ops *ops, void *data)
 {
-	const unsigned cpu = get_cpu();
+	const unsigned cpu = get_cpu_light();
 	int graph = 0;
 	u32 *prev_esp;
 
@@ -86,7 +86,7 @@ void dump_trace(struct task_struct *task, struct pt_regs *regs,
 			break;
 		touch_nmi_watchdog();
 	}
-	put_cpu();
+	put_cpu_light();
 }
 EXPORT_SYMBOL(dump_trace);
 
diff --git a/kernel/msm-3.18/arch/x86/kernel/dumpstack_64.c b/kernel/msm-3.18/arch/x86/kernel/dumpstack_64.c
index ff86f19b5..4821f2918 100644
--- a/kernel/msm-3.18/arch/x86/kernel/dumpstack_64.c
+++ b/kernel/msm-3.18/arch/x86/kernel/dumpstack_64.c
@@ -152,7 +152,7 @@ void dump_trace(struct task_struct *task, struct pt_regs *regs,
 		unsigned long *stack, unsigned long bp,
 		const struct stacktrace_ops *ops, void *data)
 {
-	const unsigned cpu = get_cpu();
+	const unsigned cpu = get_cpu_light();
 	struct thread_info *tinfo;
 	unsigned long *irq_stack = (unsigned long *)per_cpu(irq_stack_ptr, cpu);
 	unsigned long dummy;
@@ -241,7 +241,7 @@ void dump_trace(struct task_struct *task, struct pt_regs *regs,
 	 * This handles the process stack:
 	 */
 	bp = ops->walk_stack(tinfo, stack, bp, ops, data, NULL, &graph);
-	put_cpu();
+	put_cpu_light();
 }
 EXPORT_SYMBOL(dump_trace);
 
@@ -255,7 +255,7 @@ show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,
 	int cpu;
 	int i;
 
-	preempt_disable();
+	migrate_disable();
 	cpu = smp_processor_id();
 
 	irq_stack_end	= (unsigned long *)(per_cpu(irq_stack_ptr, cpu));
@@ -288,7 +288,7 @@ show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,
 		pr_cont(" %016lx", *stack++);
 		touch_nmi_watchdog();
 	}
-	preempt_enable();
+	migrate_enable();
 
 	pr_cont("\n");
 	show_trace_log_lvl(task, regs, sp, bp, log_lvl);
diff --git a/kernel/msm-3.18/arch/x86/kernel/entry_32.S b/kernel/msm-3.18/arch/x86/kernel/entry_32.S
index fe611c4ae..4c1a1b5bf 100644
--- a/kernel/msm-3.18/arch/x86/kernel/entry_32.S
+++ b/kernel/msm-3.18/arch/x86/kernel/entry_32.S
@@ -359,8 +359,24 @@ END(ret_from_exception)
 ENTRY(resume_kernel)
 	DISABLE_INTERRUPTS(CLBR_ANY)
 need_resched:
+	# preempt count == 0 + NEED_RS set?
 	cmpl $0,PER_CPU_VAR(__preempt_count)
+#ifndef CONFIG_PREEMPT_LAZY
 	jnz restore_all
+#else
+	jz test_int_off
+
+	# atleast preempt count == 0 ?
+	cmpl $_PREEMPT_ENABLED,PER_CPU_VAR(__preempt_count)
+	jne restore_all
+
+	cmpl $0,TI_preempt_lazy_count(%ebp)	# non-zero preempt_lazy_count ?
+	jnz restore_all
+
+	testl $_TIF_NEED_RESCHED_LAZY, TI_flags(%ebp)
+	jz restore_all
+test_int_off:
+#endif
 	testl $X86_EFLAGS_IF,PT_EFLAGS(%esp)	# interrupts off (exception path) ?
 	jz restore_all
 	call preempt_schedule_irq
@@ -591,7 +607,7 @@ ENDPROC(system_call)
 	ALIGN
 	RING0_PTREGS_FRAME		# can't unwind into user space anyway
 work_pending:
-	testb $_TIF_NEED_RESCHED, %cl
+	testl $_TIF_NEED_RESCHED_MASK, %ecx
 	jz work_notifysig
 work_resched:
 	call schedule
@@ -604,7 +620,7 @@ work_resched:
 	andl $_TIF_WORK_MASK, %ecx	# is there any work to be done other
 					# than syscall tracing?
 	jz restore_all
-	testb $_TIF_NEED_RESCHED, %cl
+	testl $_TIF_NEED_RESCHED_MASK, %ecx
 	jnz work_resched
 
 work_notifysig:				# deal with pending signals and
diff --git a/kernel/msm-3.18/arch/x86/kernel/entry_64.S b/kernel/msm-3.18/arch/x86/kernel/entry_64.S
index a3255ca21..f4cbb1272 100644
--- a/kernel/msm-3.18/arch/x86/kernel/entry_64.S
+++ b/kernel/msm-3.18/arch/x86/kernel/entry_64.S
@@ -454,8 +454,8 @@ sysret_check:
 	/* Handle reschedules */
 	/* edx:	work, edi: workmask */
 sysret_careful:
-	bt $TIF_NEED_RESCHED,%edx
-	jnc sysret_signal
+	testl $_TIF_NEED_RESCHED_MASK,%edx
+	jz sysret_signal
 	TRACE_IRQS_ON
 	ENABLE_INTERRUPTS(CLBR_NONE)
 	pushq_cfi %rdi
@@ -554,8 +554,8 @@ GLOBAL(int_with_check)
 	/* First do a reschedule test. */
 	/* edx:	work, edi: workmask */
 int_careful:
-	bt $TIF_NEED_RESCHED,%edx
-	jnc  int_very_careful
+	testl $_TIF_NEED_RESCHED_MASK,%edx
+	jz  int_very_careful
 	TRACE_IRQS_ON
 	ENABLE_INTERRUPTS(CLBR_NONE)
 	pushq_cfi %rdi
@@ -870,8 +870,8 @@ native_irq_return_ldt:
 	/* edi: workmask, edx: work */
 retint_careful:
 	CFI_RESTORE_STATE
-	bt    $TIF_NEED_RESCHED,%edx
-	jnc   retint_signal
+	testl $_TIF_NEED_RESCHED_MASK,%edx
+	jz   retint_signal
 	TRACE_IRQS_ON
 	ENABLE_INTERRUPTS(CLBR_NONE)
 	pushq_cfi %rdi
@@ -903,7 +903,22 @@ retint_signal:
 	/* rcx:	 threadinfo. interrupts off. */
 ENTRY(retint_kernel)
 	cmpl $0,PER_CPU_VAR(__preempt_count)
+#ifndef CONFIG_PREEMPT_LAZY
 	jnz  retint_restore_args
+#else
+	jz  check_int_off
+
+	# atleast preempt count == 0 ?
+	cmpl $_PREEMPT_ENABLED,PER_CPU_VAR(__preempt_count)
+	jnz retint_restore_args
+
+	cmpl $0, TI_preempt_lazy_count(%rcx)
+	jnz retint_restore_args
+
+	bt $TIF_NEED_RESCHED_LAZY,TI_flags(%rcx)
+	jnc  retint_restore_args
+check_int_off:
+#endif
 	bt   $9,EFLAGS-ARGOFFSET(%rsp)	/* interrupts off? */
 	jnc  retint_restore_args
 	call preempt_schedule_irq
@@ -1119,6 +1134,7 @@ bad_gs:
 	jmp  2b
 	.previous
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 /* Call softirq on interrupt stack. Interrupts are off. */
 ENTRY(do_softirq_own_stack)
 	CFI_STARTPROC
@@ -1138,6 +1154,7 @@ ENTRY(do_softirq_own_stack)
 	ret
 	CFI_ENDPROC
 END(do_softirq_own_stack)
+#endif
 
 #ifdef CONFIG_XEN
 idtentry xen_hypervisor_callback xen_do_hypervisor_callback has_error_code=0
@@ -1305,7 +1322,7 @@ paranoid_userspace:
 	movq %rsp,%rdi			/* &pt_regs */
 	call sync_regs
 	movq %rax,%rsp			/* switch stack for scheduling */
-	testl $_TIF_NEED_RESCHED,%ebx
+	testl $_TIF_NEED_RESCHED_MASK,%ebx
 	jnz paranoid_schedule
 	movl %ebx,%edx			/* arg3: thread flags */
 	TRACE_IRQS_ON
diff --git a/kernel/msm-3.18/arch/x86/kernel/irq_32.c b/kernel/msm-3.18/arch/x86/kernel/irq_32.c
index 63ce838e5..b889f5ba4 100644
--- a/kernel/msm-3.18/arch/x86/kernel/irq_32.c
+++ b/kernel/msm-3.18/arch/x86/kernel/irq_32.c
@@ -142,6 +142,7 @@ void irq_ctx_init(int cpu)
 	       cpu, per_cpu(hardirq_stack, cpu),  per_cpu(softirq_stack, cpu));
 }
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 void do_softirq_own_stack(void)
 {
 	struct thread_info *curstk;
@@ -160,6 +161,7 @@ void do_softirq_own_stack(void)
 
 	call_on_stack(__do_softirq, isp);
 }
+#endif
 
 bool handle_irq(unsigned irq, struct pt_regs *regs)
 {
diff --git a/kernel/msm-3.18/arch/x86/kernel/kvm.c b/kernel/msm-3.18/arch/x86/kernel/kvm.c
index dc3d52a24..66ada0bb5 100644
--- a/kernel/msm-3.18/arch/x86/kernel/kvm.c
+++ b/kernel/msm-3.18/arch/x86/kernel/kvm.c
@@ -36,6 +36,7 @@
 #include <linux/kprobes.h>
 #include <linux/debugfs.h>
 #include <linux/nmi.h>
+#include <linux/wait-simple.h>
 #include <asm/timer.h>
 #include <asm/cpu.h>
 #include <asm/traps.h>
@@ -91,14 +92,14 @@ static void kvm_io_delay(void)
 
 struct kvm_task_sleep_node {
 	struct hlist_node link;
-	wait_queue_head_t wq;
+	struct swait_head wq;
 	u32 token;
 	int cpu;
 	bool halted;
 };
 
 static struct kvm_task_sleep_head {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct hlist_head list;
 } async_pf_sleepers[KVM_TASK_SLEEP_HASHSIZE];
 
@@ -122,17 +123,17 @@ void kvm_async_pf_task_wait(u32 token)
 	u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
 	struct kvm_task_sleep_head *b = &async_pf_sleepers[key];
 	struct kvm_task_sleep_node n, *e;
-	DEFINE_WAIT(wait);
+	DEFINE_SWAITER(wait);
 
 	rcu_irq_enter();
 
-	spin_lock(&b->lock);
+	raw_spin_lock(&b->lock);
 	e = _find_apf_task(b, token);
 	if (e) {
 		/* dummy entry exist -> wake up was delivered ahead of PF */
 		hlist_del(&e->link);
 		kfree(e);
-		spin_unlock(&b->lock);
+		raw_spin_unlock(&b->lock);
 
 		rcu_irq_exit();
 		return;
@@ -141,13 +142,13 @@ void kvm_async_pf_task_wait(u32 token)
 	n.token = token;
 	n.cpu = smp_processor_id();
 	n.halted = is_idle_task(current) || preempt_count() > 1;
-	init_waitqueue_head(&n.wq);
+	init_swait_head(&n.wq);
 	hlist_add_head(&n.link, &b->list);
-	spin_unlock(&b->lock);
+	raw_spin_unlock(&b->lock);
 
 	for (;;) {
 		if (!n.halted)
-			prepare_to_wait(&n.wq, &wait, TASK_UNINTERRUPTIBLE);
+			swait_prepare(&n.wq, &wait, TASK_UNINTERRUPTIBLE);
 		if (hlist_unhashed(&n.link))
 			break;
 
@@ -168,7 +169,7 @@ void kvm_async_pf_task_wait(u32 token)
 		rcu_irq_enter();
 	}
 	if (!n.halted)
-		finish_wait(&n.wq, &wait);
+		swait_finish(&n.wq, &wait);
 
 	rcu_irq_exit();
 	return;
@@ -180,8 +181,8 @@ static void apf_task_wake_one(struct kvm_task_sleep_node *n)
 	hlist_del_init(&n->link);
 	if (n->halted)
 		smp_send_reschedule(n->cpu);
-	else if (waitqueue_active(&n->wq))
-		wake_up(&n->wq);
+	else if (swaitqueue_active(&n->wq))
+		swait_wake(&n->wq);
 }
 
 static void apf_task_wake_all(void)
@@ -191,14 +192,14 @@ static void apf_task_wake_all(void)
 	for (i = 0; i < KVM_TASK_SLEEP_HASHSIZE; i++) {
 		struct hlist_node *p, *next;
 		struct kvm_task_sleep_head *b = &async_pf_sleepers[i];
-		spin_lock(&b->lock);
+		raw_spin_lock(&b->lock);
 		hlist_for_each_safe(p, next, &b->list) {
 			struct kvm_task_sleep_node *n =
 				hlist_entry(p, typeof(*n), link);
 			if (n->cpu == smp_processor_id())
 				apf_task_wake_one(n);
 		}
-		spin_unlock(&b->lock);
+		raw_spin_unlock(&b->lock);
 	}
 }
 
@@ -214,7 +215,7 @@ void kvm_async_pf_task_wake(u32 token)
 	}
 
 again:
-	spin_lock(&b->lock);
+	raw_spin_lock(&b->lock);
 	n = _find_apf_task(b, token);
 	if (!n) {
 		/*
@@ -227,17 +228,17 @@ again:
 			 * Allocation failed! Busy wait while other cpu
 			 * handles async PF.
 			 */
-			spin_unlock(&b->lock);
+			raw_spin_unlock(&b->lock);
 			cpu_relax();
 			goto again;
 		}
 		n->token = token;
 		n->cpu = smp_processor_id();
-		init_waitqueue_head(&n->wq);
+		init_swait_head(&n->wq);
 		hlist_add_head(&n->link, &b->list);
 	} else
 		apf_task_wake_one(n);
-	spin_unlock(&b->lock);
+	raw_spin_unlock(&b->lock);
 	return;
 }
 EXPORT_SYMBOL_GPL(kvm_async_pf_task_wake);
@@ -488,7 +489,7 @@ void __init kvm_guest_init(void)
 	paravirt_ops_setup();
 	register_reboot_notifier(&kvm_pv_reboot_nb);
 	for (i = 0; i < KVM_TASK_SLEEP_HASHSIZE; i++)
-		spin_lock_init(&async_pf_sleepers[i].lock);
+		raw_spin_lock_init(&async_pf_sleepers[i].lock);
 	if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF))
 		x86_init.irqs.trap_init = kvm_apf_trap_init;
 
diff --git a/kernel/msm-3.18/arch/x86/kernel/process_32.c b/kernel/msm-3.18/arch/x86/kernel/process_32.c
index 603c4f99c..ae3ac253f 100644
--- a/kernel/msm-3.18/arch/x86/kernel/process_32.c
+++ b/kernel/msm-3.18/arch/x86/kernel/process_32.c
@@ -35,6 +35,7 @@
 #include <linux/uaccess.h>
 #include <linux/io.h>
 #include <linux/kdebug.h>
+#include <linux/highmem.h>
 
 #include <asm/pgtable.h>
 #include <asm/ldt.h>
@@ -214,6 +215,35 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
 }
 EXPORT_SYMBOL_GPL(start_thread);
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+static void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p)
+{
+	int i;
+
+	/*
+	 * Clear @prev's kmap_atomic mappings
+	 */
+	for (i = 0; i < prev_p->kmap_idx; i++) {
+		int idx = i + KM_TYPE_NR * smp_processor_id();
+		pte_t *ptep = kmap_pte - idx;
+
+		kpte_clear_flush(ptep, __fix_to_virt(FIX_KMAP_BEGIN + idx));
+	}
+	/*
+	 * Restore @next_p's kmap_atomic mappings
+	 */
+	for (i = 0; i < next_p->kmap_idx; i++) {
+		int idx = i + KM_TYPE_NR * smp_processor_id();
+
+		if (!pte_none(next_p->kmap_pte[i]))
+			set_pte(kmap_pte - idx, next_p->kmap_pte[i]);
+	}
+}
+#else
+static inline void
+switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p) { }
+#endif
+
 
 /*
  *	switch_to(x,y) should switch tasks from x to y.
@@ -301,6 +331,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 		     task_thread_info(next_p)->flags & _TIF_WORK_CTXSW_NEXT))
 		__switch_to_xtra(prev_p, next_p, tss);
 
+	switch_kmaps(prev_p, next_p);
+
 	/*
 	 * Leave lazy mode, flushing any hypercalls made here.
 	 * This must be done before restoring TLS segments so
diff --git a/kernel/msm-3.18/arch/x86/kernel/signal.c b/kernel/msm-3.18/arch/x86/kernel/signal.c
index 0a62df4ab..dabf7a235 100644
--- a/kernel/msm-3.18/arch/x86/kernel/signal.c
+++ b/kernel/msm-3.18/arch/x86/kernel/signal.c
@@ -746,6 +746,14 @@ do_notify_resume(struct pt_regs *regs, void *unused, __u32 thread_info_flags)
 		mce_notify_process();
 #endif /* CONFIG_X86_64 && CONFIG_X86_MCE */
 
+#ifdef ARCH_RT_DELAYS_SIGNAL_SEND
+	if (unlikely(current->forced_info.si_signo)) {
+		struct task_struct *t = current;
+		force_sig_info(t->forced_info.si_signo,	&t->forced_info, t);
+		t->forced_info.si_signo = 0;
+	}
+#endif
+
 	if (thread_info_flags & _TIF_UPROBE)
 		uprobe_notify_resume(regs);
 
diff --git a/kernel/msm-3.18/arch/x86/kernel/traps.c b/kernel/msm-3.18/arch/x86/kernel/traps.c
index 07ab8e973..4f0662bb6 100644
--- a/kernel/msm-3.18/arch/x86/kernel/traps.c
+++ b/kernel/msm-3.18/arch/x86/kernel/traps.c
@@ -87,9 +87,21 @@ static inline void conditional_sti(struct pt_regs *regs)
 		local_irq_enable();
 }
 
-static inline void preempt_conditional_sti(struct pt_regs *regs)
+static inline void conditional_sti_ist(struct pt_regs *regs)
 {
+#ifdef CONFIG_X86_64
+	/*
+	 * X86_64 uses a per CPU stack on the IST for certain traps
+	 * like int3. The task can not be preempted when using one
+	 * of these stacks, thus preemption must be disabled, otherwise
+	 * the stack can be corrupted if the task is scheduled out,
+	 * and another task comes in and uses this stack.
+	 *
+	 * On x86_32 the task keeps its own stack and it is OK if the
+	 * task schedules out.
+	 */
 	preempt_count_inc();
+#endif
 	if (regs->flags & X86_EFLAGS_IF)
 		local_irq_enable();
 }
@@ -100,11 +112,13 @@ static inline void conditional_cli(struct pt_regs *regs)
 		local_irq_disable();
 }
 
-static inline void preempt_conditional_cli(struct pt_regs *regs)
+static inline void conditional_cli_ist(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
 		local_irq_disable();
+#ifdef CONFIG_X86_64
 	preempt_count_dec();
+#endif
 }
 
 static nokprobe_inline int
@@ -372,9 +386,9 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 	 * as we may switch to the interrupt stack.
 	 */
 	debug_stack_usage_inc();
-	preempt_conditional_sti(regs);
+	conditional_sti_ist(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, NULL);
-	preempt_conditional_cli(regs);
+	conditional_cli_ist(regs);
 	debug_stack_usage_dec();
 exit:
 	exception_exit(prev_state);
@@ -517,12 +531,12 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_inc();
 
 	/* It's safe to allow irq's after DR6 has been saved */
-	preempt_conditional_sti(regs);
+	conditional_sti_ist(regs);
 
 	if (regs->flags & X86_VM_MASK) {
 		handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code,
 					X86_TRAP_DB);
-		preempt_conditional_cli(regs);
+		conditional_cli_ist(regs);
 		debug_stack_usage_dec();
 		goto exit;
 	}
@@ -542,7 +556,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	si_code = get_si_code(tsk->thread.debugreg6);
 	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)
 		send_sigtrap(tsk, regs, error_code, si_code);
-	preempt_conditional_cli(regs);
+	conditional_cli_ist(regs);
 	debug_stack_usage_dec();
 
 exit:
diff --git a/kernel/msm-3.18/arch/x86/kvm/lapic.c b/kernel/msm-3.18/arch/x86/kvm/lapic.c
index de8e50040..33caf685e 100644
--- a/kernel/msm-3.18/arch/x86/kvm/lapic.c
+++ b/kernel/msm-3.18/arch/x86/kvm/lapic.c
@@ -1034,8 +1034,38 @@ static void update_divide_count(struct kvm_lapic *apic)
 				   apic->divide_count);
 }
 
+
+static enum hrtimer_restart apic_timer_fn(struct hrtimer *data);
+
+static void apic_timer_expired(struct hrtimer *data)
+{
+	int ret, i = 0;
+	enum hrtimer_restart r;
+	struct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);
+
+	r = apic_timer_fn(data);
+
+	if (r == HRTIMER_RESTART) {
+		do {
+			ret = hrtimer_start_expires(data, HRTIMER_MODE_ABS);
+			if (ret == -ETIME)
+				hrtimer_add_expires_ns(&ktimer->timer,
+							ktimer->period);
+			i++;
+		} while (ret == -ETIME && i < 10);
+
+		if (ret == -ETIME) {
+			printk_once(KERN_ERR "%s: failed to reprogram timer\n",
+			       __func__);
+			WARN_ON_ONCE(1);
+		}
+	}
+}
+
+
 static void start_apic_timer(struct kvm_lapic *apic)
 {
+	int ret;
 	ktime_t now;
 	atomic_set(&apic->lapic_timer.pending, 0);
 
@@ -1065,9 +1095,11 @@ static void start_apic_timer(struct kvm_lapic *apic)
 			}
 		}
 
-		hrtimer_start(&apic->lapic_timer.timer,
+		ret = hrtimer_start(&apic->lapic_timer.timer,
 			      ktime_add_ns(now, apic->lapic_timer.period),
 			      HRTIMER_MODE_ABS);
+		if (ret == -ETIME)
+			apic_timer_expired(&apic->lapic_timer.timer);
 
 		apic_debug("%s: bus cycle is %" PRId64 "ns, now 0x%016"
 			   PRIx64 ", "
@@ -1097,8 +1129,10 @@ static void start_apic_timer(struct kvm_lapic *apic)
 			ns = (tscdeadline - guest_tsc) * 1000000ULL;
 			do_div(ns, this_tsc_khz);
 		}
-		hrtimer_start(&apic->lapic_timer.timer,
+		ret = hrtimer_start(&apic->lapic_timer.timer,
 			ktime_add_ns(now, ns), HRTIMER_MODE_ABS);
+		if (ret == -ETIME)
+			apic_timer_expired(&apic->lapic_timer.timer);
 
 		local_irq_restore(flags);
 	}
@@ -1539,7 +1573,7 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 	struct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);
 	struct kvm_lapic *apic = container_of(ktimer, struct kvm_lapic, lapic_timer);
 	struct kvm_vcpu *vcpu = apic->vcpu;
-	wait_queue_head_t *q = &vcpu->wq;
+	struct swait_head *q = &vcpu->wq;
 
 	/*
 	 * There is a race window between reading and incrementing, but we do
@@ -1553,8 +1587,8 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		kvm_make_request(KVM_REQ_PENDING_TIMER, vcpu);
 	}
 
-	if (waitqueue_active(q))
-		wake_up_interruptible(q);
+	if (swaitqueue_active(q))
+		swait_wake_interruptible(q);
 
 	if (lapic_is_periodic(apic)) {
 		hrtimer_add_expires_ns(&ktimer->timer, ktimer->period);
@@ -1587,6 +1621,7 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
 	hrtimer_init(&apic->lapic_timer.timer, CLOCK_MONOTONIC,
 		     HRTIMER_MODE_ABS);
 	apic->lapic_timer.timer.function = apic_timer_fn;
+	apic->lapic_timer.timer.irqsafe = 1;
 
 	/*
 	 * APIC is created enabled. This will prevent kvm_lapic_set_base from
@@ -1708,7 +1743,8 @@ void __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)
 
 	timer = &vcpu->arch.apic->lapic_timer.timer;
 	if (hrtimer_cancel(timer))
-		hrtimer_start_expires(timer, HRTIMER_MODE_ABS);
+		if (hrtimer_start_expires(timer, HRTIMER_MODE_ABS) == -ETIME)
+			apic_timer_expired(timer);
 }
 
 /*
diff --git a/kernel/msm-3.18/arch/x86/kvm/x86.c b/kernel/msm-3.18/arch/x86/kvm/x86.c
index 3411283e0..03297b924 100644
--- a/kernel/msm-3.18/arch/x86/kvm/x86.c
+++ b/kernel/msm-3.18/arch/x86/kvm/x86.c
@@ -5773,6 +5773,13 @@ int kvm_arch_init(void *opaque)
 		goto out;
 	}
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+	if (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {
+		printk(KERN_ERR "RT requires X86_FEATURE_CONSTANT_TSC\n");
+		return -EOPNOTSUPP;
+	}
+#endif
+
 	r = kvm_mmu_module_init();
 	if (r)
 		goto out_free_percpu;
diff --git a/kernel/msm-3.18/arch/x86/mm/fault.c b/kernel/msm-3.18/arch/x86/mm/fault.c
index 6fa245ae5..bc77c3ce4 100644
--- a/kernel/msm-3.18/arch/x86/mm/fault.c
+++ b/kernel/msm-3.18/arch/x86/mm/fault.c
@@ -1128,7 +1128,7 @@ __do_page_fault(struct pt_regs *regs, unsigned long error_code,
 	 * If we're in an interrupt, have no user context or are running
 	 * in an atomic region then we must not take the fault:
 	 */
-	if (unlikely(in_atomic() || !mm)) {
+	if (unlikely(!mm || pagefault_disabled())) {
 		bad_area_nosemaphore(regs, error_code, address);
 		return;
 	}
diff --git a/kernel/msm-3.18/arch/x86/mm/highmem_32.c b/kernel/msm-3.18/arch/x86/mm/highmem_32.c
index 4500142bc..7f9684447 100644
--- a/kernel/msm-3.18/arch/x86/mm/highmem_32.c
+++ b/kernel/msm-3.18/arch/x86/mm/highmem_32.c
@@ -32,6 +32,7 @@ EXPORT_SYMBOL(kunmap);
  */
 void *kmap_atomic_prot(struct page *page, pgprot_t prot)
 {
+	pte_t pte = mk_pte(page, prot);
 	unsigned long vaddr;
 	int idx, type;
 
@@ -45,7 +46,10 @@ void *kmap_atomic_prot(struct page *page, pgprot_t prot)
 	idx = type + KM_TYPE_NR*smp_processor_id();
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
 	BUG_ON(!pte_none(*(kmap_pte-idx)));
-	set_pte(kmap_pte-idx, mk_pte(page, prot));
+#ifdef CONFIG_PREEMPT_RT_FULL
+	current->kmap_pte[type] = pte;
+#endif
+	set_pte(kmap_pte-idx, pte);
 	arch_flush_lazy_mmu_mode();
 
 	return (void *)vaddr;
@@ -88,6 +92,9 @@ void __kunmap_atomic(void *kvaddr)
 		 * is a bad idea also, in case the page changes cacheability
 		 * attributes or becomes a protected page in a hypervisor.
 		 */
+#ifdef CONFIG_PREEMPT_RT_FULL
+		current->kmap_pte[type] = __pte(0);
+#endif
 		kpte_clear_flush(kmap_pte-idx, vaddr);
 		kmap_atomic_idx_pop();
 		arch_flush_lazy_mmu_mode();
diff --git a/kernel/msm-3.18/arch/x86/mm/iomap_32.c b/kernel/msm-3.18/arch/x86/mm/iomap_32.c
index 7b179b499..62377d67a 100644
--- a/kernel/msm-3.18/arch/x86/mm/iomap_32.c
+++ b/kernel/msm-3.18/arch/x86/mm/iomap_32.c
@@ -56,6 +56,7 @@ EXPORT_SYMBOL_GPL(iomap_free);
 
 void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
 {
+	pte_t pte = pfn_pte(pfn, prot);
 	unsigned long vaddr;
 	int idx, type;
 
@@ -64,7 +65,12 @@ void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
 	type = kmap_atomic_idx_push();
 	idx = type + KM_TYPE_NR * smp_processor_id();
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
-	set_pte(kmap_pte - idx, pfn_pte(pfn, prot));
+	WARN_ON(!pte_none(*(kmap_pte - idx)));
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+	current->kmap_pte[type] = pte;
+#endif
+	set_pte(kmap_pte - idx, pte);
 	arch_flush_lazy_mmu_mode();
 
 	return (void *)vaddr;
@@ -110,6 +116,9 @@ iounmap_atomic(void __iomem *kvaddr)
 		 * is a bad idea also, in case the page changes cacheability
 		 * attributes or becomes a protected page in a hypervisor.
 		 */
+#ifdef CONFIG_PREEMPT_RT_FULL
+		current->kmap_pte[type] = __pte(0);
+#endif
 		kpte_clear_flush(kmap_pte-idx, vaddr);
 		kmap_atomic_idx_pop();
 	}
diff --git a/kernel/msm-3.18/arch/x86/mm/pageattr.c b/kernel/msm-3.18/arch/x86/mm/pageattr.c
index 29add6bd9..51a928f05 100644
--- a/kernel/msm-3.18/arch/x86/mm/pageattr.c
+++ b/kernel/msm-3.18/arch/x86/mm/pageattr.c
@@ -211,7 +211,15 @@ static void cpa_flush_array(unsigned long *start, int numpages, int cache,
 			    int in_flags, struct page **pages)
 {
 	unsigned int i, level;
+#ifdef CONFIG_PREEMPT
+	/*
+	 * Avoid wbinvd() because it causes latencies on all CPUs,
+	 * regardless of any CPU isolation that may be in effect.
+	 */
+	unsigned long do_wbinvd = 0;
+#else
 	unsigned long do_wbinvd = cache && numpages >= 1024; /* 4M threshold */
+#endif
 
 	BUG_ON(irqs_disabled());
 
diff --git a/kernel/msm-3.18/arch/x86/platform/uv/tlb_uv.c b/kernel/msm-3.18/arch/x86/platform/uv/tlb_uv.c
index 3968d67d3..7d444650b 100644
--- a/kernel/msm-3.18/arch/x86/platform/uv/tlb_uv.c
+++ b/kernel/msm-3.18/arch/x86/platform/uv/tlb_uv.c
@@ -714,9 +714,9 @@ static void destination_plugged(struct bau_desc *bau_desc,
 
 		quiesce_local_uvhub(hmaster);
 
-		spin_lock(&hmaster->queue_lock);
+		raw_spin_lock(&hmaster->queue_lock);
 		reset_with_ipi(&bau_desc->distribution, bcp);
-		spin_unlock(&hmaster->queue_lock);
+		raw_spin_unlock(&hmaster->queue_lock);
 
 		end_uvhub_quiesce(hmaster);
 
@@ -736,9 +736,9 @@ static void destination_timeout(struct bau_desc *bau_desc,
 
 		quiesce_local_uvhub(hmaster);
 
-		spin_lock(&hmaster->queue_lock);
+		raw_spin_lock(&hmaster->queue_lock);
 		reset_with_ipi(&bau_desc->distribution, bcp);
-		spin_unlock(&hmaster->queue_lock);
+		raw_spin_unlock(&hmaster->queue_lock);
 
 		end_uvhub_quiesce(hmaster);
 
@@ -759,7 +759,7 @@ static void disable_for_period(struct bau_control *bcp, struct ptc_stats *stat)
 	cycles_t tm1;
 
 	hmaster = bcp->uvhub_master;
-	spin_lock(&hmaster->disable_lock);
+	raw_spin_lock(&hmaster->disable_lock);
 	if (!bcp->baudisabled) {
 		stat->s_bau_disabled++;
 		tm1 = get_cycles();
@@ -772,7 +772,7 @@ static void disable_for_period(struct bau_control *bcp, struct ptc_stats *stat)
 			}
 		}
 	}
-	spin_unlock(&hmaster->disable_lock);
+	raw_spin_unlock(&hmaster->disable_lock);
 }
 
 static void count_max_concurr(int stat, struct bau_control *bcp,
@@ -835,7 +835,7 @@ static void record_send_stats(cycles_t time1, cycles_t time2,
  */
 static void uv1_throttle(struct bau_control *hmaster, struct ptc_stats *stat)
 {
-	spinlock_t *lock = &hmaster->uvhub_lock;
+	raw_spinlock_t *lock = &hmaster->uvhub_lock;
 	atomic_t *v;
 
 	v = &hmaster->active_descriptor_count;
@@ -968,7 +968,7 @@ static int check_enable(struct bau_control *bcp, struct ptc_stats *stat)
 	struct bau_control *hmaster;
 
 	hmaster = bcp->uvhub_master;
-	spin_lock(&hmaster->disable_lock);
+	raw_spin_lock(&hmaster->disable_lock);
 	if (bcp->baudisabled && (get_cycles() >= bcp->set_bau_on_time)) {
 		stat->s_bau_reenabled++;
 		for_each_present_cpu(tcpu) {
@@ -980,10 +980,10 @@ static int check_enable(struct bau_control *bcp, struct ptc_stats *stat)
 				tbcp->period_giveups = 0;
 			}
 		}
-		spin_unlock(&hmaster->disable_lock);
+		raw_spin_unlock(&hmaster->disable_lock);
 		return 0;
 	}
-	spin_unlock(&hmaster->disable_lock);
+	raw_spin_unlock(&hmaster->disable_lock);
 	return -1;
 }
 
@@ -1899,9 +1899,9 @@ static void __init init_per_cpu_tunables(void)
 		bcp->cong_reps			= congested_reps;
 		bcp->disabled_period =		sec_2_cycles(disabled_period);
 		bcp->giveup_limit =		giveup_limit;
-		spin_lock_init(&bcp->queue_lock);
-		spin_lock_init(&bcp->uvhub_lock);
-		spin_lock_init(&bcp->disable_lock);
+		raw_spin_lock_init(&bcp->queue_lock);
+		raw_spin_lock_init(&bcp->uvhub_lock);
+		raw_spin_lock_init(&bcp->disable_lock);
 	}
 }
 
diff --git a/kernel/msm-3.18/arch/x86/platform/uv/uv_time.c b/kernel/msm-3.18/arch/x86/platform/uv/uv_time.c
index a244237f3..a718fe0d2 100644
--- a/kernel/msm-3.18/arch/x86/platform/uv/uv_time.c
+++ b/kernel/msm-3.18/arch/x86/platform/uv/uv_time.c
@@ -58,7 +58,7 @@ static DEFINE_PER_CPU(struct clock_event_device, cpu_ced);
 
 /* There is one of these allocated per node */
 struct uv_rtc_timer_head {
-	spinlock_t	lock;
+	raw_spinlock_t	lock;
 	/* next cpu waiting for timer, local node relative: */
 	int		next_cpu;
 	/* number of cpus on this node: */
@@ -178,7 +178,7 @@ static __init int uv_rtc_allocate_timers(void)
 				uv_rtc_deallocate_timers();
 				return -ENOMEM;
 			}
-			spin_lock_init(&head->lock);
+			raw_spin_lock_init(&head->lock);
 			head->ncpus = uv_blade_nr_possible_cpus(bid);
 			head->next_cpu = -1;
 			blade_info[bid] = head;
@@ -232,7 +232,7 @@ static int uv_rtc_set_timer(int cpu, u64 expires)
 	unsigned long flags;
 	int next_cpu;
 
-	spin_lock_irqsave(&head->lock, flags);
+	raw_spin_lock_irqsave(&head->lock, flags);
 
 	next_cpu = head->next_cpu;
 	*t = expires;
@@ -244,12 +244,12 @@ static int uv_rtc_set_timer(int cpu, u64 expires)
 		if (uv_setup_intr(cpu, expires)) {
 			*t = ULLONG_MAX;
 			uv_rtc_find_next_timer(head, pnode);
-			spin_unlock_irqrestore(&head->lock, flags);
+			raw_spin_unlock_irqrestore(&head->lock, flags);
 			return -ETIME;
 		}
 	}
 
-	spin_unlock_irqrestore(&head->lock, flags);
+	raw_spin_unlock_irqrestore(&head->lock, flags);
 	return 0;
 }
 
@@ -268,7 +268,7 @@ static int uv_rtc_unset_timer(int cpu, int force)
 	unsigned long flags;
 	int rc = 0;
 
-	spin_lock_irqsave(&head->lock, flags);
+	raw_spin_lock_irqsave(&head->lock, flags);
 
 	if ((head->next_cpu == bcpu && uv_read_rtc(NULL) >= *t) || force)
 		rc = 1;
@@ -280,7 +280,7 @@ static int uv_rtc_unset_timer(int cpu, int force)
 			uv_rtc_find_next_timer(head, pnode);
 	}
 
-	spin_unlock_irqrestore(&head->lock, flags);
+	raw_spin_unlock_irqrestore(&head->lock, flags);
 
 	return rc;
 }
@@ -300,13 +300,18 @@ static int uv_rtc_unset_timer(int cpu, int force)
 static cycle_t uv_read_rtc(struct clocksource *cs)
 {
 	unsigned long offset;
+	cycle_t cycles;
 
+	preempt_disable();
 	if (uv_get_min_hub_revision_id() == 1)
 		offset = 0;
 	else
 		offset = (uv_blade_processor_id() * L1_CACHE_BYTES) % PAGE_SIZE;
 
-	return (cycle_t)uv_read_local_mmr(UVH_RTC | offset);
+	cycles = (cycle_t)uv_read_local_mmr(UVH_RTC | offset);
+	preempt_enable();
+
+	return cycles;
 }
 
 /*
diff --git a/kernel/msm-3.18/arch/xtensa/mm/fault.c b/kernel/msm-3.18/arch/xtensa/mm/fault.c
index 9e3571a65..9696ab258 100644
--- a/kernel/msm-3.18/arch/xtensa/mm/fault.c
+++ b/kernel/msm-3.18/arch/xtensa/mm/fault.c
@@ -57,7 +57,7 @@ void do_page_fault(struct pt_regs *regs)
 	/* If we're in an interrupt or have no user
 	 * context, we must not take the fault..
 	 */
-	if (in_atomic() || !mm) {
+	if (!mm || pagefault_disabled()) {
 		bad_page_fault(regs, address, SIGSEGV);
 		return;
 	}
diff --git a/kernel/msm-3.18/block/blk-core.c b/kernel/msm-3.18/block/blk-core.c
index 950eea0ee..0ee9306a3 100644
--- a/kernel/msm-3.18/block/blk-core.c
+++ b/kernel/msm-3.18/block/blk-core.c
@@ -102,6 +102,9 @@ void blk_rq_init(struct request_queue *q, struct request *rq)
 
 	INIT_LIST_HEAD(&rq->queuelist);
 	INIT_LIST_HEAD(&rq->timeout_list);
+#if CONFIG_PREEMPT_RT_FULL
+	INIT_WORK(&rq->work, __blk_mq_complete_request_remote_work);
+#endif
 	rq->cpu = -1;
 	rq->q = q;
 	rq->__sector = (sector_t) -1;
@@ -196,7 +199,7 @@ EXPORT_SYMBOL(blk_delay_queue);
  **/
 void blk_start_queue(struct request_queue *q)
 {
-	WARN_ON(!irqs_disabled());
+	WARN_ON_NONRT(!irqs_disabled());
 
 	queue_flag_clear(QUEUE_FLAG_STOPPED, q);
 	__blk_run_queue(q);
@@ -629,7 +632,7 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 	q->bypass_depth = 1;
 	__set_bit(QUEUE_FLAG_BYPASS, &q->queue_flags);
 
-	init_waitqueue_head(&q->mq_freeze_wq);
+	init_swait_head(&q->mq_freeze_wq);
 
 	if (blkcg_init_queue(q))
 		goto fail_bdi;
@@ -3074,7 +3077,7 @@ static void queue_unplugged(struct request_queue *q, unsigned int depth,
 		blk_run_queue_async(q);
 	else
 		__blk_run_queue(q);
-	spin_unlock(q->queue_lock);
+	spin_unlock_irq(q->queue_lock);
 }
 
 static void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)
@@ -3122,7 +3125,6 @@ EXPORT_SYMBOL(blk_check_plugged);
 void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	struct request_queue *q;
-	unsigned long flags;
 	struct request *rq;
 	LIST_HEAD(list);
 	unsigned int depth;
@@ -3142,11 +3144,6 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 	q = NULL;
 	depth = 0;
 
-	/*
-	 * Save and disable interrupts here, to avoid doing it for every
-	 * queue lock we have to take.
-	 */
-	local_irq_save(flags);
 	while (!list_empty(&list)) {
 		rq = list_entry_rq(list.next);
 		list_del_init(&rq->queuelist);
@@ -3159,7 +3156,7 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 				queue_unplugged(q, depth, from_schedule);
 			q = rq->q;
 			depth = 0;
-			spin_lock(q->queue_lock);
+			spin_lock_irq(q->queue_lock);
 		}
 
 		/*
@@ -3186,8 +3183,6 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 	 */
 	if (q)
 		queue_unplugged(q, depth, from_schedule);
-
-	local_irq_restore(flags);
 }
 
 void blk_finish_plug(struct blk_plug *plug)
diff --git a/kernel/msm-3.18/block/blk-ioc.c b/kernel/msm-3.18/block/blk-ioc.c
index 1a27f45ec..28f467e63 100644
--- a/kernel/msm-3.18/block/blk-ioc.c
+++ b/kernel/msm-3.18/block/blk-ioc.c
@@ -7,6 +7,7 @@
 #include <linux/bio.h>
 #include <linux/blkdev.h>
 #include <linux/slab.h>
+#include <linux/delay.h>
 
 #include "blk.h"
 
@@ -109,7 +110,7 @@ static void ioc_release_fn(struct work_struct *work)
 			spin_unlock(q->queue_lock);
 		} else {
 			spin_unlock_irqrestore(&ioc->lock, flags);
-			cpu_relax();
+			cpu_chill();
 			spin_lock_irqsave_nested(&ioc->lock, flags, 1);
 		}
 	}
@@ -187,7 +188,7 @@ retry:
 			spin_unlock(icq->q->queue_lock);
 		} else {
 			spin_unlock_irqrestore(&ioc->lock, flags);
-			cpu_relax();
+			cpu_chill();
 			goto retry;
 		}
 	}
diff --git a/kernel/msm-3.18/block/blk-iopoll.c b/kernel/msm-3.18/block/blk-iopoll.c
index 0736729d6..3e21e31d0 100644
--- a/kernel/msm-3.18/block/blk-iopoll.c
+++ b/kernel/msm-3.18/block/blk-iopoll.c
@@ -35,6 +35,7 @@ void blk_iopoll_sched(struct blk_iopoll *iop)
 	list_add_tail(&iop->list, this_cpu_ptr(&blk_cpu_iopoll));
 	__raise_softirq_irqoff(BLOCK_IOPOLL_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(blk_iopoll_sched);
 
@@ -132,6 +133,7 @@ static void blk_iopoll_softirq(struct softirq_action *h)
 		__raise_softirq_irqoff(BLOCK_IOPOLL_SOFTIRQ);
 
 	local_irq_enable();
+	preempt_check_resched_rt();
 }
 
 /**
@@ -201,6 +203,7 @@ static int blk_iopoll_cpu_notify(struct notifier_block *self,
 				 this_cpu_ptr(&blk_cpu_iopoll));
 		__raise_softirq_irqoff(BLOCK_IOPOLL_SOFTIRQ);
 		local_irq_enable();
+		preempt_check_resched_rt();
 	}
 
 	return NOTIFY_OK;
diff --git a/kernel/msm-3.18/block/blk-mq-cpu.c b/kernel/msm-3.18/block/blk-mq-cpu.c
index bb3ed488f..628c6c13c 100644
--- a/kernel/msm-3.18/block/blk-mq-cpu.c
+++ b/kernel/msm-3.18/block/blk-mq-cpu.c
@@ -16,7 +16,7 @@
 #include "blk-mq.h"
 
 static LIST_HEAD(blk_mq_cpu_notify_list);
-static DEFINE_RAW_SPINLOCK(blk_mq_cpu_notify_lock);
+static DEFINE_SPINLOCK(blk_mq_cpu_notify_lock);
 
 static int blk_mq_main_cpu_notify(struct notifier_block *self,
 				  unsigned long action, void *hcpu)
@@ -25,7 +25,10 @@ static int blk_mq_main_cpu_notify(struct notifier_block *self,
 	struct blk_mq_cpu_notifier *notify;
 	int ret = NOTIFY_OK;
 
-	raw_spin_lock(&blk_mq_cpu_notify_lock);
+	if (action != CPU_POST_DEAD)
+		return NOTIFY_OK;
+
+	spin_lock(&blk_mq_cpu_notify_lock);
 
 	list_for_each_entry(notify, &blk_mq_cpu_notify_list, list) {
 		ret = notify->notify(notify->data, action, cpu);
@@ -33,7 +36,7 @@ static int blk_mq_main_cpu_notify(struct notifier_block *self,
 			break;
 	}
 
-	raw_spin_unlock(&blk_mq_cpu_notify_lock);
+	spin_unlock(&blk_mq_cpu_notify_lock);
 	return ret;
 }
 
@@ -41,16 +44,16 @@ void blk_mq_register_cpu_notifier(struct blk_mq_cpu_notifier *notifier)
 {
 	BUG_ON(!notifier->notify);
 
-	raw_spin_lock(&blk_mq_cpu_notify_lock);
+	spin_lock(&blk_mq_cpu_notify_lock);
 	list_add_tail(&notifier->list, &blk_mq_cpu_notify_list);
-	raw_spin_unlock(&blk_mq_cpu_notify_lock);
+	spin_unlock(&blk_mq_cpu_notify_lock);
 }
 
 void blk_mq_unregister_cpu_notifier(struct blk_mq_cpu_notifier *notifier)
 {
-	raw_spin_lock(&blk_mq_cpu_notify_lock);
+	spin_lock(&blk_mq_cpu_notify_lock);
 	list_del(&notifier->list);
-	raw_spin_unlock(&blk_mq_cpu_notify_lock);
+	spin_unlock(&blk_mq_cpu_notify_lock);
 }
 
 void blk_mq_init_cpu_notifier(struct blk_mq_cpu_notifier *notifier,
diff --git a/kernel/msm-3.18/block/blk-mq.c b/kernel/msm-3.18/block/blk-mq.c
index 691959ecb..2c666222b 100644
--- a/kernel/msm-3.18/block/blk-mq.c
+++ b/kernel/msm-3.18/block/blk-mq.c
@@ -85,7 +85,7 @@ static int blk_mq_queue_enter(struct request_queue *q)
 		if (percpu_ref_tryget_live(&q->mq_usage_counter))
 			return 0;
 
-		ret = wait_event_interruptible(q->mq_freeze_wq,
+		ret = swait_event_interruptible(q->mq_freeze_wq,
 				!q->mq_freeze_depth || blk_queue_dying(q));
 		if (blk_queue_dying(q))
 			return -ENODEV;
@@ -104,7 +104,7 @@ static void blk_mq_usage_counter_release(struct percpu_ref *ref)
 	struct request_queue *q =
 		container_of(ref, struct request_queue, mq_usage_counter);
 
-	wake_up_all(&q->mq_freeze_wq);
+	swait_wake_all(&q->mq_freeze_wq);
 }
 
 static void blk_mq_freeze_queue_start(struct request_queue *q)
@@ -123,7 +123,7 @@ static void blk_mq_freeze_queue_start(struct request_queue *q)
 
 static void blk_mq_freeze_queue_wait(struct request_queue *q)
 {
-	wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->mq_usage_counter));
+	swait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->mq_usage_counter));
 }
 
 /*
@@ -146,7 +146,7 @@ static void blk_mq_unfreeze_queue(struct request_queue *q)
 	spin_unlock_irq(q->queue_lock);
 	if (wake) {
 		percpu_ref_reinit(&q->mq_usage_counter);
-		wake_up_all(&q->mq_freeze_wq);
+		swait_wake_all(&q->mq_freeze_wq);
 	}
 }
 
@@ -194,6 +194,9 @@ static void blk_mq_rq_ctx_init(struct request_queue *q, struct blk_mq_ctx *ctx,
 	rq->resid_len = 0;
 	rq->sense = NULL;
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+	INIT_WORK(&rq->work, __blk_mq_complete_request_remote_work);
+#endif
 	INIT_LIST_HEAD(&rq->timeout_list);
 	rq->timeout = 0;
 
@@ -313,6 +316,17 @@ void blk_mq_end_request(struct request *rq, int error)
 }
 EXPORT_SYMBOL(blk_mq_end_request);
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+
+void __blk_mq_complete_request_remote_work(struct work_struct *work)
+{
+	struct request *rq = container_of(work, struct request, work);
+
+	rq->q->softirq_done_fn(rq);
+}
+
+#else
+
 static void __blk_mq_complete_request_remote(void *data)
 {
 	struct request *rq = data;
@@ -320,6 +334,8 @@ static void __blk_mq_complete_request_remote(void *data)
 	rq->q->softirq_done_fn(rq);
 }
 
+#endif
+
 static void blk_mq_ipi_complete_request(struct request *rq)
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
@@ -331,19 +347,23 @@ static void blk_mq_ipi_complete_request(struct request *rq)
 		return;
 	}
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	if (!test_bit(QUEUE_FLAG_SAME_FORCE, &rq->q->queue_flags))
 		shared = cpus_share_cache(cpu, ctx->cpu);
 
 	if (cpu != ctx->cpu && !shared && cpu_online(ctx->cpu)) {
+#ifdef CONFIG_PREEMPT_RT_FULL
+		schedule_work_on(ctx->cpu, &rq->work);
+#else
 		rq->csd.func = __blk_mq_complete_request_remote;
 		rq->csd.info = rq;
 		rq->csd.flags = 0;
 		smp_call_function_single_async(ctx->cpu, &rq->csd);
+#endif
 	} else {
 		rq->q->softirq_done_fn(rq);
 	}
-	put_cpu();
+	put_cpu_light();
 }
 
 void __blk_mq_complete_request(struct request *rq)
@@ -814,9 +834,9 @@ void blk_mq_run_queues(struct request_queue *q, bool async)
 		    test_bit(BLK_MQ_S_STOPPED, &hctx->state))
 			continue;
 
-		preempt_disable();
+		migrate_disable();
 		blk_mq_run_hw_queue(hctx, async);
-		preempt_enable();
+		migrate_enable();
 	}
 }
 EXPORT_SYMBOL(blk_mq_run_queues);
@@ -843,9 +863,9 @@ void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
 
-	preempt_disable();
+	migrate_disable();
 	blk_mq_run_hw_queue(hctx, false);
-	preempt_enable();
+	migrate_enable();
 }
 EXPORT_SYMBOL(blk_mq_start_hw_queue);
 
@@ -870,9 +890,9 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 			continue;
 
 		clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
-		preempt_disable();
+		migrate_disable();
 		blk_mq_run_hw_queue(hctx, async);
-		preempt_enable();
+		migrate_enable();
 	}
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
@@ -1478,7 +1498,7 @@ static int blk_mq_hctx_notify(void *data, unsigned long action,
 {
 	struct blk_mq_hw_ctx *hctx = data;
 
-	if (action == CPU_DEAD || action == CPU_DEAD_FROZEN)
+	if (action == CPU_POST_DEAD)
 		return blk_mq_hctx_cpu_offline(hctx, cpu);
 
 	/*
diff --git a/kernel/msm-3.18/block/blk-mq.h b/kernel/msm-3.18/block/blk-mq.h
index d567d5283..d1d78dfe4 100644
--- a/kernel/msm-3.18/block/blk-mq.h
+++ b/kernel/msm-3.18/block/blk-mq.h
@@ -73,7 +73,10 @@ struct blk_align_bitmap {
 static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
 					   unsigned int cpu)
 {
-	return per_cpu_ptr(q->queue_ctx, cpu);
+	struct blk_mq_ctx *ctx;
+
+	ctx = per_cpu_ptr(q->queue_ctx, cpu);
+	return ctx;
 }
 
 /*
@@ -84,12 +87,12 @@ static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
  */
 static inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
 {
-	return __blk_mq_get_ctx(q, get_cpu());
+	return __blk_mq_get_ctx(q, get_cpu_light());
 }
 
 static inline void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
 {
-	put_cpu();
+	put_cpu_light();
 }
 
 struct blk_mq_alloc_data {
diff --git a/kernel/msm-3.18/block/blk-softirq.c b/kernel/msm-3.18/block/blk-softirq.c
index 53b1737e9..81c3c0a62 100644
--- a/kernel/msm-3.18/block/blk-softirq.c
+++ b/kernel/msm-3.18/block/blk-softirq.c
@@ -51,6 +51,7 @@ static void trigger_softirq(void *data)
 		raise_softirq_irqoff(BLOCK_SOFTIRQ);
 
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 
 /*
@@ -93,6 +94,7 @@ static int blk_cpu_notify(struct notifier_block *self, unsigned long action,
 				 this_cpu_ptr(&blk_cpu_done));
 		raise_softirq_irqoff(BLOCK_SOFTIRQ);
 		local_irq_enable();
+		preempt_check_resched_rt();
 	}
 
 	return NOTIFY_OK;
@@ -150,6 +152,7 @@ do_local:
 		goto do_local;
 
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 
 /**
diff --git a/kernel/msm-3.18/block/bounce.c b/kernel/msm-3.18/block/bounce.c
index ab21ba203..97b734aed 100644
--- a/kernel/msm-3.18/block/bounce.c
+++ b/kernel/msm-3.18/block/bounce.c
@@ -54,11 +54,11 @@ static void bounce_copy_vec(struct bio_vec *to, unsigned char *vfrom)
 	unsigned long flags;
 	unsigned char *vto;
 
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	vto = kmap_atomic(to->bv_page);
 	memcpy(vto + to->bv_offset, vfrom, to->bv_len);
 	kunmap_atomic(vto);
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 }
 
 #else /* CONFIG_HIGHMEM */
diff --git a/kernel/msm-3.18/crypto/algapi.c b/kernel/msm-3.18/crypto/algapi.c
index 314cc745f..0269c5ed4 100644
--- a/kernel/msm-3.18/crypto/algapi.c
+++ b/kernel/msm-3.18/crypto/algapi.c
@@ -698,13 +698,13 @@ EXPORT_SYMBOL_GPL(crypto_spawn_tfm2);
 
 int crypto_register_notifier(struct notifier_block *nb)
 {
-	return blocking_notifier_chain_register(&crypto_chain, nb);
+	return srcu_notifier_chain_register(&crypto_chain, nb);
 }
 EXPORT_SYMBOL_GPL(crypto_register_notifier);
 
 int crypto_unregister_notifier(struct notifier_block *nb)
 {
-	return blocking_notifier_chain_unregister(&crypto_chain, nb);
+	return srcu_notifier_chain_unregister(&crypto_chain, nb);
 }
 EXPORT_SYMBOL_GPL(crypto_unregister_notifier);
 
diff --git a/kernel/msm-3.18/crypto/api.c b/kernel/msm-3.18/crypto/api.c
index 7db2e89a3..25a77b2c2 100644
--- a/kernel/msm-3.18/crypto/api.c
+++ b/kernel/msm-3.18/crypto/api.c
@@ -31,7 +31,7 @@ EXPORT_SYMBOL_GPL(crypto_alg_list);
 DECLARE_RWSEM(crypto_alg_sem);
 EXPORT_SYMBOL_GPL(crypto_alg_sem);
 
-BLOCKING_NOTIFIER_HEAD(crypto_chain);
+SRCU_NOTIFIER_HEAD(crypto_chain);
 EXPORT_SYMBOL_GPL(crypto_chain);
 
 static struct crypto_alg *crypto_larval_wait(struct crypto_alg *alg);
@@ -236,10 +236,10 @@ int crypto_probing_notify(unsigned long val, void *v)
 {
 	int ok;
 
-	ok = blocking_notifier_call_chain(&crypto_chain, val, v);
+	ok = srcu_notifier_call_chain(&crypto_chain, val, v);
 	if (ok == NOTIFY_DONE) {
 		request_module("cryptomgr");
-		ok = blocking_notifier_call_chain(&crypto_chain, val, v);
+		ok = srcu_notifier_call_chain(&crypto_chain, val, v);
 	}
 
 	return ok;
diff --git a/kernel/msm-3.18/crypto/internal.h b/kernel/msm-3.18/crypto/internal.h
index bd39bfc92..a5db167cb 100644
--- a/kernel/msm-3.18/crypto/internal.h
+++ b/kernel/msm-3.18/crypto/internal.h
@@ -48,7 +48,7 @@ struct crypto_larval {
 
 extern struct list_head crypto_alg_list;
 extern struct rw_semaphore crypto_alg_sem;
-extern struct blocking_notifier_head crypto_chain;
+extern struct srcu_notifier_head crypto_chain;
 
 #ifdef CONFIG_PROC_FS
 void __init crypto_init_proc(void);
@@ -142,7 +142,7 @@ static inline int crypto_is_moribund(struct crypto_alg *alg)
 
 static inline void crypto_notify(unsigned long val, void *v)
 {
-	blocking_notifier_call_chain(&crypto_chain, val, v);
+	srcu_notifier_call_chain(&crypto_chain, val, v);
 }
 
 #endif	/* _CRYPTO_INTERNAL_H */
diff --git a/kernel/msm-3.18/drivers/acpi/acpica/acglobal.h b/kernel/msm-3.18/drivers/acpi/acpica/acglobal.h
index ebf02cc10..f15ef3dc4 100644
--- a/kernel/msm-3.18/drivers/acpi/acpica/acglobal.h
+++ b/kernel/msm-3.18/drivers/acpi/acpica/acglobal.h
@@ -112,7 +112,7 @@ ACPI_GLOBAL(u8, acpi_gbl_global_lock_pending);
  * interrupt level
  */
 ACPI_GLOBAL(acpi_spinlock, acpi_gbl_gpe_lock);	/* For GPE data structs and registers */
-ACPI_GLOBAL(acpi_spinlock, acpi_gbl_hardware_lock);	/* For ACPI H/W except GPE registers */
+ACPI_GLOBAL(acpi_raw_spinlock, acpi_gbl_hardware_lock);	/* For ACPI H/W except GPE registers */
 ACPI_GLOBAL(acpi_spinlock, acpi_gbl_reference_count_lock);
 
 /* Mutex for _OSI support */
diff --git a/kernel/msm-3.18/drivers/acpi/acpica/hwregs.c b/kernel/msm-3.18/drivers/acpi/acpica/hwregs.c
index a4c34d2c5..b40826b27 100644
--- a/kernel/msm-3.18/drivers/acpi/acpica/hwregs.c
+++ b/kernel/msm-3.18/drivers/acpi/acpica/hwregs.c
@@ -269,14 +269,14 @@ acpi_status acpi_hw_clear_acpi_status(void)
 			  ACPI_BITMASK_ALL_FIXED_STATUS,
 			  ACPI_FORMAT_UINT64(acpi_gbl_xpm1a_status.address)));
 
-	lock_flags = acpi_os_acquire_lock(acpi_gbl_hardware_lock);
+	raw_spin_lock_irqsave(acpi_gbl_hardware_lock, lock_flags);
 
 	/* Clear the fixed events in PM1 A/B */
 
 	status = acpi_hw_register_write(ACPI_REGISTER_PM1_STATUS,
 					ACPI_BITMASK_ALL_FIXED_STATUS);
 
-	acpi_os_release_lock(acpi_gbl_hardware_lock, lock_flags);
+	raw_spin_unlock_irqrestore(acpi_gbl_hardware_lock, lock_flags);
 
 	if (ACPI_FAILURE(status)) {
 		goto exit;
diff --git a/kernel/msm-3.18/drivers/acpi/acpica/hwxface.c b/kernel/msm-3.18/drivers/acpi/acpica/hwxface.c
index 96d007df6..e57a03ed3 100644
--- a/kernel/msm-3.18/drivers/acpi/acpica/hwxface.c
+++ b/kernel/msm-3.18/drivers/acpi/acpica/hwxface.c
@@ -374,7 +374,7 @@ acpi_status acpi_write_bit_register(u32 register_id, u32 value)
 		return_ACPI_STATUS(AE_BAD_PARAMETER);
 	}
 
-	lock_flags = acpi_os_acquire_lock(acpi_gbl_hardware_lock);
+	raw_spin_lock_irqsave(acpi_gbl_hardware_lock, lock_flags);
 
 	/*
 	 * At this point, we know that the parent register is one of the
@@ -435,7 +435,7 @@ acpi_status acpi_write_bit_register(u32 register_id, u32 value)
 
 unlock_and_exit:
 
-	acpi_os_release_lock(acpi_gbl_hardware_lock, lock_flags);
+	raw_spin_unlock_irqrestore(acpi_gbl_hardware_lock, lock_flags);
 	return_ACPI_STATUS(status);
 }
 
diff --git a/kernel/msm-3.18/drivers/acpi/acpica/utmutex.c b/kernel/msm-3.18/drivers/acpi/acpica/utmutex.c
index 82717fff9..508c5770e 100644
--- a/kernel/msm-3.18/drivers/acpi/acpica/utmutex.c
+++ b/kernel/msm-3.18/drivers/acpi/acpica/utmutex.c
@@ -88,7 +88,7 @@ acpi_status acpi_ut_mutex_initialize(void)
 		return_ACPI_STATUS (status);
 	}
 
-	status = acpi_os_create_lock (&acpi_gbl_hardware_lock);
+	status = acpi_os_create_raw_lock (&acpi_gbl_hardware_lock);
 	if (ACPI_FAILURE (status)) {
 		return_ACPI_STATUS (status);
 	}
@@ -141,7 +141,7 @@ void acpi_ut_mutex_terminate(void)
 	/* Delete the spinlocks */
 
 	acpi_os_delete_lock(acpi_gbl_gpe_lock);
-	acpi_os_delete_lock(acpi_gbl_hardware_lock);
+	acpi_os_delete_raw_lock(acpi_gbl_hardware_lock);
 	acpi_os_delete_lock(acpi_gbl_reference_count_lock);
 
 	/* Delete the reader/writer lock */
diff --git a/kernel/msm-3.18/drivers/ata/libata-sff.c b/kernel/msm-3.18/drivers/ata/libata-sff.c
index 12d337754..a6f7a089f 100644
--- a/kernel/msm-3.18/drivers/ata/libata-sff.c
+++ b/kernel/msm-3.18/drivers/ata/libata-sff.c
@@ -678,9 +678,9 @@ unsigned int ata_sff_data_xfer_noirq(struct ata_device *dev, unsigned char *buf,
 	unsigned long flags;
 	unsigned int consumed;
 
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	consumed = ata_sff_data_xfer32(dev, buf, buflen, rw);
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 
 	return consumed;
 }
@@ -719,7 +719,7 @@ static void ata_pio_sector(struct ata_queued_cmd *qc)
 		unsigned long flags;
 
 		/* FIXME: use a bounce buffer */
-		local_irq_save(flags);
+		local_irq_save_nort(flags);
 		buf = kmap_atomic(page);
 
 		/* do the actual data transfer */
@@ -727,7 +727,7 @@ static void ata_pio_sector(struct ata_queued_cmd *qc)
 				       do_write);
 
 		kunmap_atomic(buf);
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 	} else {
 		buf = page_address(page);
 		ap->ops->sff_data_xfer(qc->dev, buf + offset, qc->sect_size,
@@ -864,7 +864,7 @@ next_sg:
 		unsigned long flags;
 
 		/* FIXME: use bounce buffer */
-		local_irq_save(flags);
+		local_irq_save_nort(flags);
 		buf = kmap_atomic(page);
 
 		/* do the actual data transfer */
@@ -872,7 +872,7 @@ next_sg:
 								count, rw);
 
 		kunmap_atomic(buf);
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 	} else {
 		buf = page_address(page);
 		consumed = ap->ops->sff_data_xfer(dev,  buf + offset,
diff --git a/kernel/msm-3.18/drivers/block/zram/zram_drv.c b/kernel/msm-3.18/drivers/block/zram/zram_drv.c
index ae3908e3a..1656bb466 100644
--- a/kernel/msm-3.18/drivers/block/zram/zram_drv.c
+++ b/kernel/msm-3.18/drivers/block/zram/zram_drv.c
@@ -437,6 +437,8 @@ static struct zram_meta *zram_meta_alloc(int device_id, u64 disksize)
 		goto out_error;
 	}
 
+	zram_meta_init_table_locks(meta, disksize);
+
 	return meta;
 
 out_error:
@@ -535,12 +537,12 @@ static int zram_decompress_page(struct zram *zram, char *mem, u32 index)
 	unsigned long handle;
 	size_t size;
 
-	bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
+	zram_lock_table(&meta->table[index]);
 	handle = meta->table[index].handle;
 	size = zram_get_obj_size(meta, index);
 
 	if (!handle || zram_test_flag(meta, index, ZRAM_ZERO)) {
-		bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
+		zram_unlock_table(&meta->table[index]);
 		memset(mem, 0, PAGE_SIZE);
 		return 0;
 	}
@@ -551,7 +553,7 @@ static int zram_decompress_page(struct zram *zram, char *mem, u32 index)
 	else
 		ret = zcomp_decompress(zram->comp, cmem, size, mem);
 	zs_unmap_object(meta->mem_pool, handle);
-	bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
+	zram_unlock_table(&meta->table[index]);
 
 	/* Should NEVER happen. Return bio error if it does. */
 	if (unlikely(ret)) {
@@ -571,14 +573,14 @@ static int zram_bvec_read(struct zram *zram, struct bio_vec *bvec,
 	struct zram_meta *meta = zram->meta;
 	page = bvec->bv_page;
 
-	bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
+	zram_lock_table(&meta->table[index]);
 	if (unlikely(!meta->table[index].handle) ||
 			zram_test_flag(meta, index, ZRAM_ZERO)) {
-		bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
+		zram_unlock_table(&meta->table[index]);
 		handle_zero_page(bvec);
 		return 0;
 	}
-	bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
+	zram_unlock_table(&meta->table[index]);
 
 	if (is_partial_io(bvec))
 		/* Use  a temporary buffer to decompress the page */
@@ -674,10 +676,10 @@ static int zram_bvec_write(struct zram *zram, struct bio_vec *bvec, u32 index,
 		if (user_mem)
 			kunmap_atomic(user_mem);
 		/* Free memory associated with this sector now. */
-		bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
+		zram_lock_table(&meta->table[index]);
 		zram_free_page(zram, index);
 		zram_set_flag(meta, index, ZRAM_ZERO);
-		bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
+		zram_unlock_table(&meta->table[index]);
 
 		atomic64_inc(&zram->stats.zero_pages);
 		ret = 0;
@@ -740,12 +742,12 @@ static int zram_bvec_write(struct zram *zram, struct bio_vec *bvec, u32 index,
 	 * Free memory associated with this sector
 	 * before overwriting unused sectors.
 	 */
-	bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
+	zram_lock_table(&meta->table[index]);
 	zram_free_page(zram, index);
 
 	meta->table[index].handle = handle;
 	zram_set_obj_size(meta, index, clen);
-	bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
+	zram_unlock_table(&meta->table[index]);
 
 	/* Update stats */
 	atomic64_add(clen, &zram->stats.compr_data_size);
@@ -811,9 +813,9 @@ static void zram_bio_discard(struct zram *zram, u32 index,
 	}
 
 	while (n >= PAGE_SIZE) {
-		bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
+		zram_lock_table(&meta->table[index]);
 		zram_free_page(zram, index);
-		bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
+		zram_unlock_table(&meta->table[index]);
 		atomic64_inc(&zram->stats.notify_free);
 		index++;
 		n -= PAGE_SIZE;
@@ -1056,9 +1058,9 @@ static void zram_slot_free_notify(struct block_device *bdev,
 	zram = bdev->bd_disk->private_data;
 	meta = zram->meta;
 
-	bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
+	zram_lock_table(&meta->table[index]);
 	zram_free_page(zram, index);
-	bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
+	zram_unlock_table(&meta->table[index]);
 	atomic64_inc(&zram->stats.notify_free);
 }
 
diff --git a/kernel/msm-3.18/drivers/block/zram/zram_drv.h b/kernel/msm-3.18/drivers/block/zram/zram_drv.h
index 570c598f4..22c0173b0 100644
--- a/kernel/msm-3.18/drivers/block/zram/zram_drv.h
+++ b/kernel/msm-3.18/drivers/block/zram/zram_drv.h
@@ -78,6 +78,9 @@ enum zram_pageflags {
 struct zram_table_entry {
 	unsigned long handle;
 	unsigned long value;
+#ifdef CONFIG_PREEMPT_RT_BASE
+	spinlock_t lock;
+#endif
 };
 
 struct zram_stats {
@@ -122,4 +125,42 @@ struct zram {
 	u64 disksize;	/* bytes */
 	char compressor[10];
 };
+
+#ifndef CONFIG_PREEMPT_RT_BASE
+static inline void zram_lock_table(struct zram_table_entry *table)
+{
+	bit_spin_lock(ZRAM_ACCESS, &table->value);
+}
+
+static inline void zram_unlock_table(struct zram_table_entry *table)
+{
+	bit_spin_unlock(ZRAM_ACCESS, &table->value);
+}
+
+static inline void zram_meta_init_table_locks(struct zram_meta *meta, u64 disksize) { }
+#else /* CONFIG_PREEMPT_RT_BASE */
+static inline void zram_lock_table(struct zram_table_entry *table)
+{
+	spin_lock(&table->lock);
+	__set_bit(ZRAM_ACCESS, &table->value);
+}
+
+static inline void zram_unlock_table(struct zram_table_entry *table)
+{
+	__clear_bit(ZRAM_ACCESS, &table->value);
+	spin_unlock(&table->lock);
+}
+
+static inline void zram_meta_init_table_locks(struct zram_meta *meta, u64 disksize)
+{
+        size_t num_pages = disksize >> PAGE_SHIFT;
+        size_t index;
+
+        for (index = 0; index < num_pages; index++) {
+		spinlock_t *lock = &meta->table[index].lock;
+		spin_lock_init(lock);
+        }
+}
+#endif /* CONFIG_PREEMPT_RT_BASE */
+
 #endif
diff --git a/kernel/msm-3.18/drivers/char/random.c b/kernel/msm-3.18/drivers/char/random.c
index d55156fc0..acb8e7c21 100644
--- a/kernel/msm-3.18/drivers/char/random.c
+++ b/kernel/msm-3.18/drivers/char/random.c
@@ -776,8 +776,6 @@ static void add_timer_randomness(struct timer_rand_state *state, unsigned num)
 	} sample;
 	long delta, delta2, delta3;
 
-	preempt_disable();
-
 	sample.jiffies = jiffies;
 	sample.cycles = random_get_entropy();
 	sample.num = num;
@@ -818,7 +816,6 @@ static void add_timer_randomness(struct timer_rand_state *state, unsigned num)
 		 */
 		credit_entropy_bits(r, min_t(int, fls(delta>>1), 11));
 	}
-	preempt_enable();
 }
 
 void add_input_randomness(unsigned int type, unsigned int code,
@@ -871,28 +868,27 @@ static __u32 get_reg(struct fast_pool *f, struct pt_regs *regs)
 	return *(ptr + f->reg_idx++);
 }
 
-void add_interrupt_randomness(int irq, int irq_flags)
+void add_interrupt_randomness(int irq, int irq_flags, __u64 ip)
 {
 	struct entropy_store	*r;
 	struct fast_pool	*fast_pool = this_cpu_ptr(&irq_randomness);
-	struct pt_regs		*regs = get_irq_regs();
 	unsigned long		now = jiffies;
 	cycles_t		cycles = random_get_entropy();
 	__u32			c_high, j_high;
-	__u64			ip;
 	unsigned long		seed;
 	int			credit = 0;
 
 	if (cycles == 0)
-		cycles = get_reg(fast_pool, regs);
+		cycles = get_reg(fast_pool, NULL);
 	c_high = (sizeof(cycles) > 4) ? cycles >> 32 : 0;
 	j_high = (sizeof(now) > 4) ? now >> 32 : 0;
 	fast_pool->pool[0] ^= cycles ^ j_high ^ irq;
 	fast_pool->pool[1] ^= now ^ c_high;
-	ip = regs ? instruction_pointer(regs) : _RET_IP_;
+	if (!ip)
+		ip = _RET_IP_;
 	fast_pool->pool[2] ^= ip;
 	fast_pool->pool[3] ^= (sizeof(ip) > 4) ? ip >> 32 :
-		get_reg(fast_pool, regs);
+		get_reg(fast_pool, NULL);
 
 	fast_mix(fast_pool);
 	add_interrupt_bench(cycles);
diff --git a/kernel/msm-3.18/drivers/clocksource/tcb_clksrc.c b/kernel/msm-3.18/drivers/clocksource/tcb_clksrc.c
index 8bdbc45c6..43f1c6bc6 100644
--- a/kernel/msm-3.18/drivers/clocksource/tcb_clksrc.c
+++ b/kernel/msm-3.18/drivers/clocksource/tcb_clksrc.c
@@ -23,8 +23,7 @@
  *     this 32 bit free-running counter. the second channel is not used.
  *
  *   - The third channel may be used to provide a 16-bit clockevent
- *     source, used in either periodic or oneshot mode.  This runs
- *     at 32 KiHZ, and can handle delays of up to two seconds.
+ *     source, used in either periodic or oneshot mode.
  *
  * A boot clocksource and clockevent source are also currently needed,
  * unless the relevant platforms (ARM/AT91, AVR32/AT32) are changed so
@@ -74,6 +73,7 @@ static struct clocksource clksrc = {
 struct tc_clkevt_device {
 	struct clock_event_device	clkevt;
 	struct clk			*clk;
+	u32				freq;
 	void __iomem			*regs;
 };
 
@@ -82,13 +82,6 @@ static struct tc_clkevt_device *to_tc_clkevt(struct clock_event_device *clkevt)
 	return container_of(clkevt, struct tc_clkevt_device, clkevt);
 }
 
-/* For now, we always use the 32K clock ... this optimizes for NO_HZ,
- * because using one of the divided clocks would usually mean the
- * tick rate can never be less than several dozen Hz (vs 0.5 Hz).
- *
- * A divided clock could be good for high resolution timers, since
- * 30.5 usec resolution can seem "low".
- */
 static u32 timer_clock;
 
 static void tc_mode(enum clock_event_mode m, struct clock_event_device *d)
@@ -111,11 +104,12 @@ static void tc_mode(enum clock_event_mode m, struct clock_event_device *d)
 	case CLOCK_EVT_MODE_PERIODIC:
 		clk_enable(tcd->clk);
 
-		/* slow clock, count up to RC, then irq and restart */
+		/* count up to RC, then irq and restart */
 		__raw_writel(timer_clock
 				| ATMEL_TC_WAVE | ATMEL_TC_WAVESEL_UP_AUTO,
 				regs + ATMEL_TC_REG(2, CMR));
-		__raw_writel((32768 + HZ/2) / HZ, tcaddr + ATMEL_TC_REG(2, RC));
+		__raw_writel((tcd->freq + HZ / 2) / HZ,
+			     tcaddr + ATMEL_TC_REG(2, RC));
 
 		/* Enable clock and interrupts on RC compare */
 		__raw_writel(ATMEL_TC_CPCS, regs + ATMEL_TC_REG(2, IER));
@@ -128,7 +122,7 @@ static void tc_mode(enum clock_event_mode m, struct clock_event_device *d)
 	case CLOCK_EVT_MODE_ONESHOT:
 		clk_enable(tcd->clk);
 
-		/* slow clock, count up to RC, then irq and stop */
+		/* count up to RC, then irq and stop */
 		__raw_writel(timer_clock | ATMEL_TC_CPCSTOP
 				| ATMEL_TC_WAVE | ATMEL_TC_WAVESEL_UP_AUTO,
 				regs + ATMEL_TC_REG(2, CMR));
@@ -157,8 +151,12 @@ static struct tc_clkevt_device clkevt = {
 		.name		= "tc_clkevt",
 		.features	= CLOCK_EVT_FEAT_PERIODIC
 					| CLOCK_EVT_FEAT_ONESHOT,
+#ifdef CONFIG_ATMEL_TCB_CLKSRC_USE_SLOW_CLOCK
 		/* Should be lower than at91rm9200's system timer */
 		.rating		= 125,
+#else
+		.rating		= 200,
+#endif
 		.set_next_event	= tc_next_event,
 		.set_mode	= tc_mode,
 	},
@@ -178,8 +176,9 @@ static irqreturn_t ch2_irq(int irq, void *handle)
 	return IRQ_NONE;
 }
 
-static int __init setup_clkevents(struct atmel_tc *tc, int clk32k_divisor_idx)
+static int __init setup_clkevents(struct atmel_tc *tc, int divisor_idx)
 {
+	unsigned divisor = atmel_tc_divisors[divisor_idx];
 	int ret;
 	struct clk *t2_clk = tc->clk[2];
 	int irq = tc->irq[2];
@@ -193,7 +192,11 @@ static int __init setup_clkevents(struct atmel_tc *tc, int clk32k_divisor_idx)
 	clkevt.regs = tc->regs;
 	clkevt.clk = t2_clk;
 
-	timer_clock = clk32k_divisor_idx;
+	timer_clock = divisor_idx;
+	if (!divisor)
+		clkevt.freq = 32768;
+	else
+		clkevt.freq = clk_get_rate(t2_clk) / divisor;
 
 	clkevt.clkevt.cpumask = cpumask_of(0);
 
@@ -203,7 +206,7 @@ static int __init setup_clkevents(struct atmel_tc *tc, int clk32k_divisor_idx)
 		return ret;
 	}
 
-	clockevents_config_and_register(&clkevt.clkevt, 32768, 1, 0xffff);
+	clockevents_config_and_register(&clkevt.clkevt, clkevt.freq, 1, 0xffff);
 
 	return ret;
 }
@@ -340,7 +343,11 @@ static int __init tcb_clksrc_init(void)
 		goto err_disable_t1;
 
 	/* channel 2:  periodic and oneshot timer support */
+#ifdef CONFIG_ATMEL_TCB_CLKSRC_USE_SLOW_CLOCK
 	ret = setup_clkevents(tc, clk32k_divisor_idx);
+#else
+	ret = setup_clkevents(tc, best_divisor_idx);
+#endif
 	if (ret)
 		goto err_unregister_clksrc;
 
diff --git a/kernel/msm-3.18/drivers/clocksource/timer-atmel-pit.c b/kernel/msm-3.18/drivers/clocksource/timer-atmel-pit.c
index d5289098b..109ad4583 100644
--- a/kernel/msm-3.18/drivers/clocksource/timer-atmel-pit.c
+++ b/kernel/msm-3.18/drivers/clocksource/timer-atmel-pit.c
@@ -90,6 +90,7 @@ static cycle_t read_pit_clk(struct clocksource *cs)
 	return elapsed;
 }
 
+static struct irqaction at91sam926x_pit_irq;
 /*
  * Clockevent device:  interrupts every 1/HZ (== pit_cycles * MCK/16)
  */
@@ -100,6 +101,8 @@ pit_clkevt_mode(enum clock_event_mode mode, struct clock_event_device *dev)
 
 	switch (mode) {
 	case CLOCK_EVT_MODE_PERIODIC:
+		/* Set up irq handler */
+		setup_irq(at91sam926x_pit_irq.irq, &at91sam926x_pit_irq);
 		/* update clocksource counter */
 		data->cnt += data->cycle * PIT_PICNT(pit_read(data->base, AT91_PIT_PIVR));
 		pit_write(data->base, AT91_PIT_MR,
@@ -113,6 +116,7 @@ pit_clkevt_mode(enum clock_event_mode mode, struct clock_event_device *dev)
 		/* disable irq, leaving the clocksource active */
 		pit_write(data->base, AT91_PIT_MR,
 			  (data->cycle - 1) | AT91_PIT_PITEN);
+		remove_irq(at91sam926x_pit_irq.irq, &at91sam926x_pit_irq);
 		break;
 	case CLOCK_EVT_MODE_RESUME:
 		break;
diff --git a/kernel/msm-3.18/drivers/cpufreq/Kconfig.x86 b/kernel/msm-3.18/drivers/cpufreq/Kconfig.x86
index 89ae88f91..e3108a67e 100644
--- a/kernel/msm-3.18/drivers/cpufreq/Kconfig.x86
+++ b/kernel/msm-3.18/drivers/cpufreq/Kconfig.x86
@@ -113,7 +113,7 @@ config X86_POWERNOW_K7_ACPI
 
 config X86_POWERNOW_K8
 	tristate "AMD Opteron/Athlon64 PowerNow!"
-	depends on ACPI && ACPI_PROCESSOR && X86_ACPI_CPUFREQ
+	depends on ACPI && ACPI_PROCESSOR && X86_ACPI_CPUFREQ && !PREEMPT_RT_BASE
 	help
 	  This adds the CPUFreq driver for K8/early Opteron/Athlon64 processors.
 	  Support for K10 and newer processors is now in acpi-cpufreq.
diff --git a/kernel/msm-3.18/drivers/cpufreq/cpufreq.c b/kernel/msm-3.18/drivers/cpufreq/cpufreq.c
index 3c72aef8a..e94f3baf7 100644
--- a/kernel/msm-3.18/drivers/cpufreq/cpufreq.c
+++ b/kernel/msm-3.18/drivers/cpufreq/cpufreq.c
@@ -63,12 +63,6 @@ static inline bool has_target(void)
 	return cpufreq_driver->target_index || cpufreq_driver->target;
 }
 
-/*
- * rwsem to guarantee that cpufreq driver module doesn't unload during critical
- * sections
- */
-static DECLARE_RWSEM(cpufreq_rwsem);
-
 /* internal prototypes */
 static int __cpufreq_governor(struct cpufreq_policy *policy,
 		unsigned int event);
@@ -231,9 +225,6 @@ struct cpufreq_policy *cpufreq_cpu_get(unsigned int cpu)
 	if (cpufreq_disabled() || (cpu >= nr_cpu_ids))
 		return NULL;
 
-	if (!down_read_trylock(&cpufreq_rwsem))
-		return NULL;
-
 	/* get the cpufreq driver */
 	read_lock_irqsave(&cpufreq_driver_lock, flags);
 
@@ -246,9 +237,6 @@ struct cpufreq_policy *cpufreq_cpu_get(unsigned int cpu)
 
 	read_unlock_irqrestore(&cpufreq_driver_lock, flags);
 
-	if (!policy)
-		up_read(&cpufreq_rwsem);
-
 	return policy;
 }
 EXPORT_SYMBOL_GPL(cpufreq_cpu_get);
@@ -259,7 +247,6 @@ void cpufreq_cpu_put(struct cpufreq_policy *policy)
 		return;
 
 	kobject_put(&policy->kobj);
-	up_read(&cpufreq_rwsem);
 }
 EXPORT_SYMBOL_GPL(cpufreq_cpu_put);
 
@@ -849,9 +836,6 @@ static ssize_t show(struct kobject *kobj, struct attribute *attr, char *buf)
 	if (!cpu_online(policy->cpu))
 		goto unlock;
 
-	if (!down_read_trylock(&cpufreq_rwsem))
-		goto unlock;
-
 	down_read(&policy->rwsem);
 
 	if (fattr->show)
@@ -860,9 +844,9 @@ static ssize_t show(struct kobject *kobj, struct attribute *attr, char *buf)
 		ret = -EIO;
 
 	up_read(&policy->rwsem);
-	up_read(&cpufreq_rwsem);
 unlock:
 	put_online_cpus();
+
 	return ret;
 }
 
@@ -878,9 +862,6 @@ static ssize_t store(struct kobject *kobj, struct attribute *attr,
 	if (!cpu_online(policy->cpu))
 		goto unlock;
 
-	if (!down_read_trylock(&cpufreq_rwsem))
-		goto unlock;
-
 	down_write(&policy->rwsem);
 
 	if (fattr->store)
@@ -889,8 +870,6 @@ static ssize_t store(struct kobject *kobj, struct attribute *attr,
 		ret = -EIO;
 
 	up_write(&policy->rwsem);
-
-	up_read(&cpufreq_rwsem);
 unlock:
 	put_online_cpus();
 
@@ -1238,9 +1217,6 @@ static int __cpufreq_add_dev(struct device *dev, struct subsys_interface *sif)
 	}
 #endif
 
-	if (!down_read_trylock(&cpufreq_rwsem))
-		return 0;
-
 #ifdef CONFIG_HOTPLUG_CPU
 	/* Check if this cpu was hot-unplugged earlier and has siblings */
 	read_lock_irqsave(&cpufreq_driver_lock, flags);
@@ -1248,7 +1224,6 @@ static int __cpufreq_add_dev(struct device *dev, struct subsys_interface *sif)
 		if (cpumask_test_cpu(cpu, tpolicy->related_cpus)) {
 			read_unlock_irqrestore(&cpufreq_driver_lock, flags);
 			ret = cpufreq_add_policy_cpu(tpolicy, cpu, dev);
-			up_read(&cpufreq_rwsem);
 			return ret;
 		}
 	}
@@ -1385,8 +1360,6 @@ static int __cpufreq_add_dev(struct device *dev, struct subsys_interface *sif)
 
 	kobject_uevent(&policy->kobj, KOBJ_ADD);
 
-	up_read(&cpufreq_rwsem);
-
 	/* Callback for handling stuff after policy is ready */
 	if (cpufreq_driver->ready)
 		cpufreq_driver->ready(policy);
@@ -1413,8 +1386,6 @@ err_set_policy_cpu:
 	cpufreq_policy_free(policy);
 
 nomem_out:
-	up_read(&cpufreq_rwsem);
-
 	return ret;
 }
 
@@ -2685,17 +2656,18 @@ int cpufreq_unregister_driver(struct cpufreq_driver *driver)
 
 	pr_info("unregistering driver %s\n", driver->name);
 
+	/* Protect against concurrent cpu hotplug */
+	get_online_cpus();
 	subsys_interface_unregister(&cpufreq_interface);
 	remove_boost_sysfs_file();
 	unregister_hotcpu_notifier(&cpufreq_cpu_notifier);
 
-	down_write(&cpufreq_rwsem);
 	write_lock_irqsave(&cpufreq_driver_lock, flags);
 
 	cpufreq_driver = NULL;
 
 	write_unlock_irqrestore(&cpufreq_driver_lock, flags);
-	up_write(&cpufreq_rwsem);
+	put_online_cpus();
 
 	return 0;
 }
diff --git a/kernel/msm-3.18/drivers/gpio/gpio-omap.c b/kernel/msm-3.18/drivers/gpio/gpio-omap.c
index 415682f69..da96b23ae 100644
--- a/kernel/msm-3.18/drivers/gpio/gpio-omap.c
+++ b/kernel/msm-3.18/drivers/gpio/gpio-omap.c
@@ -57,7 +57,7 @@ struct gpio_bank {
 	u32 saved_datain;
 	u32 level_mask;
 	u32 toggle_mask;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct gpio_chip chip;
 	struct clk *dbck;
 	u32 mod_usage;
@@ -503,19 +503,19 @@ static int omap_gpio_irq_type(struct irq_data *d, unsigned type)
 		(type & (IRQ_TYPE_LEVEL_LOW|IRQ_TYPE_LEVEL_HIGH)))
 		return -EINVAL;
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	offset = GPIO_INDEX(bank, gpio);
 	retval = omap_set_gpio_triggering(bank, offset, type);
 	if (!LINE_USED(bank->mod_usage, offset)) {
 		omap_enable_gpio_module(bank, offset);
 		omap_set_gpio_direction(bank, offset, 1);
 	} else if (!omap_gpio_is_input(bank, BIT(offset))) {
-		spin_unlock_irqrestore(&bank->lock, flags);
+		raw_spin_unlock_irqrestore(&bank->lock, flags);
 		return -EINVAL;
 	}
 
 	bank->irq_usage |= BIT(GPIO_INDEX(bank, gpio));
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 
 	if (type & (IRQ_TYPE_LEVEL_LOW | IRQ_TYPE_LEVEL_HIGH))
 		__irq_set_handler_locked(d->irq, handle_level_irq);
@@ -633,14 +633,14 @@ static int omap_set_gpio_wakeup(struct gpio_bank *bank, int gpio, int enable)
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	if (enable)
 		bank->context.wake_en |= gpio_bit;
 	else
 		bank->context.wake_en &= ~gpio_bit;
 
 	writel_relaxed(bank->context.wake_en, bank->base + bank->regs->wkup_en);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 
 	return 0;
 }
@@ -675,7 +675,7 @@ static int omap_gpio_request(struct gpio_chip *chip, unsigned offset)
 	if (!BANK_USED(bank))
 		pm_runtime_get_sync(bank->dev);
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	/* Set trigger to none. You need to enable the desired trigger with
 	 * request_irq() or set_irq_type(). Only do this if the IRQ line has
 	 * not already been requested.
@@ -685,7 +685,7 @@ static int omap_gpio_request(struct gpio_chip *chip, unsigned offset)
 		omap_enable_gpio_module(bank, offset);
 	}
 	bank->mod_usage |= BIT(offset);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 
 	return 0;
 }
@@ -695,11 +695,11 @@ static void omap_gpio_free(struct gpio_chip *chip, unsigned offset)
 	struct gpio_bank *bank = container_of(chip, struct gpio_bank, chip);
 	unsigned long flags;
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	bank->mod_usage &= ~(BIT(offset));
 	omap_disable_gpio_module(bank, offset);
 	omap_reset_gpio(bank, bank->chip.base + offset);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 
 	/*
 	 * If this is the last gpio to be freed in the bank,
@@ -799,12 +799,12 @@ static void omap_gpio_irq_shutdown(struct irq_data *d)
 	unsigned long flags;
 	unsigned offset = GPIO_INDEX(bank, gpio);
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	gpio_unlock_as_irq(&bank->chip, offset);
 	bank->irq_usage &= ~(BIT(offset));
 	omap_disable_gpio_module(bank, offset);
 	omap_reset_gpio(bank, gpio);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 
 	/*
 	 * If this is the last IRQ to be freed in the bank,
@@ -828,10 +828,10 @@ static void omap_gpio_mask_irq(struct irq_data *d)
 	unsigned int gpio = omap_irq_to_gpio(bank, d->hwirq);
 	unsigned long flags;
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	omap_set_gpio_irqenable(bank, gpio, 0);
 	omap_set_gpio_triggering(bank, GPIO_INDEX(bank, gpio), IRQ_TYPE_NONE);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 }
 
 static void omap_gpio_unmask_irq(struct irq_data *d)
@@ -842,7 +842,7 @@ static void omap_gpio_unmask_irq(struct irq_data *d)
 	u32 trigger = irqd_get_trigger_type(d);
 	unsigned long flags;
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	if (trigger)
 		omap_set_gpio_triggering(bank, GPIO_INDEX(bank, gpio), trigger);
 
@@ -854,7 +854,7 @@ static void omap_gpio_unmask_irq(struct irq_data *d)
 	}
 
 	omap_set_gpio_irqenable(bank, gpio, 1);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 }
 
 /*---------------------------------------------------------------------*/
@@ -867,9 +867,9 @@ static int omap_mpuio_suspend_noirq(struct device *dev)
 					OMAP_MPUIO_GPIO_MASKIT / bank->stride;
 	unsigned long		flags;
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	writel_relaxed(0xffff & ~bank->context.wake_en, mask_reg);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 
 	return 0;
 }
@@ -882,9 +882,9 @@ static int omap_mpuio_resume_noirq(struct device *dev)
 					OMAP_MPUIO_GPIO_MASKIT / bank->stride;
 	unsigned long		flags;
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	writel_relaxed(bank->context.wake_en, mask_reg);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 
 	return 0;
 }
@@ -930,9 +930,9 @@ static int omap_gpio_get_direction(struct gpio_chip *chip, unsigned offset)
 
 	bank = container_of(chip, struct gpio_bank, chip);
 	reg = bank->base + bank->regs->direction;
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	dir = !!(readl_relaxed(reg) & BIT(offset));
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 	return dir;
 }
 
@@ -942,9 +942,9 @@ static int omap_gpio_input(struct gpio_chip *chip, unsigned offset)
 	unsigned long flags;
 
 	bank = container_of(chip, struct gpio_bank, chip);
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	omap_set_gpio_direction(bank, offset, 1);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 	return 0;
 }
 
@@ -968,10 +968,10 @@ static int omap_gpio_output(struct gpio_chip *chip, unsigned offset, int value)
 	unsigned long flags;
 
 	bank = container_of(chip, struct gpio_bank, chip);
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	bank->set_dataout(bank, offset, value);
 	omap_set_gpio_direction(bank, offset, 0);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 	return 0;
 }
 
@@ -983,9 +983,9 @@ static int omap_gpio_debounce(struct gpio_chip *chip, unsigned offset,
 
 	bank = container_of(chip, struct gpio_bank, chip);
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	omap2_set_gpio_debounce(bank, offset, debounce);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 
 	return 0;
 }
@@ -996,9 +996,9 @@ static void omap_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
 	unsigned long flags;
 
 	bank = container_of(chip, struct gpio_bank, chip);
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 	bank->set_dataout(bank, offset, value);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 }
 
 /*---------------------------------------------------------------------*/
@@ -1223,7 +1223,7 @@ static int omap_gpio_probe(struct platform_device *pdev)
 	else
 		bank->set_dataout = omap_set_gpio_dataout_mask;
 
-	spin_lock_init(&bank->lock);
+	raw_spin_lock_init(&bank->lock);
 
 	/* Static mapping, never released */
 	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
@@ -1270,7 +1270,7 @@ static int omap_gpio_runtime_suspend(struct device *dev)
 	unsigned long flags;
 	u32 wake_low, wake_hi;
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 
 	/*
 	 * Only edges can generate a wakeup event to the PRCM.
@@ -1323,7 +1323,7 @@ update_gpio_context_count:
 				bank->get_context_loss_count(bank->dev);
 
 	omap_gpio_dbck_disable(bank);
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 
 	return 0;
 }
@@ -1338,7 +1338,7 @@ static int omap_gpio_runtime_resume(struct device *dev)
 	unsigned long flags;
 	int c;
 
-	spin_lock_irqsave(&bank->lock, flags);
+	raw_spin_lock_irqsave(&bank->lock, flags);
 
 	/*
 	 * On the first resume during the probe, the context has not
@@ -1374,14 +1374,14 @@ static int omap_gpio_runtime_resume(struct device *dev)
 			if (c != bank->context_loss_count) {
 				omap_gpio_restore_context(bank);
 			} else {
-				spin_unlock_irqrestore(&bank->lock, flags);
+				raw_spin_unlock_irqrestore(&bank->lock, flags);
 				return 0;
 			}
 		}
 	}
 
 	if (!bank->workaround_enabled) {
-		spin_unlock_irqrestore(&bank->lock, flags);
+		raw_spin_unlock_irqrestore(&bank->lock, flags);
 		return 0;
 	}
 
@@ -1436,7 +1436,7 @@ static int omap_gpio_runtime_resume(struct device *dev)
 	}
 
 	bank->workaround_enabled = false;
-	spin_unlock_irqrestore(&bank->lock, flags);
+	raw_spin_unlock_irqrestore(&bank->lock, flags);
 
 	return 0;
 }
diff --git a/kernel/msm-3.18/drivers/gpu/drm/i915/i915_gem.c b/kernel/msm-3.18/drivers/gpu/drm/i915/i915_gem.c
index f56af0aaa..68560152b 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/i915/i915_gem.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/i915/i915_gem.c
@@ -1,5 +1,5 @@
 /*
- * Copyright ¬© 2008-2015 Intel Corporation
+ * Copyright ¬© 2008 Intel Corporation
  *
  * Permission is hereby granted, free of charge, to any person obtaining a
  * copy of this software and associated documentation files (the "Software"),
@@ -29,23 +29,38 @@
 #include <drm/drm_vma_manager.h>
 #include <drm/i915_drm.h>
 #include "i915_drv.h"
-#include "i915_vgpu.h"
 #include "i915_trace.h"
 #include "intel_drv.h"
+#include <linux/oom.h>
 #include <linux/shmem_fs.h>
 #include <linux/slab.h>
 #include <linux/swap.h>
 #include <linux/pci.h>
 #include <linux/dma-buf.h>
 
-#define RQ_BUG_ON(expr)
-
 static void i915_gem_object_flush_gtt_write_domain(struct drm_i915_gem_object *obj);
-static void i915_gem_object_flush_cpu_write_domain(struct drm_i915_gem_object *obj);
-static void
-i915_gem_object_retire__write(struct drm_i915_gem_object *obj);
+static void i915_gem_object_flush_cpu_write_domain(struct drm_i915_gem_object *obj,
+						   bool force);
+static __must_check int
+i915_gem_object_wait_rendering(struct drm_i915_gem_object *obj,
+			       bool readonly);
 static void
-i915_gem_object_retire__read(struct drm_i915_gem_object *obj, int ring);
+i915_gem_object_retire(struct drm_i915_gem_object *obj);
+
+static void i915_gem_write_fence(struct drm_device *dev, int reg,
+				 struct drm_i915_gem_object *obj);
+static void i915_gem_object_update_fence(struct drm_i915_gem_object *obj,
+					 struct drm_i915_fence_reg *fence,
+					 bool enable);
+
+static unsigned long i915_gem_shrinker_count(struct shrinker *shrinker,
+					     struct shrink_control *sc);
+static unsigned long i915_gem_shrinker_scan(struct shrinker *shrinker,
+					    struct shrink_control *sc);
+static int i915_gem_shrinker_oom(struct notifier_block *nb,
+				 unsigned long event,
+				 void *ptr);
+static unsigned long i915_gem_shrink_all(struct drm_i915_private *dev_priv);
 
 static bool cpu_cache_is_coherent(struct drm_device *dev,
 				  enum i915_cache_level level)
@@ -61,6 +76,18 @@ static bool cpu_write_needs_clflush(struct drm_i915_gem_object *obj)
 	return obj->pin_display;
 }
 
+static inline void i915_gem_object_fence_lost(struct drm_i915_gem_object *obj)
+{
+	if (obj->tiling_mode)
+		i915_gem_release_mmap(obj);
+
+	/* As we do not have an associated fence register, we will force
+	 * a tiling change if we ever need to acquire one.
+	 */
+	obj->fence_dirty = false;
+	obj->fence_reg = I915_FENCE_REG_NONE;
+}
+
 /* some bookkeeping */
 static void i915_gem_info_add_obj(struct drm_i915_private *dev_priv,
 				  size_t size)
@@ -126,24 +153,53 @@ int i915_mutex_lock_interruptible(struct drm_device *dev)
 	return 0;
 }
 
+static inline bool
+i915_gem_object_is_inactive(struct drm_i915_gem_object *obj)
+{
+	return i915_gem_obj_bound_any(obj) && !obj->active;
+}
+
+int
+i915_gem_init_ioctl(struct drm_device *dev, void *data,
+		    struct drm_file *file)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_init *args = data;
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		return -ENODEV;
+
+	if (args->gtt_start >= args->gtt_end ||
+	    (args->gtt_end | args->gtt_start) & (PAGE_SIZE - 1))
+		return -EINVAL;
+
+	/* GEM with user mode setting was never supported on ilk and later. */
+	if (INTEL_INFO(dev)->gen >= 5)
+		return -ENODEV;
+
+	mutex_lock(&dev->struct_mutex);
+	i915_gem_setup_global_gtt(dev, args->gtt_start, args->gtt_end,
+				  args->gtt_end);
+	dev_priv->gtt.mappable_end = args->gtt_end;
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+}
+
 int
 i915_gem_get_aperture_ioctl(struct drm_device *dev, void *data,
 			    struct drm_file *file)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_get_aperture *args = data;
-	struct i915_gtt *ggtt = &dev_priv->gtt;
-	struct i915_vma *vma;
+	struct drm_i915_gem_object *obj;
 	size_t pinned;
 
 	pinned = 0;
 	mutex_lock(&dev->struct_mutex);
-	list_for_each_entry(vma, &ggtt->base.active_list, mm_list)
-		if (vma->pin_count)
-			pinned += vma->node.size;
-	list_for_each_entry(vma, &ggtt->base.inactive_list, mm_list)
-		if (vma->pin_count)
-			pinned += vma->node.size;
+	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list)
+		if (i915_gem_obj_is_pinned(obj))
+			pinned += i915_gem_obj_ggtt_size(obj);
 	mutex_unlock(&dev->struct_mutex);
 
 	args->aper_size = dev_priv->gtt.base.total;
@@ -152,134 +208,40 @@ i915_gem_get_aperture_ioctl(struct drm_device *dev, void *data,
 	return 0;
 }
 
-static int
-i915_gem_object_get_pages_phys(struct drm_i915_gem_object *obj)
-{
-	struct address_space *mapping = file_inode(obj->base.filp)->i_mapping;
-	char *vaddr = obj->phys_handle->vaddr;
-	struct sg_table *st;
-	struct scatterlist *sg;
-	int i;
-
-	if (WARN_ON(i915_gem_object_needs_bit17_swizzle(obj)))
-		return -EINVAL;
-
-	for (i = 0; i < obj->base.size / PAGE_SIZE; i++) {
-		struct page *page;
-		char *src;
-
-		page = shmem_read_mapping_page(mapping, i);
-		if (IS_ERR(page))
-			return PTR_ERR(page);
-
-		src = kmap_atomic(page);
-		memcpy(vaddr, src, PAGE_SIZE);
-		drm_clflush_virt_range(vaddr, PAGE_SIZE);
-		kunmap_atomic(src);
-
-		page_cache_release(page);
-		vaddr += PAGE_SIZE;
-	}
-
-	i915_gem_chipset_flush(obj->base.dev);
-
-	st = kmalloc(sizeof(*st), GFP_KERNEL);
-	if (st == NULL)
-		return -ENOMEM;
-
-	if (sg_alloc_table(st, 1, GFP_KERNEL)) {
-		kfree(st);
-		return -ENOMEM;
-	}
-
-	sg = st->sgl;
-	sg->offset = 0;
-	sg->length = obj->base.size;
-
-	sg_dma_address(sg) = obj->phys_handle->busaddr;
-	sg_dma_len(sg) = obj->base.size;
-
-	obj->pages = st;
-	return 0;
-}
-
-static void
-i915_gem_object_put_pages_phys(struct drm_i915_gem_object *obj)
+static void i915_gem_object_detach_phys(struct drm_i915_gem_object *obj)
 {
-	int ret;
-
-	BUG_ON(obj->madv == __I915_MADV_PURGED);
-
-	ret = i915_gem_object_set_to_cpu_domain(obj, true);
-	if (ret) {
-		/* In the event of a disaster, abandon all caches and
-		 * hope for the best.
-		 */
-		WARN_ON(ret != -EIO);
-		obj->base.read_domains = obj->base.write_domain = I915_GEM_DOMAIN_CPU;
-	}
+	drm_dma_handle_t *phys = obj->phys_handle;
 
-	if (obj->madv == I915_MADV_DONTNEED)
-		obj->dirty = 0;
+	if (!phys)
+		return;
 
-	if (obj->dirty) {
+	if (obj->madv == I915_MADV_WILLNEED) {
 		struct address_space *mapping = file_inode(obj->base.filp)->i_mapping;
-		char *vaddr = obj->phys_handle->vaddr;
+		char *vaddr = phys->vaddr;
 		int i;
 
 		for (i = 0; i < obj->base.size / PAGE_SIZE; i++) {
-			struct page *page;
-			char *dst;
-
-			page = shmem_read_mapping_page(mapping, i);
-			if (IS_ERR(page))
-				continue;
-
-			dst = kmap_atomic(page);
-			drm_clflush_virt_range(vaddr, PAGE_SIZE);
-			memcpy(dst, vaddr, PAGE_SIZE);
-			kunmap_atomic(dst);
-
-			set_page_dirty(page);
-			if (obj->madv == I915_MADV_WILLNEED)
+			struct page *page = shmem_read_mapping_page(mapping, i);
+			if (!IS_ERR(page)) {
+				char *dst = kmap_atomic(page);
+				memcpy(dst, vaddr, PAGE_SIZE);
+				drm_clflush_virt_range(dst, PAGE_SIZE);
+				kunmap_atomic(dst);
+
+				set_page_dirty(page);
 				mark_page_accessed(page);
-			page_cache_release(page);
+				page_cache_release(page);
+			}
 			vaddr += PAGE_SIZE;
 		}
-		obj->dirty = 0;
+		i915_gem_chipset_flush(obj->base.dev);
 	}
 
-	sg_free_table(obj->pages);
-	kfree(obj->pages);
-}
-
-static void
-i915_gem_object_release_phys(struct drm_i915_gem_object *obj)
-{
-	drm_pci_free(obj->base.dev, obj->phys_handle);
-}
-
-static const struct drm_i915_gem_object_ops i915_gem_phys_ops = {
-	.get_pages = i915_gem_object_get_pages_phys,
-	.put_pages = i915_gem_object_put_pages_phys,
-	.release = i915_gem_object_release_phys,
-};
-
-static int
-drop_pages(struct drm_i915_gem_object *obj)
-{
-	struct i915_vma *vma, *next;
-	int ret;
-
-	drm_gem_object_reference(&obj->base);
-	list_for_each_entry_safe(vma, next, &obj->vma_list, vma_link)
-		if (i915_vma_unbind(vma))
-			break;
-
-	ret = i915_gem_object_put_pages(obj);
-	drm_gem_object_unreference(&obj->base);
-
-	return ret;
+#ifdef CONFIG_X86
+	set_memory_wb((unsigned long)phys->vaddr, phys->size / PAGE_SIZE);
+#endif
+	drm_pci_free(obj->base.dev, phys);
+	obj->phys_handle = NULL;
 }
 
 int
@@ -287,7 +249,9 @@ i915_gem_object_attach_phys(struct drm_i915_gem_object *obj,
 			    int align)
 {
 	drm_dma_handle_t *phys;
-	int ret;
+	struct address_space *mapping;
+	char *vaddr;
+	int i;
 
 	if (obj->phys_handle) {
 		if ((unsigned long)obj->phys_handle->vaddr & (align -1))
@@ -302,19 +266,41 @@ i915_gem_object_attach_phys(struct drm_i915_gem_object *obj,
 	if (obj->base.filp == NULL)
 		return -EINVAL;
 
-	ret = drop_pages(obj);
-	if (ret)
-		return ret;
-
 	/* create a new object */
 	phys = drm_pci_alloc(obj->base.dev, obj->base.size, align);
 	if (!phys)
 		return -ENOMEM;
 
-	obj->phys_handle = phys;
-	obj->ops = &i915_gem_phys_ops;
+	vaddr = phys->vaddr;
+#ifdef CONFIG_X86
+	set_memory_wc((unsigned long)vaddr, phys->size / PAGE_SIZE);
+#endif
+	mapping = file_inode(obj->base.filp)->i_mapping;
+	for (i = 0; i < obj->base.size / PAGE_SIZE; i++) {
+		struct page *page;
+		char *src;
+
+		page = shmem_read_mapping_page(mapping, i);
+		if (IS_ERR(page)) {
+#ifdef CONFIG_X86
+			set_memory_wb((unsigned long)phys->vaddr, phys->size / PAGE_SIZE);
+#endif
+			drm_pci_free(obj->base.dev, phys);
+			return PTR_ERR(page);
+		}
+
+		src = kmap_atomic(page);
+		memcpy(vaddr, src, PAGE_SIZE);
+		kunmap_atomic(src);
 
-	return i915_gem_object_get_pages(obj);
+		mark_page_accessed(page);
+		page_cache_release(page);
+
+		vaddr += PAGE_SIZE;
+	}
+
+	obj->phys_handle = phys;
+	return 0;
 }
 
 static int
@@ -325,16 +311,7 @@ i915_gem_phys_pwrite(struct drm_i915_gem_object *obj,
 	struct drm_device *dev = obj->base.dev;
 	void *vaddr = obj->phys_handle->vaddr + args->offset;
 	char __user *user_data = to_user_ptr(args->data_ptr);
-	int ret = 0;
 
-	/* We manually control the domain here and pretend that it
-	 * remains coherent i.e. in the GTT domain, like shmem_pwrite.
-	 */
-	ret = i915_gem_object_wait_rendering(obj, false);
-	if (ret)
-		return ret;
-
-	intel_fb_obj_invalidate(obj, ORIGIN_CPU);
 	if (__copy_from_user_inatomic_nocache(vaddr, user_data, args->size)) {
 		unsigned long unwritten;
 
@@ -345,30 +322,24 @@ i915_gem_phys_pwrite(struct drm_i915_gem_object *obj,
 		mutex_unlock(&dev->struct_mutex);
 		unwritten = copy_from_user(vaddr, user_data, args->size);
 		mutex_lock(&dev->struct_mutex);
-		if (unwritten) {
-			ret = -EFAULT;
-			goto out;
-		}
+		if (unwritten)
+			return -EFAULT;
 	}
 
-	drm_clflush_virt_range(vaddr, args->size);
 	i915_gem_chipset_flush(dev);
-
-out:
-	intel_fb_obj_flush(obj, false, ORIGIN_CPU);
-	return ret;
+	return 0;
 }
 
 void *i915_gem_object_alloc(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	return kmem_cache_zalloc(dev_priv->objects, GFP_KERNEL);
+	return kmem_cache_zalloc(dev_priv->slab, GFP_KERNEL);
 }
 
 void i915_gem_object_free(struct drm_i915_gem_object *obj)
 {
 	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
-	kmem_cache_free(dev_priv->objects, obj);
+	kmem_cache_free(dev_priv->slab, obj);
 }
 
 static int
@@ -502,6 +473,8 @@ int i915_gem_obj_prepare_shmem_read(struct drm_i915_gem_object *obj,
 		ret = i915_gem_object_wait_rendering(obj, true);
 		if (ret)
 			return ret;
+
+		i915_gem_object_retire(obj);
 	}
 
 	ret = i915_gem_object_get_pages(obj);
@@ -788,8 +761,6 @@ i915_gem_gtt_pwrite_fast(struct drm_device *dev,
 
 	offset = i915_gem_obj_ggtt_offset(obj) + args->offset;
 
-	intel_fb_obj_invalidate(obj, ORIGIN_GTT);
-
 	while (remain > 0) {
 		/* Operation in this page
 		 *
@@ -810,7 +781,7 @@ i915_gem_gtt_pwrite_fast(struct drm_device *dev,
 		if (fast_user_write(dev_priv->gtt.mappable, page_base,
 				    page_offset, user_data, page_length)) {
 			ret = -EFAULT;
-			goto out_flush;
+			goto out_unpin;
 		}
 
 		remain -= page_length;
@@ -818,8 +789,6 @@ i915_gem_gtt_pwrite_fast(struct drm_device *dev,
 		offset += page_length;
 	}
 
-out_flush:
-	intel_fb_obj_flush(obj, false, ORIGIN_GTT);
 out_unpin:
 	i915_gem_object_ggtt_unpin(obj);
 out:
@@ -921,6 +890,8 @@ i915_gem_shmem_pwrite(struct drm_device *dev,
 		ret = i915_gem_object_wait_rendering(obj, false);
 		if (ret)
 			return ret;
+
+		i915_gem_object_retire(obj);
 	}
 	/* Same trick applies to invalidate partially written cachelines read
 	 * before writing. */
@@ -932,8 +903,6 @@ i915_gem_shmem_pwrite(struct drm_device *dev,
 	if (ret)
 		return ret;
 
-	intel_fb_obj_invalidate(obj, ORIGIN_CPU);
-
 	i915_gem_object_pin_pages(obj);
 
 	offset = args->offset;
@@ -1005,16 +974,13 @@ out:
 		if (!needs_clflush_after &&
 		    obj->base.write_domain != I915_GEM_DOMAIN_CPU) {
 			if (i915_gem_clflush_object(obj, obj->pin_display))
-				needs_clflush_after = true;
+				i915_gem_chipset_flush(dev);
 		}
 	}
 
 	if (needs_clflush_after)
 		i915_gem_chipset_flush(dev);
-	else
-		obj->cache_dirty = true;
 
-	intel_fb_obj_flush(obj, false, ORIGIN_CPU);
 	return ret;
 }
 
@@ -1027,7 +993,6 @@ int
 i915_gem_pwrite_ioctl(struct drm_device *dev, void *data,
 		      struct drm_file *file)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_pwrite *args = data;
 	struct drm_i915_gem_object *obj;
 	int ret;
@@ -1047,11 +1012,9 @@ i915_gem_pwrite_ioctl(struct drm_device *dev, void *data,
 			return -EFAULT;
 	}
 
-	intel_runtime_pm_get(dev_priv);
-
 	ret = i915_mutex_lock_interruptible(dev);
 	if (ret)
-		goto put_rpm;
+		return ret;
 
 	obj = to_intel_bo(drm_gem_object_lookup(dev, file, args->handle));
 	if (&obj->base == NULL) {
@@ -1083,6 +1046,11 @@ i915_gem_pwrite_ioctl(struct drm_device *dev, void *data,
 	 * pread/pwrite currently are reading and writing from the CPU
 	 * perspective, requiring manual detiling by the client.
 	 */
+	if (obj->phys_handle) {
+		ret = i915_gem_phys_pwrite(obj, args, file);
+		goto out;
+	}
+
 	if (obj->tiling_mode == I915_TILING_NONE &&
 	    obj->base.write_domain != I915_GEM_DOMAIN_CPU &&
 	    cpu_write_needs_clflush(obj)) {
@@ -1092,20 +1060,13 @@ i915_gem_pwrite_ioctl(struct drm_device *dev, void *data,
 		 * textures). Fallback to the shmem path in that case. */
 	}
 
-	if (ret == -EFAULT || ret == -ENOSPC) {
-		if (obj->phys_handle)
-			ret = i915_gem_phys_pwrite(obj, args, file);
-		else
-			ret = i915_gem_shmem_pwrite(dev, obj, args, file);
-	}
+	if (ret == -EFAULT || ret == -ENOSPC)
+		ret = i915_gem_shmem_pwrite(dev, obj, args, file);
 
 out:
 	drm_gem_object_unreference(&obj->base);
 unlock:
 	mutex_unlock(&dev->struct_mutex);
-put_rpm:
-	intel_runtime_pm_put(dev_priv);
-
 	return ret;
 }
 
@@ -1135,6 +1096,24 @@ i915_gem_check_wedge(struct i915_gpu_error *error,
 	return 0;
 }
 
+/*
+ * Compare seqno against outstanding lazy request. Emit a request if they are
+ * equal.
+ */
+int
+i915_gem_check_olr(struct intel_engine_cs *ring, u32 seqno)
+{
+	int ret;
+
+	BUG_ON(!mutex_is_locked(&ring->dev->struct_mutex));
+
+	ret = 0;
+	if (seqno == ring->outstanding_lazy_seqno)
+		ret = i915_add_request(ring, NULL);
+
+	return ret;
+}
+
 static void fake_irq(unsigned long data)
 {
 	wake_up_process((struct task_struct *)data);
@@ -1146,84 +1125,19 @@ static bool missed_irq(struct drm_i915_private *dev_priv,
 	return test_bit(ring->id, &dev_priv->gpu_error.missed_irq_rings);
 }
 
-static unsigned long local_clock_us(unsigned *cpu)
-{
-	unsigned long t;
-
-	/* Cheaply and approximately convert from nanoseconds to microseconds.
-	 * The result and subsequent calculations are also defined in the same
-	 * approximate microseconds units. The principal source of timing
-	 * error here is from the simple truncation.
-	 *
-	 * Note that local_clock() is only defined wrt to the current CPU;
-	 * the comparisons are no longer valid if we switch CPUs. Instead of
-	 * blocking preemption for the entire busywait, we can detect the CPU
-	 * switch and use that as indicator of system load and a reason to
-	 * stop busywaiting, see busywait_stop().
-	 */
-	*cpu = get_cpu();
-	t = local_clock() >> 10;
-	put_cpu();
-
-	return t;
-}
-
-static bool busywait_stop(unsigned long timeout, unsigned cpu)
+static bool can_wait_boost(struct drm_i915_file_private *file_priv)
 {
-	unsigned this_cpu;
-
-	if (time_after(local_clock_us(&this_cpu), timeout))
+	if (file_priv == NULL)
 		return true;
 
-	return this_cpu != cpu;
-}
-
-static int __i915_spin_request(struct drm_i915_gem_request *req, int state)
-{
-	unsigned long timeout;
-	unsigned cpu;
-
-	/* When waiting for high frequency requests, e.g. during synchronous
-	 * rendering split between the CPU and GPU, the finite amount of time
-	 * required to set up the irq and wait upon it limits the response
-	 * rate. By busywaiting on the request completion for a short while we
-	 * can service the high frequency waits as quick as possible. However,
-	 * if it is a slow request, we want to sleep as quickly as possible.
-	 * The tradeoff between waiting and sleeping is roughly the time it
-	 * takes to sleep on a request, on the order of a microsecond.
-	 */
-
-	if (req->ring->irq_refcount)
-		return -EBUSY;
-
-	/* Only spin if we know the GPU is processing this request */
-	if (!i915_gem_request_started(req, true))
-		return -EAGAIN;
-
-	timeout = local_clock_us(&cpu) + 5;
-	while (!need_resched()) {
-		if (i915_gem_request_completed(req, true))
-			return 0;
-
-		if (signal_pending_state(state, current))
-			break;
-
-		if (busywait_stop(timeout, cpu))
-			break;
-
-		cpu_relax_lowlatency();
-	}
-
-	if (i915_gem_request_completed(req, false))
-		return 0;
-
-	return -EAGAIN;
+	return !atomic_xchg(&file_priv->rps_wait_boost, true);
 }
 
 /**
- * __i915_wait_request - wait until execution of request has finished
- * @req: duh!
- * @reset_counter: reset sequence associated with the given request
+ * __wait_seqno - wait until execution of seqno has finished
+ * @ring: the ring expected to report seqno
+ * @seqno: duh!
+ * @reset_counter: reset sequence associated with the given seqno
  * @interruptible: do an interruptible wait (normally yes)
  * @timeout: in - how long to wait (NULL forever); out - how much time remaining
  *
@@ -1234,21 +1148,19 @@ static int __i915_spin_request(struct drm_i915_gem_request *req, int state)
  * reset_counter _must_ be read before, and an appropriate smp_rmb must be
  * inserted.
  *
- * Returns 0 if the request was found within the alloted time. Else returns the
+ * Returns 0 if the seqno was found within the alloted time. Else returns the
  * errno with remaining time filled in timeout argument.
  */
-int __i915_wait_request(struct drm_i915_gem_request *req,
+static int __wait_seqno(struct intel_engine_cs *ring, u32 seqno,
 			unsigned reset_counter,
 			bool interruptible,
 			s64 *timeout,
-			struct intel_rps_client *rps)
+			struct drm_i915_file_private *file_priv)
 {
-	struct intel_engine_cs *ring = i915_gem_request_get_ring(req);
 	struct drm_device *dev = ring->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	const bool irq_test_in_progress =
 		ACCESS_ONCE(dev_priv->gpu_error.test_irq_rings) & intel_ring_flag(ring);
-	int state = interruptible ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE;
 	DEFINE_WAIT(wait);
 	unsigned long timeout_expire;
 	s64 before, now;
@@ -1256,44 +1168,30 @@ int __i915_wait_request(struct drm_i915_gem_request *req,
 
 	WARN(!intel_irqs_enabled(dev_priv), "IRQs disabled");
 
-	if (list_empty(&req->list))
+	if (i915_seqno_passed(ring->get_seqno(ring, true), seqno))
 		return 0;
 
-	if (i915_gem_request_completed(req, true))
-		return 0;
-
-	timeout_expire = 0;
-	if (timeout) {
-		if (WARN_ON(*timeout < 0))
-			return -EINVAL;
-
-		if (*timeout == 0)
-			return -ETIME;
+	timeout_expire = timeout ? jiffies + nsecs_to_jiffies((u64)*timeout) : 0;
 
-		timeout_expire = jiffies + nsecs_to_jiffies_timeout(*timeout);
+	if (INTEL_INFO(dev)->gen >= 6 && ring->id == RCS && can_wait_boost(file_priv)) {
+		gen6_rps_boost(dev_priv);
+		if (file_priv)
+			mod_delayed_work(dev_priv->wq,
+					 &file_priv->mm.idle_work,
+					 msecs_to_jiffies(100));
 	}
 
-	if (INTEL_INFO(dev_priv)->gen >= 6)
-		gen6_rps_boost(dev_priv, rps, req->emitted_jiffies);
+	if (!irq_test_in_progress && WARN_ON(!ring->irq_get(ring)))
+		return -ENODEV;
 
 	/* Record current time in case interrupted by signal, or wedged */
-	trace_i915_gem_request_wait_begin(req);
+	trace_i915_gem_request_wait_begin(ring, seqno);
 	before = ktime_get_raw_ns();
-
-	/* Optimistic spin for the next jiffie before touching IRQs */
-	ret = __i915_spin_request(req, state);
-	if (ret == 0)
-		goto out;
-
-	if (!irq_test_in_progress && WARN_ON(!ring->irq_get(ring))) {
-		ret = -ENODEV;
-		goto out;
-	}
-
 	for (;;) {
 		struct timer_list timer;
 
-		prepare_to_wait(&ring->irq_queue, &wait, state);
+		prepare_to_wait(&ring->irq_queue, &wait,
+				interruptible ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE);
 
 		/* We need to check whether any gpu reset happened in between
 		 * the caller grabbing the seqno and now ... */
@@ -1306,12 +1204,12 @@ int __i915_wait_request(struct drm_i915_gem_request *req,
 			break;
 		}
 
-		if (i915_gem_request_completed(req, false)) {
+		if (i915_seqno_passed(ring->get_seqno(ring, false), seqno)) {
 			ret = 0;
 			break;
 		}
 
-		if (signal_pending_state(state, current)) {
+		if (interruptible && signal_pending(current)) {
 			ret = -ERESTARTSYS;
 			break;
 		}
@@ -1337,150 +1235,67 @@ int __i915_wait_request(struct drm_i915_gem_request *req,
 			destroy_timer_on_stack(&timer);
 		}
 	}
+	now = ktime_get_raw_ns();
+	trace_i915_gem_request_wait_end(ring, seqno);
+
 	if (!irq_test_in_progress)
 		ring->irq_put(ring);
 
 	finish_wait(&ring->irq_queue, &wait);
 
-out:
-	now = ktime_get_raw_ns();
-	trace_i915_gem_request_wait_end(req);
-
 	if (timeout) {
 		s64 tres = *timeout - (now - before);
 
 		*timeout = tres < 0 ? 0 : tres;
-
-		/*
-		 * Apparently ktime isn't accurate enough and occasionally has a
-		 * bit of mismatch in the jiffies<->nsecs<->ktime loop. So patch
-		 * things up to make the test happy. We allow up to 1 jiffy.
-		 *
-		 * This is a regrssion from the timespec->ktime conversion.
-		 */
-		if (ret == -ETIME && *timeout < jiffies_to_usecs(1)*1000)
-			*timeout = 0;
 	}
 
 	return ret;
 }
 
-int i915_gem_request_add_to_client(struct drm_i915_gem_request *req,
-				   struct drm_file *file)
+/**
+ * Waits for a sequence number to be signaled, and cleans up the
+ * request and object lists appropriately for that event.
+ */
+int
+i915_wait_seqno(struct intel_engine_cs *ring, uint32_t seqno)
 {
-	struct drm_i915_private *dev_private;
-	struct drm_i915_file_private *file_priv;
+	struct drm_device *dev = ring->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	bool interruptible = dev_priv->mm.interruptible;
+	int ret;
 
-	WARN_ON(!req || !file || req->file_priv);
+	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
+	BUG_ON(seqno == 0);
 
-	if (!req || !file)
-		return -EINVAL;
+	ret = i915_gem_check_wedge(&dev_priv->gpu_error, interruptible);
+	if (ret)
+		return ret;
 
-	if (req->file_priv)
-		return -EINVAL;
-
-	dev_private = req->ring->dev->dev_private;
-	file_priv = file->driver_priv;
-
-	spin_lock(&file_priv->mm.lock);
-	req->file_priv = file_priv;
-	list_add_tail(&req->client_list, &file_priv->mm.request_list);
-	spin_unlock(&file_priv->mm.lock);
-
-	req->pid = get_pid(task_pid(current));
-
-	return 0;
-}
-
-static inline void
-i915_gem_request_remove_from_client(struct drm_i915_gem_request *request)
-{
-	struct drm_i915_file_private *file_priv = request->file_priv;
-
-	if (!file_priv)
-		return;
-
-	spin_lock(&file_priv->mm.lock);
-	list_del(&request->client_list);
-	request->file_priv = NULL;
-	spin_unlock(&file_priv->mm.lock);
+	ret = i915_gem_check_olr(ring, seqno);
+	if (ret)
+		return ret;
 
-	put_pid(request->pid);
-	request->pid = NULL;
+	return __wait_seqno(ring, seqno,
+			    atomic_read(&dev_priv->gpu_error.reset_counter),
+			    interruptible, NULL, NULL);
 }
 
-static void i915_gem_request_retire(struct drm_i915_gem_request *request)
+static int
+i915_gem_object_wait_rendering__tail(struct drm_i915_gem_object *obj,
+				     struct intel_engine_cs *ring)
 {
-	trace_i915_gem_request_retire(request);
+	if (!obj->active)
+		return 0;
 
-	/* We know the GPU must have read the request to have
-	 * sent us the seqno + interrupt, so use the position
-	 * of tail of the request to update the last known position
-	 * of the GPU head.
+	/* Manually manage the write flush as we may have not yet
+	 * retired the buffer.
 	 *
-	 * Note this requires that we are always called in request
-	 * completion order.
+	 * Note that the last_write_seqno is always the earlier of
+	 * the two (read/write) seqno, so if we haved successfully waited,
+	 * we know we have passed the last write.
 	 */
-	request->ringbuf->last_retired_head = request->postfix;
-
-	list_del_init(&request->list);
-	i915_gem_request_remove_from_client(request);
-
-	i915_gem_request_unreference(request);
-}
-
-static void
-__i915_gem_request_retire__upto(struct drm_i915_gem_request *req)
-{
-	struct intel_engine_cs *engine = req->ring;
-	struct drm_i915_gem_request *tmp;
-
-	lockdep_assert_held(&engine->dev->struct_mutex);
-
-	if (list_empty(&req->list))
-		return;
-
-	do {
-		tmp = list_first_entry(&engine->request_list,
-				       typeof(*tmp), list);
-
-		i915_gem_request_retire(tmp);
-	} while (tmp != req);
-
-	WARN_ON(i915_verify_lists(engine->dev));
-}
-
-/**
- * Waits for a request to be signaled, and cleans up the
- * request and object lists appropriately for that event.
- */
-int
-i915_wait_request(struct drm_i915_gem_request *req)
-{
-	struct drm_device *dev;
-	struct drm_i915_private *dev_priv;
-	bool interruptible;
-	int ret;
-
-	BUG_ON(req == NULL);
+	obj->last_write_seqno = 0;
 
-	dev = req->ring->dev;
-	dev_priv = dev->dev_private;
-	interruptible = dev_priv->mm.interruptible;
-
-	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
-
-	ret = i915_gem_check_wedge(&dev_priv->gpu_error, interruptible);
-	if (ret)
-		return ret;
-
-	ret = __i915_wait_request(req,
-				  atomic_read(&dev_priv->gpu_error.reset_counter),
-				  interruptible, NULL, NULL);
-	if (ret)
-		return ret;
-
-	__i915_gem_request_retire__upto(req);
 	return 0;
 }
 
@@ -1488,56 +1303,23 @@ i915_wait_request(struct drm_i915_gem_request *req)
  * Ensures that all rendering to the object has completed and the object is
  * safe to unbind from the GTT or access from the CPU.
  */
-int
+static __must_check int
 i915_gem_object_wait_rendering(struct drm_i915_gem_object *obj,
 			       bool readonly)
 {
-	int ret, i;
+	struct intel_engine_cs *ring = obj->ring;
+	u32 seqno;
+	int ret;
 
-	if (!obj->active)
+	seqno = readonly ? obj->last_write_seqno : obj->last_read_seqno;
+	if (seqno == 0)
 		return 0;
 
-	if (readonly) {
-		if (obj->last_write_req != NULL) {
-			ret = i915_wait_request(obj->last_write_req);
-			if (ret)
-				return ret;
-
-			i = obj->last_write_req->ring->id;
-			if (obj->last_read_req[i] == obj->last_write_req)
-				i915_gem_object_retire__read(obj, i);
-			else
-				i915_gem_object_retire__write(obj);
-		}
-	} else {
-		for (i = 0; i < I915_NUM_RINGS; i++) {
-			if (obj->last_read_req[i] == NULL)
-				continue;
-
-			ret = i915_wait_request(obj->last_read_req[i]);
-			if (ret)
-				return ret;
-
-			i915_gem_object_retire__read(obj, i);
-		}
-		RQ_BUG_ON(obj->active);
-	}
-
-	return 0;
-}
-
-static void
-i915_gem_object_retire_request(struct drm_i915_gem_object *obj,
-			       struct drm_i915_gem_request *req)
-{
-	int ring = req->ring->id;
-
-	if (obj->last_read_req[ring] == req)
-		i915_gem_object_retire__read(obj, ring);
-	else if (obj->last_write_req == req)
-		i915_gem_object_retire__write(obj);
+	ret = i915_wait_seqno(ring, seqno);
+	if (ret)
+		return ret;
 
-	__i915_gem_request_retire__upto(req);
+	return i915_gem_object_wait_rendering__tail(obj, ring);
 }
 
 /* A nonblocking variant of the above wait. This is a highly dangerous routine
@@ -1545,66 +1327,39 @@ i915_gem_object_retire_request(struct drm_i915_gem_object *obj,
  */
 static __must_check int
 i915_gem_object_wait_rendering__nonblocking(struct drm_i915_gem_object *obj,
-					    struct intel_rps_client *rps,
+					    struct drm_i915_file_private *file_priv,
 					    bool readonly)
 {
 	struct drm_device *dev = obj->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct drm_i915_gem_request *requests[I915_NUM_RINGS];
+	struct intel_engine_cs *ring = obj->ring;
 	unsigned reset_counter;
-	int ret, i, n = 0;
+	u32 seqno;
+	int ret;
 
 	BUG_ON(!mutex_is_locked(&dev->struct_mutex));
 	BUG_ON(!dev_priv->mm.interruptible);
 
-	if (!obj->active)
+	seqno = readonly ? obj->last_write_seqno : obj->last_read_seqno;
+	if (seqno == 0)
 		return 0;
 
 	ret = i915_gem_check_wedge(&dev_priv->gpu_error, true);
 	if (ret)
 		return ret;
 
-	reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
-
-	if (readonly) {
-		struct drm_i915_gem_request *req;
-
-		req = obj->last_write_req;
-		if (req == NULL)
-			return 0;
-
-		requests[n++] = i915_gem_request_reference(req);
-	} else {
-		for (i = 0; i < I915_NUM_RINGS; i++) {
-			struct drm_i915_gem_request *req;
-
-			req = obj->last_read_req[i];
-			if (req == NULL)
-				continue;
-
-			requests[n++] = i915_gem_request_reference(req);
-		}
-	}
+	ret = i915_gem_check_olr(ring, seqno);
+	if (ret)
+		return ret;
 
+	reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
 	mutex_unlock(&dev->struct_mutex);
-	for (i = 0; ret == 0 && i < n; i++)
-		ret = __i915_wait_request(requests[i], reset_counter, true,
-					  NULL, rps);
+	ret = __wait_seqno(ring, seqno, reset_counter, true, NULL, file_priv);
 	mutex_lock(&dev->struct_mutex);
+	if (ret)
+		return ret;
 
-	for (i = 0; i < n; i++) {
-		if (ret == 0)
-			i915_gem_object_retire_request(obj, requests[i]);
-		i915_gem_request_unreference(requests[i]);
-	}
-
-	return ret;
-}
-
-static struct intel_rps_client *to_rps_client(struct drm_file *file)
-{
-	struct drm_i915_file_private *fpriv = file->driver_priv;
-	return &fpriv->rps;
+	return i915_gem_object_wait_rendering__tail(obj, ring);
 }
 
 /**
@@ -1649,20 +1404,23 @@ i915_gem_set_domain_ioctl(struct drm_device *dev, void *data,
 	 * to catch cases where we are gazumped.
 	 */
 	ret = i915_gem_object_wait_rendering__nonblocking(obj,
-							  to_rps_client(file),
+							  file->driver_priv,
 							  !write_domain);
 	if (ret)
 		goto unref;
 
-	if (read_domains & I915_GEM_DOMAIN_GTT)
+	if (read_domains & I915_GEM_DOMAIN_GTT) {
 		ret = i915_gem_object_set_to_gtt_domain(obj, write_domain != 0);
-	else
-		ret = i915_gem_object_set_to_cpu_domain(obj, write_domain != 0);
 
-	if (write_domain != 0)
-		intel_fb_obj_invalidate(obj,
-					write_domain == I915_GEM_DOMAIN_GTT ?
-					ORIGIN_GTT : ORIGIN_CPU);
+		/* Silently promote "you're not bound, there was nothing to do"
+		 * to success, since the client was just asking us to
+		 * make sure everything was done.
+		 */
+		if (ret == -EINVAL)
+			ret = 0;
+	} else {
+		ret = i915_gem_object_set_to_cpu_domain(obj, write_domain != 0);
+	}
 
 unref:
 	drm_gem_object_unreference(&obj->base);
@@ -1694,7 +1452,7 @@ i915_gem_sw_finish_ioctl(struct drm_device *dev, void *data,
 
 	/* Pinned buffers may be scanout, so flush the cache */
 	if (obj->pin_display)
-		i915_gem_object_flush_cpu_write_domain(obj);
+		i915_gem_object_flush_cpu_write_domain(obj, true);
 
 	drm_gem_object_unreference(&obj->base);
 unlock:
@@ -1708,16 +1466,6 @@ unlock:
  *
  * While the mapping holds a reference on the contents of the object, it doesn't
  * imply a ref on the object itself.
- *
- * IMPORTANT:
- *
- * DRM driver writers who look a this function as an example for how to do GEM
- * mmap support, please don't implement mmap support like here. The modern way
- * to implement DRM mmap support is with an mmap offset ioctl (like
- * i915_gem_mmap_gtt) and then using the mmap syscall on the DRM fd directly.
- * That way debug tooling like valgrind will understand what's going on, hiding
- * the mmap call in a driver private ioctl will break that. The i915 driver only
- * does cpu mmaps this way because we didn't know better.
  */
 int
 i915_gem_mmap_ioctl(struct drm_device *dev, void *data,
@@ -1727,12 +1475,6 @@ i915_gem_mmap_ioctl(struct drm_device *dev, void *data,
 	struct drm_gem_object *obj;
 	unsigned long addr;
 
-	if (args->flags & ~(I915_MMAP_WC))
-		return -EINVAL;
-
-	if (args->flags & I915_MMAP_WC && !cpu_has_pat)
-		return -ENODEV;
-
 	obj = drm_gem_object_lookup(dev, file, args->handle);
 	if (obj == NULL)
 		return -ENOENT;
@@ -1748,19 +1490,6 @@ i915_gem_mmap_ioctl(struct drm_device *dev, void *data,
 	addr = vm_mmap(obj->filp, 0, args->size,
 		       PROT_READ | PROT_WRITE, MAP_SHARED,
 		       args->offset);
-	if (args->flags & I915_MMAP_WC) {
-		struct mm_struct *mm = current->mm;
-		struct vm_area_struct *vma;
-
-		down_write(&mm->mmap_sem);
-		vma = find_vma(mm, addr);
-		if (vma)
-			vma->vm_page_prot =
-				pgprot_writecombine(vm_get_page_prot(vma->vm_flags));
-		else
-			addr = -ENOMEM;
-		up_write(&mm->mmap_sem);
-	}
 	drm_gem_object_unreference_unlocked(obj);
 	if (IS_ERR((void *)addr))
 		return addr;
@@ -1772,8 +1501,8 @@ i915_gem_mmap_ioctl(struct drm_device *dev, void *data,
 
 /**
  * i915_gem_fault - fault a page into the GTT
- * @vma: VMA in question
- * @vmf: fault info
+ * vma: VMA in question
+ * vmf: fault info
  *
  * The fault handler is set up by drm_gem_mmap() when a object is GTT mapped
  * from userspace.  The fault handler takes care of binding the object to
@@ -1791,7 +1520,6 @@ int i915_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 	struct drm_i915_gem_object *obj = to_intel_bo(vma->vm_private_data);
 	struct drm_device *dev = obj->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct i915_ggtt_view view = i915_ggtt_view_normal;
 	pgoff_t page_offset;
 	unsigned long pfn;
 	int ret = 0;
@@ -1824,23 +1552,8 @@ int i915_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 		goto unlock;
 	}
 
-	/* Use a partial view if the object is bigger than the aperture. */
-	if (obj->base.size >= dev_priv->gtt.mappable_end &&
-	    obj->tiling_mode == I915_TILING_NONE) {
-		static const unsigned int chunk_size = 256; // 1 MiB
-
-		memset(&view, 0, sizeof(view));
-		view.type = I915_GGTT_VIEW_PARTIAL;
-		view.params.partial.offset = rounddown(page_offset, chunk_size);
-		view.params.partial.size =
-			min_t(unsigned int,
-			      chunk_size,
-			      (vma->vm_end - vma->vm_start)/PAGE_SIZE -
-			      view.params.partial.offset);
-	}
-
-	/* Now pin it into the GTT if needed */
-	ret = i915_gem_object_ggtt_pin(obj, &view, 0, PIN_MAPPABLE);
+	/* Now bind it into the GTT if needed */
+	ret = i915_gem_obj_ggtt_pin(obj, 0, PIN_MAPPABLE);
 	if (ret)
 		goto unlock;
 
@@ -1853,50 +1566,30 @@ int i915_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 		goto unpin;
 
 	/* Finally, remap it using the new GTT offset */
-	pfn = dev_priv->gtt.mappable_base +
-		i915_gem_obj_ggtt_offset_view(obj, &view);
+	pfn = dev_priv->gtt.mappable_base + i915_gem_obj_ggtt_offset(obj);
 	pfn >>= PAGE_SHIFT;
 
-	if (unlikely(view.type == I915_GGTT_VIEW_PARTIAL)) {
-		/* Overriding existing pages in partial view does not cause
-		 * us any trouble as TLBs are still valid because the fault
-		 * is due to userspace losing part of the mapping or never
-		 * having accessed it before (at this partials' range).
-		 */
-		unsigned long base = vma->vm_start +
-				     (view.params.partial.offset << PAGE_SHIFT);
-		unsigned int i;
+	if (!obj->fault_mappable) {
+		unsigned long size = min_t(unsigned long,
+					   vma->vm_end - vma->vm_start,
+					   obj->base.size);
+		int i;
 
-		for (i = 0; i < view.params.partial.size; i++) {
-			ret = vm_insert_pfn(vma, base + i * PAGE_SIZE, pfn + i);
+		for (i = 0; i < size >> PAGE_SHIFT; i++) {
+			ret = vm_insert_pfn(vma,
+					    (unsigned long)vma->vm_start + i * PAGE_SIZE,
+					    pfn + i);
 			if (ret)
 				break;
 		}
 
 		obj->fault_mappable = true;
-	} else {
-		if (!obj->fault_mappable) {
-			unsigned long size = min_t(unsigned long,
-						   vma->vm_end - vma->vm_start,
-						   obj->base.size);
-			int i;
-
-			for (i = 0; i < size >> PAGE_SHIFT; i++) {
-				ret = vm_insert_pfn(vma,
-						    (unsigned long)vma->vm_start + i * PAGE_SIZE,
-						    pfn + i);
-				if (ret)
-					break;
-			}
-
-			obj->fault_mappable = true;
-		} else
-			ret = vm_insert_pfn(vma,
-					    (unsigned long)vmf->virtual_address,
-					    pfn + page_offset);
-	}
+	} else
+		ret = vm_insert_pfn(vma,
+				    (unsigned long)vmf->virtual_address,
+				    pfn + page_offset);
 unpin:
-	i915_gem_object_ggtt_unpin_view(obj, &view);
+	i915_gem_object_ggtt_unpin(obj);
 unlock:
 	mutex_unlock(&dev->struct_mutex);
 out:
@@ -2075,6 +1768,7 @@ i915_gem_mmap_gtt(struct drm_file *file,
 		  uint32_t handle,
 		  uint64_t *offset)
 {
+	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_object *obj;
 	int ret;
 
@@ -2088,6 +1782,11 @@ i915_gem_mmap_gtt(struct drm_file *file,
 		goto unlock;
 	}
 
+	if (obj->base.size > dev_priv->gtt.mappable_end) {
+		ret = -E2BIG;
+		goto out;
+	}
+
 	if (obj->madv != I915_MADV_WILLNEED) {
 		DRM_DEBUG("Attempting to mmap a purgeable buffer\n");
 		ret = -EFAULT;
@@ -2131,6 +1830,12 @@ i915_gem_mmap_gtt_ioctl(struct drm_device *dev, void *data,
 	return i915_gem_mmap_gtt(file, dev, args->handle, &args->offset);
 }
 
+static inline int
+i915_gem_object_is_purgeable(struct drm_i915_gem_object *obj)
+{
+	return obj->madv == I915_MADV_DONTNEED;
+}
+
 /* Immediately discard the backing storage */
 static void
 i915_gem_object_truncate(struct drm_i915_gem_object *obj)
@@ -2187,8 +1892,6 @@ i915_gem_object_put_pages_gtt(struct drm_i915_gem_object *obj)
 		obj->base.read_domains = obj->base.write_domain = I915_GEM_DOMAIN_CPU;
 	}
 
-	i915_gem_gtt_finish_object(obj);
-
 	if (i915_gem_object_needs_bit17_swizzle(obj))
 		i915_gem_object_save_bit_17_swizzle(obj);
 
@@ -2238,6 +1941,96 @@ i915_gem_object_put_pages(struct drm_i915_gem_object *obj)
 	return 0;
 }
 
+unsigned long
+i915_gem_shrink(struct drm_i915_private *dev_priv,
+		long target, unsigned flags)
+{
+	const bool purgeable_only = flags & I915_SHRINK_PURGEABLE;
+	unsigned long count = 0;
+
+	/*
+	 * As we may completely rewrite the (un)bound list whilst unbinding
+	 * (due to retiring requests) we have to strictly process only
+	 * one element of the list at the time, and recheck the list
+	 * on every iteration.
+	 *
+	 * In particular, we must hold a reference whilst removing the
+	 * object as we may end up waiting for and/or retiring the objects.
+	 * This might release the final reference (held by the active list)
+	 * and result in the object being freed from under us. This is
+	 * similar to the precautions the eviction code must take whilst
+	 * removing objects.
+	 *
+	 * Also note that although these lists do not hold a reference to
+	 * the object we can safely grab one here: The final object
+	 * unreferencing and the bound_list are both protected by the
+	 * dev->struct_mutex and so we won't ever be able to observe an
+	 * object on the bound_list with a reference count equals 0.
+	 */
+	if (flags & I915_SHRINK_UNBOUND) {
+		struct list_head still_in_list;
+
+		INIT_LIST_HEAD(&still_in_list);
+		while (count < target && !list_empty(&dev_priv->mm.unbound_list)) {
+			struct drm_i915_gem_object *obj;
+
+			obj = list_first_entry(&dev_priv->mm.unbound_list,
+					       typeof(*obj), global_list);
+			list_move_tail(&obj->global_list, &still_in_list);
+
+			if (!i915_gem_object_is_purgeable(obj) && purgeable_only)
+				continue;
+
+			drm_gem_object_reference(&obj->base);
+
+			if (i915_gem_object_put_pages(obj) == 0)
+				count += obj->base.size >> PAGE_SHIFT;
+
+			drm_gem_object_unreference(&obj->base);
+		}
+		list_splice(&still_in_list, &dev_priv->mm.unbound_list);
+	}
+
+	if (flags & I915_SHRINK_BOUND) {
+		struct list_head still_in_list;
+
+		INIT_LIST_HEAD(&still_in_list);
+		while (count < target && !list_empty(&dev_priv->mm.bound_list)) {
+			struct drm_i915_gem_object *obj;
+			struct i915_vma *vma, *v;
+
+			obj = list_first_entry(&dev_priv->mm.bound_list,
+					       typeof(*obj), global_list);
+			list_move_tail(&obj->global_list, &still_in_list);
+
+			if (!i915_gem_object_is_purgeable(obj) && purgeable_only)
+				continue;
+
+			drm_gem_object_reference(&obj->base);
+
+			list_for_each_entry_safe(vma, v, &obj->vma_list, vma_link)
+				if (i915_vma_unbind(vma))
+					break;
+
+			if (i915_gem_object_put_pages(obj) == 0)
+				count += obj->base.size >> PAGE_SHIFT;
+
+			drm_gem_object_unreference(&obj->base);
+		}
+		list_splice(&still_in_list, &dev_priv->mm.bound_list);
+	}
+
+	return count;
+}
+
+static unsigned long
+i915_gem_shrink_all(struct drm_i915_private *dev_priv)
+{
+	i915_gem_evict_everything(dev_priv->dev);
+	return i915_gem_shrink(dev_priv, LONG_MAX,
+			       I915_SHRINK_BOUND | I915_SHRINK_UNBOUND);
+}
+
 static int
 i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)
 {
@@ -2249,7 +2042,6 @@ i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)
 	struct sg_page_iter sg_iter;
 	struct page *page;
 	unsigned long last_pfn = 0;	/* suppress gcc warning */
-	int ret;
 	gfp_t gfp;
 
 	/* Assert that the object is not currently in any GPU domain. As it
@@ -2275,8 +2067,9 @@ i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)
 	 * Fail silently without starting the shrinker
 	 */
 	mapping = file_inode(obj->base.filp)->i_mapping;
-	gfp = mapping_gfp_constraint(mapping, ~(__GFP_IO | __GFP_RECLAIM));
-	gfp |= __GFP_NORETRY | __GFP_NOWARN;
+	gfp = mapping_gfp_mask(mapping);
+	gfp |= __GFP_NORETRY | __GFP_NOWARN | __GFP_NO_KSWAPD;
+	gfp &= ~(__GFP_IO | __GFP_WAIT);
 	sg = st->sgl;
 	st->nents = 0;
 	for (i = 0; i < page_count; i++) {
@@ -2296,10 +2089,8 @@ i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)
 			 */
 			i915_gem_shrink_all(dev_priv);
 			page = shmem_read_mapping_page(mapping, i);
-			if (IS_ERR(page)) {
-				ret = PTR_ERR(page);
+			if (IS_ERR(page))
 				goto err_pages;
-			}
 		}
 #ifdef CONFIG_SWIOTLB
 		if (swiotlb_nr_tbl()) {
@@ -2328,17 +2119,9 @@ i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)
 		sg_mark_end(sg);
 	obj->pages = st;
 
-	ret = i915_gem_gtt_prepare_object(obj);
-	if (ret)
-		goto err_pages;
-
 	if (i915_gem_object_needs_bit17_swizzle(obj))
 		i915_gem_object_do_bit_17_swizzle(obj);
 
-	if (obj->tiling_mode != I915_TILING_NONE &&
-	    dev_priv->quirks & QUIRK_PIN_SWIZZLED_PAGES)
-		i915_gem_object_pin_pages(obj);
-
 	return 0;
 
 err_pages:
@@ -2356,10 +2139,10 @@ err_pages:
 	 * space and so want to translate the error from shmemfs back to our
 	 * usual understanding of ENOMEM.
 	 */
-	if (ret == -ENOSPC)
-		ret = -ENOMEM;
-
-	return ret;
+	if (PTR_ERR(page) == -ENOSPC)
+		return -ENOMEM;
+	else
+		return PTR_ERR(page);
 }
 
 /* Ensure that the associated pages are gathered from the backing storage
@@ -2391,74 +2174,84 @@ i915_gem_object_get_pages(struct drm_i915_gem_object *obj)
 		return ret;
 
 	list_add_tail(&obj->global_list, &dev_priv->mm.unbound_list);
-
-	obj->get_page.sg = obj->pages->sgl;
-	obj->get_page.last = 0;
-
 	return 0;
 }
 
-void i915_vma_move_to_active(struct i915_vma *vma,
-			     struct drm_i915_gem_request *req)
+static void
+i915_gem_object_move_to_active(struct drm_i915_gem_object *obj,
+			       struct intel_engine_cs *ring)
 {
-	struct drm_i915_gem_object *obj = vma->obj;
-	struct intel_engine_cs *ring;
+	u32 seqno = intel_ring_get_seqno(ring);
 
-	ring = i915_gem_request_get_ring(req);
+	BUG_ON(ring == NULL);
+	if (obj->ring != ring && obj->last_write_seqno) {
+		/* Keep the seqno relative to the current ring */
+		obj->last_write_seqno = seqno;
+	}
+	obj->ring = ring;
 
 	/* Add a reference if we're newly entering the active list. */
-	if (obj->active == 0)
+	if (!obj->active) {
 		drm_gem_object_reference(&obj->base);
-	obj->active |= intel_ring_flag(ring);
+		obj->active = 1;
+	}
 
-	list_move_tail(&obj->ring_list[ring->id], &ring->active_list);
-	i915_gem_request_assign(&obj->last_read_req[ring->id], req);
+	list_move_tail(&obj->ring_list, &ring->active_list);
 
-	list_move_tail(&vma->mm_list, &vma->vm->active_list);
+	obj->last_read_seqno = seqno;
 }
 
-static void
-i915_gem_object_retire__write(struct drm_i915_gem_object *obj)
+void i915_vma_move_to_active(struct i915_vma *vma,
+			     struct intel_engine_cs *ring)
 {
-	RQ_BUG_ON(obj->last_write_req == NULL);
-	RQ_BUG_ON(!(obj->active & intel_ring_flag(obj->last_write_req->ring)));
-
-	i915_gem_request_assign(&obj->last_write_req, NULL);
-	intel_fb_obj_flush(obj, true, ORIGIN_CS);
+	list_move_tail(&vma->mm_list, &vma->vm->active_list);
+	return i915_gem_object_move_to_active(vma->obj, ring);
 }
 
 static void
-i915_gem_object_retire__read(struct drm_i915_gem_object *obj, int ring)
+i915_gem_object_move_to_inactive(struct drm_i915_gem_object *obj)
 {
+	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
+	struct i915_address_space *vm;
 	struct i915_vma *vma;
 
-	RQ_BUG_ON(obj->last_read_req[ring] == NULL);
-	RQ_BUG_ON(!(obj->active & (1 << ring)));
+	BUG_ON(obj->base.write_domain & ~I915_GEM_GPU_DOMAINS);
+	BUG_ON(!obj->active);
 
-	list_del_init(&obj->ring_list[ring]);
-	i915_gem_request_assign(&obj->last_read_req[ring], NULL);
+	list_for_each_entry(vm, &dev_priv->vm_list, global_link) {
+		vma = i915_gem_obj_to_vma(obj, vm);
+		if (vma && !list_empty(&vma->mm_list))
+			list_move_tail(&vma->mm_list, &vm->inactive_list);
+	}
 
-	if (obj->last_write_req && obj->last_write_req->ring->id == ring)
-		i915_gem_object_retire__write(obj);
+	intel_fb_obj_flush(obj, true);
 
-	obj->active &= ~(1 << ring);
-	if (obj->active)
-		return;
+	list_del_init(&obj->ring_list);
+	obj->ring = NULL;
 
-	/* Bump our place on the bound list to keep it roughly in LRU order
-	 * so that we don't steal from recently used but inactive objects
-	 * (unless we are forced to ofc!)
-	 */
-	list_move_tail(&obj->global_list,
-		       &to_i915(obj->base.dev)->mm.bound_list);
+	obj->last_read_seqno = 0;
+	obj->last_write_seqno = 0;
+	obj->base.write_domain = 0;
 
-	list_for_each_entry(vma, &obj->vma_list, vma_link) {
-		if (!list_empty(&vma->mm_list))
-			list_move_tail(&vma->mm_list, &vma->vm->inactive_list);
-	}
+	obj->last_fenced_seqno = 0;
 
-	i915_gem_request_assign(&obj->last_fenced_req, NULL);
+	obj->active = 0;
 	drm_gem_object_unreference(&obj->base);
+
+	WARN_ON(i915_verify_lists(dev));
+}
+
+static void
+i915_gem_object_retire(struct drm_i915_gem_object *obj)
+{
+	struct intel_engine_cs *ring = obj->ring;
+
+	if (ring == NULL)
+		return;
+
+	if (i915_seqno_passed(ring->get_seqno(ring, true),
+			      obj->last_read_seqno))
+		i915_gem_object_move_to_inactive(obj);
 }
 
 static int
@@ -2531,34 +2324,26 @@ i915_gem_get_seqno(struct drm_device *dev, u32 *seqno)
 	return 0;
 }
 
-/*
- * NB: This function is not allowed to fail. Doing so would mean the the
- * request is not being tracked for completion but the work itself is
- * going to happen on the hardware. This would be a Bad Thing(tm).
- */
-void __i915_add_request(struct drm_i915_gem_request *request,
-			struct drm_i915_gem_object *obj,
-			bool flush_caches)
+int __i915_add_request(struct intel_engine_cs *ring,
+		       struct drm_file *file,
+		       struct drm_i915_gem_object *obj,
+		       u32 *out_seqno)
 {
-	struct intel_engine_cs *ring;
-	struct drm_i915_private *dev_priv;
+	struct drm_i915_private *dev_priv = ring->dev->dev_private;
+	struct drm_i915_gem_request *request;
 	struct intel_ringbuffer *ringbuf;
-	u32 request_start;
+	u32 request_ring_position, request_start;
 	int ret;
 
+	request = ring->preallocated_lazy_request;
 	if (WARN_ON(request == NULL))
-		return;
-
-	ring = request->ring;
-	dev_priv = ring->dev->dev_private;
-	ringbuf = request->ringbuf;
+		return -ENOMEM;
 
-	/*
-	 * To ensure that this call will not fail, space for its emissions
-	 * should already have been reserved in the ring buffer. Let the ring
-	 * know that it is time to use that space up.
-	 */
-	intel_ring_reserved_space_use(ringbuf);
+	if (i915.enable_execlists) {
+		struct intel_context *ctx = request->ctx;
+		ringbuf = ctx->engine[ring->id].ringbuf;
+	} else
+		ringbuf = ring->buffer;
 
 	request_start = intel_ring_get_tail(ringbuf);
 	/*
@@ -2568,13 +2353,14 @@ void __i915_add_request(struct drm_i915_gem_request *request,
 	 * is that the flush _must_ happen before the next request, no matter
 	 * what.
 	 */
-	if (flush_caches) {
-		if (i915.enable_execlists)
-			ret = logical_ring_flush_all_caches(request);
-		else
-			ret = intel_ring_flush_all_caches(request);
-		/* Not allowed to fail! */
-		WARN(ret, "*_ring_flush_all_caches failed: %d!\n", ret);
+	if (i915.enable_execlists) {
+		ret = logical_ring_flush_all_caches(ringbuf);
+		if (ret)
+			return ret;
+	} else {
+		ret = intel_ring_flush_all_caches(ring);
+		if (ret)
+			return ret;
 	}
 
 	/* Record the position of the start of the request so that
@@ -2582,19 +2368,22 @@ void __i915_add_request(struct drm_i915_gem_request *request,
 	 * GPU processing the request, we never over-estimate the
 	 * position of the head.
 	 */
-	request->postfix = intel_ring_get_tail(ringbuf);
+	request_ring_position = intel_ring_get_tail(ringbuf);
 
-	if (i915.enable_execlists)
-		ret = ring->emit_request(request);
-	else {
-		ret = ring->add_request(request);
-
-		request->tail = intel_ring_get_tail(ringbuf);
+	if (i915.enable_execlists) {
+		ret = ring->emit_request(ringbuf);
+		if (ret)
+			return ret;
+	} else {
+		ret = ring->add_request(ring);
+		if (ret)
+			return ret;
 	}
-	/* Not allowed to fail! */
-	WARN(ret, "emit|add_request failed: %d!\n", ret);
 
+	request->seqno = intel_ring_get_seqno(ring);
+	request->ring = ring;
 	request->head = request_start;
+	request->tail = request_ring_position;
 
 	/* Whilst this request exists, batch_obj will be on the
 	 * active_list, and so will hold the active reference. Only when this
@@ -2604,25 +2393,63 @@ void __i915_add_request(struct drm_i915_gem_request *request,
 	 */
 	request->batch_obj = obj;
 
+	if (!i915.enable_execlists) {
+		/* Hold a reference to the current context so that we can inspect
+		 * it later in case a hangcheck error event fires.
+		 */
+		request->ctx = ring->last_context;
+		if (request->ctx)
+			i915_gem_context_reference(request->ctx);
+	}
+
 	request->emitted_jiffies = jiffies;
-	request->previous_seqno = ring->last_submitted_seqno;
-	ring->last_submitted_seqno = request->seqno;
 	list_add_tail(&request->list, &ring->request_list);
+	request->file_priv = NULL;
+
+	if (file) {
+		struct drm_i915_file_private *file_priv = file->driver_priv;
+
+		spin_lock(&file_priv->mm.lock);
+		request->file_priv = file_priv;
+		list_add_tail(&request->client_list,
+			      &file_priv->mm.request_list);
+		spin_unlock(&file_priv->mm.lock);
+	}
 
-	trace_i915_gem_request_add(request);
+	trace_i915_gem_request_add(ring, request->seqno);
+	ring->outstanding_lazy_seqno = 0;
+	ring->preallocated_lazy_request = NULL;
 
-	i915_queue_hangcheck(ring->dev);
+	if (!dev_priv->ums.mm_suspended) {
+		i915_queue_hangcheck(ring->dev);
 
-	queue_delayed_work(dev_priv->wq,
-			   &dev_priv->mm.retire_work,
-			   round_jiffies_up_relative(HZ));
-	intel_mark_busy(dev_priv->dev);
+		cancel_delayed_work_sync(&dev_priv->mm.idle_work);
+		queue_delayed_work(dev_priv->wq,
+				   &dev_priv->mm.retire_work,
+				   round_jiffies_up_relative(HZ));
+		intel_mark_busy(dev_priv->dev);
+	}
 
-	/* Sanity check that the reserved size was large enough. */
-	intel_ring_reserved_space_end(ringbuf);
+	if (out_seqno)
+		*out_seqno = request->seqno;
+	return 0;
 }
 
-static bool i915_context_is_banned(struct drm_i915_private *dev_priv,
+static inline void
+i915_gem_request_remove_from_client(struct drm_i915_gem_request *request)
+{
+	struct drm_i915_file_private *file_priv = request->file_priv;
+
+	if (!file_priv)
+		return;
+
+	spin_lock(&file_priv->mm.lock);
+	list_del(&request->client_list);
+	request->file_priv = NULL;
+	spin_unlock(&file_priv->mm.lock);
+}
+
+static bool i915_context_is_banned(struct drm_i915_private *dev_priv,
 				   const struct intel_context *ctx)
 {
 	unsigned long elapsed;
@@ -2632,8 +2459,7 @@ static bool i915_context_is_banned(struct drm_i915_private *dev_priv,
 	if (ctx->hang_stats.banned)
 		return true;
 
-	if (ctx->hang_stats.ban_period_seconds &&
-	    elapsed <= ctx->hang_stats.ban_period_seconds) {
+	if (elapsed <= DRM_I915_CTX_BAN_PERIOD) {
 		if (!i915_gem_context_is_default(ctx)) {
 			DRM_DEBUG("context hanging too fast, banning!\n");
 			return true;
@@ -2667,106 +2493,27 @@ static void i915_set_reset_status(struct drm_i915_private *dev_priv,
 	}
 }
 
-void i915_gem_request_free(struct kref *req_ref)
-{
-	struct drm_i915_gem_request *req = container_of(req_ref,
-						 typeof(*req), ref);
-	struct intel_context *ctx = req->ctx;
-
-	if (req->file_priv)
-		i915_gem_request_remove_from_client(req);
-
-	if (ctx) {
-		if (i915.enable_execlists) {
-			if (ctx != req->ring->default_context)
-				intel_lr_context_unpin(req);
-		}
-
-		i915_gem_context_unreference(ctx);
-	}
-
-	kmem_cache_free(req->i915->requests, req);
-}
-
-int i915_gem_request_alloc(struct intel_engine_cs *ring,
-			   struct intel_context *ctx,
-			   struct drm_i915_gem_request **req_out)
+static void i915_gem_free_request(struct drm_i915_gem_request *request)
 {
-	struct drm_i915_private *dev_priv = to_i915(ring->dev);
-	struct drm_i915_gem_request *req;
-	int ret;
-
-	if (!req_out)
-		return -EINVAL;
-
-	*req_out = NULL;
-
-	req = kmem_cache_zalloc(dev_priv->requests, GFP_KERNEL);
-	if (req == NULL)
-		return -ENOMEM;
-
-	ret = i915_gem_get_seqno(ring->dev, &req->seqno);
-	if (ret)
-		goto err;
-
-	kref_init(&req->ref);
-	req->i915 = dev_priv;
-	req->ring = ring;
-	req->ctx  = ctx;
-	i915_gem_context_reference(req->ctx);
-
-	if (i915.enable_execlists)
-		ret = intel_logical_ring_alloc_request_extras(req);
-	else
-		ret = intel_ring_alloc_request_extras(req);
-	if (ret) {
-		i915_gem_context_unreference(req->ctx);
-		goto err;
-	}
-
-	/*
-	 * Reserve space in the ring buffer for all the commands required to
-	 * eventually emit this request. This is to guarantee that the
-	 * i915_add_request() call can't fail. Note that the reserve may need
-	 * to be redone if the request is not actually submitted straight
-	 * away, e.g. because a GPU scheduler has deferred it.
-	 */
-	if (i915.enable_execlists)
-		ret = intel_logical_ring_reserve_space(req);
-	else
-		ret = intel_ring_reserve_space(req);
-	if (ret) {
-		/*
-		 * At this point, the request is fully allocated even if not
-		 * fully prepared. Thus it can be cleaned up using the proper
-		 * free code.
-		 */
-		i915_gem_request_cancel(req);
-		return ret;
-	}
-
-	*req_out = req;
-	return 0;
-
-err:
-	kmem_cache_free(dev_priv->requests, req);
-	return ret;
-}
+	list_del(&request->list);
+	i915_gem_request_remove_from_client(request);
 
-void i915_gem_request_cancel(struct drm_i915_gem_request *req)
-{
-	intel_ring_reserved_space_cancel(req->ringbuf);
+	if (request->ctx)
+		i915_gem_context_unreference(request->ctx);
 
-	i915_gem_request_unreference(req);
+	kfree(request);
 }
 
 struct drm_i915_gem_request *
 i915_gem_find_active_request(struct intel_engine_cs *ring)
 {
 	struct drm_i915_gem_request *request;
+	u32 completed_seqno;
+
+	completed_seqno = ring->get_seqno(ring, false);
 
 	list_for_each_entry(request, &ring->request_list, list) {
-		if (i915_gem_request_completed(request, false))
+		if (i915_seqno_passed(completed_seqno, request->seqno))
 			continue;
 
 		return request;
@@ -2802,28 +2549,9 @@ static void i915_gem_reset_ring_cleanup(struct drm_i915_private *dev_priv,
 
 		obj = list_first_entry(&ring->active_list,
 				       struct drm_i915_gem_object,
-				       ring_list[ring->id]);
-
-		i915_gem_object_retire__read(obj, ring->id);
-	}
-
-	/*
-	 * Clear the execlists queue up before freeing the requests, as those
-	 * are the ones that keep the context and ringbuffer backing objects
-	 * pinned in place.
-	 */
-	while (!list_empty(&ring->execlist_queue)) {
-		struct drm_i915_gem_request *submit_req;
+				       ring_list);
 
-		submit_req = list_first_entry(&ring->execlist_queue,
-				struct drm_i915_gem_request,
-				execlist_link);
-		list_del(&submit_req->execlist_link);
-
-		if (submit_req->ctx != ring->default_context)
-			intel_lr_context_unpin(submit_req);
-
-		i915_gem_request_unreference(submit_req);
+		i915_gem_object_move_to_inactive(obj);
 	}
 
 	/*
@@ -2840,7 +2568,45 @@ static void i915_gem_reset_ring_cleanup(struct drm_i915_private *dev_priv,
 					   struct drm_i915_gem_request,
 					   list);
 
-		i915_gem_request_retire(request);
+		i915_gem_free_request(request);
+	}
+
+	while (!list_empty(&ring->execlist_queue)) {
+		struct intel_ctx_submit_request *submit_req;
+
+		submit_req = list_first_entry(&ring->execlist_queue,
+				struct intel_ctx_submit_request,
+				execlist_link);
+		list_del(&submit_req->execlist_link);
+		intel_runtime_pm_put(dev_priv);
+		i915_gem_context_unreference(submit_req->ctx);
+		kfree(submit_req);
+	}
+
+	/* These may not have been flush before the reset, do so now */
+	kfree(ring->preallocated_lazy_request);
+	ring->preallocated_lazy_request = NULL;
+	ring->outstanding_lazy_seqno = 0;
+}
+
+void i915_gem_restore_fences(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int i;
+
+	for (i = 0; i < dev_priv->num_fence_regs; i++) {
+		struct drm_i915_fence_reg *reg = &dev_priv->fence_regs[i];
+
+		/*
+		 * Commit delayed tiling changes if we have an object still
+		 * attached to the fence, otherwise just clear the fence.
+		 */
+		if (reg->obj) {
+			i915_gem_object_update_fence(reg->obj, reg,
+						     reg->obj->tiling_mode);
+		} else {
+			i915_gem_write_fence(dev, i, NULL);
+		}
 	}
 }
 
@@ -2864,8 +2630,6 @@ void i915_gem_reset(struct drm_device *dev)
 	i915_gem_context_reset(dev);
 
 	i915_gem_restore_fences(dev);
-
-	WARN_ON(i915_verify_lists(dev));
 }
 
 /**
@@ -2874,25 +2638,14 @@ void i915_gem_reset(struct drm_device *dev)
 void
 i915_gem_retire_requests_ring(struct intel_engine_cs *ring)
 {
-	WARN_ON(i915_verify_lists(ring->dev));
-
-	/* Retire requests first as we use it above for the early return.
-	 * If we retire requests last, we may use a later seqno and so clear
-	 * the requests lists without clearing the active list, leading to
-	 * confusion.
-	 */
-	while (!list_empty(&ring->request_list)) {
-		struct drm_i915_gem_request *request;
+	uint32_t seqno;
 
-		request = list_first_entry(&ring->request_list,
-					   struct drm_i915_gem_request,
-					   list);
+	if (list_empty(&ring->request_list))
+		return;
 
-		if (!i915_gem_request_completed(request, true))
-			break;
+	WARN_ON(i915_verify_lists(ring->dev));
 
-		i915_gem_request_retire(request);
-	}
+	seqno = ring->get_seqno(ring, true);
 
 	/* Move any buffers on the active list that are no longer referenced
 	 * by the ringbuffer to the flushing/inactive lists as appropriate,
@@ -2903,18 +2656,53 @@ i915_gem_retire_requests_ring(struct intel_engine_cs *ring)
 
 		obj = list_first_entry(&ring->active_list,
 				      struct drm_i915_gem_object,
-				      ring_list[ring->id]);
+				      ring_list);
+
+		if (!i915_seqno_passed(seqno, obj->last_read_seqno))
+			break;
+
+		i915_gem_object_move_to_inactive(obj);
+	}
+
+
+	while (!list_empty(&ring->request_list)) {
+		struct drm_i915_gem_request *request;
+		struct intel_ringbuffer *ringbuf;
+
+		request = list_first_entry(&ring->request_list,
+					   struct drm_i915_gem_request,
+					   list);
 
-		if (!list_empty(&obj->last_read_req[ring->id]->list))
+		if (!i915_seqno_passed(seqno, request->seqno))
 			break;
 
-		i915_gem_object_retire__read(obj, ring->id);
+		trace_i915_gem_request_retire(ring, request->seqno);
+
+		/* This is one of the few common intersection points
+		 * between legacy ringbuffer submission and execlists:
+		 * we need to tell them apart in order to find the correct
+		 * ringbuffer to which the request belongs to.
+		 */
+		if (i915.enable_execlists) {
+			struct intel_context *ctx = request->ctx;
+			ringbuf = ctx->engine[ring->id].ringbuf;
+		} else
+			ringbuf = ring->buffer;
+
+		/* We know the GPU must have read the request to have
+		 * sent us the seqno + interrupt, so use the position
+		 * of tail of the request to update the last known position
+		 * of the GPU head.
+		 */
+		ringbuf->last_retired_head = request->tail;
+
+		i915_gem_free_request(request);
 	}
 
-	if (unlikely(ring->trace_irq_req &&
-		     i915_gem_request_completed(ring->trace_irq_req, true))) {
+	if (unlikely(ring->trace_irq_seqno &&
+		     i915_seqno_passed(seqno, ring->trace_irq_seqno))) {
 		ring->irq_put(ring);
-		i915_gem_request_assign(&ring->trace_irq_req, NULL);
+		ring->trace_irq_seqno = 0;
 	}
 
 	WARN_ON(i915_verify_lists(ring->dev));
@@ -2931,15 +2719,6 @@ i915_gem_retire_requests(struct drm_device *dev)
 	for_each_ring(ring, dev_priv, i) {
 		i915_gem_retire_requests_ring(ring);
 		idle &= list_empty(&ring->request_list);
-		if (i915.enable_execlists) {
-			unsigned long flags;
-
-			spin_lock_irqsave(&ring->execlist_lock, flags);
-			idle &= list_empty(&ring->execlist_queue);
-			spin_unlock_irqrestore(&ring->execlist_lock, flags);
-
-			intel_execlists_retire_requests(ring);
-		}
 	}
 
 	if (idle)
@@ -2974,25 +2753,8 @@ i915_gem_idle_work_handler(struct work_struct *work)
 {
 	struct drm_i915_private *dev_priv =
 		container_of(work, typeof(*dev_priv), mm.idle_work.work);
-	struct drm_device *dev = dev_priv->dev;
-	struct intel_engine_cs *ring;
-	int i;
-
-	for_each_ring(ring, dev_priv, i)
-		if (!list_empty(&ring->request_list))
-			return;
-
-	intel_mark_idle(dev);
-
-	if (mutex_trylock(&dev->struct_mutex)) {
-		struct intel_engine_cs *ring;
-		int i;
-
-		for_each_ring(ring, dev_priv, i)
-			i915_gem_batch_pool_fini(&ring->batch_pool);
 
-		mutex_unlock(&dev->struct_mutex);
-	}
+	intel_mark_idle(dev_priv->dev);
 }
 
 /**
@@ -3003,26 +2765,14 @@ i915_gem_idle_work_handler(struct work_struct *work)
 static int
 i915_gem_object_flush_active(struct drm_i915_gem_object *obj)
 {
-	int i;
-
-	if (!obj->active)
-		return 0;
-
-	for (i = 0; i < I915_NUM_RINGS; i++) {
-		struct drm_i915_gem_request *req;
-
-		req = obj->last_read_req[i];
-		if (req == NULL)
-			continue;
+	int ret;
 
-		if (list_empty(&req->list))
-			goto retire;
+	if (obj->active) {
+		ret = i915_gem_check_olr(obj->ring, obj->last_read_seqno);
+		if (ret)
+			return ret;
 
-		if (i915_gem_request_completed(req, true)) {
-			__i915_gem_request_retire__upto(req);
-retire:
-			i915_gem_object_retire__read(obj, i);
-		}
+		i915_gem_retire_requests_ring(obj->ring);
 	}
 
 	return 0;
@@ -3056,13 +2806,10 @@ i915_gem_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *file)
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_wait *args = data;
 	struct drm_i915_gem_object *obj;
-	struct drm_i915_gem_request *req[I915_NUM_RINGS];
+	struct intel_engine_cs *ring = NULL;
 	unsigned reset_counter;
-	int i, n = 0;
-	int ret;
-
-	if (args->flags != 0)
-		return -EINVAL;
+	u32 seqno = 0;
+	int ret = 0;
 
 	ret = i915_mutex_lock_interruptible(dev);
 	if (ret)
@@ -3079,37 +2826,28 @@ i915_gem_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *file)
 	if (ret)
 		goto out;
 
-	if (!obj->active)
-		goto out;
+	if (obj->active) {
+		seqno = obj->last_read_seqno;
+		ring = obj->ring;
+	}
+
+	if (seqno == 0)
+		 goto out;
 
 	/* Do this after OLR check to make sure we make forward progress polling
-	 * on this IOCTL with a timeout == 0 (like busy ioctl)
+	 * on this IOCTL with a timeout <=0 (like busy ioctl)
 	 */
-	if (args->timeout_ns == 0) {
+	if (args->timeout_ns <= 0) {
 		ret = -ETIME;
 		goto out;
 	}
 
 	drm_gem_object_unreference(&obj->base);
 	reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
-
-	for (i = 0; i < I915_NUM_RINGS; i++) {
-		if (obj->last_read_req[i] == NULL)
-			continue;
-
-		req[n++] = i915_gem_request_reference(obj->last_read_req[i]);
-	}
-
 	mutex_unlock(&dev->struct_mutex);
 
-	for (i = 0; i < n; i++) {
-		if (ret == 0)
-			ret = __i915_wait_request(req[i], reset_counter, true,
-						  args->timeout_ns > 0 ? &args->timeout_ns : NULL,
-						  file->driver_priv);
-		i915_gem_request_unreference__unlocked(req[i]);
-	}
-	return ret;
+	return __wait_seqno(ring, seqno, reset_counter, true, &args->timeout_ns,
+			    file->driver_priv);
 
 out:
 	drm_gem_object_unreference(&obj->base);
@@ -3117,130 +2855,54 @@ out:
 	return ret;
 }
 
-static int
-__i915_gem_object_sync(struct drm_i915_gem_object *obj,
-		       struct intel_engine_cs *to,
-		       struct drm_i915_gem_request *from_req,
-		       struct drm_i915_gem_request **to_req)
-{
-	struct intel_engine_cs *from;
-	int ret;
-
-	from = i915_gem_request_get_ring(from_req);
-	if (to == from)
-		return 0;
-
-	if (i915_gem_request_completed(from_req, true))
-		return 0;
-
-	if (!i915_semaphore_is_enabled(obj->base.dev)) {
-		struct drm_i915_private *i915 = to_i915(obj->base.dev);
-		ret = __i915_wait_request(from_req,
-					  atomic_read(&i915->gpu_error.reset_counter),
-					  i915->mm.interruptible,
-					  NULL,
-					  &i915->rps.semaphores);
-		if (ret)
-			return ret;
-
-		i915_gem_object_retire_request(obj, from_req);
-	} else {
-		int idx = intel_ring_sync_index(from, to);
-		u32 seqno = i915_gem_request_get_seqno(from_req);
-
-		WARN_ON(!to_req);
-
-		if (seqno <= from->semaphore.sync_seqno[idx])
-			return 0;
-
-		if (*to_req == NULL) {
-			ret = i915_gem_request_alloc(to, to->default_context, to_req);
-			if (ret)
-				return ret;
-		}
-
-		trace_i915_gem_ring_sync_to(*to_req, from, from_req);
-		ret = to->semaphore.sync_to(*to_req, from, seqno);
-		if (ret)
-			return ret;
-
-		/* We use last_read_req because sync_to()
-		 * might have just caused seqno wrap under
-		 * the radar.
-		 */
-		from->semaphore.sync_seqno[idx] =
-			i915_gem_request_get_seqno(obj->last_read_req[from->id]);
-	}
-
-	return 0;
-}
-
 /**
  * i915_gem_object_sync - sync an object to a ring.
  *
  * @obj: object which may be in use on another ring.
  * @to: ring we wish to use the object on. May be NULL.
- * @to_req: request we wish to use the object for. See below.
- *          This will be allocated and returned if a request is
- *          required but not passed in.
  *
  * This code is meant to abstract object synchronization with the GPU.
  * Calling with NULL implies synchronizing the object with the CPU
- * rather than a particular GPU ring. Conceptually we serialise writes
- * between engines inside the GPU. We only allow one engine to write
- * into a buffer at any time, but multiple readers. To ensure each has
- * a coherent view of memory, we must:
- *
- * - If there is an outstanding write request to the object, the new
- *   request must wait for it to complete (either CPU or in hw, requests
- *   on the same ring will be naturally ordered).
- *
- * - If we are a write request (pending_write_domain is set), the new
- *   request must wait for outstanding read requests to complete.
- *
- * For CPU synchronisation (NULL to) no request is required. For syncing with
- * rings to_req must be non-NULL. However, a request does not have to be
- * pre-allocated. If *to_req is NULL and sync commands will be emitted then a
- * request will be allocated automatically and returned through *to_req. Note
- * that it is not guaranteed that commands will be emitted (because the system
- * might already be idle). Hence there is no need to create a request that
- * might never have any work submitted. Note further that if a request is
- * returned in *to_req, it is the responsibility of the caller to submit
- * that request (after potentially adding more work to it).
+ * rather than a particular GPU ring.
  *
  * Returns 0 if successful, else propagates up the lower layer error.
  */
 int
 i915_gem_object_sync(struct drm_i915_gem_object *obj,
-		     struct intel_engine_cs *to,
-		     struct drm_i915_gem_request **to_req)
+		     struct intel_engine_cs *to)
 {
-	const bool readonly = obj->base.pending_write_domain == 0;
-	struct drm_i915_gem_request *req[I915_NUM_RINGS];
-	int ret, i, n;
+	struct intel_engine_cs *from = obj->ring;
+	u32 seqno;
+	int ret, idx;
 
-	if (!obj->active)
+	if (from == NULL || to == from)
 		return 0;
 
-	if (to == NULL)
-		return i915_gem_object_wait_rendering(obj, readonly);
+	if (to == NULL || !i915_semaphore_is_enabled(obj->base.dev))
+		return i915_gem_object_wait_rendering(obj, false);
 
-	n = 0;
-	if (readonly) {
-		if (obj->last_write_req)
-			req[n++] = obj->last_write_req;
-	} else {
-		for (i = 0; i < I915_NUM_RINGS; i++)
-			if (obj->last_read_req[i])
-				req[n++] = obj->last_read_req[i];
-	}
-	for (i = 0; i < n; i++) {
-		ret = __i915_gem_object_sync(obj, to, req[i], to_req);
-		if (ret)
-			return ret;
-	}
+	idx = intel_ring_sync_index(from, to);
 
-	return 0;
+	seqno = obj->last_read_seqno;
+	/* Optimization: Avoid semaphore sync when we are sure we already
+	 * waited for an object with higher seqno */
+	if (seqno <= from->semaphore.sync_seqno[idx])
+		return 0;
+
+	ret = i915_gem_check_olr(obj->ring, seqno);
+	if (ret)
+		return ret;
+
+	trace_i915_gem_ring_sync_to(from, to, seqno);
+	ret = to->semaphore.sync_to(to, from, seqno);
+	if (!ret)
+		/* We use last_read_seqno because sync_to()
+		 * might have just caused seqno wrap under
+		 * the radar.
+		 */
+		from->semaphore.sync_seqno[idx] = obj->last_read_seqno;
+
+	return ret;
 }
 
 static void i915_gem_object_finish_gtt(struct drm_i915_gem_object *obj)
@@ -3267,7 +2929,7 @@ static void i915_gem_object_finish_gtt(struct drm_i915_gem_object *obj)
 					    old_write_domain);
 }
 
-static int __i915_vma_unbind(struct i915_vma *vma, bool wait)
+int i915_vma_unbind(struct i915_vma *vma)
 {
 	struct drm_i915_gem_object *obj = vma->obj;
 	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
@@ -3286,14 +2948,18 @@ static int __i915_vma_unbind(struct i915_vma *vma, bool wait)
 
 	BUG_ON(obj->pages == NULL);
 
-	if (wait) {
-		ret = i915_gem_object_wait_rendering(obj, false);
-		if (ret)
-			return ret;
-	}
+	ret = i915_gem_object_finish_gpu(obj);
+	if (ret)
+		return ret;
+	/* Continue on if we fail due to EIO, the GPU is hung so we
+	 * should be safe and we need to cleanup or else we might
+	 * cause memory corruption through use-after-free.
+	 */
+
+	/* Throw away the active reference before moving to the unbound list */
+	i915_gem_object_retire(obj);
 
-	if (i915_is_ggtt(vma->vm) &&
-	    vma->ggtt_view.type == I915_GGTT_VIEW_NORMAL) {
+	if (i915_is_ggtt(vma->vm)) {
 		i915_gem_object_finish_gtt(obj);
 
 		/* release the fence reg _after_ flushing */
@@ -3304,27 +2970,21 @@ static int __i915_vma_unbind(struct i915_vma *vma, bool wait)
 
 	trace_i915_vma_unbind(vma);
 
-	vma->vm->unbind_vma(vma);
-	vma->bound = 0;
+	vma->unbind_vma(vma);
 
 	list_del_init(&vma->mm_list);
-	if (i915_is_ggtt(vma->vm)) {
-		if (vma->ggtt_view.type == I915_GGTT_VIEW_NORMAL) {
-			obj->map_and_fenceable = false;
-		} else if (vma->ggtt_view.pages) {
-			sg_free_table(vma->ggtt_view.pages);
-			kfree(vma->ggtt_view.pages);
-		}
-		vma->ggtt_view.pages = NULL;
-	}
+	if (i915_is_ggtt(vma->vm))
+		obj->map_and_fenceable = false;
 
 	drm_mm_remove_node(&vma->node);
 	i915_gem_vma_destroy(vma);
 
 	/* Since the unbound list is global, only move to that list if
 	 * no more VMAs exist. */
-	if (list_empty(&obj->vma_list))
+	if (list_empty(&obj->vma_list)) {
+		i915_gem_gtt_finish_object(obj);
 		list_move_tail(&obj->global_list, &dev_priv->mm.unbound_list);
+	}
 
 	/* And finally now the object is completely decoupled from this vma,
 	 * we can drop its hold on the backing storage and allow it to be
@@ -3335,16 +2995,6 @@ static int __i915_vma_unbind(struct i915_vma *vma, bool wait)
 	return 0;
 }
 
-int i915_vma_unbind(struct i915_vma *vma)
-{
-	return __i915_vma_unbind(vma, true);
-}
-
-int __i915_vma_unbind_no_wait(struct i915_vma *vma)
-{
-	return __i915_vma_unbind(vma, false);
-}
-
 int i915_gpu_idle(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -3354,19 +3004,9 @@ int i915_gpu_idle(struct drm_device *dev)
 	/* Flush everything onto the inactive list. */
 	for_each_ring(ring, dev_priv, i) {
 		if (!i915.enable_execlists) {
-			struct drm_i915_gem_request *req;
-
-			ret = i915_gem_request_alloc(ring, ring->default_context, &req);
+			ret = i915_switch_context(ring, ring->default_context);
 			if (ret)
 				return ret;
-
-			ret = i915_switch_context(req);
-			if (ret) {
-				i915_gem_request_cancel(req);
-				return ret;
-			}
-
-			i915_add_request_no_flush(req);
 		}
 
 		ret = intel_ring_idle(ring);
@@ -3374,23 +3014,363 @@ int i915_gpu_idle(struct drm_device *dev)
 			return ret;
 	}
 
-	WARN_ON(i915_verify_lists(dev));
 	return 0;
 }
 
-static bool i915_gem_valid_gtt_space(struct i915_vma *vma,
-				     unsigned long cache_level)
+static void i965_write_fence_reg(struct drm_device *dev, int reg,
+				 struct drm_i915_gem_object *obj)
 {
-	struct drm_mm_node *gtt_space = &vma->node;
-	struct drm_mm_node *other;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int fence_reg;
+	int fence_pitch_shift;
 
-	/*
-	 * On some machines we have to be careful when putting differing types
-	 * of snoopable memory together to avoid the prefetcher crossing memory
-	 * domains and dying. During vm initialisation, we decide whether or not
-	 * these constraints apply and set the drm_mm.color_adjust
-	 * appropriately.
-	 */
+	if (INTEL_INFO(dev)->gen >= 6) {
+		fence_reg = FENCE_REG_SANDYBRIDGE_0;
+		fence_pitch_shift = SANDYBRIDGE_FENCE_PITCH_SHIFT;
+	} else {
+		fence_reg = FENCE_REG_965_0;
+		fence_pitch_shift = I965_FENCE_PITCH_SHIFT;
+	}
+
+	fence_reg += reg * 8;
+
+	/* To w/a incoherency with non-atomic 64-bit register updates,
+	 * we split the 64-bit update into two 32-bit writes. In order
+	 * for a partial fence not to be evaluated between writes, we
+	 * precede the update with write to turn off the fence register,
+	 * and only enable the fence as the last step.
+	 *
+	 * For extra levels of paranoia, we make sure each step lands
+	 * before applying the next step.
+	 */
+	I915_WRITE(fence_reg, 0);
+	POSTING_READ(fence_reg);
+
+	if (obj) {
+		u32 size = i915_gem_obj_ggtt_size(obj);
+		uint64_t val;
+
+		/* Adjust fence size to match tiled area */
+		if (obj->tiling_mode != I915_TILING_NONE) {
+			uint32_t row_size = obj->stride *
+				(obj->tiling_mode == I915_TILING_Y ? 32 : 8);
+			size = (size / row_size) * row_size;
+		}
+
+		val = (uint64_t)((i915_gem_obj_ggtt_offset(obj) + size - 4096) &
+				 0xfffff000) << 32;
+		val |= i915_gem_obj_ggtt_offset(obj) & 0xfffff000;
+		val |= (uint64_t)((obj->stride / 128) - 1) << fence_pitch_shift;
+		if (obj->tiling_mode == I915_TILING_Y)
+			val |= 1 << I965_FENCE_TILING_Y_SHIFT;
+		val |= I965_FENCE_REG_VALID;
+
+		I915_WRITE(fence_reg + 4, val >> 32);
+		POSTING_READ(fence_reg + 4);
+
+		I915_WRITE(fence_reg + 0, val);
+		POSTING_READ(fence_reg);
+	} else {
+		I915_WRITE(fence_reg + 4, 0);
+		POSTING_READ(fence_reg + 4);
+	}
+}
+
+static void i915_write_fence_reg(struct drm_device *dev, int reg,
+				 struct drm_i915_gem_object *obj)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u32 val;
+
+	if (obj) {
+		u32 size = i915_gem_obj_ggtt_size(obj);
+		int pitch_val;
+		int tile_width;
+
+		WARN((i915_gem_obj_ggtt_offset(obj) & ~I915_FENCE_START_MASK) ||
+		     (size & -size) != size ||
+		     (i915_gem_obj_ggtt_offset(obj) & (size - 1)),
+		     "object 0x%08lx [fenceable? %d] not 1M or pot-size (0x%08x) aligned\n",
+		     i915_gem_obj_ggtt_offset(obj), obj->map_and_fenceable, size);
+
+		if (obj->tiling_mode == I915_TILING_Y && HAS_128_BYTE_Y_TILING(dev))
+			tile_width = 128;
+		else
+			tile_width = 512;
+
+		/* Note: pitch better be a power of two tile widths */
+		pitch_val = obj->stride / tile_width;
+		pitch_val = ffs(pitch_val) - 1;
+
+		val = i915_gem_obj_ggtt_offset(obj);
+		if (obj->tiling_mode == I915_TILING_Y)
+			val |= 1 << I830_FENCE_TILING_Y_SHIFT;
+		val |= I915_FENCE_SIZE_BITS(size);
+		val |= pitch_val << I830_FENCE_PITCH_SHIFT;
+		val |= I830_FENCE_REG_VALID;
+	} else
+		val = 0;
+
+	if (reg < 8)
+		reg = FENCE_REG_830_0 + reg * 4;
+	else
+		reg = FENCE_REG_945_8 + (reg - 8) * 4;
+
+	I915_WRITE(reg, val);
+	POSTING_READ(reg);
+}
+
+static void i830_write_fence_reg(struct drm_device *dev, int reg,
+				struct drm_i915_gem_object *obj)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	uint32_t val;
+
+	if (obj) {
+		u32 size = i915_gem_obj_ggtt_size(obj);
+		uint32_t pitch_val;
+
+		WARN((i915_gem_obj_ggtt_offset(obj) & ~I830_FENCE_START_MASK) ||
+		     (size & -size) != size ||
+		     (i915_gem_obj_ggtt_offset(obj) & (size - 1)),
+		     "object 0x%08lx not 512K or pot-size 0x%08x aligned\n",
+		     i915_gem_obj_ggtt_offset(obj), size);
+
+		pitch_val = obj->stride / 128;
+		pitch_val = ffs(pitch_val) - 1;
+
+		val = i915_gem_obj_ggtt_offset(obj);
+		if (obj->tiling_mode == I915_TILING_Y)
+			val |= 1 << I830_FENCE_TILING_Y_SHIFT;
+		val |= I830_FENCE_SIZE_BITS(size);
+		val |= pitch_val << I830_FENCE_PITCH_SHIFT;
+		val |= I830_FENCE_REG_VALID;
+	} else
+		val = 0;
+
+	I915_WRITE(FENCE_REG_830_0 + reg * 4, val);
+	POSTING_READ(FENCE_REG_830_0 + reg * 4);
+}
+
+inline static bool i915_gem_object_needs_mb(struct drm_i915_gem_object *obj)
+{
+	return obj && obj->base.read_domains & I915_GEM_DOMAIN_GTT;
+}
+
+static void i915_gem_write_fence(struct drm_device *dev, int reg,
+				 struct drm_i915_gem_object *obj)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	/* Ensure that all CPU reads are completed before installing a fence
+	 * and all writes before removing the fence.
+	 */
+	if (i915_gem_object_needs_mb(dev_priv->fence_regs[reg].obj))
+		mb();
+
+	WARN(obj && (!obj->stride || !obj->tiling_mode),
+	     "bogus fence setup with stride: 0x%x, tiling mode: %i\n",
+	     obj->stride, obj->tiling_mode);
+
+	switch (INTEL_INFO(dev)->gen) {
+	case 8:
+	case 7:
+	case 6:
+	case 5:
+	case 4: i965_write_fence_reg(dev, reg, obj); break;
+	case 3: i915_write_fence_reg(dev, reg, obj); break;
+	case 2: i830_write_fence_reg(dev, reg, obj); break;
+	default: BUG();
+	}
+
+	/* And similarly be paranoid that no direct access to this region
+	 * is reordered to before the fence is installed.
+	 */
+	if (i915_gem_object_needs_mb(obj))
+		mb();
+}
+
+static inline int fence_number(struct drm_i915_private *dev_priv,
+			       struct drm_i915_fence_reg *fence)
+{
+	return fence - dev_priv->fence_regs;
+}
+
+static void i915_gem_object_update_fence(struct drm_i915_gem_object *obj,
+					 struct drm_i915_fence_reg *fence,
+					 bool enable)
+{
+	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
+	int reg = fence_number(dev_priv, fence);
+
+	i915_gem_write_fence(obj->base.dev, reg, enable ? obj : NULL);
+
+	if (enable) {
+		obj->fence_reg = reg;
+		fence->obj = obj;
+		list_move_tail(&fence->lru_list, &dev_priv->mm.fence_list);
+	} else {
+		obj->fence_reg = I915_FENCE_REG_NONE;
+		fence->obj = NULL;
+		list_del_init(&fence->lru_list);
+	}
+	obj->fence_dirty = false;
+}
+
+static int
+i915_gem_object_wait_fence(struct drm_i915_gem_object *obj)
+{
+	if (obj->last_fenced_seqno) {
+		int ret = i915_wait_seqno(obj->ring, obj->last_fenced_seqno);
+		if (ret)
+			return ret;
+
+		obj->last_fenced_seqno = 0;
+	}
+
+	return 0;
+}
+
+int
+i915_gem_object_put_fence(struct drm_i915_gem_object *obj)
+{
+	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
+	struct drm_i915_fence_reg *fence;
+	int ret;
+
+	ret = i915_gem_object_wait_fence(obj);
+	if (ret)
+		return ret;
+
+	if (obj->fence_reg == I915_FENCE_REG_NONE)
+		return 0;
+
+	fence = &dev_priv->fence_regs[obj->fence_reg];
+
+	if (WARN_ON(fence->pin_count))
+		return -EBUSY;
+
+	i915_gem_object_fence_lost(obj);
+	i915_gem_object_update_fence(obj, fence, false);
+
+	return 0;
+}
+
+static struct drm_i915_fence_reg *
+i915_find_fence_reg(struct drm_device *dev)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_fence_reg *reg, *avail;
+	int i;
+
+	/* First try to find a free reg */
+	avail = NULL;
+	for (i = dev_priv->fence_reg_start; i < dev_priv->num_fence_regs; i++) {
+		reg = &dev_priv->fence_regs[i];
+		if (!reg->obj)
+			return reg;
+
+		if (!reg->pin_count)
+			avail = reg;
+	}
+
+	if (avail == NULL)
+		goto deadlock;
+
+	/* None available, try to steal one or wait for a user to finish */
+	list_for_each_entry(reg, &dev_priv->mm.fence_list, lru_list) {
+		if (reg->pin_count)
+			continue;
+
+		return reg;
+	}
+
+deadlock:
+	/* Wait for completion of pending flips which consume fences */
+	if (intel_has_pending_fb_unpin(dev))
+		return ERR_PTR(-EAGAIN);
+
+	return ERR_PTR(-EDEADLK);
+}
+
+/**
+ * i915_gem_object_get_fence - set up fencing for an object
+ * @obj: object to map through a fence reg
+ *
+ * When mapping objects through the GTT, userspace wants to be able to write
+ * to them without having to worry about swizzling if the object is tiled.
+ * This function walks the fence regs looking for a free one for @obj,
+ * stealing one if it can't find any.
+ *
+ * It then sets up the reg based on the object's properties: address, pitch
+ * and tiling format.
+ *
+ * For an untiled surface, this removes any existing fence.
+ */
+int
+i915_gem_object_get_fence(struct drm_i915_gem_object *obj)
+{
+	struct drm_device *dev = obj->base.dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	bool enable = obj->tiling_mode != I915_TILING_NONE;
+	struct drm_i915_fence_reg *reg;
+	int ret;
+
+	/* Have we updated the tiling parameters upon the object and so
+	 * will need to serialise the write to the associated fence register?
+	 */
+	if (obj->fence_dirty) {
+		ret = i915_gem_object_wait_fence(obj);
+		if (ret)
+			return ret;
+	}
+
+	/* Just update our place in the LRU if our fence is getting reused. */
+	if (obj->fence_reg != I915_FENCE_REG_NONE) {
+		reg = &dev_priv->fence_regs[obj->fence_reg];
+		if (!obj->fence_dirty) {
+			list_move_tail(&reg->lru_list,
+				       &dev_priv->mm.fence_list);
+			return 0;
+		}
+	} else if (enable) {
+		if (WARN_ON(!obj->map_and_fenceable))
+			return -EINVAL;
+
+		reg = i915_find_fence_reg(dev);
+		if (IS_ERR(reg))
+			return PTR_ERR(reg);
+
+		if (reg->obj) {
+			struct drm_i915_gem_object *old = reg->obj;
+
+			ret = i915_gem_object_wait_fence(old);
+			if (ret)
+				return ret;
+
+			i915_gem_object_fence_lost(old);
+		}
+	} else
+		return 0;
+
+	i915_gem_object_update_fence(obj, reg, enable);
+
+	return 0;
+}
+
+static bool i915_gem_valid_gtt_space(struct i915_vma *vma,
+				     unsigned long cache_level)
+{
+	struct drm_mm_node *gtt_space = &vma->node;
+	struct drm_mm_node *other;
+
+	/*
+	 * On some machines we have to be careful when putting differing types
+	 * of snoopable memory together to avoid the prefetcher crossing memory
+	 * domains and dying. During vm initialisation, we decide whether or not
+	 * these constraints apply and set the drm_mm.color_adjust
+	 * appropriately.
+	 */
 	if (vma->vm->mm.color_adjust == NULL)
 		return true;
 
@@ -3411,87 +3391,92 @@ static bool i915_gem_valid_gtt_space(struct i915_vma *vma,
 	return true;
 }
 
+static void i915_gem_verify_gtt(struct drm_device *dev)
+{
+#if WATCH_GTT
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_object *obj;
+	int err = 0;
+
+	list_for_each_entry(obj, &dev_priv->mm.gtt_list, global_list) {
+		if (obj->gtt_space == NULL) {
+			printk(KERN_ERR "object found on GTT list with no space reserved\n");
+			err++;
+			continue;
+		}
+
+		if (obj->cache_level != obj->gtt_space->color) {
+			printk(KERN_ERR "object reserved space [%08lx, %08lx] with wrong color, cache_level=%x, color=%lx\n",
+			       i915_gem_obj_ggtt_offset(obj),
+			       i915_gem_obj_ggtt_offset(obj) + i915_gem_obj_ggtt_size(obj),
+			       obj->cache_level,
+			       obj->gtt_space->color);
+			err++;
+			continue;
+		}
+
+		if (!i915_gem_valid_gtt_space(dev,
+					      obj->gtt_space,
+					      obj->cache_level)) {
+			printk(KERN_ERR "invalid GTT space found at [%08lx, %08lx] - color=%x\n",
+			       i915_gem_obj_ggtt_offset(obj),
+			       i915_gem_obj_ggtt_offset(obj) + i915_gem_obj_ggtt_size(obj),
+			       obj->cache_level);
+			err++;
+			continue;
+		}
+	}
+
+	WARN_ON(err);
+#endif
+}
+
 /**
- * Finds free space in the GTT aperture and binds the object or a view of it
- * there.
+ * Finds free space in the GTT aperture and binds the object there.
  */
 static struct i915_vma *
 i915_gem_object_bind_to_vm(struct drm_i915_gem_object *obj,
 			   struct i915_address_space *vm,
-			   const struct i915_ggtt_view *ggtt_view,
 			   unsigned alignment,
 			   uint64_t flags)
 {
 	struct drm_device *dev = obj->base.dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	u32 fence_alignment, unfenced_alignment;
-	u32 search_flag, alloc_flag;
-	u64 start, end;
-	u64 size, fence_size;
+	u32 size, fence_size, fence_alignment, unfenced_alignment;
+	unsigned long start =
+		flags & PIN_OFFSET_BIAS ? flags & PIN_OFFSET_MASK : 0;
+	unsigned long end =
+		flags & PIN_MAPPABLE ? dev_priv->gtt.mappable_end : vm->total;
 	struct i915_vma *vma;
 	int ret;
 
-	if (i915_is_ggtt(vm)) {
-		u32 view_size;
-
-		if (WARN_ON(!ggtt_view))
-			return ERR_PTR(-EINVAL);
-
-		view_size = i915_ggtt_view_size(obj, ggtt_view);
-
-		fence_size = i915_gem_get_gtt_size(dev,
-						   view_size,
-						   obj->tiling_mode);
-		fence_alignment = i915_gem_get_gtt_alignment(dev,
-							     view_size,
-							     obj->tiling_mode,
-							     true);
-		unfenced_alignment = i915_gem_get_gtt_alignment(dev,
-								view_size,
-								obj->tiling_mode,
-								false);
-		size = flags & PIN_MAPPABLE ? fence_size : view_size;
-	} else {
-		fence_size = i915_gem_get_gtt_size(dev,
-						   obj->base.size,
-						   obj->tiling_mode);
-		fence_alignment = i915_gem_get_gtt_alignment(dev,
-							     obj->base.size,
-							     obj->tiling_mode,
-							     true);
-		unfenced_alignment =
-			i915_gem_get_gtt_alignment(dev,
-						   obj->base.size,
-						   obj->tiling_mode,
-						   false);
-		size = flags & PIN_MAPPABLE ? fence_size : obj->base.size;
-	}
-
-	start = flags & PIN_OFFSET_BIAS ? flags & PIN_OFFSET_MASK : 0;
-	end = vm->total;
-	if (flags & PIN_MAPPABLE)
-		end = min_t(u64, end, dev_priv->gtt.mappable_end);
-	if (flags & PIN_ZONE_4G)
-		end = min_t(u64, end, (1ULL << 32));
+	fence_size = i915_gem_get_gtt_size(dev,
+					   obj->base.size,
+					   obj->tiling_mode);
+	fence_alignment = i915_gem_get_gtt_alignment(dev,
+						     obj->base.size,
+						     obj->tiling_mode, true);
+	unfenced_alignment =
+		i915_gem_get_gtt_alignment(dev,
+					   obj->base.size,
+					   obj->tiling_mode, false);
 
 	if (alignment == 0)
 		alignment = flags & PIN_MAPPABLE ? fence_alignment :
 						unfenced_alignment;
 	if (flags & PIN_MAPPABLE && alignment & (fence_alignment - 1)) {
-		DRM_DEBUG("Invalid object (view type=%u) alignment requested %u\n",
-			  ggtt_view ? ggtt_view->type : 0,
-			  alignment);
+		DRM_DEBUG("Invalid object alignment requested %u\n", alignment);
 		return ERR_PTR(-EINVAL);
 	}
 
-	/* If binding the object/GGTT view requires more space than the entire
-	 * aperture has, reject it early before evicting everything in a vain
-	 * attempt to find space.
+	size = flags & PIN_MAPPABLE ? fence_size : obj->base.size;
+
+	/* If the object is bigger than the entire aperture, reject it early
+	 * before evicting everything in a vain attempt to find space.
 	 */
-	if (size > end) {
-		DRM_DEBUG("Attempting to bind an object (view type=%u) larger than the aperture: size=%llu > %s aperture=%llu\n",
-			  ggtt_view ? ggtt_view->type : 0,
-			  size,
+	if (obj->base.size > end) {
+		DRM_DEBUG("Attempting to bind an object larger than the aperture: object=%zd > %s aperture=%lu\n",
+			  obj->base.size,
 			  flags & PIN_MAPPABLE ? "mappable" : "total",
 			  end);
 		return ERR_PTR(-E2BIG);
@@ -3503,27 +3488,17 @@ i915_gem_object_bind_to_vm(struct drm_i915_gem_object *obj,
 
 	i915_gem_object_pin_pages(obj);
 
-	vma = ggtt_view ? i915_gem_obj_lookup_or_create_ggtt_vma(obj, ggtt_view) :
-			  i915_gem_obj_lookup_or_create_vma(obj, vm);
-
+	vma = i915_gem_obj_lookup_or_create_vma(obj, vm);
 	if (IS_ERR(vma))
 		goto err_unpin;
 
-	if (flags & PIN_HIGH) {
-		search_flag = DRM_MM_SEARCH_BELOW;
-		alloc_flag = DRM_MM_CREATE_TOP;
-	} else {
-		search_flag = DRM_MM_SEARCH_DEFAULT;
-		alloc_flag = DRM_MM_CREATE_DEFAULT;
-	}
-
 search_free:
 	ret = drm_mm_insert_node_in_range_generic(&vm->mm, &vma->node,
 						  size, alignment,
 						  obj->cache_level,
 						  start, end,
-						  search_flag,
-						  alloc_flag);
+						  DRM_MM_SEARCH_DEFAULT,
+						  DRM_MM_CREATE_DEFAULT);
 	if (ret) {
 		ret = i915_gem_evict_something(dev, vm, size, alignment,
 					       obj->cache_level,
@@ -3539,14 +3514,32 @@ search_free:
 		goto err_remove_node;
 	}
 
-	trace_i915_vma_bind(vma, flags);
-	ret = i915_vma_bind(vma, obj->cache_level, flags);
+	ret = i915_gem_gtt_prepare_object(obj);
 	if (ret)
 		goto err_remove_node;
 
 	list_move_tail(&obj->global_list, &dev_priv->mm.bound_list);
 	list_add_tail(&vma->mm_list, &vm->inactive_list);
 
+	if (i915_is_ggtt(vm)) {
+		bool mappable, fenceable;
+
+		fenceable = (vma->node.size == fence_size &&
+			     (vma->node.start & (fence_alignment - 1)) == 0);
+
+		mappable = (vma->node.start + obj->base.size <=
+			    dev_priv->gtt.mappable_end);
+
+		obj->map_and_fenceable = mappable && fenceable;
+	}
+
+	WARN_ON(flags & PIN_MAPPABLE && !obj->map_and_fenceable);
+
+	trace_i915_vma_bind(vma, flags);
+	vma->bind_vma(vma, obj->cache_level,
+		      flags & (PIN_MAPPABLE | PIN_GLOBAL) ? GLOBAL_BIND : 0);
+
+	i915_gem_verify_gtt(dev);
 	return vma;
 
 err_remove_node:
@@ -3574,7 +3567,7 @@ i915_gem_clflush_object(struct drm_i915_gem_object *obj,
 	 * Stolen memory is always coherent with the GPU as it is explicitly
 	 * marked as wc by the system, or the system is cache-coherent.
 	 */
-	if (obj->stolen || obj->phys_handle)
+	if (obj->stolen)
 		return false;
 
 	/* If the GPU is snooping the contents of the CPU cache,
@@ -3585,14 +3578,11 @@ i915_gem_clflush_object(struct drm_i915_gem_object *obj,
 	 * snooping behaviour occurs naturally as the result of our domain
 	 * tracking.
 	 */
-	if (!force && cpu_cache_is_coherent(obj->base.dev, obj->cache_level)) {
-		obj->cache_dirty = true;
+	if (!force && cpu_cache_is_coherent(obj->base.dev, obj->cache_level))
 		return false;
-	}
 
 	trace_i915_gem_object_clflush(obj);
 	drm_clflush_sg(obj->pages);
-	obj->cache_dirty = false;
 
 	return true;
 }
@@ -3619,7 +3609,7 @@ i915_gem_object_flush_gtt_write_domain(struct drm_i915_gem_object *obj)
 	old_write_domain = obj->base.write_domain;
 	obj->base.write_domain = 0;
 
-	intel_fb_obj_flush(obj, false, ORIGIN_GTT);
+	intel_fb_obj_flush(obj, false);
 
 	trace_i915_gem_object_change_domain(obj,
 					    obj->base.read_domains,
@@ -3628,20 +3618,21 @@ i915_gem_object_flush_gtt_write_domain(struct drm_i915_gem_object *obj)
 
 /** Flushes the CPU write domain for the object if it's dirty. */
 static void
-i915_gem_object_flush_cpu_write_domain(struct drm_i915_gem_object *obj)
+i915_gem_object_flush_cpu_write_domain(struct drm_i915_gem_object *obj,
+				       bool force)
 {
 	uint32_t old_write_domain;
 
 	if (obj->base.write_domain != I915_GEM_DOMAIN_CPU)
 		return;
 
-	if (i915_gem_clflush_object(obj, obj->pin_display))
+	if (i915_gem_clflush_object(obj, force))
 		i915_gem_chipset_flush(obj->base.dev);
 
 	old_write_domain = obj->base.write_domain;
 	obj->base.write_domain = 0;
 
-	intel_fb_obj_flush(obj, false, ORIGIN_CPU);
+	intel_fb_obj_flush(obj, false);
 
 	trace_i915_gem_object_change_domain(obj,
 					    obj->base.read_domains,
@@ -3657,10 +3648,15 @@ i915_gem_object_flush_cpu_write_domain(struct drm_i915_gem_object *obj)
 int
 i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj, bool write)
 {
+	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
+	struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
 	uint32_t old_write_domain, old_read_domains;
-	struct i915_vma *vma;
 	int ret;
 
+	/* Not valid to be called on unbound objects. */
+	if (vma == NULL)
+		return -EINVAL;
+
 	if (obj->base.write_domain == I915_GEM_DOMAIN_GTT)
 		return 0;
 
@@ -3668,19 +3664,8 @@ i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj, bool write)
 	if (ret)
 		return ret;
 
-	/* Flush and acquire obj->pages so that we are coherent through
-	 * direct access in memory with previous cached writes through
-	 * shmemfs and that our cache domain tracking remains valid.
-	 * For example, if the obj->filp was moved to swap without us
-	 * being notified and releasing the pages, we would mistakenly
-	 * continue to assume that the obj remained out of the CPU cached
-	 * domain.
-	 */
-	ret = i915_gem_object_get_pages(obj);
-	if (ret)
-		return ret;
-
-	i915_gem_object_flush_cpu_write_domain(obj);
+	i915_gem_object_retire(obj);
+	i915_gem_object_flush_cpu_write_domain(obj, false);
 
 	/* Serialise direct access to this object with the barriers for
 	 * coherent writes from the GPU, by effectively invalidating the
@@ -3703,137 +3688,95 @@ i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj, bool write)
 		obj->dirty = 1;
 	}
 
+	if (write)
+		intel_fb_obj_invalidate(obj, NULL);
+
 	trace_i915_gem_object_change_domain(obj,
 					    old_read_domains,
 					    old_write_domain);
 
 	/* And bump the LRU for this access */
-	vma = i915_gem_obj_to_ggtt(obj);
-	if (vma && drm_mm_node_allocated(&vma->node) && !obj->active)
+	if (i915_gem_object_is_inactive(obj))
 		list_move_tail(&vma->mm_list,
-			       &to_i915(obj->base.dev)->gtt.base.inactive_list);
+			       &dev_priv->gtt.base.inactive_list);
 
 	return 0;
 }
 
-/**
- * Changes the cache-level of an object across all VMA.
- *
- * After this function returns, the object will be in the new cache-level
- * across all GTT and the contents of the backing storage will be coherent,
- * with respect to the new cache-level. In order to keep the backing storage
- * coherent for all users, we only allow a single cache level to be set
- * globally on the object and prevent it from being changed whilst the
- * hardware is reading from the object. That is if the object is currently
- * on the scanout it will be set to uncached (or equivalent display
- * cache coherency) and all non-MOCS GPU access will also be uncached so
- * that all direct access to the scanout remains coherent.
- */
 int i915_gem_object_set_cache_level(struct drm_i915_gem_object *obj,
 				    enum i915_cache_level cache_level)
 {
 	struct drm_device *dev = obj->base.dev;
 	struct i915_vma *vma, *next;
-	bool bound = false;
-	int ret = 0;
+	int ret;
 
 	if (obj->cache_level == cache_level)
-		goto out;
-
-	/* Inspect the list of currently bound VMA and unbind any that would
-	 * be invalid given the new cache-level. This is principally to
-	 * catch the issue of the CS prefetch crossing page boundaries and
-	 * reading an invalid PTE on older architectures.
-	 */
-	list_for_each_entry_safe(vma, next, &obj->vma_list, vma_link) {
-		if (!drm_mm_node_allocated(&vma->node))
-			continue;
+		return 0;
 
-		if (vma->pin_count) {
-			DRM_DEBUG("can not change the cache level of pinned objects\n");
-			return -EBUSY;
-		}
+	if (i915_gem_obj_is_pinned(obj)) {
+		DRM_DEBUG("can not change the cache level of pinned objects\n");
+		return -EBUSY;
+	}
 
+	list_for_each_entry_safe(vma, next, &obj->vma_list, vma_link) {
 		if (!i915_gem_valid_gtt_space(vma, cache_level)) {
 			ret = i915_vma_unbind(vma);
 			if (ret)
 				return ret;
-		} else
-			bound = true;
+		}
 	}
 
-	/* We can reuse the existing drm_mm nodes but need to change the
-	 * cache-level on the PTE. We could simply unbind them all and
-	 * rebind with the correct cache-level on next use. However since
-	 * we already have a valid slot, dma mapping, pages etc, we may as
-	 * rewrite the PTE in the belief that doing so tramples upon less
-	 * state and so involves less work.
-	 */
-	if (bound) {
-		/* Before we change the PTE, the GPU must not be accessing it.
-		 * If we wait upon the object, we know that all the bound
-		 * VMA are no longer active.
-		 */
-		ret = i915_gem_object_wait_rendering(obj, false);
+	if (i915_gem_obj_bound_any(obj)) {
+		ret = i915_gem_object_finish_gpu(obj);
 		if (ret)
 			return ret;
 
-		if (!HAS_LLC(dev) && cache_level != I915_CACHE_NONE) {
-			/* Access to snoopable pages through the GTT is
-			 * incoherent and on some machines causes a hard
-			 * lockup. Relinquish the CPU mmaping to force
-			 * userspace to refault in the pages and we can
-			 * then double check if the GTT mapping is still
-			 * valid for that pointer access.
-			 */
-			i915_gem_release_mmap(obj);
-
-			/* As we no longer need a fence for GTT access,
-			 * we can relinquish it now (and so prevent having
-			 * to steal a fence from someone else on the next
-			 * fence request). Note GPU activity would have
-			 * dropped the fence as all snoopable access is
-			 * supposed to be linear.
-			 */
+		i915_gem_object_finish_gtt(obj);
+
+		/* Before SandyBridge, you could not use tiling or fence
+		 * registers with snooped memory, so relinquish any fences
+		 * currently pointing to our region in the aperture.
+		 */
+		if (INTEL_INFO(dev)->gen < 6) {
 			ret = i915_gem_object_put_fence(obj);
 			if (ret)
 				return ret;
-		} else {
-			/* We either have incoherent backing store and
-			 * so no GTT access or the architecture is fully
-			 * coherent. In such cases, existing GTT mmaps
-			 * ignore the cache bit in the PTE and we can
-			 * rewrite it without confusing the GPU or having
-			 * to force userspace to fault back in its mmaps.
-			 */
 		}
 
-		list_for_each_entry(vma, &obj->vma_list, vma_link) {
-			if (!drm_mm_node_allocated(&vma->node))
-				continue;
-
-			ret = i915_vma_bind(vma, cache_level, PIN_UPDATE);
-			if (ret)
-				return ret;
-		}
+		list_for_each_entry(vma, &obj->vma_list, vma_link)
+			if (drm_mm_node_allocated(&vma->node))
+				vma->bind_vma(vma, cache_level,
+					      obj->has_global_gtt_mapping ? GLOBAL_BIND : 0);
 	}
 
 	list_for_each_entry(vma, &obj->vma_list, vma_link)
 		vma->node.color = cache_level;
 	obj->cache_level = cache_level;
 
-out:
-	/* Flush the dirty CPU caches to the backing storage so that the
-	 * object is now coherent at its new cache level (with respect
-	 * to the access domain).
-	 */
-	if (obj->cache_dirty &&
-	    obj->base.write_domain != I915_GEM_DOMAIN_CPU &&
-	    cpu_write_needs_clflush(obj)) {
-		if (i915_gem_clflush_object(obj, true))
-			i915_gem_chipset_flush(obj->base.dev);
+	if (cpu_write_needs_clflush(obj)) {
+		u32 old_read_domains, old_write_domain;
+
+		/* If we're coming from LLC cached, then we haven't
+		 * actually been tracking whether the data is in the
+		 * CPU cache or not, since we only allow one bit set
+		 * in obj->write_domain and have been skipping the clflushes.
+		 * Just set it to the CPU cache for now.
+		 */
+		i915_gem_object_retire(obj);
+		WARN_ON(obj->base.write_domain & ~I915_GEM_DOMAIN_CPU);
+
+		old_read_domains = obj->base.read_domains;
+		old_write_domain = obj->base.write_domain;
+
+		obj->base.read_domains = I915_GEM_DOMAIN_CPU;
+		obj->base.write_domain = I915_GEM_DOMAIN_CPU;
+
+		trace_i915_gem_object_change_domain(obj,
+						    old_read_domains,
+						    old_write_domain);
 	}
 
+	i915_gem_verify_gtt(dev);
 	return 0;
 }
 
@@ -3842,10 +3785,17 @@ int i915_gem_get_caching_ioctl(struct drm_device *dev, void *data,
 {
 	struct drm_i915_gem_caching *args = data;
 	struct drm_i915_gem_object *obj;
+	int ret;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
 
 	obj = to_intel_bo(drm_gem_object_lookup(dev, file, args->handle));
-	if (&obj->base == NULL)
-		return -ENOENT;
+	if (&obj->base == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
 
 	switch (obj->cache_level) {
 	case I915_CACHE_LLC:
@@ -3862,14 +3812,15 @@ int i915_gem_get_caching_ioctl(struct drm_device *dev, void *data,
 		break;
 	}
 
-	drm_gem_object_unreference_unlocked(&obj->base);
-	return 0;
+	drm_gem_object_unreference(&obj->base);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
 }
 
 int i915_gem_set_caching_ioctl(struct drm_device *dev, void *data,
 			       struct drm_file *file)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_caching *args = data;
 	struct drm_i915_gem_object *obj;
 	enum i915_cache_level level;
@@ -3880,15 +3831,6 @@ int i915_gem_set_caching_ioctl(struct drm_device *dev, void *data,
 		level = I915_CACHE_NONE;
 		break;
 	case I915_CACHING_CACHED:
-		/*
-		 * Due to a HW issue on BXT A stepping, GPU stores via a
-		 * snooped mapping may leave stale data in a corresponding CPU
-		 * cacheline, whereas normally such cachelines would get
-		 * invalidated.
-		 */
-		if (IS_BROXTON(dev) && INTEL_REVID(dev) < BXT_REVID_B0)
-			return -ENODEV;
-
 		level = I915_CACHE_LLC;
 		break;
 	case I915_CACHING_DISPLAY:
@@ -3898,11 +3840,9 @@ int i915_gem_set_caching_ioctl(struct drm_device *dev, void *data,
 		return -EINVAL;
 	}
 
-	intel_runtime_pm_get(dev_priv);
-
 	ret = i915_mutex_lock_interruptible(dev);
 	if (ret)
-		goto rpm_put;
+		return ret;
 
 	obj = to_intel_bo(drm_gem_object_lookup(dev, file, args->handle));
 	if (&obj->base == NULL) {
@@ -3915,12 +3855,31 @@ int i915_gem_set_caching_ioctl(struct drm_device *dev, void *data,
 	drm_gem_object_unreference(&obj->base);
 unlock:
 	mutex_unlock(&dev->struct_mutex);
-rpm_put:
-	intel_runtime_pm_put(dev_priv);
-
 	return ret;
 }
 
+static bool is_pin_display(struct drm_i915_gem_object *obj)
+{
+	struct i915_vma *vma;
+
+	vma = i915_gem_obj_to_ggtt(obj);
+	if (!vma)
+		return false;
+
+	/* There are 3 sources that pin objects:
+	 *   1. The display engine (scanouts, sprites, cursors);
+	 *   2. Reservations for execbuffer;
+	 *   3. The user.
+	 *
+	 * We can ignore reservations as we hold the struct_mutex and
+	 * are only called outside of the reservation path.  The user
+	 * can only increment pin_count once, and so if after
+	 * subtracting the potential reference by the user, any pin_count
+	 * remains, it must be due to another use by the display engine.
+	 */
+	return vma->pin_count - !!obj->user_pin_count;
+}
+
 /*
  * Prepare buffer for display plane (scanout, cursors, etc).
  * Can be called from an uninterruptible phase (modesetting) and allows
@@ -3929,21 +3888,23 @@ rpm_put:
 int
 i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
 				     u32 alignment,
-				     struct intel_engine_cs *pipelined,
-				     struct drm_i915_gem_request **pipelined_request,
-				     const struct i915_ggtt_view *view)
+				     struct intel_engine_cs *pipelined)
 {
 	u32 old_read_domains, old_write_domain;
+	bool was_pin_display;
 	int ret;
 
-	ret = i915_gem_object_sync(obj, pipelined, pipelined_request);
-	if (ret)
-		return ret;
+	if (pipelined != obj->ring) {
+		ret = i915_gem_object_sync(obj, pipelined);
+		if (ret)
+			return ret;
+	}
 
 	/* Mark the pin_display early so that we account for the
 	 * display coherency whilst setting up the cache domains.
 	 */
-	obj->pin_display++;
+	was_pin_display = obj->pin_display;
+	obj->pin_display = true;
 
 	/* The display engine is not coherent with the LLC cache on gen6.  As
 	 * a result, we make sure that the pinning that is about to occur is
@@ -3963,13 +3924,11 @@ i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
 	 * (e.g. libkms for the bootup splash), we have to ensure that we
 	 * always use map_and_fenceable for all scanout buffers.
 	 */
-	ret = i915_gem_object_ggtt_pin(obj, view, alignment,
-				       view->type == I915_GGTT_VIEW_NORMAL ?
-				       PIN_MAPPABLE : 0);
+	ret = i915_gem_obj_ggtt_pin(obj, alignment, PIN_MAPPABLE);
 	if (ret)
 		goto err_unpin_display;
 
-	i915_gem_object_flush_cpu_write_domain(obj);
+	i915_gem_object_flush_cpu_write_domain(obj, true);
 
 	old_write_domain = obj->base.write_domain;
 	old_read_domains = obj->base.read_domains;
@@ -3987,20 +3946,33 @@ i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
 	return 0;
 
 err_unpin_display:
-	obj->pin_display--;
+	WARN_ON(was_pin_display != is_pin_display(obj));
+	obj->pin_display = was_pin_display;
 	return ret;
 }
 
 void
-i915_gem_object_unpin_from_display_plane(struct drm_i915_gem_object *obj,
-					 const struct i915_ggtt_view *view)
+i915_gem_object_unpin_from_display_plane(struct drm_i915_gem_object *obj)
 {
-	if (WARN_ON(obj->pin_display == 0))
-		return;
+	i915_gem_object_ggtt_unpin(obj);
+	obj->pin_display = is_pin_display(obj);
+}
 
-	i915_gem_object_ggtt_unpin_view(obj, view);
+int
+i915_gem_object_finish_gpu(struct drm_i915_gem_object *obj)
+{
+	int ret;
+
+	if ((obj->base.read_domains & I915_GEM_GPU_DOMAINS) == 0)
+		return 0;
 
-	obj->pin_display--;
+	ret = i915_gem_object_wait_rendering(obj, false);
+	if (ret)
+		return ret;
+
+	/* Ensure that we invalidate the GPU's caches and TLBs. */
+	obj->base.read_domains &= ~I915_GEM_GPU_DOMAINS;
+	return 0;
 }
 
 /**
@@ -4022,6 +3994,7 @@ i915_gem_object_set_to_cpu_domain(struct drm_i915_gem_object *obj, bool write)
 	if (ret)
 		return ret;
 
+	i915_gem_object_retire(obj);
 	i915_gem_object_flush_gtt_write_domain(obj);
 
 	old_write_domain = obj->base.write_domain;
@@ -4047,6 +4020,9 @@ i915_gem_object_set_to_cpu_domain(struct drm_i915_gem_object *obj, bool write)
 		obj->base.write_domain = I915_GEM_DOMAIN_CPU;
 	}
 
+	if (write)
+		intel_fb_obj_invalidate(obj, NULL);
+
 	trace_i915_gem_object_change_domain(obj,
 					    old_read_domains,
 					    old_write_domain);
@@ -4069,9 +4045,11 @@ i915_gem_ring_throttle(struct drm_device *dev, struct drm_file *file)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_file_private *file_priv = file->driver_priv;
-	unsigned long recent_enough = jiffies - DRM_I915_THROTTLE_JIFFIES;
-	struct drm_i915_gem_request *request, *target = NULL;
+	unsigned long recent_enough = jiffies - msecs_to_jiffies(20);
+	struct drm_i915_gem_request *request;
+	struct intel_engine_cs *ring = NULL;
 	unsigned reset_counter;
+	u32 seqno = 0;
 	int ret;
 
 	ret = i915_gem_wait_for_error(&dev_priv->gpu_error);
@@ -4087,29 +4065,19 @@ i915_gem_ring_throttle(struct drm_device *dev, struct drm_file *file)
 		if (time_after_eq(request->emitted_jiffies, recent_enough))
 			break;
 
-		/*
-		 * Note that the request might not have been submitted yet.
-		 * In which case emitted_jiffies will be zero.
-		 */
-		if (!request->emitted_jiffies)
-			continue;
-
-		target = request;
+		ring = request->ring;
+		seqno = request->seqno;
 	}
 	reset_counter = atomic_read(&dev_priv->gpu_error.reset_counter);
-	if (target)
-		i915_gem_request_reference(target);
 	spin_unlock(&file_priv->mm.lock);
 
-	if (target == NULL)
+	if (seqno == 0)
 		return 0;
 
-	ret = __i915_wait_request(target, reset_counter, true, NULL, NULL);
+	ret = __wait_seqno(ring, seqno, reset_counter, true, NULL, NULL);
 	if (ret == 0)
 		queue_delayed_work(dev_priv->wq, &dev_priv->mm.retire_work, 0);
 
-	i915_gem_request_unreference__unlocked(target);
-
 	return ret;
 }
 
@@ -4132,39 +4100,14 @@ i915_vma_misplaced(struct i915_vma *vma, uint32_t alignment, uint64_t flags)
 	return false;
 }
 
-void __i915_vma_set_map_and_fenceable(struct i915_vma *vma)
-{
-	struct drm_i915_gem_object *obj = vma->obj;
-	bool mappable, fenceable;
-	u32 fence_size, fence_alignment;
-
-	fence_size = i915_gem_get_gtt_size(obj->base.dev,
-					   obj->base.size,
-					   obj->tiling_mode);
-	fence_alignment = i915_gem_get_gtt_alignment(obj->base.dev,
-						     obj->base.size,
-						     obj->tiling_mode,
-						     true);
-
-	fenceable = (vma->node.size == fence_size &&
-		     (vma->node.start & (fence_alignment - 1)) == 0);
-
-	mappable = (vma->node.start + fence_size <=
-		    to_i915(obj->base.dev)->gtt.mappable_end);
-
-	obj->map_and_fenceable = mappable && fenceable;
-}
-
-static int
-i915_gem_object_do_pin(struct drm_i915_gem_object *obj,
-		       struct i915_address_space *vm,
-		       const struct i915_ggtt_view *ggtt_view,
-		       uint32_t alignment,
-		       uint64_t flags)
+int
+i915_gem_object_pin(struct drm_i915_gem_object *obj,
+		    struct i915_address_space *vm,
+		    uint32_t alignment,
+		    uint64_t flags)
 {
 	struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
 	struct i915_vma *vma;
-	unsigned bound;
 	int ret;
 
 	if (WARN_ON(vm == &dev_priv->mm.aliasing_ppgtt->base))
@@ -4173,31 +4116,17 @@ i915_gem_object_do_pin(struct drm_i915_gem_object *obj,
 	if (WARN_ON(flags & (PIN_GLOBAL | PIN_MAPPABLE) && !i915_is_ggtt(vm)))
 		return -EINVAL;
 
-	if (WARN_ON((flags & (PIN_MAPPABLE | PIN_GLOBAL)) == PIN_MAPPABLE))
-		return -EINVAL;
-
-	if (WARN_ON(i915_is_ggtt(vm) != !!ggtt_view))
-		return -EINVAL;
-
-	vma = ggtt_view ? i915_gem_obj_to_ggtt_view(obj, ggtt_view) :
-			  i915_gem_obj_to_vma(obj, vm);
-
-	if (IS_ERR(vma))
-		return PTR_ERR(vma);
-
+	vma = i915_gem_obj_to_vma(obj, vm);
 	if (vma) {
 		if (WARN_ON(vma->pin_count == DRM_I915_GEM_OBJECT_MAX_PIN_COUNT))
 			return -EBUSY;
 
 		if (i915_vma_misplaced(vma, alignment, flags)) {
 			WARN(vma->pin_count,
-			     "bo is already pinned in %s with incorrect alignment:"
-			     " offset=%08x %08x, req.alignment=%x, req.map_and_fenceable=%d,"
+			     "bo is already pinned with incorrect alignment:"
+			     " offset=%lx, req.alignment=%x, req.map_and_fenceable=%d,"
 			     " obj->map_and_fenceable=%d\n",
-			     ggtt_view ? "ggtt" : "ppgtt",
-			     upper_32_bits(vma->node.start),
-			     lower_32_bits(vma->node.start),
-			     alignment,
+			     i915_gem_obj_offset(obj, vm), alignment,
 			     !!(flags & PIN_MAPPABLE),
 			     obj->map_and_fenceable);
 			ret = i915_vma_unbind(vma);
@@ -4208,63 +4137,155 @@ i915_gem_object_do_pin(struct drm_i915_gem_object *obj,
 		}
 	}
 
-	bound = vma ? vma->bound : 0;
 	if (vma == NULL || !drm_mm_node_allocated(&vma->node)) {
-		vma = i915_gem_object_bind_to_vm(obj, vm, ggtt_view, alignment,
-						 flags);
+		vma = i915_gem_object_bind_to_vm(obj, vm, alignment, flags);
 		if (IS_ERR(vma))
 			return PTR_ERR(vma);
-	} else {
-		ret = i915_vma_bind(vma, obj->cache_level, flags);
-		if (ret)
-			return ret;
 	}
 
-	if (ggtt_view && ggtt_view->type == I915_GGTT_VIEW_NORMAL &&
-	    (bound ^ vma->bound) & GLOBAL_BIND) {
-		__i915_vma_set_map_and_fenceable(vma);
-		WARN_ON(flags & PIN_MAPPABLE && !obj->map_and_fenceable);
-	}
+	if (flags & PIN_GLOBAL && !obj->has_global_gtt_mapping)
+		vma->bind_vma(vma, obj->cache_level, GLOBAL_BIND);
 
 	vma->pin_count++;
+	if (flags & PIN_MAPPABLE)
+		obj->pin_mappable |= true;
+
 	return 0;
 }
 
-int
-i915_gem_object_pin(struct drm_i915_gem_object *obj,
-		    struct i915_address_space *vm,
-		    uint32_t alignment,
-		    uint64_t flags)
+void
+i915_gem_object_ggtt_unpin(struct drm_i915_gem_object *obj)
 {
-	return i915_gem_object_do_pin(obj, vm,
-				      i915_is_ggtt(vm) ? &i915_ggtt_view_normal : NULL,
-				      alignment, flags);
+	struct i915_vma *vma = i915_gem_obj_to_ggtt(obj);
+
+	BUG_ON(!vma);
+	BUG_ON(vma->pin_count == 0);
+	BUG_ON(!i915_gem_obj_ggtt_bound(obj));
+
+	if (--vma->pin_count == 0)
+		obj->pin_mappable = false;
 }
 
-int
-i915_gem_object_ggtt_pin(struct drm_i915_gem_object *obj,
-			 const struct i915_ggtt_view *view,
-			 uint32_t alignment,
-			 uint64_t flags)
+bool
+i915_gem_object_pin_fence(struct drm_i915_gem_object *obj)
 {
-	if (WARN_ONCE(!view, "no view specified"))
-		return -EINVAL;
+	if (obj->fence_reg != I915_FENCE_REG_NONE) {
+		struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
+		struct i915_vma *ggtt_vma = i915_gem_obj_to_ggtt(obj);
 
-	return i915_gem_object_do_pin(obj, i915_obj_to_ggtt(obj), view,
-				      alignment, flags | PIN_GLOBAL);
+		WARN_ON(!ggtt_vma ||
+			dev_priv->fence_regs[obj->fence_reg].pin_count >
+			ggtt_vma->pin_count);
+		dev_priv->fence_regs[obj->fence_reg].pin_count++;
+		return true;
+	} else
+		return false;
 }
 
 void
-i915_gem_object_ggtt_unpin_view(struct drm_i915_gem_object *obj,
-				const struct i915_ggtt_view *view)
+i915_gem_object_unpin_fence(struct drm_i915_gem_object *obj)
 {
-	struct i915_vma *vma = i915_gem_obj_to_ggtt_view(obj, view);
+	if (obj->fence_reg != I915_FENCE_REG_NONE) {
+		struct drm_i915_private *dev_priv = obj->base.dev->dev_private;
+		WARN_ON(dev_priv->fence_regs[obj->fence_reg].pin_count <= 0);
+		dev_priv->fence_regs[obj->fence_reg].pin_count--;
+	}
+}
 
-	BUG_ON(!vma);
-	WARN_ON(vma->pin_count == 0);
-	WARN_ON(!i915_gem_obj_ggtt_bound_view(obj, view));
+int
+i915_gem_pin_ioctl(struct drm_device *dev, void *data,
+		   struct drm_file *file)
+{
+	struct drm_i915_gem_pin *args = data;
+	struct drm_i915_gem_object *obj;
+	int ret;
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		return -ENODEV;
+
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	obj = to_intel_bo(drm_gem_object_lookup(dev, file, args->handle));
+	if (&obj->base == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
+
+	if (obj->madv != I915_MADV_WILLNEED) {
+		DRM_DEBUG("Attempting to pin a purgeable buffer\n");
+		ret = -EFAULT;
+		goto out;
+	}
+
+	if (obj->pin_filp != NULL && obj->pin_filp != file) {
+		DRM_DEBUG("Already pinned in i915_gem_pin_ioctl(): %d\n",
+			  args->handle);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (obj->user_pin_count == ULONG_MAX) {
+		ret = -EBUSY;
+		goto out;
+	}
+
+	if (obj->user_pin_count == 0) {
+		ret = i915_gem_obj_ggtt_pin(obj, args->alignment, PIN_MAPPABLE);
+		if (ret)
+			goto out;
+	}
+
+	obj->user_pin_count++;
+	obj->pin_filp = file;
+
+	args->offset = i915_gem_obj_ggtt_offset(obj);
+out:
+	drm_gem_object_unreference(&obj->base);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+int
+i915_gem_unpin_ioctl(struct drm_device *dev, void *data,
+		     struct drm_file *file)
+{
+	struct drm_i915_gem_pin *args = data;
+	struct drm_i915_gem_object *obj;
+	int ret;
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		return -ENODEV;
 
-	--vma->pin_count;
+	ret = i915_mutex_lock_interruptible(dev);
+	if (ret)
+		return ret;
+
+	obj = to_intel_bo(drm_gem_object_lookup(dev, file, args->handle));
+	if (&obj->base == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
+
+	if (obj->pin_filp != file) {
+		DRM_DEBUG("Not pinned by caller in i915_gem_pin_ioctl(): %d\n",
+			  args->handle);
+		ret = -EINVAL;
+		goto out;
+	}
+	obj->user_pin_count--;
+	if (obj->user_pin_count == 0) {
+		obj->pin_filp = NULL;
+		i915_gem_object_ggtt_unpin(obj);
+	}
+
+out:
+	drm_gem_object_unreference(&obj->base);
+unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
 }
 
 int
@@ -4291,15 +4312,13 @@ i915_gem_busy_ioctl(struct drm_device *dev, void *data,
 	 * necessary flushes here.
 	 */
 	ret = i915_gem_object_flush_active(obj);
-	if (ret)
-		goto unref;
 
-	BUILD_BUG_ON(I915_NUM_RINGS > 16);
-	args->busy = obj->active << 16;
-	if (obj->last_write_req)
-		args->busy |= obj->last_write_req->ring->id;
+	args->busy = obj->active;
+	if (obj->ring) {
+		BUILD_BUG_ON(I915_NUM_RINGS > 16);
+		args->busy |= intel_ring_flag(obj->ring) << 16;
+	}
 
-unref:
 	drm_gem_object_unreference(&obj->base);
 unlock:
 	mutex_unlock(&dev->struct_mutex);
@@ -4317,7 +4336,6 @@ int
 i915_gem_madvise_ioctl(struct drm_device *dev, void *data,
 		       struct drm_file *file_priv)
 {
-	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct drm_i915_gem_madvise *args = data;
 	struct drm_i915_gem_object *obj;
 	int ret;
@@ -4345,20 +4363,11 @@ i915_gem_madvise_ioctl(struct drm_device *dev, void *data,
 		goto out;
 	}
 
-	if (obj->pages &&
-	    obj->tiling_mode != I915_TILING_NONE &&
-	    dev_priv->quirks & QUIRK_PIN_SWIZZLED_PAGES) {
-		if (obj->madv == I915_MADV_WILLNEED)
-			i915_gem_object_unpin_pages(obj);
-		if (args->madv == I915_MADV_WILLNEED)
-			i915_gem_object_pin_pages(obj);
-	}
-
 	if (obj->madv != __I915_MADV_PURGED)
 		obj->madv = args->madv;
 
 	/* if the object is no longer attached, discard its backing storage */
-	if (obj->madv == I915_MADV_DONTNEED && obj->pages == NULL)
+	if (i915_gem_object_is_purgeable(obj) && obj->pages == NULL)
 		i915_gem_object_truncate(obj);
 
 	args->retained = obj->madv != __I915_MADV_PURGED;
@@ -4373,14 +4382,10 @@ unlock:
 void i915_gem_object_init(struct drm_i915_gem_object *obj,
 			  const struct drm_i915_gem_object_ops *ops)
 {
-	int i;
-
 	INIT_LIST_HEAD(&obj->global_list);
-	for (i = 0; i < I915_NUM_RINGS; i++)
-		INIT_LIST_HEAD(&obj->ring_list[i]);
+	INIT_LIST_HEAD(&obj->ring_list);
 	INIT_LIST_HEAD(&obj->obj_exec_link);
 	INIT_LIST_HEAD(&obj->vma_list);
-	INIT_LIST_HEAD(&obj->batch_pool_link);
 
 	obj->ops = ops;
 
@@ -4500,6 +4505,8 @@ void i915_gem_free_object(struct drm_gem_object *gem_obj)
 		}
 	}
 
+	i915_gem_object_detach_phys(obj);
+
 	/* Stolen objects don't hold a ref, but do hold pin count. Fix that up
 	 * before progressing. */
 	if (obj->stolen)
@@ -4507,11 +4514,6 @@ void i915_gem_free_object(struct drm_gem_object *gem_obj)
 
 	WARN_ON(obj->frontbuffer_bits);
 
-	if (obj->pages && obj->madv == I915_MADV_WILLNEED &&
-	    dev_priv->quirks & QUIRK_PIN_SWIZZLED_PAGES &&
-	    obj->tiling_mode != I915_TILING_NONE)
-		i915_gem_object_unpin_pages(obj);
-
 	if (WARN_ON(obj->pages_pin_count))
 		obj->pages_pin_count = 0;
 	if (discard_backing_storage(obj))
@@ -4540,29 +4542,10 @@ struct i915_vma *i915_gem_obj_to_vma(struct drm_i915_gem_object *obj,
 				     struct i915_address_space *vm)
 {
 	struct i915_vma *vma;
-	list_for_each_entry(vma, &obj->vma_list, vma_link) {
-		if (i915_is_ggtt(vma->vm) &&
-		    vma->ggtt_view.type != I915_GGTT_VIEW_NORMAL)
-			continue;
+	list_for_each_entry(vma, &obj->vma_list, vma_link)
 		if (vma->vm == vm)
 			return vma;
-	}
-	return NULL;
-}
-
-struct i915_vma *i915_gem_obj_to_ggtt_view(struct drm_i915_gem_object *obj,
-					   const struct i915_ggtt_view *view)
-{
-	struct i915_address_space *ggtt = i915_obj_to_ggtt(obj);
-	struct i915_vma *vma;
-
-	if (WARN_ONCE(!view, "no view specified"))
-		return ERR_PTR(-EINVAL);
 
-	list_for_each_entry(vma, &obj->vma_list, vma_link)
-		if (vma->vm == ggtt &&
-		    i915_ggtt_view_equal(&vma->ggtt_view, view))
-			return vma;
 	return NULL;
 }
 
@@ -4582,7 +4565,7 @@ void i915_gem_vma_destroy(struct i915_vma *vma)
 
 	list_del(&vma->vma_link);
 
-	kmem_cache_free(to_i915(vma->obj->base.dev)->vmas, vma);
+	kfree(vma);
 }
 
 static void
@@ -4603,24 +4586,34 @@ i915_gem_suspend(struct drm_device *dev)
 	int ret = 0;
 
 	mutex_lock(&dev->struct_mutex);
+	if (dev_priv->ums.mm_suspended)
+		goto err;
+
 	ret = i915_gpu_idle(dev);
 	if (ret)
 		goto err;
 
 	i915_gem_retire_requests(dev);
 
+	/* Under UMS, be paranoid and evict. */
+	if (!drm_core_check_feature(dev, DRIVER_MODESET))
+		i915_gem_evict_everything(dev);
+
+	i915_kernel_lost_context(dev);
 	i915_gem_stop_ringbuffers(dev);
+
+	/* Hack!  Don't let anybody do execbuf while we don't control the chip.
+	 * We need to replace this with a semaphore, or something.
+	 * And not confound ums.mm_suspended!
+	 */
+	dev_priv->ums.mm_suspended = !drm_core_check_feature(dev,
+							     DRIVER_MODESET);
 	mutex_unlock(&dev->struct_mutex);
 
-	cancel_delayed_work_sync(&dev_priv->gpu_error.hangcheck_work);
+	del_timer_sync(&dev_priv->gpu_error.hangcheck_timer);
 	cancel_delayed_work_sync(&dev_priv->mm.retire_work);
 	flush_delayed_work(&dev_priv->mm.idle_work);
 
-	/* Assert that we sucessfully flushed all the work and
-	 * reset the GPU back to its idle, low power state.
-	 */
-	WARN_ON(dev_priv->mm.busy);
-
 	return 0;
 
 err:
@@ -4628,9 +4621,8 @@ err:
 	return ret;
 }
 
-int i915_gem_l3_remap(struct drm_i915_gem_request *req, int slice)
+int i915_gem_l3_remap(struct intel_engine_cs *ring, int slice)
 {
-	struct intel_engine_cs *ring = req->ring;
 	struct drm_device *dev = ring->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	u32 reg_base = GEN7_L3LOG_BASE + (slice * 0x200);
@@ -4640,7 +4632,7 @@ int i915_gem_l3_remap(struct drm_i915_gem_request *req, int slice)
 	if (!HAS_L3_DPF(dev) || !remap_info)
 		return 0;
 
-	ret = intel_ring_begin(req, GEN7_L3LOG_SIZE / 4 * 3);
+	ret = intel_ring_begin(ring, GEN7_L3LOG_SIZE / 4 * 3);
 	if (ret)
 		return ret;
 
@@ -4685,6 +4677,22 @@ void i915_gem_init_swizzling(struct drm_device *dev)
 		BUG();
 }
 
+static bool
+intel_enable_blt(struct drm_device *dev)
+{
+	if (!HAS_BLT(dev))
+		return false;
+
+	/* The blitter was dysfunctional on early prototypes */
+	if (IS_GEN6(dev) && dev->pdev->revision < 8) {
+		DRM_INFO("BLT not supported on this pre-production hardware;"
+			 " graphics performance will be degraded.\n");
+		return false;
+	}
+
+	return true;
+}
+
 static void init_unused_ring(struct drm_device *dev, u32 base)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
@@ -4717,6 +4725,14 @@ int i915_gem_init_rings(struct drm_device *dev)
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	int ret;
 
+	/*
+	 * At least 830 can leave some of the unused rings
+	 * "active" (ie. head != tail) after resume which
+	 * will prevent c3 entry. Makes sure all unused rings
+	 * are totally idle.
+	 */
+	init_unused_rings(dev);
+
 	ret = intel_init_render_ring_buffer(dev);
 	if (ret)
 		return ret;
@@ -4727,7 +4743,7 @@ int i915_gem_init_rings(struct drm_device *dev)
 			goto cleanup_render_ring;
 	}
 
-	if (HAS_BLT(dev)) {
+	if (intel_enable_blt(dev)) {
 		ret = intel_init_blt_ring_buffer(dev);
 		if (ret)
 			goto cleanup_bsd_ring;
@@ -4745,8 +4761,14 @@ int i915_gem_init_rings(struct drm_device *dev)
 			goto cleanup_vebox_ring;
 	}
 
+	ret = i915_gem_set_seqno(dev, ((u32)~0 - 0x1000));
+	if (ret)
+		goto cleanup_bsd2_ring;
+
 	return 0;
 
+cleanup_bsd2_ring:
+	intel_cleanup_ring_buffer(&dev_priv->ring[VCS2]);
 cleanup_vebox_ring:
 	intel_cleanup_ring_buffer(&dev_priv->ring[VECS]);
 cleanup_blt_ring:
@@ -4763,15 +4785,11 @@ int
 i915_gem_init_hw(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_engine_cs *ring;
-	int ret, i, j;
+	int ret, i;
 
 	if (INTEL_INFO(dev)->gen < 6 && !intel_enable_gtt())
 		return -EIO;
 
-	/* Double layer security blanket, see i915_gem_init() */
-	intel_uncore_forcewake_get(dev_priv, FORCEWAKE_ALL);
-
 	if (dev_priv->ellc_size)
 		I915_WRITE(HSW_IDICR, I915_READ(HSW_IDICR) | IDIHASHMSK(0xf));
 
@@ -4793,94 +4811,27 @@ i915_gem_init_hw(struct drm_device *dev)
 
 	i915_gem_init_swizzling(dev);
 
-	/*
-	 * At least 830 can leave some of the unused rings
-	 * "active" (ie. head != tail) after resume which
-	 * will prevent c3 entry. Makes sure all unused rings
-	 * are totally idle.
-	 */
-	init_unused_rings(dev);
+	ret = dev_priv->gt.init_rings(dev);
+	if (ret)
+		return ret;
 
-	BUG_ON(!dev_priv->ring[RCS].default_context);
+	for (i = 0; i < NUM_L3_SLICES(dev); i++)
+		i915_gem_l3_remap(&dev_priv->ring[RCS], i);
 
 	ret = i915_ppgtt_init_hw(dev);
-	if (ret) {
-		DRM_ERROR("PPGTT enable HW failed %d\n", ret);
-		goto out;
+	if (ret && ret != -EIO) {
+		DRM_ERROR("PPGTT enable failed %d\n", ret);
+		i915_gem_cleanup_ringbuffer(dev);
 	}
 
-	/* Need to do basic initialisation of all rings first: */
-	for_each_ring(ring, dev_priv, i) {
-		ret = ring->init_hw(ring);
-		if (ret)
-			goto out;
-	}
-
-	/* We can't enable contexts until all firmware is loaded */
-	if (HAS_GUC_UCODE(dev)) {
-		ret = intel_guc_ucode_load(dev);
-		if (ret) {
-			/*
-			 * If we got an error and GuC submission is enabled, map
-			 * the error to -EIO so the GPU will be declared wedged.
-			 * OTOH, if we didn't intend to use the GuC anyway, just
-			 * discard the error and carry on.
-			 */
-			DRM_ERROR("Failed to initialize GuC, error %d%s\n", ret,
-				  i915.enable_guc_submission ? "" :
-				  " (ignored)");
-			ret = i915.enable_guc_submission ? -EIO : 0;
-			if (ret)
-				goto out;
-		}
-	}
-
-	/*
-	 * Increment the next seqno by 0x100 so we have a visible break
-	 * on re-initialisation
-	 */
-	ret = i915_gem_set_seqno(dev, dev_priv->next_seqno+0x100);
-	if (ret)
-		goto out;
-
-	/* Now it is safe to go back round and do everything else: */
-	for_each_ring(ring, dev_priv, i) {
-		struct drm_i915_gem_request *req;
+	ret = i915_gem_context_enable(dev_priv);
+	if (ret && ret != -EIO) {
+		DRM_ERROR("Context enable failed %d\n", ret);
+		i915_gem_cleanup_ringbuffer(dev);
 
-		WARN_ON(!ring->default_context);
-
-		ret = i915_gem_request_alloc(ring, ring->default_context, &req);
-		if (ret) {
-			i915_gem_cleanup_ringbuffer(dev);
-			goto out;
-		}
-
-		if (ring->id == RCS) {
-			for (j = 0; j < NUM_L3_SLICES(dev); j++)
-				i915_gem_l3_remap(req, j);
-		}
-
-		ret = i915_ppgtt_init_ring(req);
-		if (ret && ret != -EIO) {
-			DRM_ERROR("PPGTT enable ring #%d failed %d\n", i, ret);
-			i915_gem_request_cancel(req);
-			i915_gem_cleanup_ringbuffer(dev);
-			goto out;
-		}
-
-		ret = i915_gem_context_enable(req);
-		if (ret && ret != -EIO) {
-			DRM_ERROR("Context enable ring #%d failed %d\n", i, ret);
-			i915_gem_request_cancel(req);
-			i915_gem_cleanup_ringbuffer(dev);
-			goto out;
-		}
-
-		i915_add_request_no_flush(req);
+		return ret;
 	}
 
-out:
-	intel_uncore_forcewake_put(dev_priv, FORCEWAKE_ALL);
 	return ret;
 }
 
@@ -4903,38 +4854,30 @@ int i915_gem_init(struct drm_device *dev)
 	}
 
 	if (!i915.enable_execlists) {
-		dev_priv->gt.execbuf_submit = i915_gem_ringbuffer_submission;
+		dev_priv->gt.do_execbuf = i915_gem_ringbuffer_submission;
 		dev_priv->gt.init_rings = i915_gem_init_rings;
 		dev_priv->gt.cleanup_ring = intel_cleanup_ring_buffer;
 		dev_priv->gt.stop_ring = intel_stop_ring_buffer;
 	} else {
-		dev_priv->gt.execbuf_submit = intel_execlists_submission;
+		dev_priv->gt.do_execbuf = intel_execlists_submission;
 		dev_priv->gt.init_rings = intel_logical_rings_init;
 		dev_priv->gt.cleanup_ring = intel_logical_ring_cleanup;
 		dev_priv->gt.stop_ring = intel_logical_ring_stop;
 	}
 
-	/* This is just a security blanket to placate dragons.
-	 * On some systems, we very sporadically observe that the first TLBs
-	 * used by the CS may be stale, despite us poking the TLB reset. If
-	 * we hold the forcewake during initialisation these problems
-	 * just magically go away.
-	 */
-	intel_uncore_forcewake_get(dev_priv, FORCEWAKE_ALL);
-
 	ret = i915_gem_init_userptr(dev);
-	if (ret)
-		goto out_unlock;
+	if (ret) {
+		mutex_unlock(&dev->struct_mutex);
+		return ret;
+	}
 
 	i915_gem_init_global_gtt(dev);
 
 	ret = i915_gem_context_init(dev);
-	if (ret)
-		goto out_unlock;
-
-	ret = dev_priv->gt.init_rings(dev);
-	if (ret)
-		goto out_unlock;
+	if (ret) {
+		mutex_unlock(&dev->struct_mutex);
+		return ret;
+	}
 
 	ret = i915_gem_init_hw(dev);
 	if (ret == -EIO) {
@@ -4943,14 +4886,14 @@ int i915_gem_init(struct drm_device *dev)
 		 * for all other failure, such as an allocation failure, bail.
 		 */
 		DRM_ERROR("Failed to initialize GPU, declaring it wedged\n");
-		atomic_or(I915_WEDGED, &dev_priv->gpu_error.reset_counter);
+		atomic_set_mask(I915_WEDGED, &dev_priv->gpu_error.reset_counter);
 		ret = 0;
 	}
-
-out_unlock:
-	intel_uncore_forcewake_put(dev_priv, FORCEWAKE_ALL);
 	mutex_unlock(&dev->struct_mutex);
 
+	/* Allow hardware batchbuffers unless told otherwise, but not for KMS. */
+	if (!drm_core_check_feature(dev, DRIVER_MODESET))
+		dev_priv->dri1.allow_batchbuffer = 1;
 	return ret;
 }
 
@@ -4963,14 +4906,74 @@ i915_gem_cleanup_ringbuffer(struct drm_device *dev)
 
 	for_each_ring(ring, dev_priv, i)
 		dev_priv->gt.cleanup_ring(ring);
+}
+
+int
+i915_gem_entervt_ioctl(struct drm_device *dev, void *data,
+		       struct drm_file *file_priv)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	int ret;
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		return 0;
+
+	if (i915_reset_in_progress(&dev_priv->gpu_error)) {
+		DRM_ERROR("Reenabling wedged hardware, good luck\n");
+		atomic_set(&dev_priv->gpu_error.reset_counter, 0);
+	}
+
+	mutex_lock(&dev->struct_mutex);
+	dev_priv->ums.mm_suspended = 0;
+
+	ret = i915_gem_init_hw(dev);
+	if (ret != 0) {
+		mutex_unlock(&dev->struct_mutex);
+		return ret;
+	}
 
-    if (i915.enable_execlists)
-            /*
-             * Neither the BIOS, ourselves or any other kernel
-             * expects the system to be in execlists mode on startup,
-             * so we need to reset the GPU back to legacy mode.
-             */
-            intel_gpu_reset(dev);
+	BUG_ON(!list_empty(&dev_priv->gtt.base.active_list));
+
+	ret = drm_irq_install(dev, dev->pdev->irq);
+	if (ret)
+		goto cleanup_ringbuffer;
+	mutex_unlock(&dev->struct_mutex);
+
+	return 0;
+
+cleanup_ringbuffer:
+	i915_gem_cleanup_ringbuffer(dev);
+	dev_priv->ums.mm_suspended = 1;
+	mutex_unlock(&dev->struct_mutex);
+
+	return ret;
+}
+
+int
+i915_gem_leavevt_ioctl(struct drm_device *dev, void *data,
+		       struct drm_file *file_priv)
+{
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		return 0;
+
+	mutex_lock(&dev->struct_mutex);
+	drm_irq_uninstall(dev);
+	mutex_unlock(&dev->struct_mutex);
+
+	return i915_gem_suspend(dev);
+}
+
+void
+i915_gem_lastclose(struct drm_device *dev)
+{
+	int ret;
+
+	if (drm_core_check_feature(dev, DRIVER_MODESET))
+		return;
+
+	ret = i915_gem_suspend(dev);
+	if (ret)
+		DRM_ERROR("failed to idle hardware: %d\n", ret);
 }
 
 static void
@@ -4980,29 +4983,33 @@ init_ring_lists(struct intel_engine_cs *ring)
 	INIT_LIST_HEAD(&ring->request_list);
 }
 
+void i915_init_vm(struct drm_i915_private *dev_priv,
+		  struct i915_address_space *vm)
+{
+	if (!i915_is_ggtt(vm))
+		drm_mm_init(&vm->mm, vm->start, vm->total);
+	vm->dev = dev_priv->dev;
+	INIT_LIST_HEAD(&vm->active_list);
+	INIT_LIST_HEAD(&vm->inactive_list);
+	INIT_LIST_HEAD(&vm->global_link);
+	list_add_tail(&vm->global_link, &dev_priv->vm_list);
+}
+
 void
 i915_gem_load(struct drm_device *dev)
 {
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	int i;
 
-	dev_priv->objects =
+	dev_priv->slab =
 		kmem_cache_create("i915_gem_object",
 				  sizeof(struct drm_i915_gem_object), 0,
 				  SLAB_HWCACHE_ALIGN,
 				  NULL);
-	dev_priv->vmas =
-		kmem_cache_create("i915_gem_vma",
-				  sizeof(struct i915_vma), 0,
-				  SLAB_HWCACHE_ALIGN,
-				  NULL);
-	dev_priv->requests =
-		kmem_cache_create("i915_gem_request",
-				  sizeof(struct drm_i915_gem_request), 0,
-				  SLAB_HWCACHE_ALIGN,
-				  NULL);
 
 	INIT_LIST_HEAD(&dev_priv->vm_list);
+	i915_init_vm(dev_priv, &dev_priv->gtt.base);
+
 	INIT_LIST_HEAD(&dev_priv->context_list);
 	INIT_LIST_HEAD(&dev_priv->mm.unbound_list);
 	INIT_LIST_HEAD(&dev_priv->mm.bound_list);
@@ -5017,8 +5024,18 @@ i915_gem_load(struct drm_device *dev)
 			  i915_gem_idle_work_handler);
 	init_waitqueue_head(&dev_priv->gpu_error.reset_queue);
 
+	/* On GEN3 we really need to make sure the ARB C3 LP bit is set */
+	if (!drm_core_check_feature(dev, DRIVER_MODESET) && IS_GEN3(dev)) {
+		I915_WRITE(MI_ARB_STATE,
+			   _MASKED_BIT_ENABLE(MI_ARB_C3_LP_WRITE_ENABLE));
+	}
+
 	dev_priv->relative_constants_mode = I915_EXEC_CONSTANTS_REL_GENERAL;
 
+	/* Old X drivers will take 0-2 for front, back, depth buffers */
+	if (!drm_core_check_feature(dev, DRIVER_MODESET))
+		dev_priv->fence_reg_start = 3;
+
 	if (INTEL_INFO(dev)->gen >= 7 && !IS_VALLEYVIEW(dev))
 		dev_priv->num_fence_regs = 32;
 	else if (INTEL_INFO(dev)->gen >= 4 || IS_I945G(dev) || IS_I945GM(dev) || IS_G33(dev))
@@ -5026,18 +5043,6 @@ i915_gem_load(struct drm_device *dev)
 	else
 		dev_priv->num_fence_regs = 8;
 
-	if (intel_vgpu_active(dev))
-		dev_priv->num_fence_regs =
-				I915_READ(vgtif_reg(avail_rs.fence_num));
-
-	/*
-	 * Set initial sequence number for requests.
-	 * Using this number allows the wraparound to happen early,
-	 * catching any obvious problems.
-	 */
-	dev_priv->next_seqno = ((u32)~0 - 0x1100);
-	dev_priv->last_seqno = ((u32)~0 - 0x1101);
-
 	/* Initialize fence registers to zero */
 	INIT_LIST_HEAD(&dev_priv->mm.fence_list);
 	i915_gem_restore_fences(dev);
@@ -5047,7 +5052,13 @@ i915_gem_load(struct drm_device *dev)
 
 	dev_priv->mm.interruptible = true;
 
-	i915_gem_shrinker_init(dev_priv);
+	dev_priv->mm.shrinker.scan_objects = i915_gem_shrinker_scan;
+	dev_priv->mm.shrinker.count_objects = i915_gem_shrinker_count;
+	dev_priv->mm.shrinker.seeks = DEFAULT_SEEKS;
+	register_shrinker(&dev_priv->mm.shrinker);
+
+	dev_priv->mm.oom_notifier.notifier_call = i915_gem_shrinker_oom;
+	register_oom_notifier(&dev_priv->mm.oom_notifier);
 
 	mutex_init(&dev_priv->fb_tracking.lock);
 }
@@ -5056,6 +5067,8 @@ void i915_gem_release(struct drm_device *dev, struct drm_file *file)
 {
 	struct drm_i915_file_private *file_priv = file->driver_priv;
 
+	cancel_delayed_work_sync(&file_priv->mm.idle_work);
+
 	/* Clean up our request list when the client is going away, so that
 	 * later retire_requests won't dereference our soon-to-be-gone
 	 * file_priv.
@@ -5071,12 +5084,15 @@ void i915_gem_release(struct drm_device *dev, struct drm_file *file)
 		request->file_priv = NULL;
 	}
 	spin_unlock(&file_priv->mm.lock);
+}
 
-	if (!list_empty(&file_priv->rps.link)) {
-		spin_lock(&to_i915(dev)->rps.client_lock);
-		list_del(&file_priv->rps.link);
-		spin_unlock(&to_i915(dev)->rps.client_lock);
-	}
+static void
+i915_gem_file_idle_work_handler(struct work_struct *work)
+{
+	struct drm_i915_file_private *file_priv =
+		container_of(work, typeof(*file_priv), mm.idle_work.work);
+
+	atomic_set(&file_priv->rps_wait_boost, false);
 }
 
 int i915_gem_open(struct drm_device *dev, struct drm_file *file)
@@ -5093,10 +5109,11 @@ int i915_gem_open(struct drm_device *dev, struct drm_file *file)
 	file->driver_priv = file_priv;
 	file_priv->dev_priv = dev->dev_private;
 	file_priv->file = file;
-	INIT_LIST_HEAD(&file_priv->rps.link);
 
 	spin_lock_init(&file_priv->mm.lock);
 	INIT_LIST_HEAD(&file_priv->mm.request_list);
+	INIT_DELAYED_WORK(&file_priv->mm.idle_work,
+			  i915_gem_file_idle_work_handler);
 
 	ret = i915_gem_context_open(dev, file);
 	if (ret)
@@ -5105,15 +5122,6 @@ int i915_gem_open(struct drm_device *dev, struct drm_file *file)
 	return ret;
 }
 
-/**
- * i915_gem_track_fb - update frontbuffer tracking
- * @old: current GEM buffer for the frontbuffer slots
- * @new: new GEM buffer for the frontbuffer slots
- * @frontbuffer_bits: bitmask of frontbuffer slots
- *
- * This updates the frontbuffer tracking bits @frontbuffer_bits by clearing them
- * from @old and setting them in @new. Both @old and @new can be NULL.
- */
 void i915_gem_track_fb(struct drm_i915_gem_object *old,
 		       struct drm_i915_gem_object *new,
 		       unsigned frontbuffer_bits)
@@ -5131,69 +5139,103 @@ void i915_gem_track_fb(struct drm_i915_gem_object *old,
 	}
 }
 
-/* All the new VM stuff */
-u64 i915_gem_obj_offset(struct drm_i915_gem_object *o,
-			struct i915_address_space *vm)
+static bool mutex_is_locked_by(struct mutex *mutex, struct task_struct *task)
 {
-	struct drm_i915_private *dev_priv = o->base.dev->dev_private;
-	struct i915_vma *vma;
+	if (!mutex_is_locked(mutex))
+		return false;
 
-	WARN_ON(vm == &dev_priv->mm.aliasing_ppgtt->base);
+#if defined(CONFIG_SMP) && !defined(CONFIG_DEBUG_MUTEXES) && !defined(CONFIG_PREEMPT_RT_BASE)
+	return mutex->owner == task;
+#else
+	/* Since UP may be pre-empted, we cannot assume that we own the lock */
+	return false;
+#endif
+}
 
-	list_for_each_entry(vma, &o->vma_list, vma_link) {
-		if (i915_is_ggtt(vma->vm) &&
-		    vma->ggtt_view.type != I915_GGTT_VIEW_NORMAL)
-			continue;
-		if (vma->vm == vm)
-			return vma->node.start;
-	}
+static bool i915_gem_shrinker_lock(struct drm_device *dev, bool *unlock)
+{
+	if (!mutex_trylock(&dev->struct_mutex)) {
+		if (!mutex_is_locked_by(&dev->struct_mutex, current))
+			return false;
 
-	WARN(1, "%s vma for this object not found.\n",
-	     i915_is_ggtt(vm) ? "global" : "ppgtt");
-	return -1;
+		if (to_i915(dev)->mm.shrinker_no_lock_stealing)
+			return false;
+
+		*unlock = false;
+	} else
+		*unlock = true;
+
+	return true;
 }
 
-u64 i915_gem_obj_ggtt_offset_view(struct drm_i915_gem_object *o,
-				  const struct i915_ggtt_view *view)
+static int num_vma_bound(struct drm_i915_gem_object *obj)
 {
-	struct i915_address_space *ggtt = i915_obj_to_ggtt(o);
 	struct i915_vma *vma;
+	int count = 0;
 
-	list_for_each_entry(vma, &o->vma_list, vma_link)
-		if (vma->vm == ggtt &&
-		    i915_ggtt_view_equal(&vma->ggtt_view, view))
-			return vma->node.start;
+	list_for_each_entry(vma, &obj->vma_list, vma_link)
+		if (drm_mm_node_allocated(&vma->node))
+			count++;
 
-	WARN(1, "global vma for this object not found. (view=%u)\n", view->type);
-	return -1;
+	return count;
 }
 
-bool i915_gem_obj_bound(struct drm_i915_gem_object *o,
-			struct i915_address_space *vm)
+static unsigned long
+i915_gem_shrinker_count(struct shrinker *shrinker, struct shrink_control *sc)
+{
+	struct drm_i915_private *dev_priv =
+		container_of(shrinker, struct drm_i915_private, mm.shrinker);
+	struct drm_device *dev = dev_priv->dev;
+	struct drm_i915_gem_object *obj;
+	unsigned long count;
+	bool unlock;
+
+	if (!i915_gem_shrinker_lock(dev, &unlock))
+		return 0;
+
+	count = 0;
+	list_for_each_entry(obj, &dev_priv->mm.unbound_list, global_list)
+		if (obj->pages_pin_count == 0)
+			count += obj->base.size >> PAGE_SHIFT;
+
+	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
+		if (!i915_gem_obj_is_pinned(obj) &&
+		    obj->pages_pin_count == num_vma_bound(obj))
+			count += obj->base.size >> PAGE_SHIFT;
+	}
+
+	if (unlock)
+		mutex_unlock(&dev->struct_mutex);
+
+	return count;
+}
+
+/* All the new VM stuff */
+unsigned long i915_gem_obj_offset(struct drm_i915_gem_object *o,
+				  struct i915_address_space *vm)
 {
+	struct drm_i915_private *dev_priv = o->base.dev->dev_private;
 	struct i915_vma *vma;
 
+	WARN_ON(vm == &dev_priv->mm.aliasing_ppgtt->base);
+
 	list_for_each_entry(vma, &o->vma_list, vma_link) {
-		if (i915_is_ggtt(vma->vm) &&
-		    vma->ggtt_view.type != I915_GGTT_VIEW_NORMAL)
-			continue;
-		if (vma->vm == vm && drm_mm_node_allocated(&vma->node))
-			return true;
-	}
+		if (vma->vm == vm)
+			return vma->node.start;
 
-	return false;
+	}
+	WARN(1, "%s vma for this object not found.\n",
+	     i915_is_ggtt(vm) ? "global" : "ppgtt");
+	return -1;
 }
 
-bool i915_gem_obj_ggtt_bound_view(struct drm_i915_gem_object *o,
-				  const struct i915_ggtt_view *view)
+bool i915_gem_obj_bound(struct drm_i915_gem_object *o,
+			struct i915_address_space *vm)
 {
-	struct i915_address_space *ggtt = i915_obj_to_ggtt(o);
 	struct i915_vma *vma;
 
 	list_for_each_entry(vma, &o->vma_list, vma_link)
-		if (vma->vm == ggtt &&
-		    i915_ggtt_view_equal(&vma->ggtt_view, view) &&
-		    drm_mm_node_allocated(&vma->node))
+		if (vma->vm == vm && drm_mm_node_allocated(&vma->node))
 			return true;
 
 	return false;
@@ -5220,62 +5262,115 @@ unsigned long i915_gem_obj_size(struct drm_i915_gem_object *o,
 
 	BUG_ON(list_empty(&o->vma_list));
 
-	list_for_each_entry(vma, &o->vma_list, vma_link) {
-		if (i915_is_ggtt(vma->vm) &&
-		    vma->ggtt_view.type != I915_GGTT_VIEW_NORMAL)
-			continue;
+	list_for_each_entry(vma, &o->vma_list, vma_link)
 		if (vma->vm == vm)
 			return vma->node.size;
-	}
+
 	return 0;
 }
 
-bool i915_gem_obj_is_pinned(struct drm_i915_gem_object *obj)
+static unsigned long
+i915_gem_shrinker_scan(struct shrinker *shrinker, struct shrink_control *sc)
 {
-	struct i915_vma *vma;
-	list_for_each_entry(vma, &obj->vma_list, vma_link)
-		if (vma->pin_count > 0)
-			return true;
+	struct drm_i915_private *dev_priv =
+		container_of(shrinker, struct drm_i915_private, mm.shrinker);
+	struct drm_device *dev = dev_priv->dev;
+	unsigned long freed;
+	bool unlock;
+
+	if (!i915_gem_shrinker_lock(dev, &unlock))
+		return SHRINK_STOP;
+
+	freed = i915_gem_shrink(dev_priv,
+				sc->nr_to_scan,
+				I915_SHRINK_BOUND |
+				I915_SHRINK_UNBOUND |
+				I915_SHRINK_PURGEABLE);
+	if (freed < sc->nr_to_scan)
+		freed += i915_gem_shrink(dev_priv,
+					 sc->nr_to_scan - freed,
+					 I915_SHRINK_BOUND |
+					 I915_SHRINK_UNBOUND);
+	if (unlock)
+		mutex_unlock(&dev->struct_mutex);
 
-	return false;
+	return freed;
 }
 
-/* Allocate a new GEM object and fill it with the supplied data */
-struct drm_i915_gem_object *
-i915_gem_object_create_from_data(struct drm_device *dev,
-			         const void *data, size_t size)
+static int
+i915_gem_shrinker_oom(struct notifier_block *nb, unsigned long event, void *ptr)
 {
+	struct drm_i915_private *dev_priv =
+		container_of(nb, struct drm_i915_private, mm.oom_notifier);
+	struct drm_device *dev = dev_priv->dev;
 	struct drm_i915_gem_object *obj;
-	struct sg_table *sg;
-	size_t bytes;
-	int ret;
+	unsigned long timeout = msecs_to_jiffies(5000) + 1;
+	unsigned long pinned, bound, unbound, freed;
+	bool was_interruptible;
+	bool unlock;
 
-	obj = i915_gem_alloc_object(dev, round_up(size, PAGE_SIZE));
-	if (IS_ERR_OR_NULL(obj))
-		return obj;
+	while (!i915_gem_shrinker_lock(dev, &unlock) && --timeout) {
+		schedule_timeout_killable(1);
+		if (fatal_signal_pending(current))
+			return NOTIFY_DONE;
+	}
+	if (timeout == 0) {
+		pr_err("Unable to purge GPU memory due lock contention.\n");
+		return NOTIFY_DONE;
+	}
 
-	ret = i915_gem_object_set_to_cpu_domain(obj, true);
-	if (ret)
-		goto fail;
+	was_interruptible = dev_priv->mm.interruptible;
+	dev_priv->mm.interruptible = false;
 
-	ret = i915_gem_object_get_pages(obj);
-	if (ret)
-		goto fail;
+	freed = i915_gem_shrink_all(dev_priv);
 
-	i915_gem_object_pin_pages(obj);
-	sg = obj->pages;
-	bytes = sg_copy_from_buffer(sg->sgl, sg->nents, (void *)data, size);
-	i915_gem_object_unpin_pages(obj);
+	dev_priv->mm.interruptible = was_interruptible;
 
-	if (WARN_ON(bytes != size)) {
-		DRM_ERROR("Incomplete copy, wrote %zu of %zu", bytes, size);
-		ret = -EFAULT;
-		goto fail;
+	/* Because we may be allocating inside our own driver, we cannot
+	 * assert that there are no objects with pinned pages that are not
+	 * being pointed to by hardware.
+	 */
+	unbound = bound = pinned = 0;
+	list_for_each_entry(obj, &dev_priv->mm.unbound_list, global_list) {
+		if (!obj->base.filp) /* not backed by a freeable object */
+			continue;
+
+		if (obj->pages_pin_count)
+			pinned += obj->base.size;
+		else
+			unbound += obj->base.size;
 	}
+	list_for_each_entry(obj, &dev_priv->mm.bound_list, global_list) {
+		if (!obj->base.filp)
+			continue;
 
-	return obj;
+		if (obj->pages_pin_count)
+			pinned += obj->base.size;
+		else
+			bound += obj->base.size;
+	}
 
-fail:
-	drm_gem_object_unreference(&obj->base);
-	return ERR_PTR(ret);
+	if (unlock)
+		mutex_unlock(&dev->struct_mutex);
+
+	pr_info("Purging GPU memory, %lu bytes freed, %lu bytes still pinned.\n",
+		freed, pinned);
+	if (unbound || bound)
+		pr_err("%lu and %lu bytes still available in the "
+		       "bound and unbound GPU page lists.\n",
+		       bound, unbound);
+
+	*(unsigned long *)ptr += freed;
+	return NOTIFY_DONE;
+}
+
+struct i915_vma *i915_gem_obj_to_ggtt(struct drm_i915_gem_object *obj)
+{
+	struct i915_vma *vma;
+
+	vma = list_first_entry(&obj->vma_list, typeof(*vma), vma_link);
+	if (vma->vm != i915_obj_to_ggtt(obj))
+		return NULL;
+
+	return vma;
 }
diff --git a/kernel/msm-3.18/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/kernel/msm-3.18/drivers/gpu/drm/i915/i915_gem_execbuffer.c
index 6ed7d63a0..8ea32c161 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@ -32,7 +32,6 @@
 #include "i915_trace.h"
 #include "intel_drv.h"
 #include <linux/dma_remapping.h>
-#include <linux/uaccess.h>
 
 #define  __EXEC_OBJECT_HAS_PIN (1<<31)
 #define  __EXEC_OBJECT_HAS_FENCE (1<<30)
@@ -246,6 +245,7 @@ static inline int use_cpu_reloc(struct drm_i915_gem_object *obj)
 {
 	return (HAS_LLC(obj->base.dev) ||
 		obj->base.write_domain == I915_GEM_DOMAIN_CPU ||
+		!obj->map_and_fenceable ||
 		obj->cache_level != I915_CACHE_NONE);
 }
 
@@ -331,51 +331,6 @@ relocate_entry_gtt(struct drm_i915_gem_object *obj,
 	return 0;
 }
 
-static void
-clflush_write32(void *addr, uint32_t value)
-{
-	/* This is not a fast path, so KISS. */
-	drm_clflush_virt_range(addr, sizeof(uint32_t));
-	*(uint32_t *)addr = value;
-	drm_clflush_virt_range(addr, sizeof(uint32_t));
-}
-
-static int
-relocate_entry_clflush(struct drm_i915_gem_object *obj,
-		       struct drm_i915_gem_relocation_entry *reloc,
-		       uint64_t target_offset)
-{
-	struct drm_device *dev = obj->base.dev;
-	uint32_t page_offset = offset_in_page(reloc->offset);
-	uint64_t delta = (int)reloc->delta + target_offset;
-	char *vaddr;
-	int ret;
-
-	ret = i915_gem_object_set_to_gtt_domain(obj, true);
-	if (ret)
-		return ret;
-
-	vaddr = kmap_atomic(i915_gem_object_get_page(obj,
-				reloc->offset >> PAGE_SHIFT));
-	clflush_write32(vaddr + page_offset, lower_32_bits(delta));
-
-	if (INTEL_INFO(dev)->gen >= 8) {
-		page_offset = offset_in_page(page_offset + sizeof(uint32_t));
-
-		if (page_offset == 0) {
-			kunmap_atomic(vaddr);
-			vaddr = kmap_atomic(i915_gem_object_get_page(obj,
-			    (reloc->offset + sizeof(uint32_t)) >> PAGE_SHIFT));
-		}
-
-		clflush_write32(vaddr + page_offset, upper_32_bits(delta));
-	}
-
-	kunmap_atomic(vaddr);
-
-	return 0;
-}
-
 static int
 i915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,
 				   struct eb_vmas *eb,
@@ -401,11 +356,12 @@ i915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,
 	 * pipe_control writes because the gpu doesn't properly redirect them
 	 * through the ppgtt for non_secure batchbuffers. */
 	if (unlikely(IS_GEN6(dev) &&
-	    reloc->write_domain == I915_GEM_DOMAIN_INSTRUCTION)) {
-		ret = i915_vma_bind(target_vma, target_i915_obj->cache_level,
-				    PIN_GLOBAL);
-		if (WARN_ONCE(ret, "Unexpected failure to bind target VMA!"))
-			return ret;
+	    reloc->write_domain == I915_GEM_DOMAIN_INSTRUCTION &&
+	    !target_i915_obj->has_global_gtt_mapping)) {
+		struct i915_vma *vma =
+			list_first_entry(&target_i915_obj->vma_list,
+					 typeof(*vma), vma_link);
+		vma->bind_vma(vma, target_i915_obj->cache_level, GLOBAL_BIND);
 	}
 
 	/* Validate that the target is in a valid r/w GPU domain */
@@ -459,19 +415,13 @@ i915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,
 	}
 
 	/* We can't wait for rendering with pagefaults disabled */
-	if (obj->active && pagefault_disabled())
+	if (obj->active && in_atomic())
 		return -EFAULT;
 
 	if (use_cpu_reloc(obj))
 		ret = relocate_entry_cpu(obj, reloc, target_offset);
-	else if (obj->map_and_fenceable)
+	else
 		ret = relocate_entry_gtt(obj, reloc, target_offset);
-	else if (cpu_has_clflush)
-		ret = relocate_entry_clflush(obj, reloc, target_offset);
-	else {
-		WARN_ONCE(1, "Impossible case in relocation handling\n");
-		ret = -ENODEV;
-	}
 
 	if (ret)
 		return ret;
@@ -569,12 +519,6 @@ i915_gem_execbuffer_relocate(struct eb_vmas *eb)
 	return ret;
 }
 
-static bool only_mappable_for_reloc(unsigned int flags)
-{
-	return (flags & (EXEC_OBJECT_NEEDS_FENCE | __EXEC_OBJECT_NEEDS_MAP)) ==
-		__EXEC_OBJECT_NEEDS_MAP;
-}
-
 static int
 i915_gem_execbuffer_reserve_vma(struct i915_vma *vma,
 				struct intel_engine_cs *ring,
@@ -585,30 +529,15 @@ i915_gem_execbuffer_reserve_vma(struct i915_vma *vma,
 	uint64_t flags;
 	int ret;
 
-	flags = PIN_USER;
+	flags = 0;
+	if (entry->flags & __EXEC_OBJECT_NEEDS_MAP)
+		flags |= PIN_MAPPABLE;
 	if (entry->flags & EXEC_OBJECT_NEEDS_GTT)
 		flags |= PIN_GLOBAL;
-
-	if (!drm_mm_node_allocated(&vma->node)) {
-		/* Wa32bitGeneralStateOffset & Wa32bitInstructionBaseOffset,
-		 * limit address to the first 4GBs for unflagged objects.
-		 */
-		if ((entry->flags & EXEC_OBJECT_SUPPORTS_48B_ADDRESS) == 0)
-			flags |= PIN_ZONE_4G;
-		if (entry->flags & __EXEC_OBJECT_NEEDS_MAP)
-			flags |= PIN_GLOBAL | PIN_MAPPABLE;
-		if (entry->flags & __EXEC_OBJECT_NEEDS_BIAS)
-			flags |= BATCH_OFFSET_BIAS | PIN_OFFSET_BIAS;
-		if ((flags & PIN_MAPPABLE) == 0)
-			flags |= PIN_HIGH;
-	}
+	if (entry->flags & __EXEC_OBJECT_NEEDS_BIAS)
+		flags |= BATCH_OFFSET_BIAS | PIN_OFFSET_BIAS;
 
 	ret = i915_gem_object_pin(obj, vma->vm, entry->alignment, flags);
-	if ((ret == -ENOSPC  || ret == -E2BIG) &&
-	    only_mappable_for_reloc(entry->flags))
-		ret = i915_gem_object_pin(obj, vma->vm,
-					  entry->alignment,
-					  flags & ~PIN_MAPPABLE);
 	if (ret)
 		return ret;
 
@@ -670,16 +599,11 @@ eb_vma_misplaced(struct i915_vma *vma)
 	    vma->node.start & (entry->alignment - 1))
 		return true;
 
-	if (entry->flags & __EXEC_OBJECT_NEEDS_BIAS &&
-	    vma->node.start < BATCH_OFFSET_BIAS)
-		return true;
-
-	/* avoid costly ping-pong once a batch bo ended up non-mappable */
 	if (entry->flags & __EXEC_OBJECT_NEEDS_MAP && !obj->map_and_fenceable)
-		return !only_mappable_for_reloc(entry->flags);
+		return true;
 
-	if ((entry->flags & EXEC_OBJECT_SUPPORTS_48B_ADDRESS) == 0 &&
-	    (vma->node.start + vma->node.size - 1) >> 32)
+	if (entry->flags & __EXEC_OBJECT_NEEDS_BIAS &&
+	    vma->node.start < BATCH_OFFSET_BIAS)
 		return true;
 
 	return false;
@@ -688,7 +612,6 @@ eb_vma_misplaced(struct i915_vma *vma)
 static int
 i915_gem_execbuffer_reserve(struct intel_engine_cs *ring,
 			    struct list_head *vmas,
-			    struct intel_context *ctx,
 			    bool *need_relocs)
 {
 	struct drm_i915_gem_object *obj;
@@ -711,9 +634,6 @@ i915_gem_execbuffer_reserve(struct intel_engine_cs *ring,
 		obj = vma->obj;
 		entry = vma->exec_entry;
 
-		if (ctx->flags & CONTEXT_NO_ZEROMAP)
-			entry->flags |= __EXEC_OBJECT_NEEDS_BIAS;
-
 		if (!has_fenced_gpu_access)
 			entry->flags &= ~EXEC_OBJECT_NEEDS_FENCE;
 		need_fence =
@@ -791,8 +711,7 @@ i915_gem_execbuffer_relocate_slow(struct drm_device *dev,
 				  struct drm_file *file,
 				  struct intel_engine_cs *ring,
 				  struct eb_vmas *eb,
-				  struct drm_i915_gem_exec_object2 *exec,
-				  struct intel_context *ctx)
+				  struct drm_i915_gem_exec_object2 *exec)
 {
 	struct drm_i915_gem_relocation_entry *reloc;
 	struct i915_address_space *vm;
@@ -878,7 +797,7 @@ i915_gem_execbuffer_relocate_slow(struct drm_device *dev,
 		goto err;
 
 	need_relocs = (args->flags & I915_EXEC_NO_RELOC) == 0;
-	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, ctx, &need_relocs);
+	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, &need_relocs);
 	if (ret)
 		goto err;
 
@@ -903,10 +822,9 @@ err:
 }
 
 static int
-i915_gem_execbuffer_move_to_gpu(struct drm_i915_gem_request *req,
+i915_gem_execbuffer_move_to_gpu(struct intel_engine_cs *ring,
 				struct list_head *vmas)
 {
-	const unsigned other_rings = ~intel_ring_flag(req->ring);
 	struct i915_vma *vma;
 	uint32_t flush_domains = 0;
 	bool flush_chipset = false;
@@ -914,12 +832,9 @@ i915_gem_execbuffer_move_to_gpu(struct drm_i915_gem_request *req,
 
 	list_for_each_entry(vma, vmas, exec_list) {
 		struct drm_i915_gem_object *obj = vma->obj;
-
-		if (obj->active & other_rings) {
-			ret = i915_gem_object_sync(obj, req->ring, &req);
-			if (ret)
-				return ret;
-		}
+		ret = i915_gem_object_sync(obj, ring);
+		if (ret)
+			return ret;
 
 		if (obj->base.write_domain & I915_GEM_DOMAIN_CPU)
 			flush_chipset |= i915_gem_clflush_object(obj, false);
@@ -928,7 +843,7 @@ i915_gem_execbuffer_move_to_gpu(struct drm_i915_gem_request *req,
 	}
 
 	if (flush_chipset)
-		i915_gem_chipset_flush(req->ring->dev);
+		i915_gem_chipset_flush(ring->dev);
 
 	if (flush_domains & I915_GEM_DOMAIN_GTT)
 		wmb();
@@ -936,7 +851,7 @@ i915_gem_execbuffer_move_to_gpu(struct drm_i915_gem_request *req,
 	/* Unconditionally invalidate gpu caches and ensure that we do flush
 	 * any residual writes from the previous batch.
 	 */
-	return intel_ring_invalidate_all_caches(req);
+	return intel_ring_invalidate_all_caches(ring);
 }
 
 static bool
@@ -945,21 +860,7 @@ i915_gem_check_execbuffer(struct drm_i915_gem_execbuffer2 *exec)
 	if (exec->flags & __I915_EXEC_UNKNOWN_FLAGS)
 		return false;
 
-	/* Kernel clipping was a DRI1 misfeature */
-	if (exec->num_cliprects || exec->cliprects_ptr)
-		return false;
-
-	if (exec->DR4 == 0xffffffff) {
-		DRM_DEBUG("UXA submitting garbage DR4, fixing up\n");
-		exec->DR4 = 0;
-	}
-	if (exec->DR1 || exec->DR4)
-		return false;
-
-	if ((exec->batch_start_offset | exec->batch_len) & 0x7)
-		return false;
-
-	return true;
+	return ((exec->batch_start_offset | exec->batch_len) & 0x7) == 0;
 }
 
 static int
@@ -983,9 +884,6 @@ validate_exec_list(struct drm_device *dev,
 		if (exec[i].flags & invalid_flags)
 			return -EINVAL;
 
-		if (exec[i].alignment && !is_power_of_2(exec[i].alignment))
-			return -EINVAL;
-
 		/* First check for malicious input causing overflow in
 		 * the worst case where we need to allocate the entire
 		 * relocation tree as a single array.
@@ -1034,7 +932,7 @@ i915_gem_validate_context(struct drm_device *dev, struct drm_file *file,
 	}
 
 	if (i915.enable_execlists && !ctx->engine[ring->id].state) {
-		int ret = intel_lr_context_deferred_alloc(ctx, ring);
+		int ret = intel_lr_context_deferred_create(ctx, ring);
 		if (ret) {
 			DRM_DEBUG("Could not create LRC %u: %d\n", ctx_id, ret);
 			return ERR_PTR(ret);
@@ -1046,9 +944,9 @@ i915_gem_validate_context(struct drm_device *dev, struct drm_file *file,
 
 void
 i915_gem_execbuffer_move_to_active(struct list_head *vmas,
-				   struct drm_i915_gem_request *req)
+				   struct intel_engine_cs *ring)
 {
-	struct intel_engine_cs *ring = i915_gem_request_get_ring(req);
+	u32 seqno = intel_ring_get_seqno(ring);
 	struct i915_vma *vma;
 
 	list_for_each_entry(vma, vmas, exec_list) {
@@ -1057,23 +955,23 @@ i915_gem_execbuffer_move_to_active(struct list_head *vmas,
 		u32 old_read = obj->base.read_domains;
 		u32 old_write = obj->base.write_domain;
 
-		obj->dirty = 1; /* be paranoid  */
 		obj->base.write_domain = obj->base.pending_write_domain;
 		if (obj->base.write_domain == 0)
 			obj->base.pending_read_domains |= obj->base.read_domains;
 		obj->base.read_domains = obj->base.pending_read_domains;
 
-		i915_vma_move_to_active(vma, req);
+		i915_vma_move_to_active(vma, ring);
 		if (obj->base.write_domain) {
-			i915_gem_request_assign(&obj->last_write_req, req);
+			obj->dirty = 1;
+			obj->last_write_seqno = seqno;
 
-			intel_fb_obj_invalidate(obj, ORIGIN_CS);
+			intel_fb_obj_invalidate(obj, ring);
 
 			/* update for the implicit flush after a batch */
 			obj->base.write_domain &= ~I915_GEM_GPU_DOMAINS;
 		}
 		if (entry->flags & EXEC_OBJECT_NEEDS_FENCE) {
-			i915_gem_request_assign(&obj->last_fenced_req, req);
+			obj->last_fenced_seqno = seqno;
 			if (entry->flags & __EXEC_OBJECT_HAS_FENCE) {
 				struct drm_i915_private *dev_priv = to_i915(ring->dev);
 				list_move_tail(&dev_priv->fence_regs[obj->fence_reg].lru_list,
@@ -1086,20 +984,22 @@ i915_gem_execbuffer_move_to_active(struct list_head *vmas,
 }
 
 void
-i915_gem_execbuffer_retire_commands(struct i915_execbuffer_params *params)
+i915_gem_execbuffer_retire_commands(struct drm_device *dev,
+				    struct drm_file *file,
+				    struct intel_engine_cs *ring,
+				    struct drm_i915_gem_object *obj)
 {
 	/* Unconditionally force add_request to emit a full flush. */
-	params->ring->gpu_caches_dirty = true;
+	ring->gpu_caches_dirty = true;
 
 	/* Add a breadcrumb for the completion of the batch buffer */
-	__i915_add_request(params->request, params->batch_obj, true);
+	(void)__i915_add_request(ring, file, obj, NULL);
 }
 
 static int
 i915_reset_gen7_sol_offsets(struct drm_device *dev,
-			    struct drm_i915_gem_request *req)
+			    struct intel_engine_cs *ring)
 {
-	struct intel_engine_cs *ring = req->ring;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	int ret, i;
 
@@ -1108,7 +1008,7 @@ i915_reset_gen7_sol_offsets(struct drm_device *dev,
 		return -EINVAL;
 	}
 
-	ret = intel_ring_begin(req, 4 * 3);
+	ret = intel_ring_begin(ring, 4 * 3);
 	if (ret)
 		return ret;
 
@@ -1123,82 +1023,72 @@ i915_reset_gen7_sol_offsets(struct drm_device *dev,
 	return 0;
 }
 
-static struct drm_i915_gem_object*
-i915_gem_execbuffer_parse(struct intel_engine_cs *ring,
-			  struct drm_i915_gem_exec_object2 *shadow_exec_entry,
-			  struct eb_vmas *eb,
-			  struct drm_i915_gem_object *batch_obj,
-			  u32 batch_start_offset,
-			  u32 batch_len,
-			  bool is_master)
+int
+i915_gem_ringbuffer_submission(struct drm_device *dev, struct drm_file *file,
+			       struct intel_engine_cs *ring,
+			       struct intel_context *ctx,
+			       struct drm_i915_gem_execbuffer2 *args,
+			       struct list_head *vmas,
+			       struct drm_i915_gem_object *batch_obj,
+			       u64 exec_start, u32 flags)
 {
-	struct drm_i915_gem_object *shadow_batch_obj;
-	struct i915_vma *vma;
-	int ret;
-
-	shadow_batch_obj = i915_gem_batch_pool_get(&ring->batch_pool,
-						   PAGE_ALIGN(batch_len));
-	if (IS_ERR(shadow_batch_obj))
-		return shadow_batch_obj;
-
-	ret = i915_parse_cmds(ring,
-			      batch_obj,
-			      shadow_batch_obj,
-			      batch_start_offset,
-			      batch_len,
-			      is_master);
-	if (ret)
-		goto err;
-
-	ret = i915_gem_obj_ggtt_pin(shadow_batch_obj, 0, 0);
-	if (ret)
-		goto err;
-
-	i915_gem_object_unpin_pages(shadow_batch_obj);
+	struct drm_clip_rect *cliprects = NULL;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	u64 exec_len;
+	int instp_mode;
+	u32 instp_mask;
+	int i, ret = 0;
 
-	memset(shadow_exec_entry, 0, sizeof(*shadow_exec_entry));
+	if (args->num_cliprects != 0) {
+		if (ring != &dev_priv->ring[RCS]) {
+			DRM_DEBUG("clip rectangles are only valid with the render ring\n");
+			return -EINVAL;
+		}
 
-	vma = i915_gem_obj_to_ggtt(shadow_batch_obj);
-	vma->exec_entry = shadow_exec_entry;
-	vma->exec_entry->flags = __EXEC_OBJECT_HAS_PIN;
-	drm_gem_object_reference(&shadow_batch_obj->base);
-	list_add_tail(&vma->exec_list, &eb->vmas);
+		if (INTEL_INFO(dev)->gen >= 5) {
+			DRM_DEBUG("clip rectangles are only valid on pre-gen5\n");
+			return -EINVAL;
+		}
 
-	shadow_batch_obj->base.pending_read_domains = I915_GEM_DOMAIN_COMMAND;
+		if (args->num_cliprects > UINT_MAX / sizeof(*cliprects)) {
+			DRM_DEBUG("execbuf with %u cliprects\n",
+				  args->num_cliprects);
+			return -EINVAL;
+		}
 
-	return shadow_batch_obj;
+		cliprects = kcalloc(args->num_cliprects,
+				    sizeof(*cliprects),
+				    GFP_KERNEL);
+		if (cliprects == NULL) {
+			ret = -ENOMEM;
+			goto error;
+		}
 
-err:
-	i915_gem_object_unpin_pages(shadow_batch_obj);
-	if (ret == -EACCES) /* unhandled chained batch */
-		return batch_obj;
-	else
-		return ERR_PTR(ret);
-}
+		if (copy_from_user(cliprects,
+				   to_user_ptr(args->cliprects_ptr),
+				   sizeof(*cliprects)*args->num_cliprects)) {
+			ret = -EFAULT;
+			goto error;
+		}
+	} else {
+		if (args->DR4 == 0xffffffff) {
+			DRM_DEBUG("UXA submitting garbage DR4, fixing up\n");
+			args->DR4 = 0;
+		}
 
-int
-i915_gem_ringbuffer_submission(struct i915_execbuffer_params *params,
-			       struct drm_i915_gem_execbuffer2 *args,
-			       struct list_head *vmas)
-{
-	struct drm_device *dev = params->dev;
-	struct intel_engine_cs *ring = params->ring;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	u64 exec_start, exec_len;
-	int instp_mode;
-	u32 instp_mask;
-	int ret;
+		if (args->DR1 || args->DR4 || args->cliprects_ptr) {
+			DRM_DEBUG("0 cliprects but dirt in cliprects fields\n");
+			return -EINVAL;
+		}
+	}
 
-	ret = i915_gem_execbuffer_move_to_gpu(params->request, vmas);
+	ret = i915_gem_execbuffer_move_to_gpu(ring, vmas);
 	if (ret)
-		return ret;
+		goto error;
 
-	ret = i915_switch_context(params->request);
+	ret = i915_switch_context(ring, ctx);
 	if (ret)
-		return ret;
-
-	WARN(params->ctx->ppgtt && params->ctx->ppgtt->pd_dirty_rings & (1<<ring->id),
-	     "%s didn't clear reload\n", ring->name);
+		goto error;
 
 	instp_mode = args->flags & I915_EXEC_CONSTANTS_MASK;
 	instp_mask = I915_EXEC_CONSTANTS_MASK;
@@ -1208,19 +1098,22 @@ i915_gem_ringbuffer_submission(struct i915_execbuffer_params *params,
 	case I915_EXEC_CONSTANTS_REL_SURFACE:
 		if (instp_mode != 0 && ring != &dev_priv->ring[RCS]) {
 			DRM_DEBUG("non-0 rel constants mode on non-RCS\n");
-			return -EINVAL;
+			ret = -EINVAL;
+			goto error;
 		}
 
 		if (instp_mode != dev_priv->relative_constants_mode) {
 			if (INTEL_INFO(dev)->gen < 4) {
 				DRM_DEBUG("no rel constants on pre-gen4\n");
-				return -EINVAL;
+				ret = -EINVAL;
+				goto error;
 			}
 
 			if (INTEL_INFO(dev)->gen > 5 &&
 			    instp_mode == I915_EXEC_CONSTANTS_REL_SURFACE) {
 				DRM_DEBUG("rel surface constants mode invalid on gen5+\n");
-				return -EINVAL;
+				ret = -EINVAL;
+				goto error;
 			}
 
 			/* The HW changed the meaning on this bit on gen6 */
@@ -1230,14 +1123,15 @@ i915_gem_ringbuffer_submission(struct i915_execbuffer_params *params,
 		break;
 	default:
 		DRM_DEBUG("execbuf with unknown constants: %d\n", instp_mode);
-		return -EINVAL;
+		ret = -EINVAL;
+		goto error;
 	}
 
 	if (ring == &dev_priv->ring[RCS] &&
-	    instp_mode != dev_priv->relative_constants_mode) {
-		ret = intel_ring_begin(params->request, 4);
+			instp_mode != dev_priv->relative_constants_mode) {
+		ret = intel_ring_begin(ring, 4);
 		if (ret)
-			return ret;
+			goto error;
 
 		intel_ring_emit(ring, MI_NOOP);
 		intel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));
@@ -1249,27 +1143,43 @@ i915_gem_ringbuffer_submission(struct i915_execbuffer_params *params,
 	}
 
 	if (args->flags & I915_EXEC_GEN7_SOL_RESET) {
-		ret = i915_reset_gen7_sol_offsets(dev, params->request);
+		ret = i915_reset_gen7_sol_offsets(dev, ring);
 		if (ret)
-			return ret;
+			goto error;
 	}
 
-	exec_len   = args->batch_len;
-	exec_start = params->batch_obj_vm_offset +
-		     params->args_batch_start_offset;
+	exec_len = args->batch_len;
+	if (cliprects) {
+		for (i = 0; i < args->num_cliprects; i++) {
+			ret = i915_emit_box(dev, &cliprects[i],
+					    args->DR1, args->DR4);
+			if (ret)
+				goto error;
 
-	ret = ring->dispatch_execbuffer(params->request,
-					exec_start, exec_len,
-					params->dispatch_flags);
-	if (ret)
-		return ret;
+			ret = ring->dispatch_execbuffer(ring,
+							exec_start, exec_len,
+							flags);
+			if (ret)
+				goto error;
+		}
+	} else {
+		ret = ring->dispatch_execbuffer(ring,
+						exec_start, exec_len,
+						flags);
+		if (ret)
+			return ret;
+	}
 
-	trace_i915_gem_ring_dispatch(params->request, params->dispatch_flags);
+#ifndef CONFIG_PREEMPT_RT_BASE
+	trace_i915_gem_ring_dispatch(ring, intel_ring_get_seqno(ring), flags);
+#endif
 
-	i915_gem_execbuffer_move_to_active(vmas, params->request);
-	i915_gem_execbuffer_retire_commands(params);
+	i915_gem_execbuffer_move_to_active(vmas, ring);
+	i915_gem_execbuffer_retire_commands(dev, file, ring, batch_obj);
 
-	return 0;
+error:
+	kfree(cliprects);
+	return ret;
 }
 
 /**
@@ -1331,14 +1241,12 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct eb_vmas *eb;
 	struct drm_i915_gem_object *batch_obj;
-	struct drm_i915_gem_exec_object2 shadow_exec_entry;
 	struct intel_engine_cs *ring;
 	struct intel_context *ctx;
 	struct i915_address_space *vm;
-	struct i915_execbuffer_params params_master; /* XXX: will be removed later */
-	struct i915_execbuffer_params *params = &params_master;
 	const u32 ctx_id = i915_execbuffer2_get_context_id(*args);
-	u32 dispatch_flags;
+	u64 exec_start = args->batch_start_offset;
+	u32 flags;
 	int ret;
 	bool need_relocs;
 
@@ -1349,15 +1257,15 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 	if (ret)
 		return ret;
 
-	dispatch_flags = 0;
+	flags = 0;
 	if (args->flags & I915_EXEC_SECURE) {
 		if (!file->is_master || !capable(CAP_SYS_ADMIN))
 		    return -EPERM;
 
-		dispatch_flags |= I915_DISPATCH_SECURE;
+		flags |= I915_DISPATCH_SECURE;
 	}
 	if (args->flags & I915_EXEC_IS_PINNED)
-		dispatch_flags |= I915_DISPATCH_PINNED;
+		flags |= I915_DISPATCH_PINNED;
 
 	if ((args->flags & I915_EXEC_RING_MASK) > LAST_USER_RING) {
 		DRM_DEBUG("execbuf with unknown ring: %d\n",
@@ -1365,35 +1273,13 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 		return -EINVAL;
 	}
 
-	if (((args->flags & I915_EXEC_RING_MASK) != I915_EXEC_BSD) &&
-	    ((args->flags & I915_EXEC_BSD_MASK) != 0)) {
-		DRM_DEBUG("execbuf with non bsd ring but with invalid "
-			"bsd dispatch flags: %d\n", (int)(args->flags));
-		return -EINVAL;
-	} 
-
 	if ((args->flags & I915_EXEC_RING_MASK) == I915_EXEC_DEFAULT)
 		ring = &dev_priv->ring[RCS];
 	else if ((args->flags & I915_EXEC_RING_MASK) == I915_EXEC_BSD) {
 		if (HAS_BSD2(dev)) {
 			int ring_id;
-
-			switch (args->flags & I915_EXEC_BSD_MASK) {
-			case I915_EXEC_BSD_DEFAULT:
-				ring_id = gen8_dispatch_bsd_ring(dev, file);
-				ring = &dev_priv->ring[ring_id];
-				break;
-			case I915_EXEC_BSD_RING1:
-				ring = &dev_priv->ring[VCS];
-				break;
-			case I915_EXEC_BSD_RING2:
-				ring = &dev_priv->ring[VCS2];
-				break;
-			default:
-				DRM_DEBUG("execbuf with unknown bsd ring: %d\n",
-					  (int)(args->flags & I915_EXEC_BSD_MASK));
-				return -EINVAL;
-			}
+			ring_id = gen8_dispatch_bsd_ring(dev, file);
+			ring = &dev_priv->ring[ring_id];
 		} else
 			ring = &dev_priv->ring[VCS];
 	} else
@@ -1410,26 +1296,18 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 		return -EINVAL;
 	}
 
-	if (args->flags & I915_EXEC_RESOURCE_STREAMER) {
-		if (!HAS_RESOURCE_STREAMER(dev)) {
-			DRM_DEBUG("RS is only allowed for Haswell, Gen8 and above\n");
-			return -EINVAL;
-		}
-		if (ring->id != RCS) {
-			DRM_DEBUG("RS is not available on %s\n",
-				 ring->name);
-			return -EINVAL;
-		}
-
-		dispatch_flags |= I915_DISPATCH_RS;
-	}
-
 	intel_runtime_pm_get(dev_priv);
 
 	ret = i915_mutex_lock_interruptible(dev);
 	if (ret)
 		goto pre_mutex_err;
 
+	if (dev_priv->ums.mm_suspended) {
+		mutex_unlock(&dev->struct_mutex);
+		ret = -EBUSY;
+		goto pre_mutex_err;
+	}
+
 	ctx = i915_gem_validate_context(dev, file, ring, ctx_id);
 	if (IS_ERR(ctx)) {
 		mutex_unlock(&dev->struct_mutex);
@@ -1444,8 +1322,6 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 	else
 		vm = &dev_priv->gtt.base;
 
-	memset(&params_master, 0x00, sizeof(params_master));
-
 	eb = eb_create(args);
 	if (eb == NULL) {
 		i915_gem_context_unreference(ctx);
@@ -1464,7 +1340,7 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 
 	/* Move the objects en-masse into the GTT, evicting if necessary. */
 	need_relocs = (args->flags & I915_EXEC_NO_RELOC) == 0;
-	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, ctx, &need_relocs);
+	ret = i915_gem_execbuffer_reserve(ring, &eb->vmas, &need_relocs);
 	if (ret)
 		goto err;
 
@@ -1474,7 +1350,7 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 	if (ret) {
 		if (ret == -EFAULT) {
 			ret = i915_gem_execbuffer_relocate_slow(dev, args, file, ring,
-								eb, exec, ctx);
+								eb, exec);
 			BUG_ON(!mutex_is_locked(&dev->struct_mutex));
 		}
 		if (ret)
@@ -1487,57 +1363,37 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 		ret = -EINVAL;
 		goto err;
 	}
+	batch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;
 
-	params->args_batch_start_offset = args->batch_start_offset;
-	if (i915_needs_cmd_parser(ring) && args->batch_len) {
-		struct drm_i915_gem_object *parsed_batch_obj;
-
-		parsed_batch_obj = i915_gem_execbuffer_parse(ring,
-						      &shadow_exec_entry,
-						      eb,
-						      batch_obj,
-						      args->batch_start_offset,
-						      args->batch_len,
-						      file->is_master);
-		if (IS_ERR(parsed_batch_obj)) {
-			ret = PTR_ERR(parsed_batch_obj);
+	if (i915_needs_cmd_parser(ring)) {
+		ret = i915_parse_cmds(ring,
+				      batch_obj,
+				      args->batch_start_offset,
+				      file->is_master);
+		if (ret)
 			goto err;
-		}
 
 		/*
-		 * parsed_batch_obj == batch_obj means batch not fully parsed:
-		 * Accept, but don't promote to secure.
+		 * XXX: Actually do this when enabling batch copy...
+		 *
+		 * Set the DISPATCH_SECURE bit to remove the NON_SECURE bit
+		 * from MI_BATCH_BUFFER_START commands issued in the
+		 * dispatch_execbuffer implementations. We specifically don't
+		 * want that set when the command parser is enabled.
 		 */
-
-		if (parsed_batch_obj != batch_obj) {
-			/*
-			 * Batch parsed and accepted:
-			 *
-			 * Set the DISPATCH_SECURE bit to remove the NON_SECURE
-			 * bit from MI_BATCH_BUFFER_START commands issued in
-			 * the dispatch_execbuffer implementations. We
-			 * specifically don't want that set on batches the
-			 * command parser has accepted.
-			 */
-			dispatch_flags |= I915_DISPATCH_SECURE;
-			params->args_batch_start_offset = 0;
-			batch_obj = parsed_batch_obj;
-		}
 	}
 
-	batch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;
-
 	/* snb/ivb/vlv conflate the "batch in ppgtt" bit with the "non-secure
 	 * batch" bit. Hence we need to pin secure batches into the global gtt.
 	 * hsw should have this fixed, but bdw mucks it up again. */
-	if (dispatch_flags & I915_DISPATCH_SECURE) {
+	if (flags & I915_DISPATCH_SECURE) {
 		/*
 		 * So on first glance it looks freaky that we pin the batch here
 		 * outside of the reservation loop. But:
 		 * - The batch is already pinned into the relevant ppgtt, so we
 		 *   already have the backing storage fully allocated.
 		 * - No other BO uses the global gtt (well contexts, but meh),
-		 *   so we don't really have issues with multiple objects not
+		 *   so we don't really have issues with mutliple objects not
 		 *   fitting due to fragmentation.
 		 * So this is actually safe.
 		 */
@@ -1545,57 +1401,26 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 		if (ret)
 			goto err;
 
-		params->batch_obj_vm_offset = i915_gem_obj_ggtt_offset(batch_obj);
+		exec_start += i915_gem_obj_ggtt_offset(batch_obj);
 	} else
-		params->batch_obj_vm_offset = i915_gem_obj_offset(batch_obj, vm);
-
-	/* Allocate a request for this batch buffer nice and early. */
-	ret = i915_gem_request_alloc(ring, ctx, &params->request);
-	if (ret)
-		goto err_batch_unpin;
-
-	ret = i915_gem_request_add_to_client(params->request, file);
-	if (ret)
-		goto err_batch_unpin;
-
-	/*
-	 * Save assorted stuff away to pass through to *_submission().
-	 * NB: This data should be 'persistent' and not local as it will
-	 * kept around beyond the duration of the IOCTL once the GPU
-	 * scheduler arrives.
-	 */
-	params->dev                     = dev;
-	params->file                    = file;
-	params->ring                    = ring;
-	params->dispatch_flags          = dispatch_flags;
-	params->batch_obj               = batch_obj;
-	params->ctx                     = ctx;
+		exec_start += i915_gem_obj_offset(batch_obj, vm);
 
-	ret = dev_priv->gt.execbuf_submit(params, args, &eb->vmas);
+	ret = dev_priv->gt.do_execbuf(dev, file, ring, ctx, args,
+				      &eb->vmas, batch_obj, exec_start, flags);
 
-err_batch_unpin:
 	/*
 	 * FIXME: We crucially rely upon the active tracking for the (ppgtt)
 	 * batch vma for correctness. For less ugly and less fragility this
 	 * needs to be adjusted to also track the ggtt batch vma properly as
 	 * active.
 	 */
-	if (dispatch_flags & I915_DISPATCH_SECURE)
+	if (flags & I915_DISPATCH_SECURE)
 		i915_gem_object_ggtt_unpin(batch_obj);
-
 err:
 	/* the request owns the ref now */
 	i915_gem_context_unreference(ctx);
 	eb_destroy(eb);
 
-	/*
-	 * If the request was created but not successfully submitted then it
-	 * must be freed again. If it was submitted then it is being tracked
-	 * on the active request list and no clean up is required here.
-	 */
-	if (ret && params->request)
-		i915_gem_request_cancel(params->request);
-
 	mutex_unlock(&dev->struct_mutex);
 
 pre_mutex_err:
diff --git a/kernel/msm-3.18/drivers/gpu/drm/i915/i915_irq.c b/kernel/msm-3.18/drivers/gpu/drm/i915/i915_irq.c
index 0f42a2782..80a1db09a 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/i915/i915_irq.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/i915/i915_irq.c
@@ -812,6 +812,7 @@ static int i915_get_crtc_scanoutpos(struct drm_device *dev, unsigned int pipe,
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
 
 	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_disable_rt();
 
 	/* Get optional system timestamp before query. */
 	if (stime)
@@ -863,6 +864,7 @@ static int i915_get_crtc_scanoutpos(struct drm_device *dev, unsigned int pipe,
 		*etime = ktime_get();
 
 	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_enable_rt();
 
 	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
 
diff --git a/kernel/msm-3.18/drivers/gpu/drm/i915/intel_sprite.c b/kernel/msm-3.18/drivers/gpu/drm/i915/intel_sprite.c
index 56dc132e8..c7c118476 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/i915/intel_sprite.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/i915/intel_sprite.c
@@ -33,80 +33,51 @@
 #include <drm/drm_crtc.h>
 #include <drm/drm_fourcc.h>
 #include <drm/drm_rect.h>
-#include <drm/drm_atomic.h>
-#include <drm/drm_plane_helper.h>
 #include "intel_drv.h"
 #include <drm/i915_drm.h>
 #include "i915_drv.h"
+#include <linux/locallock.h>
 
-static bool
-format_is_yuv(uint32_t format)
-{
-	switch (format) {
-	case DRM_FORMAT_YUYV:
-	case DRM_FORMAT_UYVY:
-	case DRM_FORMAT_VYUY:
-	case DRM_FORMAT_YVYU:
-		return true;
-	default:
-		return false;
-	}
-}
-
-static int usecs_to_scanlines(const struct drm_display_mode *adjusted_mode,
-			      int usecs)
+static int usecs_to_scanlines(const struct drm_display_mode *mode, int usecs)
 {
 	/* paranoia */
-	if (!adjusted_mode->crtc_htotal)
+	if (!mode->crtc_htotal)
 		return 1;
 
-	return DIV_ROUND_UP(usecs * adjusted_mode->crtc_clock,
-			    1000 * adjusted_mode->crtc_htotal);
+	return DIV_ROUND_UP(usecs * mode->crtc_clock, 1000 * mode->crtc_htotal);
 }
 
-/**
- * intel_pipe_update_start() - start update of a set of display registers
- * @crtc: the crtc of which the registers are going to be updated
- * @start_vbl_count: vblank counter return pointer used for error checking
- *
- * Mark the start of an update to pipe registers that should be updated
- * atomically regarding vblank. If the next vblank will happens within
- * the next 100 us, this function waits until the vblank passes.
- *
- * After a successful call to this function, interrupts will be disabled
- * until a subsequent call to intel_pipe_update_end(). That is done to
- * avoid random delays. The value written to @start_vbl_count should be
- * supplied to intel_pipe_update_end() for error checking.
- */
-void intel_pipe_update_start(struct intel_crtc *crtc)
+static DEFINE_LOCAL_IRQ_LOCK(pipe_update_lock);
+
+static bool intel_pipe_update_start(struct intel_crtc *crtc, uint32_t *start_vbl_count)
 {
 	struct drm_device *dev = crtc->base.dev;
-	const struct drm_display_mode *adjusted_mode = &crtc->config->base.adjusted_mode;
+	const struct drm_display_mode *mode = &crtc->config.adjusted_mode;
 	enum pipe pipe = crtc->pipe;
 	long timeout = msecs_to_jiffies_timeout(1);
 	int scanline, min, max, vblank_start;
 	wait_queue_head_t *wq = drm_crtc_vblank_waitqueue(&crtc->base);
 	DEFINE_WAIT(wait);
 
-	vblank_start = adjusted_mode->crtc_vblank_start;
-	if (adjusted_mode->flags & DRM_MODE_FLAG_INTERLACE)
+	WARN_ON(!drm_modeset_is_locked(&crtc->base.mutex));
+
+	vblank_start = mode->crtc_vblank_start;
+	if (mode->flags & DRM_MODE_FLAG_INTERLACE)
 		vblank_start = DIV_ROUND_UP(vblank_start, 2);
 
 	/* FIXME needs to be calibrated sensibly */
-	min = vblank_start - usecs_to_scanlines(adjusted_mode, 100);
+	min = vblank_start - usecs_to_scanlines(mode, 100);
 	max = vblank_start - 1;
 
-	local_irq_disable();
-
 	if (min <= 0 || max <= 0)
-		return;
+		return false;
 
-	if (WARN_ON(drm_crtc_vblank_get(&crtc->base)))
-		return;
+	if (WARN_ON(drm_vblank_get(dev, pipe)))
+		return false;
+
+	local_lock_irq(pipe_update_lock);
 
-	crtc->debug.min_vbl = min;
-	crtc->debug.max_vbl = max;
-	trace_i915_pipe_update_start(crtc);
+	trace_i915_pipe_update_start(crtc, min, max);
 
 	for (;;) {
 		/*
@@ -126,224 +97,54 @@ void intel_pipe_update_start(struct intel_crtc *crtc)
 			break;
 		}
 
-		local_irq_enable();
+		local_unlock_irq(pipe_update_lock);
 
 		timeout = schedule_timeout(timeout);
 
-		local_irq_disable();
+		local_lock_irq(pipe_update_lock);
 	}
 
 	finish_wait(wq, &wait);
 
-	drm_crtc_vblank_put(&crtc->base);
+	drm_vblank_put(dev, pipe);
 
-	crtc->debug.scanline_start = scanline;
-	crtc->debug.start_vbl_time = ktime_get();
-	crtc->debug.start_vbl_count =
-		dev->driver->get_vblank_counter(dev, pipe);
+	*start_vbl_count = dev->driver->get_vblank_counter(dev, pipe);
 
-	trace_i915_pipe_update_vblank_evaded(crtc);
+	trace_i915_pipe_update_vblank_evaded(crtc, min, max, *start_vbl_count);
+
+	return true;
 }
 
-/**
- * intel_pipe_update_end() - end update of a set of display registers
- * @crtc: the crtc of which the registers were updated
- * @start_vbl_count: start vblank counter (used for error checking)
- *
- * Mark the end of an update started with intel_pipe_update_start(). This
- * re-enables interrupts and verifies the update was actually completed
- * before a vblank using the value of @start_vbl_count.
- */
-void intel_pipe_update_end(struct intel_crtc *crtc)
+static void intel_pipe_update_end(struct intel_crtc *crtc, u32 start_vbl_count)
 {
 	struct drm_device *dev = crtc->base.dev;
 	enum pipe pipe = crtc->pipe;
-	int scanline_end = intel_get_crtc_scanline(crtc);
 	u32 end_vbl_count = dev->driver->get_vblank_counter(dev, pipe);
-	ktime_t end_vbl_time = ktime_get();
 
-	trace_i915_pipe_update_end(crtc, end_vbl_count, scanline_end);
+	trace_i915_pipe_update_end(crtc, end_vbl_count);
 
-	local_irq_enable();
+	local_unlock_irq(pipe_update_lock);
 
-	if (crtc->debug.start_vbl_count &&
-	    crtc->debug.start_vbl_count != end_vbl_count) {
-		DRM_ERROR("Atomic update failure on pipe %c (start=%u end=%u) time %lld us, min %d, max %d, scanline start %d, end %d\n",
-			  pipe_name(pipe), crtc->debug.start_vbl_count,
-			  end_vbl_count,
-			  ktime_us_delta(end_vbl_time, crtc->debug.start_vbl_time),
-			  crtc->debug.min_vbl, crtc->debug.max_vbl,
-			  crtc->debug.scanline_start, scanline_end);
-	}
+	if (start_vbl_count != end_vbl_count)
+		DRM_ERROR("Atomic update failure on pipe %c (start=%u end=%u)\n",
+			  pipe_name(pipe), start_vbl_count, end_vbl_count);
 }
 
-static void
-skl_update_plane(struct drm_plane *drm_plane, struct drm_crtc *crtc,
-		 struct drm_framebuffer *fb,
-		 int crtc_x, int crtc_y,
-		 unsigned int crtc_w, unsigned int crtc_h,
-		 uint32_t x, uint32_t y,
-		 uint32_t src_w, uint32_t src_h)
+static void intel_update_primary_plane(struct intel_crtc *crtc)
 {
-	struct drm_device *dev = drm_plane->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_plane *intel_plane = to_intel_plane(drm_plane);
-	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
-	const int pipe = intel_plane->pipe;
-	const int plane = intel_plane->plane + 1;
-	u32 plane_ctl, stride_div, stride;
-	int pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
-	const struct drm_intel_sprite_colorkey *key =
-		&to_intel_plane_state(drm_plane->state)->ckey;
-	unsigned long surf_addr;
-	u32 tile_height, plane_offset, plane_size;
-	unsigned int rotation;
-	int x_offset, y_offset;
-	struct intel_crtc_state *crtc_state = to_intel_crtc(crtc)->config;
-	int scaler_id;
-
-	plane_ctl = PLANE_CTL_ENABLE |
-		PLANE_CTL_PIPE_GAMMA_ENABLE |
-		PLANE_CTL_PIPE_CSC_ENABLE;
-
-	plane_ctl |= skl_plane_ctl_format(fb->pixel_format);
-	plane_ctl |= skl_plane_ctl_tiling(fb->modifier[0]);
-
-	rotation = drm_plane->state->rotation;
-	plane_ctl |= skl_plane_ctl_rotation(rotation);
-
-	intel_update_sprite_watermarks(drm_plane, crtc, src_w, src_h,
-				       pixel_size, true,
-				       src_w != crtc_w || src_h != crtc_h);
-
-	stride_div = intel_fb_stride_alignment(dev, fb->modifier[0],
-					       fb->pixel_format);
-
-	scaler_id = to_intel_plane_state(drm_plane->state)->scaler_id;
-
-	/* Sizes are 0 based */
-	src_w--;
-	src_h--;
-	crtc_w--;
-	crtc_h--;
+	struct drm_i915_private *dev_priv = crtc->base.dev->dev_private;
+	int reg = DSPCNTR(crtc->plane);
 
-	if (key->flags) {
-		I915_WRITE(PLANE_KEYVAL(pipe, plane), key->min_value);
-		I915_WRITE(PLANE_KEYMAX(pipe, plane), key->max_value);
-		I915_WRITE(PLANE_KEYMSK(pipe, plane), key->channel_mask);
-	}
-
-	if (key->flags & I915_SET_COLORKEY_DESTINATION)
-		plane_ctl |= PLANE_CTL_KEY_ENABLE_DESTINATION;
-	else if (key->flags & I915_SET_COLORKEY_SOURCE)
-		plane_ctl |= PLANE_CTL_KEY_ENABLE_SOURCE;
-
-	surf_addr = intel_plane_obj_offset(intel_plane, obj, 0);
-
-	if (intel_rotation_90_or_270(rotation)) {
-		/* stride: Surface height in tiles */
-		tile_height = intel_tile_height(dev, fb->pixel_format,
-						fb->modifier[0], 0);
-		stride = DIV_ROUND_UP(fb->height, tile_height);
-		plane_size = (src_w << 16) | src_h;
-		x_offset = stride * tile_height - y - (src_h + 1);
-		y_offset = x;
-	} else {
-		stride = fb->pitches[0] / stride_div;
-		plane_size = (src_h << 16) | src_w;
-		x_offset = x;
-		y_offset = y;
-	}
-	plane_offset = y_offset << 16 | x_offset;
-
-	I915_WRITE(PLANE_OFFSET(pipe, plane), plane_offset);
-	I915_WRITE(PLANE_STRIDE(pipe, plane), stride);
-	I915_WRITE(PLANE_SIZE(pipe, plane), plane_size);
-
-	/* program plane scaler */
-	if (scaler_id >= 0) {
-		uint32_t ps_ctrl = 0;
-
-		DRM_DEBUG_KMS("plane = %d PS_PLANE_SEL(plane) = 0x%x\n", plane,
-			PS_PLANE_SEL(plane));
-		ps_ctrl = PS_SCALER_EN | PS_PLANE_SEL(plane) |
-			crtc_state->scaler_state.scalers[scaler_id].mode;
-		I915_WRITE(SKL_PS_CTRL(pipe, scaler_id), ps_ctrl);
-		I915_WRITE(SKL_PS_PWR_GATE(pipe, scaler_id), 0);
-		I915_WRITE(SKL_PS_WIN_POS(pipe, scaler_id), (crtc_x << 16) | crtc_y);
-		I915_WRITE(SKL_PS_WIN_SZ(pipe, scaler_id),
-			((crtc_w + 1) << 16)|(crtc_h + 1));
-
-		I915_WRITE(PLANE_POS(pipe, plane), 0);
-	} else {
-		I915_WRITE(PLANE_POS(pipe, plane), (crtc_y << 16) | crtc_x);
-	}
-
-	I915_WRITE(PLANE_CTL(pipe, plane), plane_ctl);
-	I915_WRITE(PLANE_SURF(pipe, plane), surf_addr);
-	POSTING_READ(PLANE_SURF(pipe, plane));
-}
-
-static void
-skl_disable_plane(struct drm_plane *dplane, struct drm_crtc *crtc)
-{
-	struct drm_device *dev = dplane->dev;
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_plane *intel_plane = to_intel_plane(dplane);
-	const int pipe = intel_plane->pipe;
-	const int plane = intel_plane->plane + 1;
-
-	I915_WRITE(PLANE_CTL(pipe, plane), 0);
-
-	I915_WRITE(PLANE_SURF(pipe, plane), 0);
-	POSTING_READ(PLANE_SURF(pipe, plane));
-
-	intel_update_sprite_watermarks(dplane, crtc, 0, 0, 0, false, false);
-}
-
-static void
-chv_update_csc(struct intel_plane *intel_plane, uint32_t format)
-{
-	struct drm_i915_private *dev_priv = intel_plane->base.dev->dev_private;
-	int plane = intel_plane->plane;
-
-	/* Seems RGB data bypasses the CSC always */
-	if (!format_is_yuv(format))
-		return;
-
-	/*
-	 * BT.601 limited range YCbCr -> full range RGB
-	 *
-	 * |r|   | 6537 4769     0|   |cr  |
-	 * |g| = |-3330 4769 -1605| x |y-64|
-	 * |b|   |    0 4769  8263|   |cb  |
-	 *
-	 * Cb and Cr apparently come in as signed already, so no
-	 * need for any offset. For Y we need to remove the offset.
-	 */
-	I915_WRITE(SPCSCYGOFF(plane), SPCSC_OOFF(0) | SPCSC_IOFF(-64));
-	I915_WRITE(SPCSCCBOFF(plane), SPCSC_OOFF(0) | SPCSC_IOFF(0));
-	I915_WRITE(SPCSCCROFF(plane), SPCSC_OOFF(0) | SPCSC_IOFF(0));
-
-	I915_WRITE(SPCSCC01(plane), SPCSC_C1(4769) | SPCSC_C0(6537));
-	I915_WRITE(SPCSCC23(plane), SPCSC_C1(-3330) | SPCSC_C0(0));
-	I915_WRITE(SPCSCC45(plane), SPCSC_C1(-1605) | SPCSC_C0(4769));
-	I915_WRITE(SPCSCC67(plane), SPCSC_C1(4769) | SPCSC_C0(0));
-	I915_WRITE(SPCSCC8(plane), SPCSC_C0(8263));
-
-	I915_WRITE(SPCSCYGICLAMP(plane), SPCSC_IMAX(940) | SPCSC_IMIN(64));
-	I915_WRITE(SPCSCCBICLAMP(plane), SPCSC_IMAX(448) | SPCSC_IMIN(-448));
-	I915_WRITE(SPCSCCRICLAMP(plane), SPCSC_IMAX(448) | SPCSC_IMIN(-448));
-
-	I915_WRITE(SPCSCYGOCLAMP(plane), SPCSC_OMAX(1023) | SPCSC_OMIN(0));
-	I915_WRITE(SPCSCCBOCLAMP(plane), SPCSC_OMAX(1023) | SPCSC_OMIN(0));
-	I915_WRITE(SPCSCCROCLAMP(plane), SPCSC_OMAX(1023) | SPCSC_OMIN(0));
+	if (crtc->primary_enabled)
+		I915_WRITE(reg, I915_READ(reg) | DISPLAY_PLANE_ENABLE);
+	else
+		I915_WRITE(reg, I915_READ(reg) & ~DISPLAY_PLANE_ENABLE);
 }
 
 static void
 vlv_update_plane(struct drm_plane *dplane, struct drm_crtc *crtc,
 		 struct drm_framebuffer *fb,
-		 int crtc_x, int crtc_y,
+		 struct drm_i915_gem_object *obj, int crtc_x, int crtc_y,
 		 unsigned int crtc_w, unsigned int crtc_h,
 		 uint32_t x, uint32_t y,
 		 uint32_t src_w, uint32_t src_h)
@@ -351,16 +152,22 @@ vlv_update_plane(struct drm_plane *dplane, struct drm_crtc *crtc,
 	struct drm_device *dev = dplane->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_plane *intel_plane = to_intel_plane(dplane);
-	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	int pipe = intel_plane->pipe;
 	int plane = intel_plane->plane;
 	u32 sprctl;
 	unsigned long sprsurf_offset, linear_offset;
 	int pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
-	const struct drm_intel_sprite_colorkey *key =
-		&to_intel_plane_state(dplane->state)->ckey;
+	u32 start_vbl_count;
+	bool atomic_update;
 
-	sprctl = SP_ENABLE;
+	sprctl = I915_READ(SPCNTR(pipe, plane));
+
+	/* Mask out pixel format bits in case we change it */
+	sprctl &= ~SP_PIXFORMAT_MASK;
+	sprctl &= ~SP_YUV_BYTE_ORDER_MASK;
+	sprctl &= ~SP_TILED;
+	sprctl &= ~SP_ROTATE_180;
 
 	switch (fb->pixel_format) {
 	case DRM_FORMAT_YUYV:
@@ -414,6 +221,12 @@ vlv_update_plane(struct drm_plane *dplane, struct drm_crtc *crtc,
 	if (obj->tiling_mode != I915_TILING_NONE)
 		sprctl |= SP_TILED;
 
+	sprctl |= SP_ENABLE;
+
+	intel_update_sprite_watermarks(dplane, crtc, src_w, src_h,
+				       pixel_size, true,
+				       src_w != crtc_w || src_h != crtc_h);
+
 	/* Sizes are 0 based */
 	src_w--;
 	src_h--;
@@ -421,14 +234,13 @@ vlv_update_plane(struct drm_plane *dplane, struct drm_crtc *crtc,
 	crtc_h--;
 
 	linear_offset = y * fb->pitches[0] + x * pixel_size;
-	sprsurf_offset = intel_gen4_compute_page_offset(dev_priv,
-							&x, &y,
+	sprsurf_offset = intel_gen4_compute_page_offset(&x, &y,
 							obj->tiling_mode,
 							pixel_size,
 							fb->pitches[0]);
 	linear_offset -= sprsurf_offset;
 
-	if (dplane->state->rotation == BIT(DRM_ROTATE_180)) {
+	if (intel_plane->rotation == BIT(DRM_ROTATE_180)) {
 		sprctl |= SP_ROTATE_180;
 
 		x += src_w;
@@ -436,17 +248,9 @@ vlv_update_plane(struct drm_plane *dplane, struct drm_crtc *crtc,
 		linear_offset += src_h * fb->pitches[0] + src_w * pixel_size;
 	}
 
-	if (key->flags) {
-		I915_WRITE(SPKEYMINVAL(pipe, plane), key->min_value);
-		I915_WRITE(SPKEYMAXVAL(pipe, plane), key->max_value);
-		I915_WRITE(SPKEYMSK(pipe, plane), key->channel_mask);
-	}
-
-	if (key->flags & I915_SET_COLORKEY_SOURCE)
-		sprctl |= SP_SOURCE_KEY;
+	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
 
-	if (IS_CHERRYVIEW(dev) && pipe == PIPE_B)
-		chv_update_csc(intel_plane, fb->pixel_format);
+	intel_update_primary_plane(intel_crtc);
 
 	I915_WRITE(SPSTRIDE(pipe, plane), fb->pitches[0]);
 	I915_WRITE(SPPOS(pipe, plane), (crtc_y << 16) | crtc_x);
@@ -456,13 +260,15 @@ vlv_update_plane(struct drm_plane *dplane, struct drm_crtc *crtc,
 	else
 		I915_WRITE(SPLINOFF(pipe, plane), linear_offset);
 
-	I915_WRITE(SPCONSTALPHA(pipe, plane), 0);
-
 	I915_WRITE(SPSIZE(pipe, plane), (crtc_h << 16) | crtc_w);
 	I915_WRITE(SPCNTR(pipe, plane), sprctl);
 	I915_WRITE(SPSURF(pipe, plane), i915_gem_obj_ggtt_offset(obj) +
 		   sprsurf_offset);
-	POSTING_READ(SPSURF(pipe, plane));
+
+	intel_flush_primary_plane(dev_priv, intel_crtc->plane);
+
+	if (atomic_update)
+		intel_pipe_update_end(intel_crtc, start_vbl_count);
 }
 
 static void
@@ -471,19 +277,84 @@ vlv_disable_plane(struct drm_plane *dplane, struct drm_crtc *crtc)
 	struct drm_device *dev = dplane->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_plane *intel_plane = to_intel_plane(dplane);
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	int pipe = intel_plane->pipe;
 	int plane = intel_plane->plane;
+	u32 start_vbl_count;
+	bool atomic_update;
+
+	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
 
-	I915_WRITE(SPCNTR(pipe, plane), 0);
+	intel_update_primary_plane(intel_crtc);
 
+	I915_WRITE(SPCNTR(pipe, plane), I915_READ(SPCNTR(pipe, plane)) &
+		   ~SP_ENABLE);
+	/* Activate double buffered register update */
 	I915_WRITE(SPSURF(pipe, plane), 0);
-	POSTING_READ(SPSURF(pipe, plane));
+
+	intel_flush_primary_plane(dev_priv, intel_crtc->plane);
+
+	if (atomic_update)
+		intel_pipe_update_end(intel_crtc, start_vbl_count);
+
+	intel_update_sprite_watermarks(dplane, crtc, 0, 0, 0, false, false);
+}
+
+static int
+vlv_update_colorkey(struct drm_plane *dplane,
+		    struct drm_intel_sprite_colorkey *key)
+{
+	struct drm_device *dev = dplane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_plane *intel_plane = to_intel_plane(dplane);
+	int pipe = intel_plane->pipe;
+	int plane = intel_plane->plane;
+	u32 sprctl;
+
+	if (key->flags & I915_SET_COLORKEY_DESTINATION)
+		return -EINVAL;
+
+	I915_WRITE(SPKEYMINVAL(pipe, plane), key->min_value);
+	I915_WRITE(SPKEYMAXVAL(pipe, plane), key->max_value);
+	I915_WRITE(SPKEYMSK(pipe, plane), key->channel_mask);
+
+	sprctl = I915_READ(SPCNTR(pipe, plane));
+	sprctl &= ~SP_SOURCE_KEY;
+	if (key->flags & I915_SET_COLORKEY_SOURCE)
+		sprctl |= SP_SOURCE_KEY;
+	I915_WRITE(SPCNTR(pipe, plane), sprctl);
+
+	POSTING_READ(SPKEYMSK(pipe, plane));
+
+	return 0;
+}
+
+static void
+vlv_get_colorkey(struct drm_plane *dplane,
+		 struct drm_intel_sprite_colorkey *key)
+{
+	struct drm_device *dev = dplane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_plane *intel_plane = to_intel_plane(dplane);
+	int pipe = intel_plane->pipe;
+	int plane = intel_plane->plane;
+	u32 sprctl;
+
+	key->min_value = I915_READ(SPKEYMINVAL(pipe, plane));
+	key->max_value = I915_READ(SPKEYMAXVAL(pipe, plane));
+	key->channel_mask = I915_READ(SPKEYMSK(pipe, plane));
+
+	sprctl = I915_READ(SPCNTR(pipe, plane));
+	if (sprctl & SP_SOURCE_KEY)
+		key->flags = I915_SET_COLORKEY_SOURCE;
+	else
+		key->flags = I915_SET_COLORKEY_NONE;
 }
 
 static void
 ivb_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 		 struct drm_framebuffer *fb,
-		 int crtc_x, int crtc_y,
+		 struct drm_i915_gem_object *obj, int crtc_x, int crtc_y,
 		 unsigned int crtc_w, unsigned int crtc_h,
 		 uint32_t x, uint32_t y,
 		 uint32_t src_w, uint32_t src_h)
@@ -491,15 +362,22 @@ ivb_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 	struct drm_device *dev = plane->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_plane *intel_plane = to_intel_plane(plane);
-	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
-	enum pipe pipe = intel_plane->pipe;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+	int pipe = intel_plane->pipe;
 	u32 sprctl, sprscale = 0;
 	unsigned long sprsurf_offset, linear_offset;
 	int pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
-	const struct drm_intel_sprite_colorkey *key =
-		&to_intel_plane_state(plane->state)->ckey;
+	u32 start_vbl_count;
+	bool atomic_update;
 
-	sprctl = SPRITE_ENABLE;
+	sprctl = I915_READ(SPRCTL(pipe));
+
+	/* Mask out pixel format bits in case we change it */
+	sprctl &= ~SPRITE_PIXFORMAT_MASK;
+	sprctl &= ~SPRITE_RGB_ORDER_RGBX;
+	sprctl &= ~SPRITE_YUV_BYTE_ORDER_MASK;
+	sprctl &= ~SPRITE_TILED;
+	sprctl &= ~SPRITE_ROTATE_180;
 
 	switch (fb->pixel_format) {
 	case DRM_FORMAT_XBGR8888:
@@ -538,6 +416,8 @@ ivb_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 	else
 		sprctl |= SPRITE_TRICKLE_FEED_DISABLE;
 
+	sprctl |= SPRITE_ENABLE;
+
 	if (IS_HASWELL(dev) || IS_BROADWELL(dev))
 		sprctl |= SPRITE_PIPE_CSC_ENABLE;
 
@@ -556,12 +436,11 @@ ivb_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 
 	linear_offset = y * fb->pitches[0] + x * pixel_size;
 	sprsurf_offset =
-		intel_gen4_compute_page_offset(dev_priv,
-					       &x, &y, obj->tiling_mode,
+		intel_gen4_compute_page_offset(&x, &y, obj->tiling_mode,
 					       pixel_size, fb->pitches[0]);
 	linear_offset -= sprsurf_offset;
 
-	if (plane->state->rotation == BIT(DRM_ROTATE_180)) {
+	if (intel_plane->rotation == BIT(DRM_ROTATE_180)) {
 		sprctl |= SPRITE_ROTATE_180;
 
 		/* HSW and BDW does this automagically in hardware */
@@ -573,16 +452,9 @@ ivb_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 		}
 	}
 
-	if (key->flags) {
-		I915_WRITE(SPRKEYVAL(pipe), key->min_value);
-		I915_WRITE(SPRKEYMAX(pipe), key->max_value);
-		I915_WRITE(SPRKEYMSK(pipe), key->channel_mask);
-	}
+	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
 
-	if (key->flags & I915_SET_COLORKEY_DESTINATION)
-		sprctl |= SPRITE_DEST_KEY;
-	else if (key->flags & I915_SET_COLORKEY_SOURCE)
-		sprctl |= SPRITE_SOURCE_KEY;
+	intel_update_primary_plane(intel_crtc);
 
 	I915_WRITE(SPRSTRIDE(pipe), fb->pitches[0]);
 	I915_WRITE(SPRPOS(pipe), (crtc_y << 16) | crtc_x);
@@ -602,7 +474,11 @@ ivb_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 	I915_WRITE(SPRCTL(pipe), sprctl);
 	I915_WRITE(SPRSURF(pipe),
 		   i915_gem_obj_ggtt_offset(obj) + sprsurf_offset);
-	POSTING_READ(SPRSURF(pipe));
+
+	intel_flush_primary_plane(dev_priv, intel_crtc->plane);
+
+	if (atomic_update)
+		intel_pipe_update_end(intel_crtc, start_vbl_count);
 }
 
 static void
@@ -611,21 +487,94 @@ ivb_disable_plane(struct drm_plane *plane, struct drm_crtc *crtc)
 	struct drm_device *dev = plane->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_plane *intel_plane = to_intel_plane(plane);
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	int pipe = intel_plane->pipe;
+	u32 start_vbl_count;
+	bool atomic_update;
+
+	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
+
+	intel_update_primary_plane(intel_crtc);
 
-	I915_WRITE(SPRCTL(pipe), 0);
+	I915_WRITE(SPRCTL(pipe), I915_READ(SPRCTL(pipe)) & ~SPRITE_ENABLE);
 	/* Can't leave the scaler enabled... */
 	if (intel_plane->can_scale)
 		I915_WRITE(SPRSCALE(pipe), 0);
-
+	/* Activate double buffered register update */
 	I915_WRITE(SPRSURF(pipe), 0);
-	POSTING_READ(SPRSURF(pipe));
+
+	intel_flush_primary_plane(dev_priv, intel_crtc->plane);
+
+	if (atomic_update)
+		intel_pipe_update_end(intel_crtc, start_vbl_count);
+
+	/*
+	 * Avoid underruns when disabling the sprite.
+	 * FIXME remove once watermark updates are done properly.
+	 */
+	intel_wait_for_vblank(dev, pipe);
+
+	intel_update_sprite_watermarks(plane, crtc, 0, 0, 0, false, false);
+}
+
+static int
+ivb_update_colorkey(struct drm_plane *plane,
+		    struct drm_intel_sprite_colorkey *key)
+{
+	struct drm_device *dev = plane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_plane *intel_plane;
+	u32 sprctl;
+	int ret = 0;
+
+	intel_plane = to_intel_plane(plane);
+
+	I915_WRITE(SPRKEYVAL(intel_plane->pipe), key->min_value);
+	I915_WRITE(SPRKEYMAX(intel_plane->pipe), key->max_value);
+	I915_WRITE(SPRKEYMSK(intel_plane->pipe), key->channel_mask);
+
+	sprctl = I915_READ(SPRCTL(intel_plane->pipe));
+	sprctl &= ~(SPRITE_SOURCE_KEY | SPRITE_DEST_KEY);
+	if (key->flags & I915_SET_COLORKEY_DESTINATION)
+		sprctl |= SPRITE_DEST_KEY;
+	else if (key->flags & I915_SET_COLORKEY_SOURCE)
+		sprctl |= SPRITE_SOURCE_KEY;
+	I915_WRITE(SPRCTL(intel_plane->pipe), sprctl);
+
+	POSTING_READ(SPRKEYMSK(intel_plane->pipe));
+
+	return ret;
+}
+
+static void
+ivb_get_colorkey(struct drm_plane *plane, struct drm_intel_sprite_colorkey *key)
+{
+	struct drm_device *dev = plane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_plane *intel_plane;
+	u32 sprctl;
+
+	intel_plane = to_intel_plane(plane);
+
+	key->min_value = I915_READ(SPRKEYVAL(intel_plane->pipe));
+	key->max_value = I915_READ(SPRKEYMAX(intel_plane->pipe));
+	key->channel_mask = I915_READ(SPRKEYMSK(intel_plane->pipe));
+	key->flags = 0;
+
+	sprctl = I915_READ(SPRCTL(intel_plane->pipe));
+
+	if (sprctl & SPRITE_DEST_KEY)
+		key->flags = I915_SET_COLORKEY_DESTINATION;
+	else if (sprctl & SPRITE_SOURCE_KEY)
+		key->flags = I915_SET_COLORKEY_SOURCE;
+	else
+		key->flags = I915_SET_COLORKEY_NONE;
 }
 
 static void
 ilk_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 		 struct drm_framebuffer *fb,
-		 int crtc_x, int crtc_y,
+		 struct drm_i915_gem_object *obj, int crtc_x, int crtc_y,
 		 unsigned int crtc_w, unsigned int crtc_h,
 		 uint32_t x, uint32_t y,
 		 uint32_t src_w, uint32_t src_h)
@@ -633,15 +582,22 @@ ilk_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 	struct drm_device *dev = plane->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_plane *intel_plane = to_intel_plane(plane);
-	struct drm_i915_gem_object *obj = intel_fb_obj(fb);
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	int pipe = intel_plane->pipe;
 	unsigned long dvssurf_offset, linear_offset;
 	u32 dvscntr, dvsscale;
 	int pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
-	const struct drm_intel_sprite_colorkey *key =
-		&to_intel_plane_state(plane->state)->ckey;
+	u32 start_vbl_count;
+	bool atomic_update;
+
+	dvscntr = I915_READ(DVSCNTR(pipe));
 
-	dvscntr = DVS_ENABLE;
+	/* Mask out pixel format bits in case we change it */
+	dvscntr &= ~DVS_PIXFORMAT_MASK;
+	dvscntr &= ~DVS_RGB_ORDER_XBGR;
+	dvscntr &= ~DVS_YUV_BYTE_ORDER_MASK;
+	dvscntr &= ~DVS_TILED;
+	dvscntr &= ~DVS_ROTATE_180;
 
 	switch (fb->pixel_format) {
 	case DRM_FORMAT_XBGR8888:
@@ -677,6 +633,7 @@ ilk_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 
 	if (IS_GEN6(dev))
 		dvscntr |= DVS_TRICKLE_FEED_DISABLE; /* must disable */
+	dvscntr |= DVS_ENABLE;
 
 	intel_update_sprite_watermarks(plane, crtc, src_w, src_h,
 				       pixel_size, true,
@@ -694,12 +651,11 @@ ilk_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 
 	linear_offset = y * fb->pitches[0] + x * pixel_size;
 	dvssurf_offset =
-		intel_gen4_compute_page_offset(dev_priv,
-					       &x, &y, obj->tiling_mode,
+		intel_gen4_compute_page_offset(&x, &y, obj->tiling_mode,
 					       pixel_size, fb->pitches[0]);
 	linear_offset -= dvssurf_offset;
 
-	if (plane->state->rotation == BIT(DRM_ROTATE_180)) {
+	if (intel_plane->rotation == BIT(DRM_ROTATE_180)) {
 		dvscntr |= DVS_ROTATE_180;
 
 		x += src_w;
@@ -707,16 +663,9 @@ ilk_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 		linear_offset += src_h * fb->pitches[0] + src_w * pixel_size;
 	}
 
-	if (key->flags) {
-		I915_WRITE(DVSKEYVAL(pipe), key->min_value);
-		I915_WRITE(DVSKEYMAX(pipe), key->max_value);
-		I915_WRITE(DVSKEYMSK(pipe), key->channel_mask);
-	}
+	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
 
-	if (key->flags & I915_SET_COLORKEY_DESTINATION)
-		dvscntr |= DVS_DEST_KEY;
-	else if (key->flags & I915_SET_COLORKEY_SOURCE)
-		dvscntr |= DVS_SOURCE_KEY;
+	intel_update_primary_plane(intel_crtc);
 
 	I915_WRITE(DVSSTRIDE(pipe), fb->pitches[0]);
 	I915_WRITE(DVSPOS(pipe), (crtc_y << 16) | crtc_x);
@@ -731,7 +680,11 @@ ilk_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
 	I915_WRITE(DVSCNTR(pipe), dvscntr);
 	I915_WRITE(DVSSURF(pipe),
 		   i915_gem_obj_ggtt_offset(obj) + dvssurf_offset);
-	POSTING_READ(DVSSURF(pipe));
+
+	intel_flush_primary_plane(dev_priv, intel_crtc->plane);
+
+	if (atomic_update)
+		intel_pipe_update_end(intel_crtc, start_vbl_count);
 }
 
 static void
@@ -740,41 +693,212 @@ ilk_disable_plane(struct drm_plane *plane, struct drm_crtc *crtc)
 	struct drm_device *dev = plane->dev;
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	struct intel_plane *intel_plane = to_intel_plane(plane);
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	int pipe = intel_plane->pipe;
+	u32 start_vbl_count;
+	bool atomic_update;
+
+	atomic_update = intel_pipe_update_start(intel_crtc, &start_vbl_count);
+
+	intel_update_primary_plane(intel_crtc);
 
-	I915_WRITE(DVSCNTR(pipe), 0);
+	I915_WRITE(DVSCNTR(pipe), I915_READ(DVSCNTR(pipe)) & ~DVS_ENABLE);
 	/* Disable the scaler */
 	I915_WRITE(DVSSCALE(pipe), 0);
-
+	/* Flush double buffered register updates */
 	I915_WRITE(DVSSURF(pipe), 0);
-	POSTING_READ(DVSSURF(pipe));
+
+	intel_flush_primary_plane(dev_priv, intel_crtc->plane);
+
+	if (atomic_update)
+		intel_pipe_update_end(intel_crtc, start_vbl_count);
+
+	/*
+	 * Avoid underruns when disabling the sprite.
+	 * FIXME remove once watermark updates are done properly.
+	 */
+	intel_wait_for_vblank(dev, pipe);
+
+	intel_update_sprite_watermarks(plane, crtc, 0, 0, 0, false, false);
+}
+
+static void
+intel_post_enable_primary(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+
+	/*
+	 * BDW signals flip done immediately if the plane
+	 * is disabled, even if the plane enable is already
+	 * armed to occur at the next vblank :(
+	 */
+	if (IS_BROADWELL(dev))
+		intel_wait_for_vblank(dev, intel_crtc->pipe);
+
+	/*
+	 * FIXME IPS should be fine as long as one plane is
+	 * enabled, but in practice it seems to have problems
+	 * when going from primary only to sprite only and vice
+	 * versa.
+	 */
+	hsw_enable_ips(intel_crtc);
+
+	mutex_lock(&dev->struct_mutex);
+	intel_update_fbc(dev);
+	mutex_unlock(&dev->struct_mutex);
+}
+
+static void
+intel_pre_disable_primary(struct drm_crtc *crtc)
+{
+	struct drm_device *dev = crtc->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
+
+	mutex_lock(&dev->struct_mutex);
+	if (dev_priv->fbc.plane == intel_crtc->plane)
+		intel_disable_fbc(dev);
+	mutex_unlock(&dev->struct_mutex);
+
+	/*
+	 * FIXME IPS should be fine as long as one plane is
+	 * enabled, but in practice it seems to have problems
+	 * when going from primary only to sprite only and vice
+	 * versa.
+	 */
+	hsw_disable_ips(intel_crtc);
+}
+
+static int
+ilk_update_colorkey(struct drm_plane *plane,
+		    struct drm_intel_sprite_colorkey *key)
+{
+	struct drm_device *dev = plane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_plane *intel_plane;
+	u32 dvscntr;
+	int ret = 0;
+
+	intel_plane = to_intel_plane(plane);
+
+	I915_WRITE(DVSKEYVAL(intel_plane->pipe), key->min_value);
+	I915_WRITE(DVSKEYMAX(intel_plane->pipe), key->max_value);
+	I915_WRITE(DVSKEYMSK(intel_plane->pipe), key->channel_mask);
+
+	dvscntr = I915_READ(DVSCNTR(intel_plane->pipe));
+	dvscntr &= ~(DVS_SOURCE_KEY | DVS_DEST_KEY);
+	if (key->flags & I915_SET_COLORKEY_DESTINATION)
+		dvscntr |= DVS_DEST_KEY;
+	else if (key->flags & I915_SET_COLORKEY_SOURCE)
+		dvscntr |= DVS_SOURCE_KEY;
+	I915_WRITE(DVSCNTR(intel_plane->pipe), dvscntr);
+
+	POSTING_READ(DVSKEYMSK(intel_plane->pipe));
+
+	return ret;
+}
+
+static void
+ilk_get_colorkey(struct drm_plane *plane, struct drm_intel_sprite_colorkey *key)
+{
+	struct drm_device *dev = plane->dev;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_plane *intel_plane;
+	u32 dvscntr;
+
+	intel_plane = to_intel_plane(plane);
+
+	key->min_value = I915_READ(DVSKEYVAL(intel_plane->pipe));
+	key->max_value = I915_READ(DVSKEYMAX(intel_plane->pipe));
+	key->channel_mask = I915_READ(DVSKEYMSK(intel_plane->pipe));
+	key->flags = 0;
+
+	dvscntr = I915_READ(DVSCNTR(intel_plane->pipe));
+
+	if (dvscntr & DVS_DEST_KEY)
+		key->flags = I915_SET_COLORKEY_DESTINATION;
+	else if (dvscntr & DVS_SOURCE_KEY)
+		key->flags = I915_SET_COLORKEY_SOURCE;
+	else
+		key->flags = I915_SET_COLORKEY_NONE;
+}
+
+static bool
+format_is_yuv(uint32_t format)
+{
+	switch (format) {
+	case DRM_FORMAT_YUYV:
+	case DRM_FORMAT_UYVY:
+	case DRM_FORMAT_VYUY:
+	case DRM_FORMAT_YVYU:
+		return true;
+	default:
+		return false;
+	}
+}
+
+static bool colorkey_enabled(struct intel_plane *intel_plane)
+{
+	struct drm_intel_sprite_colorkey key;
+
+	intel_plane->get_colorkey(&intel_plane->base, &key);
+
+	return key.flags != I915_SET_COLORKEY_NONE;
 }
 
 static int
-intel_check_sprite_plane(struct drm_plane *plane,
-			 struct intel_crtc_state *crtc_state,
-			 struct intel_plane_state *state)
+intel_update_plane(struct drm_plane *plane, struct drm_crtc *crtc,
+		   struct drm_framebuffer *fb, int crtc_x, int crtc_y,
+		   unsigned int crtc_w, unsigned int crtc_h,
+		   uint32_t src_x, uint32_t src_y,
+		   uint32_t src_w, uint32_t src_h)
 {
 	struct drm_device *dev = plane->dev;
-	struct drm_crtc *crtc = state->base.crtc;
 	struct intel_crtc *intel_crtc = to_intel_crtc(crtc);
 	struct intel_plane *intel_plane = to_intel_plane(plane);
-	struct drm_framebuffer *fb = state->base.fb;
-	int crtc_x, crtc_y;
-	unsigned int crtc_w, crtc_h;
-	uint32_t src_x, src_y, src_w, src_h;
-	struct drm_rect *src = &state->src;
-	struct drm_rect *dst = &state->dst;
-	const struct drm_rect *clip = &state->clip;
+	enum pipe pipe = intel_crtc->pipe;
+	struct intel_framebuffer *intel_fb = to_intel_framebuffer(fb);
+	struct drm_i915_gem_object *obj = intel_fb->obj;
+	struct drm_i915_gem_object *old_obj = intel_plane->obj;
+	int ret;
+	bool primary_enabled;
+	bool visible;
 	int hscale, vscale;
 	int max_scale, min_scale;
-	bool can_scale;
-	int pixel_size;
-
-	if (!fb) {
-		state->visible = false;
-		return 0;
-	}
+	int pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
+	struct drm_rect src = {
+		/* sample coordinates in 16.16 fixed point */
+		.x1 = src_x,
+		.x2 = src_x + src_w,
+		.y1 = src_y,
+		.y2 = src_y + src_h,
+	};
+	struct drm_rect dst = {
+		/* integer pixels */
+		.x1 = crtc_x,
+		.x2 = crtc_x + crtc_w,
+		.y1 = crtc_y,
+		.y2 = crtc_y + crtc_h,
+	};
+	const struct drm_rect clip = {
+		.x2 = intel_crtc->active ? intel_crtc->config.pipe_src_w : 0,
+		.y2 = intel_crtc->active ? intel_crtc->config.pipe_src_h : 0,
+	};
+	const struct {
+		int crtc_x, crtc_y;
+		unsigned int crtc_w, crtc_h;
+		uint32_t src_x, src_y, src_w, src_h;
+	} orig = {
+		.crtc_x = crtc_x,
+		.crtc_y = crtc_y,
+		.crtc_w = crtc_w,
+		.crtc_h = crtc_h,
+		.src_x = src_x,
+		.src_y = src_y,
+		.src_w = src_w,
+		.src_h = src_h,
+	};
 
 	/* Don't modify another pipe's plane */
 	if (intel_plane->pipe != intel_crtc->pipe) {
@@ -788,22 +912,14 @@ intel_check_sprite_plane(struct drm_plane *plane,
 		return -EINVAL;
 	}
 
-	/* setup can_scale, min_scale, max_scale */
-	if (INTEL_INFO(dev)->gen >= 9) {
-		/* use scaler when colorkey is not required */
-		if (state->ckey.flags == I915_SET_COLORKEY_NONE) {
-			can_scale = 1;
-			min_scale = 1;
-			max_scale = skl_max_scale(intel_crtc, crtc_state);
-		} else {
-			can_scale = 0;
-			min_scale = DRM_PLANE_HELPER_NO_SCALING;
-			max_scale = DRM_PLANE_HELPER_NO_SCALING;
-		}
-	} else {
-		can_scale = intel_plane->can_scale;
-		max_scale = intel_plane->max_downscale << 16;
-		min_scale = intel_plane->can_scale ? 1 : (1 << 16);
+	/* Sprite planes can be linear or x-tiled surfaces */
+	switch (obj->tiling_mode) {
+		case I915_TILING_NONE:
+		case I915_TILING_X:
+			break;
+		default:
+			DRM_DEBUG_KMS("Unsupported tiling mode\n");
+			return -EINVAL;
 	}
 
 	/*
@@ -811,55 +927,58 @@ intel_check_sprite_plane(struct drm_plane *plane,
 	 * coordinates and sizes. We probably need some way to decide whether
 	 * more strict checking should be done instead.
 	 */
-	drm_rect_rotate(src, fb->width << 16, fb->height << 16,
-			state->base.rotation);
+	max_scale = intel_plane->max_downscale << 16;
+	min_scale = intel_plane->can_scale ? 1 : (1 << 16);
 
-	hscale = drm_rect_calc_hscale_relaxed(src, dst, min_scale, max_scale);
+	drm_rect_rotate(&src, fb->width << 16, fb->height << 16,
+			intel_plane->rotation);
+
+	hscale = drm_rect_calc_hscale_relaxed(&src, &dst, min_scale, max_scale);
 	BUG_ON(hscale < 0);
 
-	vscale = drm_rect_calc_vscale_relaxed(src, dst, min_scale, max_scale);
+	vscale = drm_rect_calc_vscale_relaxed(&src, &dst, min_scale, max_scale);
 	BUG_ON(vscale < 0);
 
-	state->visible = drm_rect_clip_scaled(src, dst, clip, hscale, vscale);
+	visible = drm_rect_clip_scaled(&src, &dst, &clip, hscale, vscale);
 
-	crtc_x = dst->x1;
-	crtc_y = dst->y1;
-	crtc_w = drm_rect_width(dst);
-	crtc_h = drm_rect_height(dst);
+	crtc_x = dst.x1;
+	crtc_y = dst.y1;
+	crtc_w = drm_rect_width(&dst);
+	crtc_h = drm_rect_height(&dst);
 
-	if (state->visible) {
+	if (visible) {
 		/* check again in case clipping clamped the results */
-		hscale = drm_rect_calc_hscale(src, dst, min_scale, max_scale);
+		hscale = drm_rect_calc_hscale(&src, &dst, min_scale, max_scale);
 		if (hscale < 0) {
 			DRM_DEBUG_KMS("Horizontal scaling factor out of limits\n");
-			drm_rect_debug_print(src, true);
-			drm_rect_debug_print(dst, false);
+			drm_rect_debug_print(&src, true);
+			drm_rect_debug_print(&dst, false);
 
 			return hscale;
 		}
 
-		vscale = drm_rect_calc_vscale(src, dst, min_scale, max_scale);
+		vscale = drm_rect_calc_vscale(&src, &dst, min_scale, max_scale);
 		if (vscale < 0) {
 			DRM_DEBUG_KMS("Vertical scaling factor out of limits\n");
-			drm_rect_debug_print(src, true);
-			drm_rect_debug_print(dst, false);
+			drm_rect_debug_print(&src, true);
+			drm_rect_debug_print(&dst, false);
 
 			return vscale;
 		}
 
 		/* Make the source viewport size an exact multiple of the scaling factors. */
-		drm_rect_adjust_size(src,
-				     drm_rect_width(dst) * hscale - drm_rect_width(src),
-				     drm_rect_height(dst) * vscale - drm_rect_height(src));
+		drm_rect_adjust_size(&src,
+				     drm_rect_width(&dst) * hscale - drm_rect_width(&src),
+				     drm_rect_height(&dst) * vscale - drm_rect_height(&src));
 
-		drm_rect_rotate_inv(src, fb->width << 16, fb->height << 16,
-				    state->base.rotation);
+		drm_rect_rotate_inv(&src, fb->width << 16, fb->height << 16,
+				    intel_plane->rotation);
 
 		/* sanity check to make sure the src viewport wasn't enlarged */
-		WARN_ON(src->x1 < (int) state->base.src_x ||
-			src->y1 < (int) state->base.src_y ||
-			src->x2 > (int) state->base.src_x + state->base.src_w ||
-			src->y2 > (int) state->base.src_y + state->base.src_h);
+		WARN_ON(src.x1 < (int) src_x ||
+			src.y1 < (int) src_y ||
+			src.x2 > (int) (src_x + src_w) ||
+			src.y2 > (int) (src_y + src_h));
 
 		/*
 		 * Hardware doesn't handle subpixel coordinates.
@@ -867,10 +986,10 @@ intel_check_sprite_plane(struct drm_plane *plane,
 		 * increase the source viewport size, because that could
 		 * push the downscaling factor out of bounds.
 		 */
-		src_x = src->x1 >> 16;
-		src_w = drm_rect_width(src) >> 16;
-		src_y = src->y1 >> 16;
-		src_h = drm_rect_height(src) >> 16;
+		src_x = src.x1 >> 16;
+		src_w = drm_rect_width(&src) >> 16;
+		src_y = src.y1 >> 16;
+		src_h = drm_rect_height(&src) >> 16;
 
 		if (format_is_yuv(fb->pixel_format)) {
 			src_x &= ~1;
@@ -880,79 +999,168 @@ intel_check_sprite_plane(struct drm_plane *plane,
 			 * Must keep src and dst the
 			 * same if we can't scale.
 			 */
-			if (!can_scale)
+			if (!intel_plane->can_scale)
 				crtc_w &= ~1;
 
 			if (crtc_w == 0)
-				state->visible = false;
+				visible = false;
 		}
 	}
 
 	/* Check size restrictions when scaling */
-	if (state->visible && (src_w != crtc_w || src_h != crtc_h)) {
+	if (visible && (src_w != crtc_w || src_h != crtc_h)) {
 		unsigned int width_bytes;
 
-		WARN_ON(!can_scale);
+		WARN_ON(!intel_plane->can_scale);
 
 		/* FIXME interlacing min height is 6 */
 
 		if (crtc_w < 3 || crtc_h < 3)
-			state->visible = false;
+			visible = false;
 
 		if (src_w < 3 || src_h < 3)
-			state->visible = false;
+			visible = false;
 
-		pixel_size = drm_format_plane_cpp(fb->pixel_format, 0);
-		width_bytes = ((src_x * pixel_size) & 63) +
-					src_w * pixel_size;
+		width_bytes = ((src_x * pixel_size) & 63) + src_w * pixel_size;
 
-		if (INTEL_INFO(dev)->gen < 9 && (src_w > 2048 || src_h > 2048 ||
-		    width_bytes > 4096 || fb->pitches[0] > 4096)) {
+		if (src_w > 2048 || src_h > 2048 ||
+		    width_bytes > 4096 || fb->pitches[0] > 4096) {
 			DRM_DEBUG_KMS("Source dimensions exceed hardware limits\n");
 			return -EINVAL;
 		}
 	}
 
-	if (state->visible) {
-		src->x1 = src_x << 16;
-		src->x2 = (src_x + src_w) << 16;
-		src->y1 = src_y << 16;
-		src->y2 = (src_y + src_h) << 16;
+	dst.x1 = crtc_x;
+	dst.x2 = crtc_x + crtc_w;
+	dst.y1 = crtc_y;
+	dst.y2 = crtc_y + crtc_h;
+
+	/*
+	 * If the sprite is completely covering the primary plane,
+	 * we can disable the primary and save power.
+	 */
+	primary_enabled = !drm_rect_equals(&dst, &clip) || colorkey_enabled(intel_plane);
+	WARN_ON(!primary_enabled && !visible && intel_crtc->active);
+
+	mutex_lock(&dev->struct_mutex);
+
+	/* Note that this will apply the VT-d workaround for scanouts,
+	 * which is more restrictive than required for sprites. (The
+	 * primary plane requires 256KiB alignment with 64 PTE padding,
+	 * the sprite planes only require 128KiB alignment and 32 PTE padding.
+	 */
+	ret = intel_pin_and_fence_fb_obj(dev, obj, NULL);
+
+	i915_gem_track_fb(old_obj, obj,
+			  INTEL_FRONTBUFFER_SPRITE(pipe));
+	mutex_unlock(&dev->struct_mutex);
+
+	if (ret)
+		return ret;
+
+	intel_plane->crtc_x = orig.crtc_x;
+	intel_plane->crtc_y = orig.crtc_y;
+	intel_plane->crtc_w = orig.crtc_w;
+	intel_plane->crtc_h = orig.crtc_h;
+	intel_plane->src_x = orig.src_x;
+	intel_plane->src_y = orig.src_y;
+	intel_plane->src_w = orig.src_w;
+	intel_plane->src_h = orig.src_h;
+	intel_plane->obj = obj;
+
+	if (intel_crtc->active) {
+		bool primary_was_enabled = intel_crtc->primary_enabled;
+
+		intel_crtc->primary_enabled = primary_enabled;
+
+		if (primary_was_enabled != primary_enabled)
+			intel_crtc_wait_for_pending_flips(crtc);
+
+		if (primary_was_enabled && !primary_enabled)
+			intel_pre_disable_primary(crtc);
+
+		if (visible)
+			intel_plane->update_plane(plane, crtc, fb, obj,
+						  crtc_x, crtc_y, crtc_w, crtc_h,
+						  src_x, src_y, src_w, src_h);
+		else
+			intel_plane->disable_plane(plane, crtc);
+
+		intel_frontbuffer_flip(dev, INTEL_FRONTBUFFER_SPRITE(pipe));
+
+		if (!primary_was_enabled && primary_enabled)
+			intel_post_enable_primary(crtc);
 	}
 
-	dst->x1 = crtc_x;
-	dst->x2 = crtc_x + crtc_w;
-	dst->y1 = crtc_y;
-	dst->y2 = crtc_y + crtc_h;
+	/* Unpin old obj after new one is active to avoid ugliness */
+	if (old_obj) {
+		/*
+		 * It's fairly common to simply update the position of
+		 * an existing object.  In that case, we don't need to
+		 * wait for vblank to avoid ugliness, we only need to
+		 * do the pin & ref bookkeeping.
+		 */
+		if (old_obj != obj && intel_crtc->active)
+			intel_wait_for_vblank(dev, intel_crtc->pipe);
+
+		mutex_lock(&dev->struct_mutex);
+		intel_unpin_fb_obj(old_obj);
+		mutex_unlock(&dev->struct_mutex);
+	}
 
 	return 0;
 }
 
-static void
-intel_commit_sprite_plane(struct drm_plane *plane,
-			  struct intel_plane_state *state)
+static int
+intel_disable_plane(struct drm_plane *plane)
 {
-	struct drm_crtc *crtc = state->base.crtc;
+	struct drm_device *dev = plane->dev;
 	struct intel_plane *intel_plane = to_intel_plane(plane);
-	struct drm_framebuffer *fb = state->base.fb;
+	struct intel_crtc *intel_crtc;
+	enum pipe pipe;
 
-	crtc = crtc ? crtc : plane->crtc;
+	if (!plane->fb)
+		return 0;
 
-	if (!crtc->state->active)
-		return;
+	if (WARN_ON(!plane->crtc))
+		return -EINVAL;
+
+	intel_crtc = to_intel_crtc(plane->crtc);
+	pipe = intel_crtc->pipe;
+
+	if (intel_crtc->active) {
+		bool primary_was_enabled = intel_crtc->primary_enabled;
+
+		intel_crtc->primary_enabled = true;
+
+		intel_plane->disable_plane(plane, plane->crtc);
+
+		if (!primary_was_enabled && intel_crtc->primary_enabled)
+			intel_post_enable_primary(plane->crtc);
+	}
+
+	if (intel_plane->obj) {
+		if (intel_crtc->active)
+			intel_wait_for_vblank(dev, intel_plane->pipe);
+
+		mutex_lock(&dev->struct_mutex);
+		intel_unpin_fb_obj(intel_plane->obj);
+		i915_gem_track_fb(intel_plane->obj, NULL,
+				  INTEL_FRONTBUFFER_SPRITE(pipe));
+		mutex_unlock(&dev->struct_mutex);
 
-	if (state->visible) {
-		intel_plane->update_plane(plane, crtc, fb,
-					  state->dst.x1, state->dst.y1,
-					  drm_rect_width(&state->dst),
-					  drm_rect_height(&state->dst),
-					  state->src.x1 >> 16,
-					  state->src.y1 >> 16,
-					  drm_rect_width(&state->src) >> 16,
-					  drm_rect_height(&state->src) >> 16);
-	} else {
-		intel_plane->disable_plane(plane, crtc);
+		intel_plane->obj = NULL;
 	}
+
+	return 0;
+}
+
+static void intel_destroy_plane(struct drm_plane *plane)
+{
+	struct intel_plane *intel_plane = to_intel_plane(plane);
+	intel_disable_plane(plane);
+	drm_plane_cleanup(plane);
+	kfree(intel_plane);
 }
 
 int intel_sprite_set_colorkey(struct drm_device *dev, void *data,
@@ -960,66 +1168,116 @@ int intel_sprite_set_colorkey(struct drm_device *dev, void *data,
 {
 	struct drm_intel_sprite_colorkey *set = data;
 	struct drm_plane *plane;
-	struct drm_plane_state *plane_state;
-	struct drm_atomic_state *state;
-	struct drm_modeset_acquire_ctx ctx;
+	struct intel_plane *intel_plane;
 	int ret = 0;
 
+	if (!drm_core_check_feature(dev, DRIVER_MODESET))
+		return -ENODEV;
+
 	/* Make sure we don't try to enable both src & dest simultaneously */
 	if ((set->flags & (I915_SET_COLORKEY_DESTINATION | I915_SET_COLORKEY_SOURCE)) == (I915_SET_COLORKEY_DESTINATION | I915_SET_COLORKEY_SOURCE))
 		return -EINVAL;
 
-	if (IS_VALLEYVIEW(dev) &&
-	    set->flags & I915_SET_COLORKEY_DESTINATION)
-		return -EINVAL;
+	drm_modeset_lock_all(dev);
 
 	plane = drm_plane_find(dev, set->plane_id);
-	if (!plane || plane->type != DRM_PLANE_TYPE_OVERLAY)
-		return -ENOENT;
+	if (!plane || plane->type != DRM_PLANE_TYPE_OVERLAY) {
+		ret = -ENOENT;
+		goto out_unlock;
+	}
 
-	drm_modeset_acquire_init(&ctx, 0);
+	intel_plane = to_intel_plane(plane);
+	ret = intel_plane->update_colorkey(plane, set);
 
-	state = drm_atomic_state_alloc(plane->dev);
-	if (!state) {
-		ret = -ENOMEM;
-		goto out;
-	}
-	state->acquire_ctx = &ctx;
-
-	while (1) {
-		plane_state = drm_atomic_get_plane_state(state, plane);
-		ret = PTR_ERR_OR_ZERO(plane_state);
-		if (!ret) {
-			to_intel_plane_state(plane_state)->ckey = *set;
-			ret = drm_atomic_commit(state);
-		}
+out_unlock:
+	drm_modeset_unlock_all(dev);
+	return ret;
+}
 
-		if (ret != -EDEADLK)
-			break;
+int intel_sprite_get_colorkey(struct drm_device *dev, void *data,
+			      struct drm_file *file_priv)
+{
+	struct drm_intel_sprite_colorkey *get = data;
+	struct drm_plane *plane;
+	struct intel_plane *intel_plane;
+	int ret = 0;
+
+	if (!drm_core_check_feature(dev, DRIVER_MODESET))
+		return -ENODEV;
 
-		drm_atomic_state_clear(state);
-		drm_modeset_backoff(&ctx);
+	drm_modeset_lock_all(dev);
+
+	plane = drm_plane_find(dev, get->plane_id);
+	if (!plane || plane->type != DRM_PLANE_TYPE_OVERLAY) {
+		ret = -ENOENT;
+		goto out_unlock;
 	}
 
-	if (ret)
-		drm_atomic_state_free(state);
+	intel_plane = to_intel_plane(plane);
+	intel_plane->get_colorkey(plane, get);
 
-out:
-	drm_modeset_drop_locks(&ctx);
-	drm_modeset_acquire_fini(&ctx);
+out_unlock:
+	drm_modeset_unlock_all(dev);
 	return ret;
 }
 
-static const uint32_t ilk_plane_formats[] = {
-	DRM_FORMAT_XRGB8888,
-	DRM_FORMAT_YUYV,
-	DRM_FORMAT_YVYU,
-	DRM_FORMAT_UYVY,
-	DRM_FORMAT_VYUY,
+int intel_plane_set_property(struct drm_plane *plane,
+			     struct drm_property *prop,
+			     uint64_t val)
+{
+	struct drm_device *dev = plane->dev;
+	struct intel_plane *intel_plane = to_intel_plane(plane);
+	uint64_t old_val;
+	int ret = -ENOENT;
+
+	if (prop == dev->mode_config.rotation_property) {
+		/* exactly one rotation angle please */
+		if (hweight32(val & 0xf) != 1)
+			return -EINVAL;
+
+		if (intel_plane->rotation == val)
+			return 0;
+
+		old_val = intel_plane->rotation;
+		intel_plane->rotation = val;
+		ret = intel_plane_restore(plane);
+		if (ret)
+			intel_plane->rotation = old_val;
+	}
+
+	return ret;
+}
+
+int intel_plane_restore(struct drm_plane *plane)
+{
+	struct intel_plane *intel_plane = to_intel_plane(plane);
+
+	if (!plane->crtc || !plane->fb)
+		return 0;
+
+	return plane->funcs->update_plane(plane, plane->crtc, plane->fb,
+				  intel_plane->crtc_x, intel_plane->crtc_y,
+				  intel_plane->crtc_w, intel_plane->crtc_h,
+				  intel_plane->src_x, intel_plane->src_y,
+				  intel_plane->src_w, intel_plane->src_h);
+}
+
+void intel_plane_disable(struct drm_plane *plane)
+{
+	if (!plane->crtc || !plane->fb)
+		return;
+
+	intel_disable_plane(plane);
+}
+
+static const struct drm_plane_funcs intel_plane_funcs = {
+	.update_plane = intel_update_plane,
+	.disable_plane = intel_disable_plane,
+	.destroy = intel_destroy_plane,
+	.set_property = intel_plane_set_property,
 };
 
-static const uint32_t snb_plane_formats[] = {
-	DRM_FORMAT_XBGR8888,
+static uint32_t ilk_plane_formats[] = {
 	DRM_FORMAT_XRGB8888,
 	DRM_FORMAT_YUYV,
 	DRM_FORMAT_YVYU,
@@ -1027,26 +1285,23 @@ static const uint32_t snb_plane_formats[] = {
 	DRM_FORMAT_VYUY,
 };
 
-static const uint32_t vlv_plane_formats[] = {
-	DRM_FORMAT_RGB565,
-	DRM_FORMAT_ABGR8888,
-	DRM_FORMAT_ARGB8888,
+static uint32_t snb_plane_formats[] = {
 	DRM_FORMAT_XBGR8888,
 	DRM_FORMAT_XRGB8888,
-	DRM_FORMAT_XBGR2101010,
-	DRM_FORMAT_ABGR2101010,
 	DRM_FORMAT_YUYV,
 	DRM_FORMAT_YVYU,
 	DRM_FORMAT_UYVY,
 	DRM_FORMAT_VYUY,
 };
 
-static uint32_t skl_plane_formats[] = {
+static uint32_t vlv_plane_formats[] = {
 	DRM_FORMAT_RGB565,
 	DRM_FORMAT_ABGR8888,
 	DRM_FORMAT_ARGB8888,
 	DRM_FORMAT_XBGR8888,
 	DRM_FORMAT_XRGB8888,
+	DRM_FORMAT_XBGR2101010,
+	DRM_FORMAT_ABGR2101010,
 	DRM_FORMAT_YUYV,
 	DRM_FORMAT_YVYU,
 	DRM_FORMAT_UYVY,
@@ -1057,7 +1312,6 @@ int
 intel_plane_init(struct drm_device *dev, enum pipe pipe, int plane)
 {
 	struct intel_plane *intel_plane;
-	struct intel_plane_state *state;
 	unsigned long possible_crtcs;
 	const uint32_t *plane_formats;
 	int num_plane_formats;
@@ -1070,13 +1324,6 @@ intel_plane_init(struct drm_device *dev, enum pipe pipe, int plane)
 	if (!intel_plane)
 		return -ENOMEM;
 
-	state = intel_create_plane_state(&intel_plane->base);
-	if (!state) {
-		kfree(intel_plane);
-		return -ENOMEM;
-	}
-	intel_plane->base.state = &state->base;
-
 	switch (INTEL_INFO(dev)->gen) {
 	case 5:
 	case 6:
@@ -1084,6 +1331,8 @@ intel_plane_init(struct drm_device *dev, enum pipe pipe, int plane)
 		intel_plane->max_downscale = 16;
 		intel_plane->update_plane = ilk_update_plane;
 		intel_plane->disable_plane = ilk_disable_plane;
+		intel_plane->update_colorkey = ilk_update_colorkey;
+		intel_plane->get_colorkey = ilk_get_colorkey;
 
 		if (IS_GEN6(dev)) {
 			plane_formats = snb_plane_formats;
@@ -1107,26 +1356,22 @@ intel_plane_init(struct drm_device *dev, enum pipe pipe, int plane)
 		if (IS_VALLEYVIEW(dev)) {
 			intel_plane->update_plane = vlv_update_plane;
 			intel_plane->disable_plane = vlv_disable_plane;
+			intel_plane->update_colorkey = vlv_update_colorkey;
+			intel_plane->get_colorkey = vlv_get_colorkey;
 
 			plane_formats = vlv_plane_formats;
 			num_plane_formats = ARRAY_SIZE(vlv_plane_formats);
 		} else {
 			intel_plane->update_plane = ivb_update_plane;
 			intel_plane->disable_plane = ivb_disable_plane;
+			intel_plane->update_colorkey = ivb_update_colorkey;
+			intel_plane->get_colorkey = ivb_get_colorkey;
 
 			plane_formats = snb_plane_formats;
 			num_plane_formats = ARRAY_SIZE(snb_plane_formats);
 		}
 		break;
-	case 9:
-		intel_plane->can_scale = true;
-		intel_plane->update_plane = skl_update_plane;
-		intel_plane->disable_plane = skl_disable_plane;
-		state->scaler_id = -1;
 
-		plane_formats = skl_plane_formats;
-		num_plane_formats = ARRAY_SIZE(skl_plane_formats);
-		break;
 	default:
 		kfree(intel_plane);
 		return -ENODEV;
@@ -1134,9 +1379,7 @@ intel_plane_init(struct drm_device *dev, enum pipe pipe, int plane)
 
 	intel_plane->pipe = pipe;
 	intel_plane->plane = plane;
-	intel_plane->frontbuffer_bit = INTEL_FRONTBUFFER_SPRITE(pipe, plane);
-	intel_plane->check_plane = intel_check_sprite_plane;
-	intel_plane->commit_plane = intel_commit_sprite_plane;
+	intel_plane->rotation = BIT(DRM_ROTATE_0);
 	possible_crtcs = (1 << pipe);
 	ret = drm_universal_plane_init(dev, &intel_plane->base, possible_crtcs,
 				       &intel_plane_funcs,
@@ -1147,10 +1390,17 @@ intel_plane_init(struct drm_device *dev, enum pipe pipe, int plane)
 		goto out;
 	}
 
-	intel_create_rotation_property(dev, intel_plane);
+	if (!dev->mode_config.rotation_property)
+		dev->mode_config.rotation_property =
+			drm_mode_create_rotation_property(dev,
+							  BIT(DRM_ROTATE_0) |
+							  BIT(DRM_ROTATE_180));
 
-	drm_plane_helper_add(&intel_plane->base, &intel_plane_helper_funcs);
+	if (dev->mode_config.rotation_property)
+		drm_object_attach_property(&intel_plane->base.base,
+					   dev->mode_config.rotation_property,
+					   intel_plane->rotation);
 
-out:
+ out:
 	return ret;
 }
diff --git a/kernel/msm-3.18/drivers/gpu/drm/radeon/radeon_display.c b/kernel/msm-3.18/drivers/gpu/drm/radeon/radeon_display.c
index 3645b223a..642854b2e 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/radeon/radeon_display.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/radeon/radeon_display.c
@@ -1862,6 +1862,7 @@ int radeon_get_crtc_scanoutpos(struct drm_device *dev, unsigned int pipe,
 	struct radeon_device *rdev = dev->dev_private;
 
 	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_disable_rt();
 
 	/* Get optional system timestamp before query. */
 	if (stime)
@@ -1954,6 +1955,7 @@ int radeon_get_crtc_scanoutpos(struct drm_device *dev, unsigned int pipe,
 		*etime = ktime_get();
 
 	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
+	preempt_enable_rt();
 
 	/* Decode into vertical and horizontal scanout position. */
 	*vpos = position & 0x1fff;
diff --git a/kernel/msm-3.18/drivers/i2c/busses/i2c-omap.c b/kernel/msm-3.18/drivers/i2c/busses/i2c-omap.c
index 277a2288d..6d1c2960f 100644
--- a/kernel/msm-3.18/drivers/i2c/busses/i2c-omap.c
+++ b/kernel/msm-3.18/drivers/i2c/busses/i2c-omap.c
@@ -875,15 +875,12 @@ omap_i2c_isr(int irq, void *dev_id)
 	u16 mask;
 	u16 stat;
 
-	spin_lock(&dev->lock);
-	mask = omap_i2c_read_reg(dev, OMAP_I2C_IE_REG);
 	stat = omap_i2c_read_reg(dev, OMAP_I2C_STAT_REG);
+	mask = omap_i2c_read_reg(dev, OMAP_I2C_IE_REG);
 
 	if (stat & mask)
 		ret = IRQ_WAKE_THREAD;
 
-	spin_unlock(&dev->lock);
-
 	return ret;
 }
 
diff --git a/kernel/msm-3.18/drivers/ide/alim15x3.c b/kernel/msm-3.18/drivers/ide/alim15x3.c
index 36f76e28a..394f142f9 100644
--- a/kernel/msm-3.18/drivers/ide/alim15x3.c
+++ b/kernel/msm-3.18/drivers/ide/alim15x3.c
@@ -234,7 +234,7 @@ static int init_chipset_ali15x3(struct pci_dev *dev)
 
 	isa_dev = pci_get_device(PCI_VENDOR_ID_AL, PCI_DEVICE_ID_AL_M1533, NULL);
 
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 
 	if (m5229_revision < 0xC2) {
 		/*
@@ -325,7 +325,7 @@ out:
 	}
 	pci_dev_put(north);
 	pci_dev_put(isa_dev);
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 	return 0;
 }
 
diff --git a/kernel/msm-3.18/drivers/ide/hpt366.c b/kernel/msm-3.18/drivers/ide/hpt366.c
index 696b6c1ec..0d0a96629 100644
--- a/kernel/msm-3.18/drivers/ide/hpt366.c
+++ b/kernel/msm-3.18/drivers/ide/hpt366.c
@@ -1241,7 +1241,7 @@ static int init_dma_hpt366(ide_hwif_t *hwif,
 
 	dma_old = inb(base + 2);
 
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 
 	dma_new = dma_old;
 	pci_read_config_byte(dev, hwif->channel ? 0x4b : 0x43, &masterdma);
@@ -1252,7 +1252,7 @@ static int init_dma_hpt366(ide_hwif_t *hwif,
 	if (dma_new != dma_old)
 		outb(dma_new, base + 2);
 
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 
 	printk(KERN_INFO "    %s: BM-DMA at 0x%04lx-0x%04lx\n",
 			 hwif->name, base, base + 7);
diff --git a/kernel/msm-3.18/drivers/ide/ide-io-std.c b/kernel/msm-3.18/drivers/ide/ide-io-std.c
index 197639775..4169433fa 100644
--- a/kernel/msm-3.18/drivers/ide/ide-io-std.c
+++ b/kernel/msm-3.18/drivers/ide/ide-io-std.c
@@ -175,7 +175,7 @@ void ide_input_data(ide_drive_t *drive, struct ide_cmd *cmd, void *buf,
 		unsigned long uninitialized_var(flags);
 
 		if ((io_32bit & 2) && !mmio) {
-			local_irq_save(flags);
+			local_irq_save_nort(flags);
 			ata_vlb_sync(io_ports->nsect_addr);
 		}
 
@@ -186,7 +186,7 @@ void ide_input_data(ide_drive_t *drive, struct ide_cmd *cmd, void *buf,
 			insl(data_addr, buf, words);
 
 		if ((io_32bit & 2) && !mmio)
-			local_irq_restore(flags);
+			local_irq_restore_nort(flags);
 
 		if (((len + 1) & 3) < 2)
 			return;
@@ -219,7 +219,7 @@ void ide_output_data(ide_drive_t *drive, struct ide_cmd *cmd, void *buf,
 		unsigned long uninitialized_var(flags);
 
 		if ((io_32bit & 2) && !mmio) {
-			local_irq_save(flags);
+			local_irq_save_nort(flags);
 			ata_vlb_sync(io_ports->nsect_addr);
 		}
 
@@ -230,7 +230,7 @@ void ide_output_data(ide_drive_t *drive, struct ide_cmd *cmd, void *buf,
 			outsl(data_addr, buf, words);
 
 		if ((io_32bit & 2) && !mmio)
-			local_irq_restore(flags);
+			local_irq_restore_nort(flags);
 
 		if (((len + 1) & 3) < 2)
 			return;
diff --git a/kernel/msm-3.18/drivers/ide/ide-io.c b/kernel/msm-3.18/drivers/ide/ide-io.c
index 177db6d5b..079ae6beb 100644
--- a/kernel/msm-3.18/drivers/ide/ide-io.c
+++ b/kernel/msm-3.18/drivers/ide/ide-io.c
@@ -659,7 +659,7 @@ void ide_timer_expiry (unsigned long data)
 		/* disable_irq_nosync ?? */
 		disable_irq(hwif->irq);
 		/* local CPU only, as if we were handling an interrupt */
-		local_irq_disable();
+		local_irq_disable_nort();
 		if (hwif->polling) {
 			startstop = handler(drive);
 		} else if (drive_is_ready(drive)) {
diff --git a/kernel/msm-3.18/drivers/ide/ide-iops.c b/kernel/msm-3.18/drivers/ide/ide-iops.c
index 376f2dc41..f014dd1b7 100644
--- a/kernel/msm-3.18/drivers/ide/ide-iops.c
+++ b/kernel/msm-3.18/drivers/ide/ide-iops.c
@@ -129,12 +129,12 @@ int __ide_wait_stat(ide_drive_t *drive, u8 good, u8 bad,
 				if ((stat & ATA_BUSY) == 0)
 					break;
 
-				local_irq_restore(flags);
+				local_irq_restore_nort(flags);
 				*rstat = stat;
 				return -EBUSY;
 			}
 		}
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 	}
 	/*
 	 * Allow status to settle, then read it again.
diff --git a/kernel/msm-3.18/drivers/ide/ide-probe.c b/kernel/msm-3.18/drivers/ide/ide-probe.c
index a3d3b1733..3eff5828e 100644
--- a/kernel/msm-3.18/drivers/ide/ide-probe.c
+++ b/kernel/msm-3.18/drivers/ide/ide-probe.c
@@ -196,10 +196,10 @@ static void do_identify(ide_drive_t *drive, u8 cmd, u16 *id)
 	int bswap = 1;
 
 	/* local CPU only; some systems need this */
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	/* read 512 bytes of id info */
 	hwif->tp_ops->input_data(drive, NULL, id, SECTOR_SIZE);
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 
 	drive->dev_flags |= IDE_DFLAG_ID_READ;
 #ifdef DEBUG
diff --git a/kernel/msm-3.18/drivers/ide/ide-taskfile.c b/kernel/msm-3.18/drivers/ide/ide-taskfile.c
index dabb88b1c..2cecea725 100644
--- a/kernel/msm-3.18/drivers/ide/ide-taskfile.c
+++ b/kernel/msm-3.18/drivers/ide/ide-taskfile.c
@@ -250,7 +250,7 @@ void ide_pio_bytes(ide_drive_t *drive, struct ide_cmd *cmd,
 
 		page_is_high = PageHighMem(page);
 		if (page_is_high)
-			local_irq_save(flags);
+			local_irq_save_nort(flags);
 
 		buf = kmap_atomic(page) + offset;
 
@@ -271,7 +271,7 @@ void ide_pio_bytes(ide_drive_t *drive, struct ide_cmd *cmd,
 		kunmap_atomic(buf);
 
 		if (page_is_high)
-			local_irq_restore(flags);
+			local_irq_restore_nort(flags);
 
 		len -= nr_bytes;
 	}
@@ -414,7 +414,7 @@ static ide_startstop_t pre_task_out_intr(ide_drive_t *drive,
 	}
 
 	if ((drive->dev_flags & IDE_DFLAG_UNMASK) == 0)
-		local_irq_disable();
+		local_irq_disable_nort();
 
 	ide_set_handler(drive, &task_pio_intr, WAIT_WORSTCASE);
 
diff --git a/kernel/msm-3.18/drivers/infiniband/ulp/ipoib/ipoib_multicast.c b/kernel/msm-3.18/drivers/infiniband/ulp/ipoib/ipoib_multicast.c
index 6391ed0fe..b95e77616 100644
--- a/kernel/msm-3.18/drivers/infiniband/ulp/ipoib/ipoib_multicast.c
+++ b/kernel/msm-3.18/drivers/infiniband/ulp/ipoib/ipoib_multicast.c
@@ -799,7 +799,7 @@ void ipoib_mcast_restart_task(struct work_struct *work)
 
 	ipoib_mcast_stop_thread(dev, 0);
 
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	netif_addr_lock(dev);
 	spin_lock(&priv->lock);
 
@@ -881,7 +881,7 @@ void ipoib_mcast_restart_task(struct work_struct *work)
 
 	spin_unlock(&priv->lock);
 	netif_addr_unlock(dev);
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 
 	/* We have to cancel outside of the spinlock */
 	list_for_each_entry_safe(mcast, tmcast, &remove_list, list) {
diff --git a/kernel/msm-3.18/drivers/input/gameport/gameport.c b/kernel/msm-3.18/drivers/input/gameport/gameport.c
index e853a2134..5b6aa39a1 100644
--- a/kernel/msm-3.18/drivers/input/gameport/gameport.c
+++ b/kernel/msm-3.18/drivers/input/gameport/gameport.c
@@ -124,12 +124,12 @@ static int old_gameport_measure_speed(struct gameport *gameport)
 	tx = 1 << 30;
 
 	for(i = 0; i < 50; i++) {
-		local_irq_save(flags);
+		local_irq_save_nort(flags);
 		GET_TIME(t1);
 		for (t = 0; t < 50; t++) gameport_read(gameport);
 		GET_TIME(t2);
 		GET_TIME(t3);
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 		udelay(i * 10);
 		if ((t = DELTA(t2,t1) - DELTA(t3,t2)) < tx) tx = t;
 	}
@@ -148,11 +148,11 @@ static int old_gameport_measure_speed(struct gameport *gameport)
 	tx = 1 << 30;
 
 	for(i = 0; i < 50; i++) {
-		local_irq_save(flags);
+		local_irq_save_nort(flags);
 		rdtscl(t1);
 		for (t = 0; t < 50; t++) gameport_read(gameport);
 		rdtscl(t2);
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 		udelay(i * 10);
 		if (t2 - t1 < tx) tx = t2 - t1;
 	}
diff --git a/kernel/msm-3.18/drivers/leds/trigger/Kconfig b/kernel/msm-3.18/drivers/leds/trigger/Kconfig
index 49794b47b..3d7245d6b 100644
--- a/kernel/msm-3.18/drivers/leds/trigger/Kconfig
+++ b/kernel/msm-3.18/drivers/leds/trigger/Kconfig
@@ -61,7 +61,7 @@ config LEDS_TRIGGER_BACKLIGHT
 
 config LEDS_TRIGGER_CPU
 	bool "LED CPU Trigger"
-	depends on LEDS_TRIGGERS
+	depends on LEDS_TRIGGERS && !PREEMPT_RT_BASE
 	help
 	  This allows LEDs to be controlled by active CPUs. This shows
 	  the active CPUs across an array of LEDs so you can see which
diff --git a/kernel/msm-3.18/drivers/md/bcache/Kconfig b/kernel/msm-3.18/drivers/md/bcache/Kconfig
index 4d200883c..98b64ed5c 100644
--- a/kernel/msm-3.18/drivers/md/bcache/Kconfig
+++ b/kernel/msm-3.18/drivers/md/bcache/Kconfig
@@ -1,6 +1,7 @@
 
 config BCACHE
 	tristate "Block device as cache"
+	depends on !PREEMPT_RT_FULL
 	---help---
 	Allows a block device to be used as cache for other devices; uses
 	a btree for indexing and the layout is optimized for SSDs.
diff --git a/kernel/msm-3.18/drivers/md/dm.c b/kernel/msm-3.18/drivers/md/dm.c
index 256b51510..8af2377d2 100644
--- a/kernel/msm-3.18/drivers/md/dm.c
+++ b/kernel/msm-3.18/drivers/md/dm.c
@@ -1953,14 +1953,14 @@ static void dm_request_fn(struct request_queue *q)
 		if (map_request(ti, clone, md))
 			goto requeued;
 
-		BUG_ON(!irqs_disabled());
+		BUG_ON_NONRT(!irqs_disabled());
 		spin_lock(q->queue_lock);
 	}
 
 	goto out;
 
 requeued:
-	BUG_ON(!irqs_disabled());
+	BUG_ON_NONRT(!irqs_disabled());
 	spin_lock(q->queue_lock);
 
 delay_and_out:
diff --git a/kernel/msm-3.18/drivers/md/raid5.c b/kernel/msm-3.18/drivers/md/raid5.c
index 094f36064..930fcf6e5 100644
--- a/kernel/msm-3.18/drivers/md/raid5.c
+++ b/kernel/msm-3.18/drivers/md/raid5.c
@@ -1649,8 +1649,9 @@ static void raid_run_ops(struct stripe_head *sh, unsigned long ops_request)
 	struct raid5_percpu *percpu;
 	unsigned long cpu;
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	percpu = per_cpu_ptr(conf->percpu, cpu);
+	spin_lock(&percpu->lock);
 	if (test_bit(STRIPE_OP_BIOFILL, &ops_request)) {
 		ops_run_biofill(sh);
 		overlap_clear++;
@@ -1702,7 +1703,8 @@ static void raid_run_ops(struct stripe_head *sh, unsigned long ops_request)
 			if (test_and_clear_bit(R5_Overlap, &dev->flags))
 				wake_up(&sh->raid_conf->wait_for_overlap);
 		}
-	put_cpu();
+	spin_unlock(&percpu->lock);
+	put_cpu_light();
 }
 
 static int grow_one_stripe(struct r5conf *conf, int hash)
@@ -5716,6 +5718,7 @@ static int raid5_alloc_percpu(struct r5conf *conf)
 			       __func__, cpu);
 			break;
 		}
+		spin_lock_init(&per_cpu_ptr(conf->percpu, cpu)->lock);
 	}
 	put_online_cpus();
 
diff --git a/kernel/msm-3.18/drivers/md/raid5.h b/kernel/msm-3.18/drivers/md/raid5.h
index d59f5ca74..9aa9af4ab 100644
--- a/kernel/msm-3.18/drivers/md/raid5.h
+++ b/kernel/msm-3.18/drivers/md/raid5.h
@@ -457,6 +457,7 @@ struct r5conf {
 	int			recovery_disabled;
 	/* per cpu variables */
 	struct raid5_percpu {
+		spinlock_t	lock;	     /* Protection for -RT */
 		struct page	*spare_page; /* Used when checking P/Q in raid6 */
 		void		*scribble;   /* space for constructing buffer
 					      * lists and performing address
diff --git a/kernel/msm-3.18/drivers/misc/Kconfig b/kernel/msm-3.18/drivers/misc/Kconfig
index 89e7305f7..1ca91a744 100644
--- a/kernel/msm-3.18/drivers/misc/Kconfig
+++ b/kernel/msm-3.18/drivers/misc/Kconfig
@@ -54,6 +54,7 @@ config AD525X_DPOT_SPI
 config ATMEL_TCLIB
 	bool "Atmel AT32/AT91 Timer/Counter Library"
 	depends on (AVR32 || ARCH_AT91)
+	default y if PREEMPT_RT_FULL
 	help
 	  Select this if you want a library to allocate the Timer/Counter
 	  blocks found on many Atmel processors.  This facilitates using
@@ -69,8 +70,7 @@ config ATMEL_TCB_CLKSRC
 	  are combined to make a single 32-bit timer.
 
 	  When GENERIC_CLOCKEVENTS is defined, the third timer channel
-	  may be used as a clock event device supporting oneshot mode
-	  (delays of up to two seconds) based on the 32 KiHz clock.
+	  may be used as a clock event device supporting oneshot mode.
 
 config ATMEL_TCB_CLKSRC_BLOCK
 	int
@@ -84,6 +84,15 @@ config ATMEL_TCB_CLKSRC_BLOCK
 	  TC can be used for other purposes, such as PWM generation and
 	  interval timing.
 
+config ATMEL_TCB_CLKSRC_USE_SLOW_CLOCK
+	bool "TC Block use 32 KiHz clock"
+	depends on ATMEL_TCB_CLKSRC
+	default y if !PREEMPT_RT_FULL
+	help
+	  Select this to use 32 KiHz base clock rate as TC block clock
+	  source for clock events.
+
+
 config DUMMY_IRQ
 	tristate "Dummy IRQ handler"
 	default n
@@ -113,6 +122,35 @@ config IBM_ASM
 	  for information on the specific driver level and support statement
 	  for your IBM server.
 
+config HWLAT_DETECTOR
+	tristate "Testing module to detect hardware-induced latencies"
+	depends on DEBUG_FS
+	depends on RING_BUFFER
+	default m
+	---help---
+	  A simple hardware latency detector. Use this module to detect
+	  large latencies introduced by the behavior of the underlying
+	  system firmware external to Linux. We do this using periodic
+	  use of stop_machine to grab all available CPUs and measure
+	  for unexplainable gaps in the CPU timestamp counter(s). By
+	  default, the module is not enabled until the "enable" file
+	  within the "hwlat_detector" debugfs directory is toggled.
+
+	  This module is often used to detect SMI (System Management
+	  Interrupts) on x86 systems, though is not x86 specific. To
+	  this end, we default to using a sample window of 1 second,
+	  during which we will sample for 0.5 seconds. If an SMI or
+	  similar event occurs during that time, it is recorded
+	  into an 8K samples global ring buffer until retreived.
+
+	  WARNING: This software should never be enabled (it can be built
+	  but should not be turned on after it is loaded) in a production
+	  environment where high latencies are a concern since the
+	  sampling mechanism actually introduces latencies for
+	  regular tasks while the CPU(s) are being held.
+
+	  If unsure, say N
+
 config PHANTOM
 	tristate "Sensable PHANToM (PCI)"
 	depends on PCI
diff --git a/kernel/msm-3.18/drivers/misc/Makefile b/kernel/msm-3.18/drivers/misc/Makefile
index d08212274..ccf7b4624 100644
--- a/kernel/msm-3.18/drivers/misc/Makefile
+++ b/kernel/msm-3.18/drivers/misc/Makefile
@@ -39,6 +39,7 @@ obj-$(CONFIG_C2PORT)		+= c2port/
 obj-$(CONFIG_HMC6352)		+= hmc6352.o
 obj-y				+= eeprom/
 obj-y				+= cb710/
+obj-$(CONFIG_HWLAT_DETECTOR)	+= hwlat_detector.o
 obj-$(CONFIG_SPEAR13XX_PCIE_GADGET)	+= spear13xx_pcie_gadget.o
 obj-$(CONFIG_VMWARE_BALLOON)	+= vmw_balloon.o
 obj-$(CONFIG_ARM_CHARLCD)	+= arm-charlcd.o
diff --git a/kernel/msm-3.18/drivers/misc/hwlat_detector.c b/kernel/msm-3.18/drivers/misc/hwlat_detector.c
new file mode 100644
index 000000000..2429c4331
--- /dev/null
+++ b/kernel/msm-3.18/drivers/misc/hwlat_detector.c
@@ -0,0 +1,1240 @@
+/*
+ * hwlat_detector.c - A simple Hardware Latency detector.
+ *
+ * Use this module to detect large system latencies induced by the behavior of
+ * certain underlying system hardware or firmware, independent of Linux itself.
+ * The code was developed originally to detect the presence of SMIs on Intel
+ * and AMD systems, although there is no dependency upon x86 herein.
+ *
+ * The classical example usage of this module is in detecting the presence of
+ * SMIs or System Management Interrupts on Intel and AMD systems. An SMI is a
+ * somewhat special form of hardware interrupt spawned from earlier CPU debug
+ * modes in which the (BIOS/EFI/etc.) firmware arranges for the South Bridge
+ * LPC (or other device) to generate a special interrupt under certain
+ * circumstances, for example, upon expiration of a special SMI timer device,
+ * due to certain external thermal readings, on certain I/O address accesses,
+ * and other situations. An SMI hits a special CPU pin, triggers a special
+ * SMI mode (complete with special memory map), and the OS is unaware.
+ *
+ * Although certain hardware-inducing latencies are necessary (for example,
+ * a modern system often requires an SMI handler for correct thermal control
+ * and remote management) they can wreak havoc upon any OS-level performance
+ * guarantees toward low-latency, especially when the OS is not even made
+ * aware of the presence of these interrupts. For this reason, we need a
+ * somewhat brute force mechanism to detect these interrupts. In this case,
+ * we do it by hogging all of the CPU(s) for configurable timer intervals,
+ * sampling the built-in CPU timer, looking for discontiguous readings.
+ *
+ * WARNING: This implementation necessarily introduces latencies. Therefore,
+ *          you should NEVER use this module in a production environment
+ *          requiring any kind of low-latency performance guarantee(s).
+ *
+ * Copyright (C) 2008-2009 Jon Masters, Red Hat, Inc. <jcm@redhat.com>
+ *
+ * Includes useful feedback from Clark Williams <clark@redhat.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/ring_buffer.h>
+#include <linux/time.h>
+#include <linux/hrtimer.h>
+#include <linux/kthread.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/uaccess.h>
+#include <linux/version.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/trace_clock.h>
+
+#define BUF_SIZE_DEFAULT	262144UL		/* 8K*(sizeof(entry)) */
+#define BUF_FLAGS		(RB_FL_OVERWRITE)	/* no block on full */
+#define U64STR_SIZE		22			/* 20 digits max */
+
+#define VERSION			"1.0.0"
+#define BANNER			"hwlat_detector: "
+#define DRVNAME			"hwlat_detector"
+#define DEFAULT_SAMPLE_WINDOW	1000000			/* 1s */
+#define DEFAULT_SAMPLE_WIDTH	500000			/* 0.5s */
+#define DEFAULT_LAT_THRESHOLD	10			/* 10us */
+
+/* Module metadata */
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Jon Masters <jcm@redhat.com>");
+MODULE_DESCRIPTION("A simple hardware latency detector");
+MODULE_VERSION(VERSION);
+
+/* Module parameters */
+
+static int debug;
+static int enabled;
+static int threshold;
+
+module_param(debug, int, 0);			/* enable debug */
+module_param(enabled, int, 0);			/* enable detector */
+module_param(threshold, int, 0);		/* latency threshold */
+
+/* Buffering and sampling */
+
+static struct ring_buffer *ring_buffer;		/* sample buffer */
+static DEFINE_MUTEX(ring_buffer_mutex);		/* lock changes */
+static unsigned long buf_size = BUF_SIZE_DEFAULT;
+static struct task_struct *kthread;		/* sampling thread */
+
+/* DebugFS filesystem entries */
+
+static struct dentry *debug_dir;		/* debugfs directory */
+static struct dentry *debug_max;		/* maximum TSC delta */
+static struct dentry *debug_count;		/* total detect count */
+static struct dentry *debug_sample_width;	/* sample width us */
+static struct dentry *debug_sample_window;	/* sample window us */
+static struct dentry *debug_sample;		/* raw samples us */
+static struct dentry *debug_threshold;		/* threshold us */
+static struct dentry *debug_enable;		/* enable/disable */
+
+/* Individual samples and global state */
+
+struct sample;					/* latency sample */
+struct data;					/* Global state */
+
+/* Sampling functions */
+static int __buffer_add_sample(struct sample *sample);
+static struct sample *buffer_get_sample(struct sample *sample);
+
+/* Threading and state */
+static int kthread_fn(void *unused);
+static int start_kthread(void);
+static int stop_kthread(void);
+static void __reset_stats(void);
+static int init_stats(void);
+
+/* Debugfs interface */
+static ssize_t simple_data_read(struct file *filp, char __user *ubuf,
+				size_t cnt, loff_t *ppos, const u64 *entry);
+static ssize_t simple_data_write(struct file *filp, const char __user *ubuf,
+				 size_t cnt, loff_t *ppos, u64 *entry);
+static int debug_sample_fopen(struct inode *inode, struct file *filp);
+static ssize_t debug_sample_fread(struct file *filp, char __user *ubuf,
+				  size_t cnt, loff_t *ppos);
+static int debug_sample_release(struct inode *inode, struct file *filp);
+static int debug_enable_fopen(struct inode *inode, struct file *filp);
+static ssize_t debug_enable_fread(struct file *filp, char __user *ubuf,
+				  size_t cnt, loff_t *ppos);
+static ssize_t debug_enable_fwrite(struct file *file,
+				   const char __user *user_buffer,
+				   size_t user_size, loff_t *offset);
+
+/* Initialization functions */
+static int init_debugfs(void);
+static void free_debugfs(void);
+static int detector_init(void);
+static void detector_exit(void);
+
+/* Individual latency samples are stored here when detected and packed into
+ * the ring_buffer circular buffer, where they are overwritten when
+ * more than buf_size/sizeof(sample) samples are received. */
+struct sample {
+	u64		seqnum;		/* unique sequence */
+	u64		duration;	/* ktime delta */
+	u64		outer_duration;	/* ktime delta (outer loop) */
+	struct timespec	timestamp;	/* wall time */
+	unsigned long   lost;
+};
+
+/* keep the global state somewhere. */
+static struct data {
+
+	struct mutex lock;		/* protect changes */
+
+	u64	count;			/* total since reset */
+	u64	max_sample;		/* max hardware latency */
+	u64	threshold;		/* sample threshold level */
+
+	u64	sample_window;		/* total sampling window (on+off) */
+	u64	sample_width;		/* active sampling portion of window */
+
+	atomic_t sample_open;		/* whether the sample file is open */
+
+	wait_queue_head_t wq;		/* waitqeue for new sample values */
+
+} data;
+
+/**
+ * __buffer_add_sample - add a new latency sample recording to the ring buffer
+ * @sample: The new latency sample value
+ *
+ * This receives a new latency sample and records it in a global ring buffer.
+ * No additional locking is used in this case.
+ */
+static int __buffer_add_sample(struct sample *sample)
+{
+	return ring_buffer_write(ring_buffer,
+				 sizeof(struct sample), sample);
+}
+
+/**
+ * buffer_get_sample - remove a hardware latency sample from the ring buffer
+ * @sample: Pre-allocated storage for the sample
+ *
+ * This retrieves a hardware latency sample from the global circular buffer
+ */
+static struct sample *buffer_get_sample(struct sample *sample)
+{
+	struct ring_buffer_event *e = NULL;
+	struct sample *s = NULL;
+	unsigned int cpu = 0;
+
+	if (!sample)
+		return NULL;
+
+	mutex_lock(&ring_buffer_mutex);
+	for_each_online_cpu(cpu) {
+		e = ring_buffer_consume(ring_buffer, cpu, NULL, &sample->lost);
+		if (e)
+			break;
+	}
+
+	if (e) {
+		s = ring_buffer_event_data(e);
+		memcpy(sample, s, sizeof(struct sample));
+	} else
+		sample = NULL;
+	mutex_unlock(&ring_buffer_mutex);
+
+	return sample;
+}
+
+#ifndef CONFIG_TRACING
+#define time_type	ktime_t
+#define time_get()	ktime_get()
+#define time_to_us(x)	ktime_to_us(x)
+#define time_sub(a, b)	ktime_sub(a, b)
+#define init_time(a, b)	(a).tv64 = b
+#define time_u64(a)	((a).tv64)
+#else
+#define time_type	u64
+#define time_get()	trace_clock_local()
+#define time_to_us(x)	div_u64(x, 1000)
+#define time_sub(a, b)	((a) - (b))
+#define init_time(a, b)	(a = b)
+#define time_u64(a)	a
+#endif
+/**
+ * get_sample - sample the CPU TSC and look for likely hardware latencies
+ *
+ * Used to repeatedly capture the CPU TSC (or similar), looking for potential
+ * hardware-induced latency. Called with interrupts disabled and with
+ * data.lock held.
+ */
+static int get_sample(void)
+{
+	time_type start, t1, t2, last_t2;
+	s64 diff, total = 0;
+	u64 sample = 0;
+	u64 outer_sample = 0;
+	int ret = -1;
+
+	init_time(last_t2, 0);
+	start = time_get(); /* start timestamp */
+
+	do {
+
+		t1 = time_get();	/* we'll look for a discontinuity */
+		t2 = time_get();
+
+		if (time_u64(last_t2)) {
+			/* Check the delta from outer loop (t2 to next t1) */
+			diff = time_to_us(time_sub(t1, last_t2));
+			/* This shouldn't happen */
+			if (diff < 0) {
+				pr_err(BANNER "time running backwards\n");
+				goto out;
+			}
+			if (diff > outer_sample)
+				outer_sample = diff;
+		}
+		last_t2 = t2;
+
+		total = time_to_us(time_sub(t2, start)); /* sample width */
+
+		/* This checks the inner loop (t1 to t2) */
+		diff = time_to_us(time_sub(t2, t1));     /* current diff */
+
+		/* This shouldn't happen */
+		if (diff < 0) {
+			pr_err(BANNER "time running backwards\n");
+			goto out;
+		}
+
+		if (diff > sample)
+			sample = diff; /* only want highest value */
+
+	} while (total <= data.sample_width);
+
+	ret = 0;
+
+	/* If we exceed the threshold value, we have found a hardware latency */
+	if (sample > data.threshold || outer_sample > data.threshold) {
+		struct sample s;
+
+		ret = 1;
+
+		data.count++;
+		s.seqnum = data.count;
+		s.duration = sample;
+		s.outer_duration = outer_sample;
+		s.timestamp = CURRENT_TIME;
+		__buffer_add_sample(&s);
+
+		/* Keep a running maximum ever recorded hardware latency */
+		if (sample > data.max_sample)
+			data.max_sample = sample;
+	}
+
+out:
+	return ret;
+}
+
+/*
+ * kthread_fn - The CPU time sampling/hardware latency detection kernel thread
+ * @unused: A required part of the kthread API.
+ *
+ * Used to periodically sample the CPU TSC via a call to get_sample. We
+ * disable interrupts, which does (intentionally) introduce latency since we
+ * need to ensure nothing else might be running (and thus pre-empting).
+ * Obviously this should never be used in production environments.
+ *
+ * Currently this runs on which ever CPU it was scheduled on, but most
+ * real-worald hardware latency situations occur across several CPUs,
+ * but we might later generalize this if we find there are any actualy
+ * systems with alternate SMI delivery or other hardware latencies.
+ */
+static int kthread_fn(void *unused)
+{
+	int ret;
+	u64 interval;
+
+	while (!kthread_should_stop()) {
+
+		mutex_lock(&data.lock);
+
+		local_irq_disable();
+		ret = get_sample();
+		local_irq_enable();
+
+		if (ret > 0)
+			wake_up(&data.wq); /* wake up reader(s) */
+
+		interval = data.sample_window - data.sample_width;
+		do_div(interval, USEC_PER_MSEC); /* modifies interval value */
+
+		mutex_unlock(&data.lock);
+
+		if (msleep_interruptible(interval))
+			break;
+	}
+
+	return 0;
+}
+
+/**
+ * start_kthread - Kick off the hardware latency sampling/detector kthread
+ *
+ * This starts a kernel thread that will sit and sample the CPU timestamp
+ * counter (TSC or similar) and look for potential hardware latencies.
+ */
+static int start_kthread(void)
+{
+	kthread = kthread_run(kthread_fn, NULL,
+					DRVNAME);
+	if (IS_ERR(kthread)) {
+		pr_err(BANNER "could not start sampling thread\n");
+		enabled = 0;
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+/**
+ * stop_kthread - Inform the hardware latency samping/detector kthread to stop
+ *
+ * This kicks the running hardware latency sampling/detector kernel thread and
+ * tells it to stop sampling now. Use this on unload and at system shutdown.
+ */
+static int stop_kthread(void)
+{
+	int ret;
+
+	ret = kthread_stop(kthread);
+
+	return ret;
+}
+
+/**
+ * __reset_stats - Reset statistics for the hardware latency detector
+ *
+ * We use data to store various statistics and global state. We call this
+ * function in order to reset those when "enable" is toggled on or off, and
+ * also at initialization. Should be called with data.lock held.
+ */
+static void __reset_stats(void)
+{
+	data.count = 0;
+	data.max_sample = 0;
+	ring_buffer_reset(ring_buffer); /* flush out old sample entries */
+}
+
+/**
+ * init_stats - Setup global state statistics for the hardware latency detector
+ *
+ * We use data to store various statistics and global state. We also use
+ * a global ring buffer (ring_buffer) to keep raw samples of detected hardware
+ * induced system latencies. This function initializes these structures and
+ * allocates the global ring buffer also.
+ */
+static int init_stats(void)
+{
+	int ret = -ENOMEM;
+
+	mutex_init(&data.lock);
+	init_waitqueue_head(&data.wq);
+	atomic_set(&data.sample_open, 0);
+
+	ring_buffer = ring_buffer_alloc(buf_size, BUF_FLAGS);
+
+	if (WARN(!ring_buffer, KERN_ERR BANNER
+			       "failed to allocate ring buffer!\n"))
+		goto out;
+
+	__reset_stats();
+	data.threshold = threshold ?: DEFAULT_LAT_THRESHOLD; /* threshold us */
+	data.sample_window = DEFAULT_SAMPLE_WINDOW; /* window us */
+	data.sample_width = DEFAULT_SAMPLE_WIDTH;   /* width us */
+
+	ret = 0;
+
+out:
+	return ret;
+
+}
+
+/*
+ * simple_data_read - Wrapper read function for global state debugfs entries
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The userspace provided buffer to read value into
+ * @cnt: The maximum number of bytes to read
+ * @ppos: The current "file" position
+ * @entry: The entry to read from
+ *
+ * This function provides a generic read implementation for the global state
+ * "data" structure debugfs filesystem entries. It would be nice to use
+ * simple_attr_read directly, but we need to make sure that the data.lock
+ * is held during the actual read.
+ */
+static ssize_t simple_data_read(struct file *filp, char __user *ubuf,
+				size_t cnt, loff_t *ppos, const u64 *entry)
+{
+	char buf[U64STR_SIZE];
+	u64 val = 0;
+	int len = 0;
+
+	memset(buf, 0, sizeof(buf));
+
+	if (!entry)
+		return -EFAULT;
+
+	mutex_lock(&data.lock);
+	val = *entry;
+	mutex_unlock(&data.lock);
+
+	len = snprintf(buf, sizeof(buf), "%llu\n", (unsigned long long)val);
+
+	return simple_read_from_buffer(ubuf, cnt, ppos, buf, len);
+
+}
+
+/*
+ * simple_data_write - Wrapper write function for global state debugfs entries
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The userspace provided buffer to write value from
+ * @cnt: The maximum number of bytes to write
+ * @ppos: The current "file" position
+ * @entry: The entry to write to
+ *
+ * This function provides a generic write implementation for the global state
+ * "data" structure debugfs filesystem entries. It would be nice to use
+ * simple_attr_write directly, but we need to make sure that the data.lock
+ * is held during the actual write.
+ */
+static ssize_t simple_data_write(struct file *filp, const char __user *ubuf,
+				 size_t cnt, loff_t *ppos, u64 *entry)
+{
+	char buf[U64STR_SIZE];
+	int csize = min(cnt, sizeof(buf));
+	u64 val = 0;
+	int err = 0;
+
+	memset(buf, '\0', sizeof(buf));
+	if (copy_from_user(buf, ubuf, csize))
+		return -EFAULT;
+
+	buf[U64STR_SIZE-1] = '\0';			/* just in case */
+	err = kstrtoull(buf, 10, &val);
+	if (err)
+		return -EINVAL;
+
+	mutex_lock(&data.lock);
+	*entry = val;
+	mutex_unlock(&data.lock);
+
+	return csize;
+}
+
+/**
+ * debug_count_fopen - Open function for "count" debugfs entry
+ * @inode: The in-kernel inode representation of the debugfs "file"
+ * @filp: The active open file structure for the debugfs "file"
+ *
+ * This function provides an open implementation for the "count" debugfs
+ * interface to the hardware latency detector.
+ */
+static int debug_count_fopen(struct inode *inode, struct file *filp)
+{
+	return 0;
+}
+
+/**
+ * debug_count_fread - Read function for "count" debugfs entry
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The userspace provided buffer to read value into
+ * @cnt: The maximum number of bytes to read
+ * @ppos: The current "file" position
+ *
+ * This function provides a read implementation for the "count" debugfs
+ * interface to the hardware latency detector. Can be used to read the
+ * number of latency readings exceeding the configured threshold since
+ * the detector was last reset (e.g. by writing a zero into "count").
+ */
+static ssize_t debug_count_fread(struct file *filp, char __user *ubuf,
+				     size_t cnt, loff_t *ppos)
+{
+	return simple_data_read(filp, ubuf, cnt, ppos, &data.count);
+}
+
+/**
+ * debug_count_fwrite - Write function for "count" debugfs entry
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The user buffer that contains the value to write
+ * @cnt: The maximum number of bytes to write to "file"
+ * @ppos: The current position in the debugfs "file"
+ *
+ * This function provides a write implementation for the "count" debugfs
+ * interface to the hardware latency detector. Can be used to write a
+ * desired value, especially to zero the total count.
+ */
+static ssize_t  debug_count_fwrite(struct file *filp,
+				       const char __user *ubuf,
+				       size_t cnt,
+				       loff_t *ppos)
+{
+	return simple_data_write(filp, ubuf, cnt, ppos, &data.count);
+}
+
+/**
+ * debug_enable_fopen - Dummy open function for "enable" debugfs interface
+ * @inode: The in-kernel inode representation of the debugfs "file"
+ * @filp: The active open file structure for the debugfs "file"
+ *
+ * This function provides an open implementation for the "enable" debugfs
+ * interface to the hardware latency detector.
+ */
+static int debug_enable_fopen(struct inode *inode, struct file *filp)
+{
+	return 0;
+}
+
+/**
+ * debug_enable_fread - Read function for "enable" debugfs interface
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The userspace provided buffer to read value into
+ * @cnt: The maximum number of bytes to read
+ * @ppos: The current "file" position
+ *
+ * This function provides a read implementation for the "enable" debugfs
+ * interface to the hardware latency detector. Can be used to determine
+ * whether the detector is currently enabled ("0\n" or "1\n" returned).
+ */
+static ssize_t debug_enable_fread(struct file *filp, char __user *ubuf,
+				      size_t cnt, loff_t *ppos)
+{
+	char buf[4];
+
+	if ((cnt < sizeof(buf)) || (*ppos))
+		return 0;
+
+	buf[0] = enabled ? '1' : '0';
+	buf[1] = '\n';
+	buf[2] = '\0';
+	if (copy_to_user(ubuf, buf, strlen(buf)))
+		return -EFAULT;
+	return *ppos = strlen(buf);
+}
+
+/**
+ * debug_enable_fwrite - Write function for "enable" debugfs interface
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The user buffer that contains the value to write
+ * @cnt: The maximum number of bytes to write to "file"
+ * @ppos: The current position in the debugfs "file"
+ *
+ * This function provides a write implementation for the "enable" debugfs
+ * interface to the hardware latency detector. Can be used to enable or
+ * disable the detector, which will have the side-effect of possibly
+ * also resetting the global stats and kicking off the measuring
+ * kthread (on an enable) or the converse (upon a disable).
+ */
+static ssize_t  debug_enable_fwrite(struct file *filp,
+					const char __user *ubuf,
+					size_t cnt,
+					loff_t *ppos)
+{
+	char buf[4];
+	int csize = min(cnt, sizeof(buf));
+	long val = 0;
+	int err = 0;
+
+	memset(buf, '\0', sizeof(buf));
+	if (copy_from_user(buf, ubuf, csize))
+		return -EFAULT;
+
+	buf[sizeof(buf)-1] = '\0';			/* just in case */
+	err = kstrtoul(buf, 10, &val);
+	if (0 != err)
+		return -EINVAL;
+
+	if (val) {
+		if (enabled)
+			goto unlock;
+		enabled = 1;
+		__reset_stats();
+		if (start_kthread())
+			return -EFAULT;
+	} else {
+		if (!enabled)
+			goto unlock;
+		enabled = 0;
+		err = stop_kthread();
+		if (err) {
+			pr_err(BANNER "cannot stop kthread\n");
+			return -EFAULT;
+		}
+		wake_up(&data.wq);		/* reader(s) should return */
+	}
+unlock:
+	return csize;
+}
+
+/**
+ * debug_max_fopen - Open function for "max" debugfs entry
+ * @inode: The in-kernel inode representation of the debugfs "file"
+ * @filp: The active open file structure for the debugfs "file"
+ *
+ * This function provides an open implementation for the "max" debugfs
+ * interface to the hardware latency detector.
+ */
+static int debug_max_fopen(struct inode *inode, struct file *filp)
+{
+	return 0;
+}
+
+/**
+ * debug_max_fread - Read function for "max" debugfs entry
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The userspace provided buffer to read value into
+ * @cnt: The maximum number of bytes to read
+ * @ppos: The current "file" position
+ *
+ * This function provides a read implementation for the "max" debugfs
+ * interface to the hardware latency detector. Can be used to determine
+ * the maximum latency value observed since it was last reset.
+ */
+static ssize_t debug_max_fread(struct file *filp, char __user *ubuf,
+				   size_t cnt, loff_t *ppos)
+{
+	return simple_data_read(filp, ubuf, cnt, ppos, &data.max_sample);
+}
+
+/**
+ * debug_max_fwrite - Write function for "max" debugfs entry
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The user buffer that contains the value to write
+ * @cnt: The maximum number of bytes to write to "file"
+ * @ppos: The current position in the debugfs "file"
+ *
+ * This function provides a write implementation for the "max" debugfs
+ * interface to the hardware latency detector. Can be used to reset the
+ * maximum or set it to some other desired value - if, then, subsequent
+ * measurements exceed this value, the maximum will be updated.
+ */
+static ssize_t  debug_max_fwrite(struct file *filp,
+				     const char __user *ubuf,
+				     size_t cnt,
+				     loff_t *ppos)
+{
+	return simple_data_write(filp, ubuf, cnt, ppos, &data.max_sample);
+}
+
+
+/**
+ * debug_sample_fopen - An open function for "sample" debugfs interface
+ * @inode: The in-kernel inode representation of this debugfs "file"
+ * @filp: The active open file structure for the debugfs "file"
+ *
+ * This function handles opening the "sample" file within the hardware
+ * latency detector debugfs directory interface. This file is used to read
+ * raw samples from the global ring_buffer and allows the user to see a
+ * running latency history. Can be opened blocking or non-blocking,
+ * affecting whether it behaves as a buffer read pipe, or does not.
+ * Implements simple locking to prevent multiple simultaneous use.
+ */
+static int debug_sample_fopen(struct inode *inode, struct file *filp)
+{
+	if (!atomic_add_unless(&data.sample_open, 1, 1))
+		return -EBUSY;
+	else
+		return 0;
+}
+
+/**
+ * debug_sample_fread - A read function for "sample" debugfs interface
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The user buffer that will contain the samples read
+ * @cnt: The maximum bytes to read from the debugfs "file"
+ * @ppos: The current position in the debugfs "file"
+ *
+ * This function handles reading from the "sample" file within the hardware
+ * latency detector debugfs directory interface. This file is used to read
+ * raw samples from the global ring_buffer and allows the user to see a
+ * running latency history. By default this will block pending a new
+ * value written into the sample buffer, unless there are already a
+ * number of value(s) waiting in the buffer, or the sample file was
+ * previously opened in a non-blocking mode of operation.
+ */
+static ssize_t debug_sample_fread(struct file *filp, char __user *ubuf,
+					size_t cnt, loff_t *ppos)
+{
+	int len = 0;
+	char buf[64];
+	struct sample *sample = NULL;
+
+	if (!enabled)
+		return 0;
+
+	sample = kzalloc(sizeof(struct sample), GFP_KERNEL);
+	if (!sample)
+		return -ENOMEM;
+
+	while (!buffer_get_sample(sample)) {
+
+		DEFINE_WAIT(wait);
+
+		if (filp->f_flags & O_NONBLOCK) {
+			len = -EAGAIN;
+			goto out;
+		}
+
+		prepare_to_wait(&data.wq, &wait, TASK_INTERRUPTIBLE);
+		schedule();
+		finish_wait(&data.wq, &wait);
+
+		if (signal_pending(current)) {
+			len = -EINTR;
+			goto out;
+		}
+
+		if (!enabled) {			/* enable was toggled */
+			len = 0;
+			goto out;
+		}
+	}
+
+	len = snprintf(buf, sizeof(buf), "%010lu.%010lu\t%llu\t%llu\n",
+		       sample->timestamp.tv_sec,
+		       sample->timestamp.tv_nsec,
+		       sample->duration,
+		       sample->outer_duration);
+
+
+	/* handling partial reads is more trouble than it's worth */
+	if (len > cnt)
+		goto out;
+
+	if (copy_to_user(ubuf, buf, len))
+		len = -EFAULT;
+
+out:
+	kfree(sample);
+	return len;
+}
+
+/**
+ * debug_sample_release - Release function for "sample" debugfs interface
+ * @inode: The in-kernel inode represenation of the debugfs "file"
+ * @filp: The active open file structure for the debugfs "file"
+ *
+ * This function completes the close of the debugfs interface "sample" file.
+ * Frees the sample_open "lock" so that other users may open the interface.
+ */
+static int debug_sample_release(struct inode *inode, struct file *filp)
+{
+	atomic_dec(&data.sample_open);
+
+	return 0;
+}
+
+/**
+ * debug_threshold_fopen - Open function for "threshold" debugfs entry
+ * @inode: The in-kernel inode representation of the debugfs "file"
+ * @filp: The active open file structure for the debugfs "file"
+ *
+ * This function provides an open implementation for the "threshold" debugfs
+ * interface to the hardware latency detector.
+ */
+static int debug_threshold_fopen(struct inode *inode, struct file *filp)
+{
+	return 0;
+}
+
+/**
+ * debug_threshold_fread - Read function for "threshold" debugfs entry
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The userspace provided buffer to read value into
+ * @cnt: The maximum number of bytes to read
+ * @ppos: The current "file" position
+ *
+ * This function provides a read implementation for the "threshold" debugfs
+ * interface to the hardware latency detector. It can be used to determine
+ * the current threshold level at which a latency will be recorded in the
+ * global ring buffer, typically on the order of 10us.
+ */
+static ssize_t debug_threshold_fread(struct file *filp, char __user *ubuf,
+					 size_t cnt, loff_t *ppos)
+{
+	return simple_data_read(filp, ubuf, cnt, ppos, &data.threshold);
+}
+
+/**
+ * debug_threshold_fwrite - Write function for "threshold" debugfs entry
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The user buffer that contains the value to write
+ * @cnt: The maximum number of bytes to write to "file"
+ * @ppos: The current position in the debugfs "file"
+ *
+ * This function provides a write implementation for the "threshold" debugfs
+ * interface to the hardware latency detector. It can be used to configure
+ * the threshold level at which any subsequently detected latencies will
+ * be recorded into the global ring buffer.
+ */
+static ssize_t  debug_threshold_fwrite(struct file *filp,
+					const char __user *ubuf,
+					size_t cnt,
+					loff_t *ppos)
+{
+	int ret;
+
+	ret = simple_data_write(filp, ubuf, cnt, ppos, &data.threshold);
+
+	if (enabled)
+		wake_up_process(kthread);
+
+	return ret;
+}
+
+/**
+ * debug_width_fopen - Open function for "width" debugfs entry
+ * @inode: The in-kernel inode representation of the debugfs "file"
+ * @filp: The active open file structure for the debugfs "file"
+ *
+ * This function provides an open implementation for the "width" debugfs
+ * interface to the hardware latency detector.
+ */
+static int debug_width_fopen(struct inode *inode, struct file *filp)
+{
+	return 0;
+}
+
+/**
+ * debug_width_fread - Read function for "width" debugfs entry
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The userspace provided buffer to read value into
+ * @cnt: The maximum number of bytes to read
+ * @ppos: The current "file" position
+ *
+ * This function provides a read implementation for the "width" debugfs
+ * interface to the hardware latency detector. It can be used to determine
+ * for how many us of the total window us we will actively sample for any
+ * hardware-induced latecy periods. Obviously, it is not possible to
+ * sample constantly and have the system respond to a sample reader, or,
+ * worse, without having the system appear to have gone out to lunch.
+ */
+static ssize_t debug_width_fread(struct file *filp, char __user *ubuf,
+				     size_t cnt, loff_t *ppos)
+{
+	return simple_data_read(filp, ubuf, cnt, ppos, &data.sample_width);
+}
+
+/**
+ * debug_width_fwrite - Write function for "width" debugfs entry
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The user buffer that contains the value to write
+ * @cnt: The maximum number of bytes to write to "file"
+ * @ppos: The current position in the debugfs "file"
+ *
+ * This function provides a write implementation for the "width" debugfs
+ * interface to the hardware latency detector. It can be used to configure
+ * for how many us of the total window us we will actively sample for any
+ * hardware-induced latency periods. Obviously, it is not possible to
+ * sample constantly and have the system respond to a sample reader, or,
+ * worse, without having the system appear to have gone out to lunch. It
+ * is enforced that width is less that the total window size.
+ */
+static ssize_t  debug_width_fwrite(struct file *filp,
+				       const char __user *ubuf,
+				       size_t cnt,
+				       loff_t *ppos)
+{
+	char buf[U64STR_SIZE];
+	int csize = min(cnt, sizeof(buf));
+	u64 val = 0;
+	int err = 0;
+
+	memset(buf, '\0', sizeof(buf));
+	if (copy_from_user(buf, ubuf, csize))
+		return -EFAULT;
+
+	buf[U64STR_SIZE-1] = '\0';			/* just in case */
+	err = kstrtoull(buf, 10, &val);
+	if (0 != err)
+		return -EINVAL;
+
+	mutex_lock(&data.lock);
+	if (val < data.sample_window)
+		data.sample_width = val;
+	else {
+		mutex_unlock(&data.lock);
+		return -EINVAL;
+	}
+	mutex_unlock(&data.lock);
+
+	if (enabled)
+		wake_up_process(kthread);
+
+	return csize;
+}
+
+/**
+ * debug_window_fopen - Open function for "window" debugfs entry
+ * @inode: The in-kernel inode representation of the debugfs "file"
+ * @filp: The active open file structure for the debugfs "file"
+ *
+ * This function provides an open implementation for the "window" debugfs
+ * interface to the hardware latency detector. The window is the total time
+ * in us that will be considered one sample period. Conceptually, windows
+ * occur back-to-back and contain a sample width period during which
+ * actual sampling occurs.
+ */
+static int debug_window_fopen(struct inode *inode, struct file *filp)
+{
+	return 0;
+}
+
+/**
+ * debug_window_fread - Read function for "window" debugfs entry
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The userspace provided buffer to read value into
+ * @cnt: The maximum number of bytes to read
+ * @ppos: The current "file" position
+ *
+ * This function provides a read implementation for the "window" debugfs
+ * interface to the hardware latency detector. The window is the total time
+ * in us that will be considered one sample period. Conceptually, windows
+ * occur back-to-back and contain a sample width period during which
+ * actual sampling occurs. Can be used to read the total window size.
+ */
+static ssize_t debug_window_fread(struct file *filp, char __user *ubuf,
+				      size_t cnt, loff_t *ppos)
+{
+	return simple_data_read(filp, ubuf, cnt, ppos, &data.sample_window);
+}
+
+/**
+ * debug_window_fwrite - Write function for "window" debugfs entry
+ * @filp: The active open file structure for the debugfs "file"
+ * @ubuf: The user buffer that contains the value to write
+ * @cnt: The maximum number of bytes to write to "file"
+ * @ppos: The current position in the debugfs "file"
+ *
+ * This function provides a write implementation for the "window" debufds
+ * interface to the hardware latency detetector. The window is the total time
+ * in us that will be considered one sample period. Conceptually, windows
+ * occur back-to-back and contain a sample width period during which
+ * actual sampling occurs. Can be used to write a new total window size. It
+ * is enfoced that any value written must be greater than the sample width
+ * size, or an error results.
+ */
+static ssize_t  debug_window_fwrite(struct file *filp,
+					const char __user *ubuf,
+					size_t cnt,
+					loff_t *ppos)
+{
+	char buf[U64STR_SIZE];
+	int csize = min(cnt, sizeof(buf));
+	u64 val = 0;
+	int err = 0;
+
+	memset(buf, '\0', sizeof(buf));
+	if (copy_from_user(buf, ubuf, csize))
+		return -EFAULT;
+
+	buf[U64STR_SIZE-1] = '\0';			/* just in case */
+	err = kstrtoull(buf, 10, &val);
+	if (0 != err)
+		return -EINVAL;
+
+	mutex_lock(&data.lock);
+	if (data.sample_width < val)
+		data.sample_window = val;
+	else {
+		mutex_unlock(&data.lock);
+		return -EINVAL;
+	}
+	mutex_unlock(&data.lock);
+
+	return csize;
+}
+
+/*
+ * Function pointers for the "count" debugfs file operations
+ */
+static const struct file_operations count_fops = {
+	.open		= debug_count_fopen,
+	.read		= debug_count_fread,
+	.write		= debug_count_fwrite,
+	.owner		= THIS_MODULE,
+};
+
+/*
+ * Function pointers for the "enable" debugfs file operations
+ */
+static const struct file_operations enable_fops = {
+	.open		= debug_enable_fopen,
+	.read		= debug_enable_fread,
+	.write		= debug_enable_fwrite,
+	.owner		= THIS_MODULE,
+};
+
+/*
+ * Function pointers for the "max" debugfs file operations
+ */
+static const struct file_operations max_fops = {
+	.open		= debug_max_fopen,
+	.read		= debug_max_fread,
+	.write		= debug_max_fwrite,
+	.owner		= THIS_MODULE,
+};
+
+/*
+ * Function pointers for the "sample" debugfs file operations
+ */
+static const struct file_operations sample_fops = {
+	.open		= debug_sample_fopen,
+	.read		= debug_sample_fread,
+	.release	= debug_sample_release,
+	.owner		= THIS_MODULE,
+};
+
+/*
+ * Function pointers for the "threshold" debugfs file operations
+ */
+static const struct file_operations threshold_fops = {
+	.open		= debug_threshold_fopen,
+	.read		= debug_threshold_fread,
+	.write		= debug_threshold_fwrite,
+	.owner		= THIS_MODULE,
+};
+
+/*
+ * Function pointers for the "width" debugfs file operations
+ */
+static const struct file_operations width_fops = {
+	.open		= debug_width_fopen,
+	.read		= debug_width_fread,
+	.write		= debug_width_fwrite,
+	.owner		= THIS_MODULE,
+};
+
+/*
+ * Function pointers for the "window" debugfs file operations
+ */
+static const struct file_operations window_fops = {
+	.open		= debug_window_fopen,
+	.read		= debug_window_fread,
+	.write		= debug_window_fwrite,
+	.owner		= THIS_MODULE,
+};
+
+/**
+ * init_debugfs - A function to initialize the debugfs interface files
+ *
+ * This function creates entries in debugfs for "hwlat_detector", including
+ * files to read values from the detector, current samples, and the
+ * maximum sample that has been captured since the hardware latency
+ * dectector was started.
+ */
+static int init_debugfs(void)
+{
+	int ret = -ENOMEM;
+
+	debug_dir = debugfs_create_dir(DRVNAME, NULL);
+	if (!debug_dir)
+		goto err_debug_dir;
+
+	debug_sample = debugfs_create_file("sample", 0444,
+					       debug_dir, NULL,
+					       &sample_fops);
+	if (!debug_sample)
+		goto err_sample;
+
+	debug_count = debugfs_create_file("count", 0444,
+					      debug_dir, NULL,
+					      &count_fops);
+	if (!debug_count)
+		goto err_count;
+
+	debug_max = debugfs_create_file("max", 0444,
+					    debug_dir, NULL,
+					    &max_fops);
+	if (!debug_max)
+		goto err_max;
+
+	debug_sample_window = debugfs_create_file("window", 0644,
+						      debug_dir, NULL,
+						      &window_fops);
+	if (!debug_sample_window)
+		goto err_window;
+
+	debug_sample_width = debugfs_create_file("width", 0644,
+						     debug_dir, NULL,
+						     &width_fops);
+	if (!debug_sample_width)
+		goto err_width;
+
+	debug_threshold = debugfs_create_file("threshold", 0644,
+						  debug_dir, NULL,
+						  &threshold_fops);
+	if (!debug_threshold)
+		goto err_threshold;
+
+	debug_enable = debugfs_create_file("enable", 0644,
+					       debug_dir, &enabled,
+					       &enable_fops);
+	if (!debug_enable)
+		goto err_enable;
+
+	else {
+		ret = 0;
+		goto out;
+	}
+
+err_enable:
+	debugfs_remove(debug_threshold);
+err_threshold:
+	debugfs_remove(debug_sample_width);
+err_width:
+	debugfs_remove(debug_sample_window);
+err_window:
+	debugfs_remove(debug_max);
+err_max:
+	debugfs_remove(debug_count);
+err_count:
+	debugfs_remove(debug_sample);
+err_sample:
+	debugfs_remove(debug_dir);
+err_debug_dir:
+out:
+	return ret;
+}
+
+/**
+ * free_debugfs - A function to cleanup the debugfs file interface
+ */
+static void free_debugfs(void)
+{
+	/* could also use a debugfs_remove_recursive */
+	debugfs_remove(debug_enable);
+	debugfs_remove(debug_threshold);
+	debugfs_remove(debug_sample_width);
+	debugfs_remove(debug_sample_window);
+	debugfs_remove(debug_max);
+	debugfs_remove(debug_count);
+	debugfs_remove(debug_sample);
+	debugfs_remove(debug_dir);
+}
+
+/**
+ * detector_init - Standard module initialization code
+ */
+static int detector_init(void)
+{
+	int ret = -ENOMEM;
+
+	pr_info(BANNER "version %s\n", VERSION);
+
+	ret = init_stats();
+	if (0 != ret)
+		goto out;
+
+	ret = init_debugfs();
+	if (0 != ret)
+		goto err_stats;
+
+	if (enabled)
+		ret = start_kthread();
+
+	goto out;
+
+err_stats:
+	ring_buffer_free(ring_buffer);
+out:
+	return ret;
+
+}
+
+/**
+ * detector_exit - Standard module cleanup code
+ */
+static void detector_exit(void)
+{
+	int err;
+
+	if (enabled) {
+		enabled = 0;
+		err = stop_kthread();
+		if (err)
+			pr_err(BANNER "cannot stop kthread\n");
+	}
+
+	free_debugfs();
+	ring_buffer_free(ring_buffer);	/* free up the ring buffer */
+
+}
+
+module_init(detector_init);
+module_exit(detector_exit);
diff --git a/kernel/msm-3.18/drivers/mmc/host/mmci.c b/kernel/msm-3.18/drivers/mmc/host/mmci.c
index 43af791e2..863a9e6f2 100644
--- a/kernel/msm-3.18/drivers/mmc/host/mmci.c
+++ b/kernel/msm-3.18/drivers/mmc/host/mmci.c
@@ -1153,15 +1153,12 @@ static irqreturn_t mmci_pio_irq(int irq, void *dev_id)
 	struct sg_mapping_iter *sg_miter = &host->sg_miter;
 	struct variant_data *variant = host->variant;
 	void __iomem *base = host->base;
-	unsigned long flags;
 	u32 status;
 
 	status = readl(base + MMCISTATUS);
 
 	dev_dbg(mmc_dev(host->mmc), "irq1 (pio) %08x\n", status);
 
-	local_irq_save(flags);
-
 	do {
 		unsigned int remain, len;
 		char *buffer;
@@ -1201,8 +1198,6 @@ static irqreturn_t mmci_pio_irq(int irq, void *dev_id)
 
 	sg_miter_stop(sg_miter);
 
-	local_irq_restore(flags);
-
 	/*
 	 * If we have less than the fifo 'half-full' threshold to transfer,
 	 * trigger a PIO interrupt as soon as any data is available.
diff --git a/kernel/msm-3.18/drivers/net/ethernet/3com/3c59x.c b/kernel/msm-3.18/drivers/net/ethernet/3com/3c59x.c
index 41095ebad..b0a0cb22a 100644
--- a/kernel/msm-3.18/drivers/net/ethernet/3com/3c59x.c
+++ b/kernel/msm-3.18/drivers/net/ethernet/3com/3c59x.c
@@ -842,9 +842,9 @@ static void poll_vortex(struct net_device *dev)
 {
 	struct vortex_private *vp = netdev_priv(dev);
 	unsigned long flags;
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	(vp->full_bus_master_rx ? boomerang_interrupt:vortex_interrupt)(dev->irq,dev);
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 }
 #endif
 
@@ -1916,12 +1916,12 @@ static void vortex_tx_timeout(struct net_device *dev)
 			 * Block interrupts because vortex_interrupt does a bare spin_lock()
 			 */
 			unsigned long flags;
-			local_irq_save(flags);
+			local_irq_save_nort(flags);
 			if (vp->full_bus_master_tx)
 				boomerang_interrupt(dev->irq, dev);
 			else
 				vortex_interrupt(dev->irq, dev);
-			local_irq_restore(flags);
+			local_irq_restore_nort(flags);
 		}
 	}
 
diff --git a/kernel/msm-3.18/drivers/net/ethernet/atheros/atl1c/atl1c_main.c b/kernel/msm-3.18/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
index 067f2cb9b..c2010fcac 100644
--- a/kernel/msm-3.18/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
+++ b/kernel/msm-3.18/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
@@ -2212,11 +2212,7 @@ static netdev_tx_t atl1c_xmit_frame(struct sk_buff *skb,
 	}
 
 	tpd_req = atl1c_cal_tpd_req(skb);
-	if (!spin_trylock_irqsave(&adapter->tx_lock, flags)) {
-		if (netif_msg_pktdata(adapter))
-			dev_info(&adapter->pdev->dev, "tx locked\n");
-		return NETDEV_TX_LOCKED;
-	}
+	spin_lock_irqsave(&adapter->tx_lock, flags);
 
 	if (atl1c_tpd_avail(adapter, type) < tpd_req) {
 		/* no enough descriptor, just stop queue */
diff --git a/kernel/msm-3.18/drivers/net/ethernet/atheros/atl1e/atl1e_main.c b/kernel/msm-3.18/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
index 2326579f9..a042658ff 100644
--- a/kernel/msm-3.18/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
+++ b/kernel/msm-3.18/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
@@ -1880,8 +1880,7 @@ static netdev_tx_t atl1e_xmit_frame(struct sk_buff *skb,
 		return NETDEV_TX_OK;
 	}
 	tpd_req = atl1e_cal_tdp_req(skb);
-	if (!spin_trylock_irqsave(&adapter->tx_lock, flags))
-		return NETDEV_TX_LOCKED;
+	spin_lock_irqsave(&adapter->tx_lock, flags);
 
 	if (atl1e_tpd_avail(adapter) < tpd_req) {
 		/* no enough descriptor, just stop queue */
diff --git a/kernel/msm-3.18/drivers/net/ethernet/chelsio/cxgb/sge.c b/kernel/msm-3.18/drivers/net/ethernet/chelsio/cxgb/sge.c
index 4c5879389..1adb83c01 100644
--- a/kernel/msm-3.18/drivers/net/ethernet/chelsio/cxgb/sge.c
+++ b/kernel/msm-3.18/drivers/net/ethernet/chelsio/cxgb/sge.c
@@ -1663,8 +1663,7 @@ static int t1_sge_tx(struct sk_buff *skb, struct adapter *adapter,
 	struct cmdQ *q = &sge->cmdQ[qid];
 	unsigned int credits, pidx, genbit, count, use_sched_skb = 0;
 
-	if (!spin_trylock(&q->lock))
-		return NETDEV_TX_LOCKED;
+	spin_lock(&q->lock);
 
 	reclaim_completed_tx(sge, q);
 
diff --git a/kernel/msm-3.18/drivers/net/ethernet/freescale/gianfar.c b/kernel/msm-3.18/drivers/net/ethernet/freescale/gianfar.c
index 4fdf0aa16..bb7aaf0d9 100644
--- a/kernel/msm-3.18/drivers/net/ethernet/freescale/gianfar.c
+++ b/kernel/msm-3.18/drivers/net/ethernet/freescale/gianfar.c
@@ -1483,7 +1483,7 @@ static int gfar_suspend(struct device *dev)
 
 	if (netif_running(ndev)) {
 
-		local_irq_save(flags);
+		local_irq_save_nort(flags);
 		lock_tx_qs(priv);
 
 		gfar_halt_nodisable(priv);
@@ -1499,7 +1499,7 @@ static int gfar_suspend(struct device *dev)
 		gfar_write(&regs->maccfg1, tempval);
 
 		unlock_tx_qs(priv);
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 
 		disable_napi(priv);
 
@@ -1541,7 +1541,7 @@ static int gfar_resume(struct device *dev)
 	/* Disable Magic Packet mode, in case something
 	 * else woke us up.
 	 */
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	lock_tx_qs(priv);
 
 	tempval = gfar_read(&regs->maccfg2);
@@ -1551,7 +1551,7 @@ static int gfar_resume(struct device *dev)
 	gfar_start(priv);
 
 	unlock_tx_qs(priv);
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 
 	netif_device_attach(ndev);
 
@@ -3307,14 +3307,14 @@ static irqreturn_t gfar_error(int irq, void *grp_id)
 			dev->stats.tx_dropped++;
 			atomic64_inc(&priv->extra_stats.tx_underrun);
 
-			local_irq_save(flags);
+			local_irq_save_nort(flags);
 			lock_tx_qs(priv);
 
 			/* Reactivate the Tx Queues */
 			gfar_write(&regs->tstat, gfargrp->tstat);
 
 			unlock_tx_qs(priv);
-			local_irq_restore(flags);
+			local_irq_restore_nort(flags);
 		}
 		netif_dbg(priv, tx_err, dev, "Transmit Error\n");
 	}
diff --git a/kernel/msm-3.18/drivers/net/ethernet/neterion/s2io.c b/kernel/msm-3.18/drivers/net/ethernet/neterion/s2io.c
index f5e4b8201..631175b5d 100644
--- a/kernel/msm-3.18/drivers/net/ethernet/neterion/s2io.c
+++ b/kernel/msm-3.18/drivers/net/ethernet/neterion/s2io.c
@@ -4084,12 +4084,7 @@ static netdev_tx_t s2io_xmit(struct sk_buff *skb, struct net_device *dev)
 			[skb->priority & (MAX_TX_FIFOS - 1)];
 	fifo = &mac_control->fifos[queue];
 
-	if (do_spin_lock)
-		spin_lock_irqsave(&fifo->tx_lock, flags);
-	else {
-		if (unlikely(!spin_trylock_irqsave(&fifo->tx_lock, flags)))
-			return NETDEV_TX_LOCKED;
-	}
+	spin_lock_irqsave(&fifo->tx_lock, flags);
 
 	if (sp->config.multiq) {
 		if (__netif_subqueue_stopped(dev, fifo->fifo_no)) {
diff --git a/kernel/msm-3.18/drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c b/kernel/msm-3.18/drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
index 3b98b263b..ca4add749 100644
--- a/kernel/msm-3.18/drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
+++ b/kernel/msm-3.18/drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
@@ -2137,10 +2137,8 @@ static int pch_gbe_xmit_frame(struct sk_buff *skb, struct net_device *netdev)
 	struct pch_gbe_tx_ring *tx_ring = adapter->tx_ring;
 	unsigned long flags;
 
-	if (!spin_trylock_irqsave(&tx_ring->tx_lock, flags)) {
-		/* Collision - tell upper layer to requeue */
-		return NETDEV_TX_LOCKED;
-	}
+	spin_lock_irqsave(&tx_ring->tx_lock, flags);
+
 	if (unlikely(!PCH_GBE_DESC_UNUSED(tx_ring))) {
 		netif_stop_queue(netdev);
 		spin_unlock_irqrestore(&tx_ring->tx_lock, flags);
diff --git a/kernel/msm-3.18/drivers/net/ethernet/realtek/8139too.c b/kernel/msm-3.18/drivers/net/ethernet/realtek/8139too.c
index 007b38cce..7858f2bd8 100644
--- a/kernel/msm-3.18/drivers/net/ethernet/realtek/8139too.c
+++ b/kernel/msm-3.18/drivers/net/ethernet/realtek/8139too.c
@@ -2215,7 +2215,7 @@ static void rtl8139_poll_controller(struct net_device *dev)
 	struct rtl8139_private *tp = netdev_priv(dev);
 	const int irq = tp->pci_dev->irq;
 
-	disable_irq(irq);
+	disable_irq_nosync(irq);
 	rtl8139_interrupt(irq, dev);
 	enable_irq(irq);
 }
diff --git a/kernel/msm-3.18/drivers/net/ethernet/tehuti/tehuti.c b/kernel/msm-3.18/drivers/net/ethernet/tehuti/tehuti.c
index 6ab36d9ff..6b7d989c4 100644
--- a/kernel/msm-3.18/drivers/net/ethernet/tehuti/tehuti.c
+++ b/kernel/msm-3.18/drivers/net/ethernet/tehuti/tehuti.c
@@ -1629,13 +1629,8 @@ static netdev_tx_t bdx_tx_transmit(struct sk_buff *skb,
 	unsigned long flags;
 
 	ENTER;
-	local_irq_save(flags);
-	if (!spin_trylock(&priv->tx_lock)) {
-		local_irq_restore(flags);
-		DBG("%s[%s]: TX locked, returning NETDEV_TX_LOCKED\n",
-		    BDX_DRV_NAME, ndev->name);
-		return NETDEV_TX_LOCKED;
-	}
+
+	spin_lock_irqsave(&priv->tx_lock, flags);
 
 	/* build tx descriptor */
 	BDX_ASSERT(f->m.wptr >= f->m.memsz);	/* started with valid wptr */
diff --git a/kernel/msm-3.18/drivers/net/rionet.c b/kernel/msm-3.18/drivers/net/rionet.c
index 18cc2c8d5..a5e0ef3c0 100644
--- a/kernel/msm-3.18/drivers/net/rionet.c
+++ b/kernel/msm-3.18/drivers/net/rionet.c
@@ -174,11 +174,7 @@ static int rionet_start_xmit(struct sk_buff *skb, struct net_device *ndev)
 	unsigned long flags;
 	int add_num = 1;
 
-	local_irq_save(flags);
-	if (!spin_trylock(&rnet->tx_lock)) {
-		local_irq_restore(flags);
-		return NETDEV_TX_LOCKED;
-	}
+	spin_lock_irqsave(&rnet->tx_lock, flags);
 
 	if (is_multicast_ether_addr(eth->h_dest))
 		add_num = nets[rnet->mport->id].nact;
diff --git a/kernel/msm-3.18/drivers/net/wireless/orinoco/orinoco_usb.c b/kernel/msm-3.18/drivers/net/wireless/orinoco/orinoco_usb.c
index 995846422..dfda85005 100644
--- a/kernel/msm-3.18/drivers/net/wireless/orinoco/orinoco_usb.c
+++ b/kernel/msm-3.18/drivers/net/wireless/orinoco/orinoco_usb.c
@@ -699,7 +699,7 @@ static void ezusb_req_ctx_wait(struct ezusb_priv *upriv,
 			while (!ctx->done.done && msecs--)
 				udelay(1000);
 		} else {
-			wait_event_interruptible(ctx->done.wait,
+			swait_event_interruptible(ctx->done.wait,
 						 ctx->done.done);
 		}
 		break;
diff --git a/kernel/msm-3.18/drivers/pci/access.c b/kernel/msm-3.18/drivers/pci/access.c
index b965c1216..5908d6e3b 100644
--- a/kernel/msm-3.18/drivers/pci/access.c
+++ b/kernel/msm-3.18/drivers/pci/access.c
@@ -580,7 +580,7 @@ void pci_cfg_access_unlock(struct pci_dev *dev)
 	WARN_ON(!dev->block_cfg_access);
 
 	dev->block_cfg_access = 0;
-	wake_up_all(&pci_cfg_wait);
+	wake_up_all_locked(&pci_cfg_wait);
 	raw_spin_unlock_irqrestore(&pci_lock, flags);
 }
 EXPORT_SYMBOL_GPL(pci_cfg_access_unlock);
diff --git a/kernel/msm-3.18/drivers/pinctrl/qcom/pinctrl-msm.c b/kernel/msm-3.18/drivers/pinctrl/qcom/pinctrl-msm.c
index d596e71e1..a414fe4ca 100644
--- a/kernel/msm-3.18/drivers/pinctrl/qcom/pinctrl-msm.c
+++ b/kernel/msm-3.18/drivers/pinctrl/qcom/pinctrl-msm.c
@@ -63,7 +63,7 @@ struct msm_pinctrl {
 	struct irq_chip *irq_chip_extn;
 	int irq;
 
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	DECLARE_BITMAP(dual_edge_irqs, MAX_NR_GPIO);
 	DECLARE_BITMAP(enabled_irqs, MAX_NR_GPIO);
@@ -161,14 +161,14 @@ static int msm_pinmux_set_mux(struct pinctrl_dev *pctldev,
 	if (WARN_ON(i == g->nfuncs))
 		return -EINVAL;
 
-	spin_lock_irqsave(&pctrl->lock, flags);
+	raw_spin_lock_irqsave(&pctrl->lock, flags);
 
 	val = readl(pctrl->regs + g->ctl_reg);
 	val &= ~(0x7 << g->mux_bit);
 	val |= i << g->mux_bit;
 	writel(val, pctrl->regs + g->ctl_reg);
 
-	spin_unlock_irqrestore(&pctrl->lock, flags);
+	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 
 	return 0;
 }
@@ -349,14 +349,14 @@ static int msm_config_group_set(struct pinctrl_dev *pctldev,
 			break;
 		case PIN_CONFIG_OUTPUT:
 			/* set output value */
-			spin_lock_irqsave(&pctrl->lock, flags);
+			raw_spin_lock_irqsave(&pctrl->lock, flags);
 			val = readl(pctrl->regs + g->io_reg);
 			if (arg)
 				val |= BIT(g->out_bit);
 			else
 				val &= ~BIT(g->out_bit);
 			writel(val, pctrl->regs + g->io_reg);
-			spin_unlock_irqrestore(&pctrl->lock, flags);
+			raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 
 			/* enable output */
 			arg = 1;
@@ -377,12 +377,12 @@ static int msm_config_group_set(struct pinctrl_dev *pctldev,
 			return -EINVAL;
 		}
 
-		spin_lock_irqsave(&pctrl->lock, flags);
+		raw_spin_lock_irqsave(&pctrl->lock, flags);
 		val = readl(pctrl->regs + g->ctl_reg);
 		val &= ~(mask << bit);
 		val |= arg << bit;
 		writel(val, pctrl->regs + g->ctl_reg);
-		spin_unlock_irqrestore(&pctrl->lock, flags);
+		raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 	}
 
 	return 0;
@@ -411,13 +411,13 @@ static int msm_gpio_direction_input(struct gpio_chip *chip, unsigned offset)
 
 	g = &pctrl->soc->groups[offset];
 
-	spin_lock_irqsave(&pctrl->lock, flags);
+	raw_spin_lock_irqsave(&pctrl->lock, flags);
 
 	val = readl(pctrl->regs + g->ctl_reg);
 	val &= ~BIT(g->oe_bit);
 	writel(val, pctrl->regs + g->ctl_reg);
 
-	spin_unlock_irqrestore(&pctrl->lock, flags);
+	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 
 	return 0;
 }
@@ -431,7 +431,7 @@ static int msm_gpio_direction_output(struct gpio_chip *chip, unsigned offset, in
 
 	g = &pctrl->soc->groups[offset];
 
-	spin_lock_irqsave(&pctrl->lock, flags);
+	raw_spin_lock_irqsave(&pctrl->lock, flags);
 
 	val = readl(pctrl->regs + g->io_reg);
 	if (value)
@@ -444,7 +444,7 @@ static int msm_gpio_direction_output(struct gpio_chip *chip, unsigned offset, in
 	val |= BIT(g->oe_bit);
 	writel(val, pctrl->regs + g->ctl_reg);
 
-	spin_unlock_irqrestore(&pctrl->lock, flags);
+	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 
 	return 0;
 }
@@ -470,7 +470,7 @@ static void msm_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
 
 	g = &pctrl->soc->groups[offset];
 
-	spin_lock_irqsave(&pctrl->lock, flags);
+	raw_spin_lock_irqsave(&pctrl->lock, flags);
 
 	val = readl(pctrl->regs + g->io_reg);
 	if (value)
@@ -479,7 +479,7 @@ static void msm_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
 		val &= ~BIT(g->out_bit);
 	writel(val, pctrl->regs + g->io_reg);
 
-	spin_unlock_irqrestore(&pctrl->lock, flags);
+	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 }
 
 static int msm_gpio_request(struct gpio_chip *chip, unsigned offset)
@@ -610,7 +610,7 @@ static void msm_gpio_irq_mask(struct irq_data *d)
 
 	g = &pctrl->soc->groups[d->hwirq];
 
-	spin_lock_irqsave(&pctrl->lock, flags);
+	raw_spin_lock_irqsave(&pctrl->lock, flags);
 
 	val = readl(pctrl->regs + g->intr_cfg_reg);
 	val &= ~BIT(g->intr_enable_bit);
@@ -618,7 +618,7 @@ static void msm_gpio_irq_mask(struct irq_data *d)
 
 	clear_bit(d->hwirq, pctrl->enabled_irqs);
 
-	spin_unlock_irqrestore(&pctrl->lock, flags);
+	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 	if (pctrl->irq_chip_extn->irq_mask)
 		pctrl->irq_chip_extn->irq_mask(d);
 }
@@ -633,7 +633,7 @@ static void msm_gpio_irq_unmask(struct irq_data *d)
 
 	g = &pctrl->soc->groups[d->hwirq];
 
-	spin_lock_irqsave(&pctrl->lock, flags);
+	raw_spin_lock_irqsave(&pctrl->lock, flags);
 
 	val = readl(pctrl->regs + g->intr_cfg_reg);
 	val |= BIT(g->intr_enable_bit);
@@ -641,9 +641,7 @@ static void msm_gpio_irq_unmask(struct irq_data *d)
 
 	set_bit(d->hwirq, pctrl->enabled_irqs);
 
-	spin_unlock_irqrestore(&pctrl->lock, flags);
-	if (pctrl->irq_chip_extn->irq_unmask)
-		pctrl->irq_chip_extn->irq_unmask(d);
+	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 }
 
 static void msm_gpio_irq_ack(struct irq_data *d)
@@ -656,7 +654,7 @@ static void msm_gpio_irq_ack(struct irq_data *d)
 
 	g = &pctrl->soc->groups[d->hwirq];
 
-	spin_lock_irqsave(&pctrl->lock, flags);
+	raw_spin_lock_irqsave(&pctrl->lock, flags);
 
 	val = readl(pctrl->regs + g->intr_status_reg);
 	if (g->intr_ack_high)
@@ -668,7 +666,7 @@ static void msm_gpio_irq_ack(struct irq_data *d)
 	if (test_bit(d->hwirq, pctrl->dual_edge_irqs))
 		msm_gpio_update_dual_edge_pos(pctrl, g, d);
 
-	spin_unlock_irqrestore(&pctrl->lock, flags);
+	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 }
 
 static int msm_gpio_irq_set_type(struct irq_data *d, unsigned int type)
@@ -681,7 +679,7 @@ static int msm_gpio_irq_set_type(struct irq_data *d, unsigned int type)
 
 	g = &pctrl->soc->groups[d->hwirq];
 
-	spin_lock_irqsave(&pctrl->lock, flags);
+	raw_spin_lock_irqsave(&pctrl->lock, flags);
 
 	/*
 	 * For hw without possibility of detecting both edges
@@ -755,7 +753,7 @@ static int msm_gpio_irq_set_type(struct irq_data *d, unsigned int type)
 	if (test_bit(d->hwirq, pctrl->dual_edge_irqs))
 		msm_gpio_update_dual_edge_pos(pctrl, g, d);
 
-	spin_unlock_irqrestore(&pctrl->lock, flags);
+	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 
 	if (type & (IRQ_TYPE_LEVEL_LOW | IRQ_TYPE_LEVEL_HIGH))
 		__irq_set_handler_locked(d->irq, handle_level_irq);
@@ -773,11 +771,11 @@ static int msm_gpio_irq_set_wake(struct irq_data *d, unsigned int on)
 	struct msm_pinctrl *pctrl = to_msm_pinctrl(gc);
 	unsigned long flags;
 
-	spin_lock_irqsave(&pctrl->lock, flags);
+	raw_spin_lock_irqsave(&pctrl->lock, flags);
 
 	irq_set_irq_wake(pctrl->irq, on);
 
-	spin_unlock_irqrestore(&pctrl->lock, flags);
+	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 
 	if (pctrl->irq_chip_extn->irq_set_wake)
 		pctrl->irq_chip_extn->irq_set_wake(d, on);
@@ -993,7 +991,7 @@ int msm_pinctrl_probe(struct platform_device *pdev,
 	pctrl->soc = soc_data;
 	pctrl->chip = msm_gpio_template;
 
-	spin_lock_init(&pctrl->lock);
+	raw_spin_lock_init(&pctrl->lock);
 
 	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	pctrl->regs = devm_ioremap_resource(&pdev->dev, res);
diff --git a/kernel/msm-3.18/drivers/scsi/fcoe/fcoe.c b/kernel/msm-3.18/drivers/scsi/fcoe/fcoe.c
index 4a8ac7d8c..8933c02b6 100644
--- a/kernel/msm-3.18/drivers/scsi/fcoe/fcoe.c
+++ b/kernel/msm-3.18/drivers/scsi/fcoe/fcoe.c
@@ -1286,7 +1286,7 @@ static void fcoe_percpu_thread_destroy(unsigned int cpu)
 	struct sk_buff *skb;
 #ifdef CONFIG_SMP
 	struct fcoe_percpu_s *p0;
-	unsigned targ_cpu = get_cpu();
+	unsigned targ_cpu = get_cpu_light();
 #endif /* CONFIG_SMP */
 
 	FCOE_DBG("Destroying receive thread for CPU %d\n", cpu);
@@ -1342,7 +1342,7 @@ static void fcoe_percpu_thread_destroy(unsigned int cpu)
 			kfree_skb(skb);
 		spin_unlock_bh(&p->fcoe_rx_list.lock);
 	}
-	put_cpu();
+	put_cpu_light();
 #else
 	/*
 	 * This a non-SMP scenario where the singular Rx thread is
@@ -1566,11 +1566,11 @@ err2:
 static int fcoe_alloc_paged_crc_eof(struct sk_buff *skb, int tlen)
 {
 	struct fcoe_percpu_s *fps;
-	int rc;
+	int rc, cpu = get_cpu_light();
 
-	fps = &get_cpu_var(fcoe_percpu);
+	fps = &per_cpu(fcoe_percpu, cpu);
 	rc = fcoe_get_paged_crc_eof(skb, tlen, fps);
-	put_cpu_var(fcoe_percpu);
+	put_cpu_light();
 
 	return rc;
 }
@@ -1768,11 +1768,11 @@ static inline int fcoe_filter_frames(struct fc_lport *lport,
 		return 0;
 	}
 
-	stats = per_cpu_ptr(lport->stats, get_cpu());
+	stats = per_cpu_ptr(lport->stats, get_cpu_light());
 	stats->InvalidCRCCount++;
 	if (stats->InvalidCRCCount < 5)
 		printk(KERN_WARNING "fcoe: dropping frame with CRC error\n");
-	put_cpu();
+	put_cpu_light();
 	return -EINVAL;
 }
 
@@ -1816,7 +1816,7 @@ static void fcoe_recv_frame(struct sk_buff *skb)
 	 */
 	hp = (struct fcoe_hdr *) skb_network_header(skb);
 
-	stats = per_cpu_ptr(lport->stats, get_cpu());
+	stats = per_cpu_ptr(lport->stats, get_cpu_light());
 	if (unlikely(FC_FCOE_DECAPS_VER(hp) != FC_FCOE_VER)) {
 		if (stats->ErrorFrames < 5)
 			printk(KERN_WARNING "fcoe: FCoE version "
@@ -1848,13 +1848,13 @@ static void fcoe_recv_frame(struct sk_buff *skb)
 		goto drop;
 
 	if (!fcoe_filter_frames(lport, fp)) {
-		put_cpu();
+		put_cpu_light();
 		fc_exch_recv(lport, fp);
 		return;
 	}
 drop:
 	stats->ErrorFrames++;
-	put_cpu();
+	put_cpu_light();
 	kfree_skb(skb);
 }
 
diff --git a/kernel/msm-3.18/drivers/scsi/fcoe/fcoe_ctlr.c b/kernel/msm-3.18/drivers/scsi/fcoe/fcoe_ctlr.c
index 34a1b1f33..d91131210 100644
--- a/kernel/msm-3.18/drivers/scsi/fcoe/fcoe_ctlr.c
+++ b/kernel/msm-3.18/drivers/scsi/fcoe/fcoe_ctlr.c
@@ -831,7 +831,7 @@ static unsigned long fcoe_ctlr_age_fcfs(struct fcoe_ctlr *fip)
 
 	INIT_LIST_HEAD(&del_list);
 
-	stats = per_cpu_ptr(fip->lp->stats, get_cpu());
+	stats = per_cpu_ptr(fip->lp->stats, get_cpu_light());
 
 	list_for_each_entry_safe(fcf, next, &fip->fcfs, list) {
 		deadline = fcf->time + fcf->fka_period + fcf->fka_period / 2;
@@ -867,7 +867,7 @@ static unsigned long fcoe_ctlr_age_fcfs(struct fcoe_ctlr *fip)
 				sel_time = fcf->time;
 		}
 	}
-	put_cpu();
+	put_cpu_light();
 
 	list_for_each_entry_safe(fcf, next, &del_list, list) {
 		/* Removes fcf from current list */
diff --git a/kernel/msm-3.18/drivers/scsi/libfc/fc_exch.c b/kernel/msm-3.18/drivers/scsi/libfc/fc_exch.c
index 30f9ef0c0..6c686bc01 100644
--- a/kernel/msm-3.18/drivers/scsi/libfc/fc_exch.c
+++ b/kernel/msm-3.18/drivers/scsi/libfc/fc_exch.c
@@ -814,10 +814,10 @@ static struct fc_exch *fc_exch_em_alloc(struct fc_lport *lport,
 	}
 	memset(ep, 0, sizeof(*ep));
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	pool = per_cpu_ptr(mp->pool, cpu);
 	spin_lock_bh(&pool->lock);
-	put_cpu();
+	put_cpu_light();
 
 	/* peek cache of free slot */
 	if (pool->left != FC_XID_UNKNOWN) {
diff --git a/kernel/msm-3.18/drivers/scsi/libsas/sas_ata.c b/kernel/msm-3.18/drivers/scsi/libsas/sas_ata.c
index 3f0c3e0b5..352934536 100644
--- a/kernel/msm-3.18/drivers/scsi/libsas/sas_ata.c
+++ b/kernel/msm-3.18/drivers/scsi/libsas/sas_ata.c
@@ -191,7 +191,7 @@ static unsigned int sas_ata_qc_issue(struct ata_queued_cmd *qc)
 	/* TODO: audit callers to ensure they are ready for qc_issue to
 	 * unconditionally re-enable interrupts
 	 */
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	spin_unlock(ap->lock);
 
 	/* If the device fell off, no sense in issuing commands */
@@ -261,7 +261,7 @@ static unsigned int sas_ata_qc_issue(struct ata_queued_cmd *qc)
 
  out:
 	spin_lock(ap->lock);
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 	return ret;
 }
 
diff --git a/kernel/msm-3.18/drivers/scsi/qla2xxx/qla_inline.h b/kernel/msm-3.18/drivers/scsi/qla2xxx/qla_inline.h
index fee9eb7c8..b42d4adc4 100644
--- a/kernel/msm-3.18/drivers/scsi/qla2xxx/qla_inline.h
+++ b/kernel/msm-3.18/drivers/scsi/qla2xxx/qla_inline.h
@@ -59,12 +59,12 @@ qla2x00_poll(struct rsp_que *rsp)
 {
 	unsigned long flags;
 	struct qla_hw_data *ha = rsp->hw;
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	if (IS_P3P_TYPE(ha))
 		qla82xx_poll(0, rsp);
 	else
 		ha->isp_ops->intr_handler(0, rsp);
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 }
 
 static inline uint8_t *
diff --git a/kernel/msm-3.18/drivers/thermal/x86_pkg_temp_thermal.c b/kernel/msm-3.18/drivers/thermal/x86_pkg_temp_thermal.c
index 9ea3d9d49..e3c2663e0 100644
--- a/kernel/msm-3.18/drivers/thermal/x86_pkg_temp_thermal.c
+++ b/kernel/msm-3.18/drivers/thermal/x86_pkg_temp_thermal.c
@@ -29,6 +29,7 @@
 #include <linux/pm.h>
 #include <linux/thermal.h>
 #include <linux/debugfs.h>
+#include <linux/work-simple.h>
 #include <asm/cpu_device_id.h>
 #include <asm/mce.h>
 
@@ -352,7 +353,7 @@ static void pkg_temp_thermal_threshold_work_fn(struct work_struct *work)
 	}
 }
 
-static int pkg_temp_thermal_platform_thermal_notify(__u64 msr_val)
+static void platform_thermal_notify_work(struct swork_event *event)
 {
 	unsigned long flags;
 	int cpu = smp_processor_id();
@@ -369,7 +370,7 @@ static int pkg_temp_thermal_platform_thermal_notify(__u64 msr_val)
 			pkg_work_scheduled[phy_id]) {
 		disable_pkg_thres_interrupt();
 		spin_unlock_irqrestore(&pkg_work_lock, flags);
-		return -EINVAL;
+		return;
 	}
 	pkg_work_scheduled[phy_id] = 1;
 	spin_unlock_irqrestore(&pkg_work_lock, flags);
@@ -378,9 +379,48 @@ static int pkg_temp_thermal_platform_thermal_notify(__u64 msr_val)
 	schedule_delayed_work_on(cpu,
 				&per_cpu(pkg_temp_thermal_threshold_work, cpu),
 				msecs_to_jiffies(notify_delay_ms));
+}
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+static struct swork_event notify_work;
+
+static int thermal_notify_work_init(void)
+{
+	int err;
+
+	err = swork_get();
+	if (err)
+		return err;
+
+	INIT_SWORK(&notify_work, platform_thermal_notify_work);
 	return 0;
 }
 
+static void thermal_notify_work_cleanup(void)
+{
+	swork_put();
+}
+
+static int pkg_temp_thermal_platform_thermal_notify(__u64 msr_val)
+{
+	swork_queue(&notify_work);
+	return 0;
+}
+
+#else  /* !CONFIG_PREEMPT_RT_FULL */
+
+static int thermal_notify_work_init(void) { return 0; }
+
+static int thermal_notify_work_cleanup(void) {  }
+
+static int pkg_temp_thermal_platform_thermal_notify(__u64 msr_val)
+{
+	platform_thermal_notify_work(NULL);
+
+	return 0;
+}
+#endif /* CONFIG_PREEMPT_RT_FULL */
+
 static int find_siblings_cpu(int cpu)
 {
 	int i;
@@ -584,6 +624,9 @@ static int __init pkg_temp_thermal_init(void)
 	if (!x86_match_cpu(pkg_temp_thermal_ids))
 		return -ENODEV;
 
+	if (!thermal_notify_work_init())
+		return -ENODEV;
+
 	spin_lock_init(&pkg_work_lock);
 	platform_thermal_package_notify =
 			pkg_temp_thermal_platform_thermal_notify;
@@ -608,7 +651,7 @@ err_ret:
 	kfree(pkg_work_scheduled);
 	platform_thermal_package_notify = NULL;
 	platform_thermal_package_rate_control = NULL;
-
+	thermal_notify_work_cleanup();
 	return -ENODEV;
 }
 
@@ -633,6 +676,7 @@ static void __exit pkg_temp_thermal_exit(void)
 	mutex_unlock(&phy_dev_list_mutex);
 	platform_thermal_package_notify = NULL;
 	platform_thermal_package_rate_control = NULL;
+	thermal_notify_work_cleanup();
 	for_each_online_cpu(i)
 		cancel_delayed_work_sync(
 			&per_cpu(pkg_temp_thermal_threshold_work, i));
diff --git a/kernel/msm-3.18/drivers/tty/serial/8250/8250_core.c b/kernel/msm-3.18/drivers/tty/serial/8250/8250_core.c
index 04da6f0e3..37ea80c69 100644
--- a/kernel/msm-3.18/drivers/tty/serial/8250/8250_core.c
+++ b/kernel/msm-3.18/drivers/tty/serial/8250/8250_core.c
@@ -37,6 +37,7 @@
 #include <linux/nmi.h>
 #include <linux/mutex.h>
 #include <linux/slab.h>
+#include <linux/kdb.h>
 #include <linux/uaccess.h>
 #include <linux/pm_runtime.h>
 #ifdef CONFIG_SPARC
@@ -81,7 +82,16 @@ static unsigned int skip_txen_test; /* force skip of txen test at init time */
 #define DEBUG_INTR(fmt...)	do { } while (0)
 #endif
 
-#define PASS_LIMIT	512
+/*
+ * On -rt we can have a more delays, and legitimately
+ * so - so don't drop work spuriously and spam the
+ * syslog:
+ */
+#ifdef CONFIG_PREEMPT_RT_FULL
+# define PASS_LIMIT	1000000
+#else
+# define PASS_LIMIT	512
+#endif
 
 #define BOTH_EMPTY 	(UART_LSR_TEMT | UART_LSR_THRE)
 
@@ -3191,7 +3201,7 @@ serial8250_console_write(struct console *co, const char *s, unsigned int count)
 
 	serial8250_rpm_get(up);
 
-	if (port->sysrq || oops_in_progress)
+	if (port->sysrq || oops_in_progress || in_kdb_printk())
 		locked = spin_trylock_irqsave(&port->lock, flags);
 	else
 		spin_lock_irqsave(&port->lock, flags);
diff --git a/kernel/msm-3.18/drivers/tty/serial/amba-pl011.c b/kernel/msm-3.18/drivers/tty/serial/amba-pl011.c
index 83c8f721c..a35541c0d 100644
--- a/kernel/msm-3.18/drivers/tty/serial/amba-pl011.c
+++ b/kernel/msm-3.18/drivers/tty/serial/amba-pl011.c
@@ -1937,13 +1937,19 @@ pl011_console_write(struct console *co, const char *s, unsigned int count)
 
 	clk_enable(uap->clk);
 
-	local_irq_save(flags);
+	/*
+	 * local_irq_save(flags);
+	 *
+	 * This local_irq_save() is nonsense. If we come in via sysrq
+	 * handling then interrupts are already disabled. Aside of
+	 * that the port.sysrq check is racy on SMP regardless.
+	*/
 	if (uap->port.sysrq)
 		locked = 0;
 	else if (oops_in_progress)
-		locked = spin_trylock(&uap->port.lock);
+		locked = spin_trylock_irqsave(&uap->port.lock, flags);
 	else
-		spin_lock(&uap->port.lock);
+		spin_lock_irqsave(&uap->port.lock, flags);
 
 	/*
 	 *	First save the CR then disable the interrupts
@@ -1965,8 +1971,7 @@ pl011_console_write(struct console *co, const char *s, unsigned int count)
 	writew(old_cr, uap->port.membase + UART011_CR);
 
 	if (locked)
-		spin_unlock(&uap->port.lock);
-	local_irq_restore(flags);
+		spin_unlock_irqrestore(&uap->port.lock, flags);
 
 	clk_disable(uap->clk);
 }
diff --git a/kernel/msm-3.18/drivers/tty/serial/omap-serial.c b/kernel/msm-3.18/drivers/tty/serial/omap-serial.c
index ef46bbb10..10c1c2608 100644
--- a/kernel/msm-3.18/drivers/tty/serial/omap-serial.c
+++ b/kernel/msm-3.18/drivers/tty/serial/omap-serial.c
@@ -1270,13 +1270,10 @@ serial_omap_console_write(struct console *co, const char *s,
 
 	pm_runtime_get_sync(up->dev);
 
-	local_irq_save(flags);
-	if (up->port.sysrq)
-		locked = 0;
-	else if (oops_in_progress)
-		locked = spin_trylock(&up->port.lock);
+	if (up->port.sysrq || oops_in_progress)
+		locked = spin_trylock_irqsave(&up->port.lock, flags);
 	else
-		spin_lock(&up->port.lock);
+		spin_lock_irqsave(&up->port.lock, flags);
 
 	/*
 	 * First save the IER then disable the interrupts
@@ -1305,8 +1302,7 @@ serial_omap_console_write(struct console *co, const char *s,
 	pm_runtime_mark_last_busy(up->dev);
 	pm_runtime_put_autosuspend(up->dev);
 	if (locked)
-		spin_unlock(&up->port.lock);
-	local_irq_restore(flags);
+		spin_unlock_irqrestore(&up->port.lock, flags);
 }
 
 static int __init
diff --git a/kernel/msm-3.18/drivers/usb/core/hcd.c b/kernel/msm-3.18/drivers/usb/core/hcd.c
index c63874e4a..adc9925a5 100644
--- a/kernel/msm-3.18/drivers/usb/core/hcd.c
+++ b/kernel/msm-3.18/drivers/usb/core/hcd.c
@@ -1687,9 +1687,9 @@ static void __usb_hcd_giveback_urb(struct urb *urb)
 	 * and no one may trigger the above deadlock situation when
 	 * running complete() in tasklet.
 	 */
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	urb->complete(urb);
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 
 	usb_anchor_resume_wakeups(anchor);
 	atomic_dec(&urb->use_count);
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_fs.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_fs.c
index e05444c1c..7ff27d471 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_fs.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_fs.c
@@ -1561,7 +1561,7 @@ static void ffs_data_put(struct ffs_data *ffs)
 		pr_info("%s(): freeing\n", __func__);
 		ffs_data_clear(ffs);
 		BUG_ON(waitqueue_active(&ffs->ev.waitq) ||
-		       waitqueue_active(&ffs->ep0req_completion.wait));
+		       swaitqueue_active(&ffs->ep0req_completion.wait));
 		kfree(ffs->dev_name);
 		kfree(ffs);
 	}
diff --git a/kernel/msm-3.18/drivers/usb/gadget/legacy/inode.c b/kernel/msm-3.18/drivers/usb/gadget/legacy/inode.c
index db2becd31..d74f46063 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/legacy/inode.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/legacy/inode.c
@@ -339,7 +339,7 @@ ep_io (struct ep_data *epdata, void *buf, unsigned len)
 	spin_unlock_irq (&epdata->dev->lock);
 
 	if (likely (value == 0)) {
-		value = wait_event_interruptible (done.wait, done.done);
+		value = swait_event_interruptible (done.wait, done.done);
 		if (value != 0) {
 			spin_lock_irq (&epdata->dev->lock);
 			if (likely (epdata->ep != NULL)) {
@@ -348,7 +348,7 @@ ep_io (struct ep_data *epdata, void *buf, unsigned len)
 				usb_ep_dequeue (epdata->ep, epdata->req);
 				spin_unlock_irq (&epdata->dev->lock);
 
-				wait_event (done.wait, done.done);
+				swait_event (done.wait, done.done);
 				if (epdata->status == -ECONNRESET)
 					epdata->status = -EINTR;
 			} else {
diff --git a/kernel/msm-3.18/fs/aio.c b/kernel/msm-3.18/fs/aio.c
index 3eec98470..a820b5e69 100644
--- a/kernel/msm-3.18/fs/aio.c
+++ b/kernel/msm-3.18/fs/aio.c
@@ -40,6 +40,7 @@
 #include <linux/ramfs.h>
 #include <linux/percpu-refcount.h>
 #include <linux/mount.h>
+#include <linux/work-simple.h>
 
 #include <asm/kmap_types.h>
 #include <asm/uaccess.h>
@@ -110,7 +111,7 @@ struct kioctx {
 	struct page		**ring_pages;
 	long			nr_pages;
 
-	struct work_struct	free_work;
+	struct swork_event	free_work;
 
 	/*
 	 * signals when all in-flight requests are done
@@ -226,6 +227,7 @@ static int __init aio_setup(void)
 		.mount		= aio_mount,
 		.kill_sb	= kill_anon_super,
 	};
+	BUG_ON(swork_get());
 	aio_mnt = kern_mount(&aio_fs);
 	if (IS_ERR(aio_mnt))
 		panic("Failed to create aio fs mount.");
@@ -506,9 +508,9 @@ static int kiocb_cancel(struct kiocb *kiocb)
 	return cancel(kiocb);
 }
 
-static void free_ioctx(struct work_struct *work)
+static void free_ioctx(struct swork_event *sev)
 {
-	struct kioctx *ctx = container_of(work, struct kioctx, free_work);
+	struct kioctx *ctx = container_of(sev, struct kioctx, free_work);
 
 	pr_debug("freeing %p\n", ctx);
 
@@ -527,8 +529,8 @@ static void free_ioctx_reqs(struct percpu_ref *ref)
 	if (ctx->requests_done)
 		complete(ctx->requests_done);
 
-	INIT_WORK(&ctx->free_work, free_ioctx);
-	schedule_work(&ctx->free_work);
+	INIT_SWORK(&ctx->free_work, free_ioctx);
+	swork_queue(&ctx->free_work);
 }
 
 /*
@@ -536,9 +538,9 @@ static void free_ioctx_reqs(struct percpu_ref *ref)
  * and ctx->users has dropped to 0, so we know no more kiocbs can be submitted -
  * now it's safe to cancel any that need to be.
  */
-static void free_ioctx_users(struct percpu_ref *ref)
+static void free_ioctx_users_work(struct swork_event *sev)
 {
-	struct kioctx *ctx = container_of(ref, struct kioctx, users);
+	struct kioctx *ctx = container_of(sev, struct kioctx, free_work);
 	struct kiocb *req;
 
 	spin_lock_irq(&ctx->ctx_lock);
@@ -557,6 +559,14 @@ static void free_ioctx_users(struct percpu_ref *ref)
 	percpu_ref_put(&ctx->reqs);
 }
 
+static void free_ioctx_users(struct percpu_ref *ref)
+{
+	struct kioctx *ctx = container_of(ref, struct kioctx, users);
+
+	INIT_SWORK(&ctx->free_work, free_ioctx_users_work);
+	swork_queue(&ctx->free_work);
+}
+
 static int ioctx_add_table(struct kioctx *ctx, struct mm_struct *mm)
 {
 	unsigned i, new_nr;
diff --git a/kernel/msm-3.18/fs/autofs4/autofs_i.h b/kernel/msm-3.18/fs/autofs4/autofs_i.h
index cecd252a3..cd334f184 100644
--- a/kernel/msm-3.18/fs/autofs4/autofs_i.h
+++ b/kernel/msm-3.18/fs/autofs4/autofs_i.h
@@ -34,6 +34,7 @@
 #include <linux/sched.h>
 #include <linux/mount.h>
 #include <linux/namei.h>
+#include <linux/delay.h>
 #include <asm/current.h>
 #include <asm/uaccess.h>
 
diff --git a/kernel/msm-3.18/fs/autofs4/expire.c b/kernel/msm-3.18/fs/autofs4/expire.c
index 513b8e5d1..3bd55ae47 100644
--- a/kernel/msm-3.18/fs/autofs4/expire.c
+++ b/kernel/msm-3.18/fs/autofs4/expire.c
@@ -151,7 +151,7 @@ again:
 			parent = p->d_parent;
 			if (!spin_trylock(&parent->d_lock)) {
 				spin_unlock(&p->d_lock);
-				cpu_relax();
+				cpu_chill();
 				goto relock;
 			}
 			spin_unlock(&p->d_lock);
diff --git a/kernel/msm-3.18/fs/buffer.c b/kernel/msm-3.18/fs/buffer.c
index d2b5be9f4..f88e562ec 100644
--- a/kernel/msm-3.18/fs/buffer.c
+++ b/kernel/msm-3.18/fs/buffer.c
@@ -301,8 +301,7 @@ static void end_buffer_async_read(struct buffer_head *bh, int uptodate)
 	 * decide that the page is now completely done.
 	 */
 	first = page_buffers(page);
-	local_irq_save(flags);
-	bit_spin_lock(BH_Uptodate_Lock, &first->b_state);
+	flags = bh_uptodate_lock_irqsave(first);
 	clear_buffer_async_read(bh);
 	unlock_buffer(bh);
 	tmp = bh;
@@ -315,8 +314,7 @@ static void end_buffer_async_read(struct buffer_head *bh, int uptodate)
 		}
 		tmp = tmp->b_this_page;
 	} while (tmp != bh);
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
+	bh_uptodate_unlock_irqrestore(first, flags);
 
 	/*
 	 * If none of the buffers had errors and they are all
@@ -328,9 +326,7 @@ static void end_buffer_async_read(struct buffer_head *bh, int uptodate)
 	return;
 
 still_busy:
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
-	return;
+	bh_uptodate_unlock_irqrestore(first, flags);
 }
 
 /*
@@ -358,8 +354,7 @@ void end_buffer_async_write(struct buffer_head *bh, int uptodate)
 	}
 
 	first = page_buffers(page);
-	local_irq_save(flags);
-	bit_spin_lock(BH_Uptodate_Lock, &first->b_state);
+	flags = bh_uptodate_lock_irqsave(first);
 
 	clear_buffer_async_write(bh);
 	unlock_buffer(bh);
@@ -371,15 +366,12 @@ void end_buffer_async_write(struct buffer_head *bh, int uptodate)
 		}
 		tmp = tmp->b_this_page;
 	}
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
+	bh_uptodate_unlock_irqrestore(first, flags);
 	end_page_writeback(page);
 	return;
 
 still_busy:
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
-	return;
+	bh_uptodate_unlock_irqrestore(first, flags);
 }
 EXPORT_SYMBOL(end_buffer_async_write);
 
@@ -3381,6 +3373,7 @@ struct buffer_head *alloc_buffer_head(gfp_t gfp_flags)
 	struct buffer_head *ret = kmem_cache_zalloc(bh_cachep, gfp_flags);
 	if (ret) {
 		INIT_LIST_HEAD(&ret->b_assoc_buffers);
+		buffer_head_init_locks(ret);
 		preempt_disable();
 		__this_cpu_inc(bh_accounting.nr);
 		recalc_bh_state();
diff --git a/kernel/msm-3.18/fs/dcache.c b/kernel/msm-3.18/fs/dcache.c
index 08bb8b0c2..b894f1f29 100644
--- a/kernel/msm-3.18/fs/dcache.c
+++ b/kernel/msm-3.18/fs/dcache.c
@@ -19,6 +19,7 @@
 #include <linux/mm.h>
 #include <linux/fs.h>
 #include <linux/fsnotify.h>
+#include <linux/delay.h>
 #include <linux/slab.h>
 #include <linux/init.h>
 #include <linux/hash.h>
@@ -723,6 +724,8 @@ static inline bool fast_dput(struct dentry *dentry)
  */
 void dput(struct dentry *dentry)
 {
+	struct dentry *parent;
+
 	if (unlikely(!dentry))
 		return;
 
@@ -759,9 +762,18 @@ repeat:
 	return;
 
 kill_it:
-	dentry = dentry_kill(dentry);
-	if (dentry) {
-		cond_resched();
+	parent = dentry_kill(dentry);
+	if (parent) {
+		int r;
+
+		if (parent == dentry) {
+			/* the task with the highest priority won't schedule */
+			r = cond_resched();
+			if (!r)
+				cpu_chill();
+		} else {
+			dentry = parent;
+		}
 		goto repeat;
 	}
 }
@@ -2410,7 +2422,7 @@ again:
 	if (dentry->d_lockref.count == 1) {
 		if (!spin_trylock(&inode->i_lock)) {
 			spin_unlock(&dentry->d_lock);
-			cpu_relax();
+			cpu_chill();
 			goto again;
 		}
 		dentry->d_flags &= ~DCACHE_CANT_MOUNT;
diff --git a/kernel/msm-3.18/fs/eventpoll.c b/kernel/msm-3.18/fs/eventpoll.c
index 1c6bba7e6..317a60768 100644
--- a/kernel/msm-3.18/fs/eventpoll.c
+++ b/kernel/msm-3.18/fs/eventpoll.c
@@ -506,12 +506,12 @@ static int ep_poll_wakeup_proc(void *priv, void *cookie, int call_nests)
  */
 static void ep_poll_safewake(wait_queue_head_t *wq)
 {
-	int this_cpu = get_cpu();
+	int this_cpu = get_cpu_light();
 
 	ep_call_nested(&poll_safewake_ncalls, EP_MAX_NESTS,
 		       ep_poll_wakeup_proc, NULL, wq, (void *) (long) this_cpu);
 
-	put_cpu();
+	put_cpu_light();
 }
 
 static void ep_remove_wait_queue(struct eppoll_entry *pwq)
diff --git a/kernel/msm-3.18/fs/exec.c b/kernel/msm-3.18/fs/exec.c
index a552a192d..13120023a 100644
--- a/kernel/msm-3.18/fs/exec.c
+++ b/kernel/msm-3.18/fs/exec.c
@@ -865,12 +865,14 @@ static int exec_mmap(struct mm_struct *mm)
 		}
 	}
 	task_lock(tsk);
+	preempt_disable_rt();
 	active_mm = tsk->active_mm;
 	tsk->mm = mm;
 	tsk->active_mm = mm;
 	activate_mm(active_mm, mm);
 	tsk->mm->vmacache_seqnum = 0;
 	vmacache_flush(tsk);
+	preempt_enable_rt();
 	task_unlock(tsk);
 	if (old_mm) {
 		up_read(&old_mm->mmap_sem);
diff --git a/kernel/msm-3.18/fs/f2fs/f2fs.h b/kernel/msm-3.18/fs/f2fs/f2fs.h
index 7db3d0201..5bb02f8e1 100644
--- a/kernel/msm-3.18/fs/f2fs/f2fs.h
+++ b/kernel/msm-3.18/fs/f2fs/f2fs.h
@@ -19,56 +19,19 @@
 #include <linux/magic.h>
 #include <linux/kobject.h>
 #include <linux/sched.h>
-#include <linux/vmalloc.h>
-#include <linux/bio.h>
-#include <linux/blkdev.h>
-#include <linux/quotaops.h>
-#ifdef CONFIG_F2FS_FS_ENCRYPTION
-#include <linux/fscrypt_supp.h>
-#else
-#include <linux/fscrypt_notsupp.h>
-#endif
-#include <crypto/hash.h>
-#include <linux/writeback.h>
 
 #ifdef CONFIG_F2FS_CHECK_FS
 #define f2fs_bug_on(sbi, condition)	BUG_ON(condition)
-#define f2fs_down_write(x, y)	down_write_nest_lock(x, y)
 #else
 #define f2fs_bug_on(sbi, condition)					\
 	do {								\
 		if (unlikely(condition)) {				\
 			WARN_ON(1);					\
-			set_sbi_flag(sbi, SBI_NEED_FSCK);		\
+			sbi->need_fsck = true;				\
 		}							\
 	} while (0)
 #endif
 
-#ifdef CONFIG_F2FS_FAULT_INJECTION
-enum {
-	FAULT_KMALLOC,
-	FAULT_PAGE_ALLOC,
-	FAULT_ALLOC_NID,
-	FAULT_ORPHAN,
-	FAULT_BLOCK,
-	FAULT_DIR_DEPTH,
-	FAULT_EVICT_INODE,
-	FAULT_TRUNCATE,
-	FAULT_IO,
-	FAULT_CHECKPOINT,
-	FAULT_MAX,
-};
-
-struct f2fs_fault_info {
-	atomic_t inject_ops;
-	unsigned int inject_rate;
-	unsigned int inject_type;
-};
-
-extern char *fault_name[FAULT_MAX];
-#define IS_FAULT_SET(fi, type) ((fi)->inject_type & (1 << (type)))
-#endif
-
 /*
  * For mount options
  */
@@ -81,22 +44,12 @@ extern char *fault_name[FAULT_MAX];
 #define F2FS_MOUNT_DISABLE_EXT_IDENTIFY	0x00000040
 #define F2FS_MOUNT_INLINE_XATTR		0x00000080
 #define F2FS_MOUNT_INLINE_DATA		0x00000100
-#define F2FS_MOUNT_INLINE_DENTRY	0x00000200
-#define F2FS_MOUNT_FLUSH_MERGE		0x00000400
-#define F2FS_MOUNT_NOBARRIER		0x00000800
-#define F2FS_MOUNT_FASTBOOT		0x00001000
-#define F2FS_MOUNT_EXTENT_CACHE		0x00002000
-#define F2FS_MOUNT_FORCE_FG_GC		0x00004000
-#define F2FS_MOUNT_DATA_FLUSH		0x00008000
-#define F2FS_MOUNT_FAULT_INJECTION	0x00010000
-#define F2FS_MOUNT_ADAPTIVE		0x00020000
-#define F2FS_MOUNT_LFS			0x00040000
-#define F2FS_MOUNT_USRQUOTA		0x00080000
-#define F2FS_MOUNT_GRPQUOTA		0x00100000
-
-#define clear_opt(sbi, option)	((sbi)->mount_opt.opt &= ~F2FS_MOUNT_##option)
-#define set_opt(sbi, option)	((sbi)->mount_opt.opt |= F2FS_MOUNT_##option)
-#define test_opt(sbi, option)	((sbi)->mount_opt.opt & F2FS_MOUNT_##option)
+#define F2FS_MOUNT_FLUSH_MERGE		0x00000200
+#define F2FS_MOUNT_NOBARRIER		0x00000400
+
+#define clear_opt(sbi, option)	(sbi->mount_opt.opt &= ~F2FS_MOUNT_##option)
+#define set_opt(sbi, option)	(sbi->mount_opt.opt |= F2FS_MOUNT_##option)
+#define test_opt(sbi, option)	(sbi->mount_opt.opt & F2FS_MOUNT_##option)
 
 #define ver_after(a, b)	(typecheck(unsigned long long, a) &&		\
 		typecheck(unsigned long long, b) &&			\
@@ -112,112 +65,25 @@ struct f2fs_mount_info {
 	unsigned int	opt;
 };
 
-#define F2FS_FEATURE_ENCRYPT	0x0001
-#define F2FS_FEATURE_BLKZONED	0x0002
-#define F2FS_FEATURE_ATOMIC_WRITE 0x0004
-
-#define F2FS_HAS_FEATURE(sb, mask)					\
-	((F2FS_SB(sb)->raw_super->feature & cpu_to_le32(mask)) != 0)
-#define F2FS_SET_FEATURE(sb, mask)					\
-	(F2FS_SB(sb)->raw_super->feature |= cpu_to_le32(mask))
-#define F2FS_CLEAR_FEATURE(sb, mask)					\
-	(F2FS_SB(sb)->raw_super->feature &= ~cpu_to_le32(mask))
-
-/* bio stuffs */
-#define REQ_OP_READ	READ
-#define REQ_OP_WRITE	WRITE
-#define bio_op(bio)	((bio)->bi_rw & 1)
-
-static inline void bio_set_op_attrs(struct bio *bio, unsigned op,
-		unsigned op_flags)
-{
-	bio->bi_rw = op | op_flags;
-}
-
-static inline int wbc_to_write_flags(struct writeback_control *wbc)
-{
-	if (wbc->sync_mode == WB_SYNC_ALL)
-		return REQ_SYNC;
-	return 0;
-}
-
-static inline void inode_lock(struct inode *inode)
-{
-	mutex_lock(&inode->i_mutex);
-}
-
-static inline int inode_trylock(struct inode *inode)
-{
-	return mutex_trylock(&inode->i_mutex);
-}
-
-static inline void inode_unlock(struct inode *inode)
-{
-	mutex_unlock(&inode->i_mutex);
-}
-
-#define rb_entry_safe(ptr, type, member) \
-	({ typeof(ptr) ____ptr = (ptr); \
-	   ____ptr ? rb_entry(____ptr, type, member) : NULL; \
-	})
-
-#define list_last_entry(ptr, type, member) \
-	list_entry((ptr)->prev, type, member)
-
-#define list_first_entry(ptr, type, member) \
-	list_entry((ptr)->next, type, member)
+#define CRCPOLY_LE 0xedb88320
 
-/**
- * wq_has_sleeper - check if there are any waiting processes
- * @wq: wait queue head
- *
- * Returns true if wq has waiting processes
- *
- * Please refer to the comment for waitqueue_active.
- */
-static inline bool wq_has_sleeper(wait_queue_head_t *wq)
+static inline __u32 f2fs_crc32(void *buf, size_t len)
 {
-	/*
-	 * We need to be sure we are in sync with the
-	 * add_wait_queue modifications to the wait queue.
-	 *
-	 * This memory barrier should be paired with one on the
-	 * waiting side.
-	 */
-	smp_mb();
-	return waitqueue_active(wq);
-}
+	unsigned char *p = (unsigned char *)buf;
+	__u32 crc = F2FS_SUPER_MAGIC;
+	int i;
 
-static inline struct dentry *file_dentry(const struct file *file)
-{
-	return file->f_path.dentry;
+	while (len--) {
+		crc ^= *p++;
+		for (i = 0; i < 8; i++)
+			crc = (crc >> 1) ^ ((crc & 1) ? CRCPOLY_LE : 0);
+	}
+	return crc;
 }
 
-static inline void inode_nohighmem(struct inode *inode)
+static inline bool f2fs_crc_valid(__u32 blk_crc, void *buf, size_t buf_size)
 {
-	mapping_set_gfp_mask(inode->i_mapping, GFP_USER);
-}
-
-/**
- * current_time - Return FS time
- * @inode: inode.
- *
- * Return the current time truncated to the time granularity supported by
- * the fs.
- *
- * Note that inode and inode->sb cannot be NULL.
- * Otherwise, the function warns and returns time without truncation.
- */
-static inline struct timespec current_time(struct inode *inode)
-{
-	struct timespec now = current_kernel_time();
-
-	if (unlikely(!inode->i_sb)) {
-		WARN(1, "current_time() called with uninitialized super_block in the inode");
-		return now;
-	}
-
-	return timespec_trunc(now, inode->i_sb->s_time_gran);
+	return f2fs_crc32(buf, buf_size) == blk_crc;
 }
 
 /*
@@ -228,22 +94,11 @@ enum {
 	SIT_BITMAP
 };
 
-#define	CP_UMOUNT	0x00000001
-#define	CP_FASTBOOT	0x00000002
-#define	CP_SYNC		0x00000004
-#define	CP_RECOVERY	0x00000008
-#define	CP_DISCARD	0x00000010
-#define CP_TRIMMED	0x00000020
-
-#define DEF_BATCHED_TRIM_SECTIONS	2048
-#define BATCHED_TRIM_SEGMENTS(sbi)	\
-		(GET_SEG_FROM_SEC(sbi, SM_I(sbi)->trim_sections))
-#define BATCHED_TRIM_BLOCKS(sbi)	\
-		(BATCHED_TRIM_SEGMENTS(sbi) << (sbi)->log_blocks_per_seg)
-#define MAX_DISCARD_BLOCKS(sbi)		BLKS_PER_SEC(sbi)
-#define DISCARD_ISSUE_RATE		8
-#define DEF_CP_INTERVAL			60	/* 60 secs */
-#define DEF_IDLE_INTERVAL		5	/* 5 secs */
+enum {
+	CP_UMOUNT,
+	CP_SYNC,
+	CP_DISCARD,
+};
 
 struct cp_control {
 	int reason;
@@ -277,69 +132,17 @@ struct ino_entry {
 	nid_t ino;		/* inode number */
 };
 
-/* for the list of inodes to be GCed */
-struct inode_entry {
+/* for the list of directory inodes */
+struct dir_inode_entry {
 	struct list_head list;	/* list head */
 	struct inode *inode;	/* vfs inode pointer */
 };
 
-/* for the bitmap indicate blocks to be discarded */
+/* for the list of blockaddresses to be discarded */
 struct discard_entry {
 	struct list_head list;	/* list head */
-	block_t start_blkaddr;	/* start blockaddr of current segment */
-	unsigned char discard_map[SIT_VBLOCK_MAP_SIZE];	/* segment discard bitmap */
-};
-
-/* max discard pend list number */
-#define MAX_PLIST_NUM		512
-#define plist_idx(blk_num)	((blk_num) >= MAX_PLIST_NUM ?		\
-					(MAX_PLIST_NUM - 1) : (blk_num - 1))
-
-enum {
-	D_PREP,
-	D_SUBMIT,
-	D_DONE,
-};
-
-struct discard_info {
-	block_t lstart;			/* logical start address */
-	block_t len;			/* length */
-	block_t start;			/* actual start address in dev */
-};
-
-struct discard_cmd {
-	struct rb_node rb_node;		/* rb node located in rb-tree */
-	union {
-		struct {
-			block_t lstart;	/* logical start address */
-			block_t len;	/* length */
-			block_t start;	/* actual start address in dev */
-		};
-		struct discard_info di;	/* discard info */
-
-	};
-	struct list_head list;		/* command list */
-	struct completion wait;		/* compleation */
-	struct block_device *bdev;	/* bdev */
-	unsigned short ref;		/* reference count */
-	unsigned char state;		/* state */
-	int error;			/* bio error */
-};
-
-struct discard_cmd_control {
-	struct task_struct *f2fs_issue_discard;	/* discard thread */
-	struct list_head entry_list;		/* 4KB discard entry list */
-	struct list_head pend_list[MAX_PLIST_NUM];/* store pending entries */
-	struct list_head wait_list;		/* store on-flushing entries */
-	wait_queue_head_t discard_wait_queue;	/* waiting queue for wake-up */
-	struct mutex cmd_lock;
-	unsigned int nr_discards;		/* # of discards in the list */
-	unsigned int max_discards;		/* max. discards to be issued */
-	unsigned int undiscard_blks;		/* # of undiscard blocks */
-	atomic_t issued_discard;		/* # of issued discard */
-	atomic_t issing_discard;		/* # of issing discard */
-	atomic_t discard_cmd_cnt;		/* # of cached cmd count */
-	struct rb_root root;			/* root of discard rb-tree */
+	block_t blkaddr;	/* block address to be discarded */
+	int len;		/* # of consecutive blocks of the discard */
 };
 
 /* for the list of fsync inodes, used only during recovery */
@@ -348,41 +151,40 @@ struct fsync_inode_entry {
 	struct inode *inode;	/* vfs inode pointer */
 	block_t blkaddr;	/* block address locating the last fsync */
 	block_t last_dentry;	/* block address locating the last dentry */
+	block_t last_inode;	/* block address locating the last inode */
 };
 
-#define nats_in_cursum(jnl)		(le16_to_cpu((jnl)->n_nats))
-#define sits_in_cursum(jnl)		(le16_to_cpu((jnl)->n_sits))
+#define nats_in_cursum(sum)		(le16_to_cpu(sum->n_nats))
+#define sits_in_cursum(sum)		(le16_to_cpu(sum->n_sits))
 
-#define nat_in_journal(jnl, i)		((jnl)->nat_j.entries[i].ne)
-#define nid_in_journal(jnl, i)		((jnl)->nat_j.entries[i].nid)
-#define sit_in_journal(jnl, i)		((jnl)->sit_j.entries[i].se)
-#define segno_in_journal(jnl, i)	((jnl)->sit_j.entries[i].segno)
+#define nat_in_journal(sum, i)		(sum->nat_j.entries[i].ne)
+#define nid_in_journal(sum, i)		(sum->nat_j.entries[i].nid)
+#define sit_in_journal(sum, i)		(sum->sit_j.entries[i].se)
+#define segno_in_journal(sum, i)	(sum->sit_j.entries[i].segno)
 
-#define MAX_NAT_JENTRIES(jnl)	(NAT_JOURNAL_ENTRIES - nats_in_cursum(jnl))
-#define MAX_SIT_JENTRIES(jnl)	(SIT_JOURNAL_ENTRIES - sits_in_cursum(jnl))
+#define MAX_NAT_JENTRIES(sum)	(NAT_JOURNAL_ENTRIES - nats_in_cursum(sum))
+#define MAX_SIT_JENTRIES(sum)	(SIT_JOURNAL_ENTRIES - sits_in_cursum(sum))
 
-static inline int update_nats_in_cursum(struct f2fs_journal *journal, int i)
+static inline int update_nats_in_cursum(struct f2fs_summary_block *rs, int i)
 {
-	int before = nats_in_cursum(journal);
-
-	journal->n_nats = cpu_to_le16(before + i);
+	int before = nats_in_cursum(rs);
+	rs->n_nats = cpu_to_le16(before + i);
 	return before;
 }
 
-static inline int update_sits_in_cursum(struct f2fs_journal *journal, int i)
+static inline int update_sits_in_cursum(struct f2fs_summary_block *rs, int i)
 {
-	int before = sits_in_cursum(journal);
-
-	journal->n_sits = cpu_to_le16(before + i);
+	int before = sits_in_cursum(rs);
+	rs->n_sits = cpu_to_le16(before + i);
 	return before;
 }
 
-static inline bool __has_cursum_space(struct f2fs_journal *journal,
-							int size, int type)
+static inline bool __has_cursum_space(struct f2fs_summary_block *sum, int size,
+								int type)
 {
 	if (type == NAT_JOURNAL)
-		return size <= MAX_NAT_JENTRIES(journal);
-	return size <= MAX_SIT_JENTRIES(journal);
+		return size <= MAX_NAT_JENTRIES(sum);
+	return size <= MAX_SIT_JENTRIES(sum);
 }
 
 /*
@@ -390,104 +192,23 @@ static inline bool __has_cursum_space(struct f2fs_journal *journal,
  */
 #define F2FS_IOC_GETFLAGS		FS_IOC_GETFLAGS
 #define F2FS_IOC_SETFLAGS		FS_IOC_SETFLAGS
-#define F2FS_IOC_GETVERSION		FS_IOC_GETVERSION
 
 #define F2FS_IOCTL_MAGIC		0xf5
 #define F2FS_IOC_START_ATOMIC_WRITE	_IO(F2FS_IOCTL_MAGIC, 1)
 #define F2FS_IOC_COMMIT_ATOMIC_WRITE	_IO(F2FS_IOCTL_MAGIC, 2)
 #define F2FS_IOC_START_VOLATILE_WRITE	_IO(F2FS_IOCTL_MAGIC, 3)
-#define F2FS_IOC_RELEASE_VOLATILE_WRITE	_IO(F2FS_IOCTL_MAGIC, 4)
-#define F2FS_IOC_ABORT_VOLATILE_WRITE	_IO(F2FS_IOCTL_MAGIC, 5)
-#define F2FS_IOC_GARBAGE_COLLECT	_IOW(F2FS_IOCTL_MAGIC, 6, __u32)
-#define F2FS_IOC_WRITE_CHECKPOINT	_IO(F2FS_IOCTL_MAGIC, 7)
-#define F2FS_IOC_DEFRAGMENT		_IOWR(F2FS_IOCTL_MAGIC, 8,	\
-						struct f2fs_defragment)
-#define F2FS_IOC_MOVE_RANGE		_IOWR(F2FS_IOCTL_MAGIC, 9,	\
-						struct f2fs_move_range)
-#define F2FS_IOC_FLUSH_DEVICE		_IOW(F2FS_IOCTL_MAGIC, 10,	\
-						struct f2fs_flush_device)
-#define F2FS_IOC_GARBAGE_COLLECT_RANGE	_IOW(F2FS_IOCTL_MAGIC, 11,	\
-						struct f2fs_gc_range)
-#define F2FS_IOC_GET_FEATURES		_IOR(F2FS_IOCTL_MAGIC, 12, __u32)
-
-#define F2FS_IOC_SET_ENCRYPTION_POLICY	FS_IOC_SET_ENCRYPTION_POLICY
-#define F2FS_IOC_GET_ENCRYPTION_POLICY	FS_IOC_GET_ENCRYPTION_POLICY
-#define F2FS_IOC_GET_ENCRYPTION_PWSALT	FS_IOC_GET_ENCRYPTION_PWSALT
-
-/*
- * should be same as XFS_IOC_GOINGDOWN.
- * Flags for going down operation used by FS_IOC_GOINGDOWN
- */
-#define F2FS_IOC_SHUTDOWN	_IOR('X', 125, __u32)	/* Shutdown */
-#define F2FS_GOING_DOWN_FULLSYNC	0x0	/* going down with full sync */
-#define F2FS_GOING_DOWN_METASYNC	0x1	/* going down with metadata */
-#define F2FS_GOING_DOWN_NOSYNC		0x2	/* going down */
-#define F2FS_GOING_DOWN_METAFLUSH	0x3	/* going down with meta flush */
 
 #if defined(__KERNEL__) && defined(CONFIG_COMPAT)
 /*
  * ioctl commands in 32 bit emulation
  */
-#define F2FS_IOC32_GETFLAGS		FS_IOC32_GETFLAGS
-#define F2FS_IOC32_SETFLAGS		FS_IOC32_SETFLAGS
-#define F2FS_IOC32_GETVERSION		FS_IOC32_GETVERSION
+#define F2FS_IOC32_GETFLAGS             FS_IOC32_GETFLAGS
+#define F2FS_IOC32_SETFLAGS             FS_IOC32_SETFLAGS
 #endif
 
-struct f2fs_gc_range {
-	u32 sync;
-	u64 start;
-	u64 len;
-};
-
-struct f2fs_defragment {
-	u64 start;
-	u64 len;
-};
-
-struct f2fs_move_range {
-	u32 dst_fd;		/* destination fd */
-	u64 pos_in;		/* start position in src_fd */
-	u64 pos_out;		/* start position in dst_fd */
-	u64 len;		/* size to move */
-};
-
-struct f2fs_flush_device {
-	u32 dev_num;		/* device number to flush */
-	u32 segments;		/* # of segments to flush */
-};
-
 /*
  * For INODE and NODE manager
  */
-/* for directory operations */
-struct f2fs_dentry_ptr {
-	struct inode *inode;
-	const void *bitmap;
-	struct f2fs_dir_entry *dentry;
-	__u8 (*filename)[F2FS_SLOT_LEN];
-	int max;
-};
-
-static inline void make_dentry_ptr_block(struct inode *inode,
-		struct f2fs_dentry_ptr *d, struct f2fs_dentry_block *t)
-{
-	d->inode = inode;
-	d->max = NR_DENTRY_IN_BLOCK;
-	d->bitmap = &t->dentry_bitmap;
-	d->dentry = t->dentry;
-	d->filename = t->filename;
-}
-
-static inline void make_dentry_ptr_inline(struct inode *inode,
-		struct f2fs_dentry_ptr *d, struct f2fs_inline_dentry *t)
-{
-	d->inode = inode;
-	d->max = NR_INLINE_DENTRY;
-	d->bitmap = &t->dentry_bitmap;
-	d->dentry = t->dentry;
-	d->filename = t->filename;
-}
-
 /*
  * XATTR_NODE_OFFSET stores xattrs to one node block per file keeping -1
  * as its node offset to distinguish from index node blocks.
@@ -504,105 +225,25 @@ enum {
 					 */
 };
 
-#define F2FS_LINK_MAX	0xffffffff	/* maximum link count per file */
+#define F2FS_LINK_MAX		32000	/* maximum link count per file */
 
 #define MAX_DIR_RA_PAGES	4	/* maximum ra pages of dir */
 
-/* vector size for gang look-up from extent cache that consists of radix tree */
-#define EXT_TREE_VEC_SIZE	64
-
 /* for in-memory extent cache entry */
-#define F2FS_MIN_EXTENT_LEN	64	/* minimum extent length */
-
-/* number of extent info in extent cache we try to shrink */
-#define EXTENT_CACHE_SHRINK_NUMBER	128
-
-struct rb_entry {
-	struct rb_node rb_node;		/* rb node located in rb-tree */
-	unsigned int ofs;		/* start offset of the entry */
-	unsigned int len;		/* length of the entry */
-};
+#define F2FS_MIN_EXTENT_LEN	16	/* minimum extent length */
 
 struct extent_info {
-	unsigned int fofs;		/* start offset in a file */
-	unsigned int len;		/* length of the extent */
-	u32 blk;			/* start block address of the extent */
-};
-
-struct extent_node {
-	struct rb_node rb_node;
-	union {
-		struct {
-			unsigned int fofs;
-			unsigned int len;
-			u32 blk;
-		};
-		struct extent_info ei;	/* extent info */
-
-	};
-	struct list_head list;		/* node in global extent list of sbi */
-	struct extent_tree *et;		/* extent tree pointer */
-};
-
-struct extent_tree {
-	nid_t ino;			/* inode number */
-	struct rb_root root;		/* root of extent info rb-tree */
-	struct extent_node *cached_en;	/* recently accessed extent node */
-	struct extent_info largest;	/* largested extent info */
-	struct list_head list;		/* to be used by sbi->zombie_list */
-	rwlock_t lock;			/* protect extent info rb-tree */
-	atomic_t node_cnt;		/* # of extent node in rb-tree*/
-};
-
-/*
- * This structure is taken from ext4_map_blocks.
- *
- * Note that, however, f2fs uses NEW and MAPPED flags for f2fs_map_blocks().
- */
-#define F2FS_MAP_NEW		(1 << BH_New)
-#define F2FS_MAP_MAPPED		(1 << BH_Mapped)
-#define F2FS_MAP_UNWRITTEN	(1 << BH_Unwritten)
-#define F2FS_MAP_FLAGS		(F2FS_MAP_NEW | F2FS_MAP_MAPPED |\
-				F2FS_MAP_UNWRITTEN)
-
-struct f2fs_map_blocks {
-	block_t m_pblk;
-	block_t m_lblk;
-	unsigned int m_len;
-	unsigned int m_flags;
-	pgoff_t *m_next_pgofs;		/* point next possible non-hole pgofs */
+	rwlock_t ext_lock;	/* rwlock for consistency */
+	unsigned int fofs;	/* start offset in a file */
+	u32 blk_addr;		/* start block address of the extent */
+	unsigned int len;	/* length of the extent */
 };
 
-/* for flag in get_data_block */
-#define F2FS_GET_BLOCK_READ		0
-#define F2FS_GET_BLOCK_DIO		1
-#define F2FS_GET_BLOCK_FIEMAP		2
-#define F2FS_GET_BLOCK_BMAP		3
-#define F2FS_GET_BLOCK_PRE_DIO		4
-#define F2FS_GET_BLOCK_PRE_AIO		5
-
 /*
  * i_advise uses FADVISE_XXX_BIT. We can add additional hints later.
  */
 #define FADVISE_COLD_BIT	0x01
 #define FADVISE_LOST_PINO_BIT	0x02
-#define FADVISE_ENCRYPT_BIT	0x04
-#define FADVISE_ENC_NAME_BIT	0x08
-#define FADVISE_KEEP_SIZE_BIT	0x10
-
-#define file_is_cold(inode)	is_file(inode, FADVISE_COLD_BIT)
-#define file_wrong_pino(inode)	is_file(inode, FADVISE_LOST_PINO_BIT)
-#define file_set_cold(inode)	set_file(inode, FADVISE_COLD_BIT)
-#define file_lost_pino(inode)	set_file(inode, FADVISE_LOST_PINO_BIT)
-#define file_clear_cold(inode)	clear_file(inode, FADVISE_COLD_BIT)
-#define file_got_pino(inode)	clear_file(inode, FADVISE_LOST_PINO_BIT)
-#define file_is_encrypt(inode)	is_file(inode, FADVISE_ENCRYPT_BIT)
-#define file_set_encrypt(inode)	set_file(inode, FADVISE_ENCRYPT_BIT)
-#define file_clear_encrypt(inode) clear_file(inode, FADVISE_ENCRYPT_BIT)
-#define file_enc_name(inode)	is_file(inode, FADVISE_ENC_NAME_BIT)
-#define file_set_enc_name(inode) set_file(inode, FADVISE_ENC_NAME_BIT)
-#define file_keep_isize(inode)	is_file(inode, FADVISE_KEEP_SIZE_BIT)
-#define file_set_keep_isize(inode) set_file(inode, FADVISE_KEEP_SIZE_BIT)
 
 #define DEF_DIR_LEVEL		0
 
@@ -621,139 +262,59 @@ struct f2fs_inode_info {
 	atomic_t dirty_pages;		/* # of dirty pages */
 	f2fs_hash_t chash;		/* hash value of given file name */
 	unsigned int clevel;		/* maximum level of given file name */
-	struct task_struct *task;	/* lookup and create consistency */
 	nid_t i_xattr_nid;		/* node id that contains xattrs */
-	loff_t	last_disk_size;		/* lastly written file size */
+	unsigned long long xattr_ver;	/* cp version of xattr modification */
+	struct extent_info ext;		/* in-memory extent cache entry */
+	struct dir_inode_entry *dirty_dir;	/* the pointer of dirty dir */
 
-#ifdef CONFIG_QUOTA
-	/* quota space reservation, managed internally by quota code */
-	qsize_t i_reserved_quota;
-#endif
-	struct list_head dirty_list;	/* dirty list for dirs and files */
-	struct list_head gdirty_list;	/* linked in global dirty list */
 	struct list_head inmem_pages;	/* inmemory pages managed by f2fs */
-	struct task_struct *inmem_task;	/* store inmemory task */
 	struct mutex inmem_lock;	/* lock for inmemory pages */
-	struct extent_tree *extent_tree;	/* cached extent_tree entry */
-	struct rw_semaphore dio_rwsem[2];/* avoid racing between dio and gc */
-	struct rw_semaphore i_mmap_sem;
 };
 
 static inline void get_extent_info(struct extent_info *ext,
-					struct f2fs_extent *i_ext)
+					struct f2fs_extent i_ext)
 {
-	ext->fofs = le32_to_cpu(i_ext->fofs);
-	ext->blk = le32_to_cpu(i_ext->blk);
-	ext->len = le32_to_cpu(i_ext->len);
+	write_lock(&ext->ext_lock);
+	ext->fofs = le32_to_cpu(i_ext.fofs);
+	ext->blk_addr = le32_to_cpu(i_ext.blk_addr);
+	ext->len = le32_to_cpu(i_ext.len);
+	write_unlock(&ext->ext_lock);
 }
 
 static inline void set_raw_extent(struct extent_info *ext,
 					struct f2fs_extent *i_ext)
 {
+	read_lock(&ext->ext_lock);
 	i_ext->fofs = cpu_to_le32(ext->fofs);
-	i_ext->blk = cpu_to_le32(ext->blk);
+	i_ext->blk_addr = cpu_to_le32(ext->blk_addr);
 	i_ext->len = cpu_to_le32(ext->len);
+	read_unlock(&ext->ext_lock);
 }
 
-static inline void set_extent_info(struct extent_info *ei, unsigned int fofs,
-						u32 blk, unsigned int len)
-{
-	ei->fofs = fofs;
-	ei->blk = blk;
-	ei->len = len;
-}
-
-static inline bool __is_discard_mergeable(struct discard_info *back,
-						struct discard_info *front)
-{
-	return back->lstart + back->len == front->lstart;
-}
-
-static inline bool __is_discard_back_mergeable(struct discard_info *cur,
-						struct discard_info *back)
-{
-	return __is_discard_mergeable(back, cur);
-}
-
-static inline bool __is_discard_front_mergeable(struct discard_info *cur,
-						struct discard_info *front)
-{
-	return __is_discard_mergeable(cur, front);
-}
-
-static inline bool __is_extent_mergeable(struct extent_info *back,
-						struct extent_info *front)
-{
-	return (back->fofs + back->len == front->fofs &&
-			back->blk + back->len == front->blk);
-}
-
-static inline bool __is_back_mergeable(struct extent_info *cur,
-						struct extent_info *back)
-{
-	return __is_extent_mergeable(back, cur);
-}
-
-static inline bool __is_front_mergeable(struct extent_info *cur,
-						struct extent_info *front)
-{
-	return __is_extent_mergeable(cur, front);
-}
-
-extern void f2fs_mark_inode_dirty_sync(struct inode *inode, bool sync);
-static inline void __try_update_largest_extent(struct inode *inode,
-			struct extent_tree *et, struct extent_node *en)
-{
-	if (en->ei.len > et->largest.len) {
-		et->largest = en->ei;
-		f2fs_mark_inode_dirty_sync(inode, true);
-	}
-}
-
-enum nid_list {
-	FREE_NID_LIST,
-	ALLOC_NID_LIST,
-	MAX_NID_LIST,
-};
-
 struct f2fs_nm_info {
 	block_t nat_blkaddr;		/* base disk address of NAT */
 	nid_t max_nid;			/* maximum possible node ids */
-	nid_t available_nids;		/* # of available node ids */
+	nid_t available_nids;		/* maximum available node ids */
 	nid_t next_scan_nid;		/* the next nid to be scanned */
 	unsigned int ram_thresh;	/* control the memory footprint */
-	unsigned int ra_nid_pages;	/* # of nid pages to be readaheaded */
-	unsigned int dirty_nats_ratio;	/* control dirty nats ratio threshold */
 
 	/* NAT cache management */
 	struct radix_tree_root nat_root;/* root of the nat entry cache */
 	struct radix_tree_root nat_set_root;/* root of the nat set cache */
-	struct rw_semaphore nat_tree_lock;	/* protect nat_tree_lock */
+	rwlock_t nat_tree_lock;		/* protect nat_tree_lock */
 	struct list_head nat_entries;	/* cached nat entry list (clean) */
 	unsigned int nat_cnt;		/* the # of cached nat entries */
 	unsigned int dirty_nat_cnt;	/* total num of nat entries in set */
-	unsigned int nat_blocks;	/* # of nat blocks */
 
 	/* free node ids management */
 	struct radix_tree_root free_nid_root;/* root of the free_nid cache */
-	struct list_head nid_list[MAX_NID_LIST];/* lists for free nids */
-	unsigned int nid_cnt[MAX_NID_LIST];	/* the number of free node id */
-	spinlock_t nid_list_lock;	/* protect nid lists ops */
+	struct list_head free_nid_list;	/* a list for free nids */
+	spinlock_t free_nid_list_lock;	/* protect free nid list */
+	unsigned int fcnt;		/* the number of free node id */
 	struct mutex build_lock;	/* lock for build free nids */
-	unsigned char (*free_nid_bitmap)[NAT_ENTRY_BITMAP_SIZE];
-	unsigned char *nat_block_bitmap;
-	unsigned short *free_nid_count;	/* free nid count of NAT block */
 
 	/* for checkpoint */
 	char *nat_bitmap;		/* NAT bitmap pointer */
-
-	unsigned int nat_bits_blocks;	/* # of nat bits blocks */
-	unsigned char *nat_bits;	/* NAT bits blocks */
-	unsigned char *full_nat_bits;	/* full NAT pages */
-	unsigned char *empty_nat_bits;	/* empty NAT pages */
-#ifdef CONFIG_F2FS_CHECK_FS
-	char *nat_bitmap_mir;		/* NAT bitmap mirror */
-#endif
 	int bitmap_size;		/* bitmap size */
 };
 
@@ -769,9 +330,6 @@ struct dnode_of_data {
 	nid_t nid;			/* node id of the direct node block */
 	unsigned int ofs_in_node;	/* data offset in the node page */
 	bool inode_page_locked;		/* inode page is locked or not */
-	bool node_changed;		/* is node block changed */
-	char cur_level;			/* level of hole node page */
-	char max_level;			/* level of current page located */
 	block_t	data_blkaddr;		/* block address of the node block */
 };
 
@@ -809,7 +367,7 @@ enum {
 	CURSEG_HOT_NODE,	/* direct node blocks of directory files */
 	CURSEG_WARM_NODE,	/* direct node blocks of normal files */
 	CURSEG_COLD_NODE,	/* indirect node blocks */
-	NO_CHECK_TYPE,
+	NO_CHECK_TYPE
 };
 
 struct flush_cmd {
@@ -821,8 +379,6 @@ struct flush_cmd {
 struct flush_cmd_control {
 	struct task_struct *f2fs_issue_flush;	/* flush thread */
 	wait_queue_head_t flush_wait_queue;	/* waiting queue for wake-up */
-	atomic_t issued_flush;			/* # of issued flushes */
-	atomic_t issing_flush;			/* # of issing flushes */
 	struct llist_head issue_list;		/* list for command issue */
 	struct llist_node *dispatch_list;	/* list for command dispatch */
 };
@@ -845,21 +401,20 @@ struct f2fs_sm_info {
 	/* a threshold to reclaim prefree segments */
 	unsigned int rec_prefree_segments;
 
-	/* for batched trimming */
-	unsigned int trim_sections;		/* # of sections to trim */
+	/* for small discard management */
+	struct list_head discard_list;		/* 4KB discard list */
+	int nr_discards;			/* # of discards in the list */
+	int max_discards;			/* max. discards to be issued */
 
 	struct list_head sit_entry_set;	/* sit entry set list */
 
 	unsigned int ipu_policy;	/* in-place-update policy */
 	unsigned int min_ipu_util;	/* in-place-update threshold */
 	unsigned int min_fsync_blocks;	/* threshold for fsync */
-	unsigned int min_hot_blocks;	/* threshold for hot block allocation */
 
 	/* for flush command control */
-	struct flush_cmd_control *fcc_info;
+	struct flush_cmd_control *cmd_control_info;
 
-	/* for discard command control */
-	struct discard_cmd_control *dcc_info;
 };
 
 /*
@@ -871,16 +426,11 @@ struct f2fs_sm_info {
  * f2fs monitors the number of several block types such as on-writeback,
  * dirty dentry blocks, dirty node blocks, and dirty meta blocks.
  */
-#define WB_DATA_TYPE(p)	(__is_cp_guaranteed(p) ? F2FS_WB_CP_DATA : F2FS_WB_DATA)
 enum count_type {
+	F2FS_WRITEBACK,
 	F2FS_DIRTY_DENTS,
-	F2FS_DIRTY_DATA,
 	F2FS_DIRTY_NODES,
 	F2FS_DIRTY_META,
-	F2FS_INMEM_PAGES,
-	F2FS_DIRTY_IMETA,
-	F2FS_WB_CP_DATA,
-	F2FS_WB_DATA,
 	NR_COUNT_TYPE,
 };
 
@@ -902,110 +452,29 @@ enum page_type {
 	META,
 	NR_PAGE_TYPE,
 	META_FLUSH,
-	INMEM,		/* the below types are used by tracepoints only. */
-	INMEM_DROP,
-	INMEM_INVALIDATE,
-	INMEM_REVOKE,
-	IPU,
-	OPU,
-};
-
-enum temp_type {
-	HOT = 0,	/* must be zero for meta bio */
-	WARM,
-	COLD,
-	NR_TEMP_TYPE,
-};
-
-enum need_lock_type {
-	LOCK_REQ = 0,
-	LOCK_DONE,
-	LOCK_RETRY,
 };
 
 struct f2fs_io_info {
-	struct f2fs_sb_info *sbi;	/* f2fs_sb_info pointer */
 	enum page_type type;	/* contains DATA/NODE/META/META_FLUSH */
-	enum temp_type temp;	/* contains HOT/WARM/COLD */
-	int op;			/* contains REQ_OP_ */
-	int op_flags;		/* req_flag_bits */
-	block_t new_blkaddr;	/* new block address to be written */
-	block_t old_blkaddr;	/* old block address before Cow */
-	struct page *page;	/* page to be written */
-	struct page *encrypted_page;	/* encrypted page */
-	struct list_head list;		/* serialize IOs */
-	bool submitted;		/* indicate IO submission */
-	int need_lock;		/* indicate we need to lock cp_rwsem */
-	bool in_list;		/* indicate fio is in io_list */
+	int rw;			/* contains R/RS/W/WS with REQ_META/REQ_PRIO */
 };
 
-#define is_read_io(rw) ((rw) == READ)
+#define is_read_io(rw)	(((rw) & 1) == READ)
 struct f2fs_bio_info {
 	struct f2fs_sb_info *sbi;	/* f2fs superblock */
 	struct bio *bio;		/* bios to merge */
 	sector_t last_block_in_bio;	/* last block number */
 	struct f2fs_io_info fio;	/* store buffered io info. */
 	struct rw_semaphore io_rwsem;	/* blocking op for bio */
-	spinlock_t io_lock;		/* serialize DATA/NODE IOs */
-	struct list_head io_list;	/* track fios */
-};
-
-#define FDEV(i)				(sbi->devs[i])
-#define RDEV(i)				(raw_super->devs[i])
-struct f2fs_dev_info {
-	struct block_device *bdev;
-	char path[MAX_PATH_LEN];
-	unsigned int total_segments;
-	block_t start_blk;
-	block_t end_blk;
-#ifdef CONFIG_BLK_DEV_ZONED
-	unsigned int nr_blkz;			/* Total number of zones */
-	u8 *blkz_type;				/* Array of zones type */
-#endif
-};
-
-enum inode_type {
-	DIR_INODE,			/* for dirty dir inode */
-	FILE_INODE,			/* for dirty regular/symlink inode */
-	DIRTY_META,			/* for all dirtied inode metadata */
-	NR_INODE_TYPE,
-};
-
-/* for inner inode cache management */
-struct inode_management {
-	struct radix_tree_root ino_root;	/* ino entry array */
-	spinlock_t ino_lock;			/* for ino entry lock */
-	struct list_head ino_list;		/* inode list head */
-	unsigned long ino_num;			/* number of entries */
-};
-
-/* For s_flag in struct f2fs_sb_info */
-enum {
-	SBI_IS_DIRTY,				/* dirty flag for checkpoint */
-	SBI_IS_CLOSE,				/* specify unmounting */
-	SBI_NEED_FSCK,				/* need fsck.f2fs to fix */
-	SBI_POR_DOING,				/* recovery is doing or not */
-	SBI_NEED_SB_WRITE,			/* need to recover superblock */
-	SBI_NEED_CP,				/* need to checkpoint */
-};
-
-enum {
-	CP_TIME,
-	REQ_TIME,
-	MAX_TIME,
 };
 
 struct f2fs_sb_info {
 	struct super_block *sb;			/* pointer to VFS super block */
 	struct proc_dir_entry *s_proc;		/* proc entry */
+	struct buffer_head *raw_super_buf;	/* buffer head of raw sb */
 	struct f2fs_super_block *raw_super;	/* raw super block pointer */
-	int valid_super_block;			/* valid super block no */
-	unsigned long s_flag;				/* flags for sbi */
-
-#ifdef CONFIG_BLK_DEV_ZONED
-	unsigned int blocks_per_blkz;		/* F2FS blocks per zone */
-	unsigned int log_blocks_per_blkz;	/* log2 F2FS blocks per zone */
-#endif
+	int s_dirty;				/* dirty flag for checkpoint */
+	bool need_fsck;				/* need fsck.f2fs to fix */
 
 	/* for node-related operations */
 	struct f2fs_nm_info *nm_info;		/* node manager */
@@ -1015,43 +484,32 @@ struct f2fs_sb_info {
 	struct f2fs_sm_info *sm_info;		/* segment manager */
 
 	/* for bio operations */
-	struct f2fs_bio_info *write_io[NR_PAGE_TYPE];	/* for write bios */
-	struct mutex wio_mutex[NR_PAGE_TYPE - 1][NR_TEMP_TYPE];
-						/* bio ordering for NODE/DATA */
-	int write_io_size_bits;			/* Write IO size bits */
-	mempool_t *write_io_dummy;		/* Dummy pages */
+	struct f2fs_bio_info read_io;			/* for read bios */
+	struct f2fs_bio_info write_io[NR_PAGE_TYPE];	/* for write bios */
+	struct completion *wait_io;		/* for completion bios */
 
 	/* for checkpoint */
 	struct f2fs_checkpoint *ckpt;		/* raw checkpoint pointer */
-	int cur_cp_pack;			/* remain current cp pack */
-	spinlock_t cp_lock;			/* for flag in ckpt */
 	struct inode *meta_inode;		/* cache meta blocks */
 	struct mutex cp_mutex;			/* checkpoint procedure lock */
 	struct rw_semaphore cp_rwsem;		/* blocking FS operations */
 	struct rw_semaphore node_write;		/* locking node writes */
-	struct rw_semaphore node_change;	/* locking node change */
+	struct mutex writepages;		/* mutex for writepages() */
+	bool por_doing;				/* recovery is doing or not */
 	wait_queue_head_t cp_wait;
-	unsigned long last_time[MAX_TIME];	/* to store time in jiffies */
-	long interval_time[MAX_TIME];		/* to store thresholds */
 
-	struct inode_management im[MAX_INO_ENTRY];      /* manage inode cache */
+	/* for inode management */
+	struct radix_tree_root ino_root[MAX_INO_ENTRY];	/* ino entry array */
+	spinlock_t ino_lock[MAX_INO_ENTRY];		/* for ino entry lock */
+	struct list_head ino_list[MAX_INO_ENTRY];	/* inode list head */
 
 	/* for orphan inode, use 0'th array */
+	unsigned int n_orphans;			/* # of orphan inodes */
 	unsigned int max_orphans;		/* max orphan inodes */
 
-	/* for inode management */
-	struct list_head inode_list[NR_INODE_TYPE];	/* dirty inode list */
-	spinlock_t inode_lock[NR_INODE_TYPE];	/* for dirty inode list lock */
-
-	/* for extent tree cache */
-	struct radix_tree_root extent_tree_root;/* cache extent cache entries */
-	struct mutex extent_tree_lock;	/* locking extent radix tree */
-	struct list_head extent_list;		/* lru list for shrinker */
-	spinlock_t extent_lock;			/* locking extent lru list */
-	atomic_t total_ext_tree;		/* extent tree count */
-	struct list_head zombie_list;		/* extent zombie tree list */
-	atomic_t total_zombie_tree;		/* extent zombie tree count */
-	atomic_t total_ext_node;		/* extent info count */
+	/* for directory inode management */
+	struct list_head dir_inode_list;	/* dir inode list */
+	spinlock_t dir_inode_lock;		/* for dir inode list lock */
 
 	/* basic filesystem units */
 	unsigned int log_sectors_per_block;	/* log2 sectors per block */
@@ -1067,28 +525,16 @@ struct f2fs_sb_info {
 	unsigned int total_sections;		/* total section count */
 	unsigned int total_node_count;		/* total node block count */
 	unsigned int total_valid_node_count;	/* valid node block count */
-	loff_t max_file_blocks;			/* max block index of file */
+	unsigned int total_valid_inode_count;	/* valid inode count */
 	int active_logs;			/* # of active logs */
 	int dir_level;				/* directory level */
 
 	block_t user_block_count;		/* # of user blocks */
 	block_t total_valid_block_count;	/* # of valid blocks */
-	block_t discard_blks;			/* discard command candidats */
+	block_t alloc_valid_block_count;	/* # of allocated blocks */
 	block_t last_valid_block_count;		/* for recovery */
-	block_t reserved_blocks;		/* configurable reserved blocks */
-
 	u32 s_next_generation;			/* for NFS support */
-
-	/* # of pages, see count_type */
-	atomic_t nr_pages[NR_COUNT_TYPE];
-	/* # of allocated blocks */
-	struct percpu_counter alloc_valid_block_count;
-
-	/* writeback control */
-	atomic_t wb_sync_req;			/* count # of WB_SYNC threads */
-
-	/* valid inode count */
-	struct percpu_counter total_valid_inode_count;
+	atomic_t nr_pages[NR_COUNT_TYPE];	/* # of pages, see count_type */
 
 	struct f2fs_mount_info mount_opt;	/* mount options */
 
@@ -1097,9 +543,6 @@ struct f2fs_sb_info {
 	struct f2fs_gc_kthread	*gc_thread;	/* GC thread */
 	unsigned int cur_victim_sec;		/* current victim section num */
 
-	/* threshold for converting bg victims for fg */
-	u64 fggc_threshold;
-
 	/* maximum # of trials to find a victim segment for SSR and GC */
 	unsigned int max_victim_search;
 
@@ -1111,129 +554,22 @@ struct f2fs_sb_info {
 	struct f2fs_stat_info *stat_info;	/* FS status information */
 	unsigned int segment_count[2];		/* # of allocated segments */
 	unsigned int block_count[2];		/* # of allocated blocks */
-	atomic_t inplace_count;		/* # of inplace update */
-	atomic64_t total_hit_ext;		/* # of lookup extent cache */
-	atomic64_t read_hit_rbtree;		/* # of hit rbtree extent node */
-	atomic64_t read_hit_largest;		/* # of hit largest extent node */
-	atomic64_t read_hit_cached;		/* # of hit cached extent node */
-	atomic_t inline_xattr;			/* # of inline_xattr inodes */
-	atomic_t inline_inode;			/* # of inline_data inodes */
-	atomic_t inline_dir;			/* # of inline_dentry inodes */
-	atomic_t aw_cnt;			/* # of atomic writes */
-	atomic_t vw_cnt;			/* # of volatile writes */
-	atomic_t max_aw_cnt;			/* max # of atomic writes */
-	atomic_t max_vw_cnt;			/* max # of volatile writes */
+	int total_hit_ext, read_hit_ext;	/* extent cache hit ratio */
+	int inline_inode;			/* # of inline_data inodes */
 	int bg_gc;				/* background gc calls */
-	unsigned int ndirty_inode[NR_INODE_TYPE];	/* # of dirty inodes */
+	unsigned int n_dirty_dirs;		/* # of dir inodes */
 #endif
+	unsigned int last_victim[2];		/* last victim segment # */
 	spinlock_t stat_lock;			/* lock for stat operations */
 
 	/* For sysfs suppport */
 	struct kobject s_kobj;
 	struct completion s_kobj_unregister;
-
-	/* For shrinker support */
-	struct list_head s_list;
-	int s_ndevs;				/* number of devices */
-	struct f2fs_dev_info *devs;		/* for device list */
-	struct mutex umount_mutex;
-	unsigned int shrinker_run_no;
-
-	/* For write statistics */
-	u64 sectors_written_start;
-	u64 kbytes_written;
-
-	/* Reference to checksum algorithm driver via cryptoapi */
-	struct crypto_shash *s_chksum_driver;
-
-	/* For fault injection */
-#ifdef CONFIG_F2FS_FAULT_INJECTION
-	struct f2fs_fault_info fault_info;
-#endif
 };
 
-#ifdef CONFIG_F2FS_FAULT_INJECTION
-#define f2fs_show_injection_info(type)				\
-	printk("%sF2FS-fs : inject %s in %s of %pF\n",		\
-		KERN_INFO, fault_name[type],			\
-		__func__, __builtin_return_address(0))
-static inline bool time_to_inject(struct f2fs_sb_info *sbi, int type)
-{
-	struct f2fs_fault_info *ffi = &sbi->fault_info;
-
-	if (!ffi->inject_rate)
-		return false;
-
-	if (!IS_FAULT_SET(ffi, type))
-		return false;
-
-	atomic_inc(&ffi->inject_ops);
-	if (atomic_read(&ffi->inject_ops) >= ffi->inject_rate) {
-		atomic_set(&ffi->inject_ops, 0);
-		return true;
-	}
-	return false;
-}
-#endif
-
-/* For write statistics. Suppose sector size is 512 bytes,
- * and the return value is in kbytes. s is of struct f2fs_sb_info.
- */
-#define BD_PART_WRITTEN(s)						 \
-(((u64)part_stat_read((s)->sb->s_bdev->bd_part, sectors[1]) -		 \
-		(s)->sectors_written_start) >> 1)
-
-static inline void f2fs_update_time(struct f2fs_sb_info *sbi, int type)
-{
-	sbi->last_time[type] = jiffies;
-}
-
-static inline bool f2fs_time_over(struct f2fs_sb_info *sbi, int type)
-{
-	struct timespec ts = {sbi->interval_time[type], 0};
-	unsigned long interval = timespec_to_jiffies(&ts);
-
-	return time_after(jiffies, sbi->last_time[type] + interval);
-}
-
-static inline bool is_idle(struct f2fs_sb_info *sbi)
-{
-	struct block_device *bdev = sbi->sb->s_bdev;
-	struct request_queue *q = bdev_get_queue(bdev);
-	struct request_list *rl = &q->root_rl;
-
-	if (rl->count[BLK_RW_SYNC] || rl->count[BLK_RW_ASYNC])
-		return 0;
-
-	return f2fs_time_over(sbi, REQ_TIME);
-}
-
 /*
  * Inline functions
  */
-static inline u32 f2fs_crc32(struct f2fs_sb_info *sbi, const void *address,
-			   unsigned int length)
-{
-	SHASH_DESC_ON_STACK(shash, sbi->s_chksum_driver);
-	u32 *ctx = (u32 *)shash_desc_ctx(shash);
-	int err;
-
-	shash->tfm = sbi->s_chksum_driver;
-	shash->flags = 0;
-	*ctx = F2FS_SUPER_MAGIC;
-
-	err = crypto_shash_update(shash, address, length);
-	BUG_ON(err);
-
-	return *ctx;
-}
-
-static inline bool f2fs_crc_valid(struct f2fs_sb_info *sbi, __u32 blk_crc,
-				  void *buf, size_t buf_size)
-{
-	return f2fs_crc32(sbi, buf, buf_size) == blk_crc;
-}
-
 static inline struct f2fs_inode_info *F2FS_I(struct inode *inode)
 {
 	return container_of(inode, struct f2fs_inode_info, vfs_inode);
@@ -1314,19 +650,14 @@ static inline struct address_space *NODE_MAPPING(struct f2fs_sb_info *sbi)
 	return sbi->node_inode->i_mapping;
 }
 
-static inline bool is_sbi_flag_set(struct f2fs_sb_info *sbi, unsigned int type)
-{
-	return test_bit(type, &sbi->s_flag);
-}
-
-static inline void set_sbi_flag(struct f2fs_sb_info *sbi, unsigned int type)
+static inline void F2FS_SET_SB_DIRT(struct f2fs_sb_info *sbi)
 {
-	set_bit(type, &sbi->s_flag);
+	sbi->s_dirty = 1;
 }
 
-static inline void clear_sbi_flag(struct f2fs_sb_info *sbi, unsigned int type)
+static inline void F2FS_RESET_SB_DIRT(struct f2fs_sb_info *sbi)
 {
-	clear_bit(type, &sbi->s_flag);
+	sbi->s_dirty = 0;
 }
 
 static inline unsigned long long cur_cp_version(struct f2fs_checkpoint *cp)
@@ -1334,93 +665,31 @@ static inline unsigned long long cur_cp_version(struct f2fs_checkpoint *cp)
 	return le64_to_cpu(cp->checkpoint_ver);
 }
 
-static inline __u64 cur_cp_crc(struct f2fs_checkpoint *cp)
-{
-	size_t crc_offset = le32_to_cpu(cp->checksum_offset);
-	return le32_to_cpu(*((__le32 *)((unsigned char *)cp + crc_offset)));
-}
-
-static inline bool __is_set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)
+static inline bool is_set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)
 {
 	unsigned int ckpt_flags = le32_to_cpu(cp->ckpt_flags);
-
 	return ckpt_flags & f;
 }
 
-static inline bool is_set_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)
+static inline void set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)
 {
-	return __is_set_ckpt_flags(F2FS_CKPT(sbi), f);
-}
-
-static inline void __set_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)
-{
-	unsigned int ckpt_flags;
-
-	ckpt_flags = le32_to_cpu(cp->ckpt_flags);
+	unsigned int ckpt_flags = le32_to_cpu(cp->ckpt_flags);
 	ckpt_flags |= f;
 	cp->ckpt_flags = cpu_to_le32(ckpt_flags);
 }
 
-static inline void set_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&sbi->cp_lock, flags);
-	__set_ckpt_flags(F2FS_CKPT(sbi), f);
-	spin_unlock_irqrestore(&sbi->cp_lock, flags);
-}
-
-static inline void __clear_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)
+static inline void clear_ckpt_flags(struct f2fs_checkpoint *cp, unsigned int f)
 {
-	unsigned int ckpt_flags;
-
-	ckpt_flags = le32_to_cpu(cp->ckpt_flags);
+	unsigned int ckpt_flags = le32_to_cpu(cp->ckpt_flags);
 	ckpt_flags &= (~f);
 	cp->ckpt_flags = cpu_to_le32(ckpt_flags);
 }
 
-static inline void clear_ckpt_flags(struct f2fs_sb_info *sbi, unsigned int f)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&sbi->cp_lock, flags);
-	__clear_ckpt_flags(F2FS_CKPT(sbi), f);
-	spin_unlock_irqrestore(&sbi->cp_lock, flags);
-}
-
-static inline void disable_nat_bits(struct f2fs_sb_info *sbi, bool lock)
-{
-	unsigned long flags;
-
-	set_sbi_flag(sbi, SBI_NEED_FSCK);
-
-	if (lock)
-		spin_lock_irqsave(&sbi->cp_lock, flags);
-	__clear_ckpt_flags(F2FS_CKPT(sbi), CP_NAT_BITS_FLAG);
-	kfree(NM_I(sbi)->nat_bits);
-	NM_I(sbi)->nat_bits = NULL;
-	if (lock)
-		spin_unlock_irqrestore(&sbi->cp_lock, flags);
-}
-
-static inline bool enabled_nat_bits(struct f2fs_sb_info *sbi,
-					struct cp_control *cpc)
-{
-	bool set = is_set_ckpt_flags(sbi, CP_NAT_BITS_FLAG);
-
-	return (cpc) ? (cpc->reason & CP_UMOUNT) && set : set;
-}
-
 static inline void f2fs_lock_op(struct f2fs_sb_info *sbi)
 {
 	down_read(&sbi->cp_rwsem);
 }
 
-static inline int f2fs_trylock_op(struct f2fs_sb_info *sbi)
-{
-	return down_read_trylock(&sbi->cp_rwsem);
-}
-
 static inline void f2fs_unlock_op(struct f2fs_sb_info *sbi)
 {
 	up_read(&sbi->cp_rwsem);
@@ -1436,28 +705,6 @@ static inline void f2fs_unlock_all(struct f2fs_sb_info *sbi)
 	up_write(&sbi->cp_rwsem);
 }
 
-static inline int __get_cp_reason(struct f2fs_sb_info *sbi)
-{
-	int reason = CP_SYNC;
-
-	if (test_opt(sbi, FASTBOOT))
-		reason = CP_FASTBOOT;
-	if (is_sbi_flag_set(sbi, SBI_IS_CLOSE))
-		reason = CP_UMOUNT;
-	return reason;
-}
-
-static inline bool __remain_node_summaries(int reason)
-{
-	return (reason & (CP_UMOUNT | CP_FASTBOOT));
-}
-
-static inline bool __exist_node_summaries(struct f2fs_sb_info *sbi)
-{
-	return (is_set_ckpt_flags(sbi, CP_UMOUNT_FLAG) ||
-			is_set_ckpt_flags(sbi, CP_FASTBOOT_FLAG));
-}
-
 /*
  * Check whether the given nid is within node id range.
  */
@@ -1470,14 +717,17 @@ static inline int check_nid_range(struct f2fs_sb_info *sbi, nid_t nid)
 	return 0;
 }
 
+#define F2FS_DEFAULT_ALLOCATED_BLOCKS	1
+
 /*
  * Check whether the inode has blocks or not
  */
 static inline int F2FS_HAS_BLOCKS(struct inode *inode)
 {
-	block_t xattr_block = F2FS_I(inode)->i_xattr_nid ? 1 : 0;
-
-	return (inode->i_blocks >> F2FS_LOG_SECTORS_PER_BLOCK) > xattr_block;
+	if (F2FS_I(inode)->i_xattr_nid)
+		return inode->i_blocks > F2FS_DEFAULT_ALLOCATED_BLOCKS + 1;
+	else
+		return inode->i_blocks > F2FS_DEFAULT_ALLOCATED_BLOCKS;
 }
 
 static inline bool f2fs_has_xattr_block(unsigned int ofs)
@@ -1485,87 +735,48 @@ static inline bool f2fs_has_xattr_block(unsigned int ofs)
 	return ofs == XATTR_NODE_OFFSET;
 }
 
-static inline void f2fs_i_blocks_write(struct inode *, block_t, bool, bool);
-static inline int inc_valid_block_count(struct f2fs_sb_info *sbi,
-				 struct inode *inode, blkcnt_t *count)
+static inline bool inc_valid_block_count(struct f2fs_sb_info *sbi,
+				 struct inode *inode, blkcnt_t count)
 {
-	blkcnt_t diff = 0, release = 0;
-	block_t avail_user_block_count;
-	int ret;
-
-	ret = dquot_reserve_block(inode, *count);
-	if (ret)
-		return ret;
-
-#ifdef CONFIG_F2FS_FAULT_INJECTION
-	if (time_to_inject(sbi, FAULT_BLOCK)) {
-		f2fs_show_injection_info(FAULT_BLOCK);
-		release = *count;
-		goto enospc;
-	}
-#endif
-	/*
-	 * let's increase this in prior to actual block count change in order
-	 * for f2fs_sync_file to avoid data races when deciding checkpoint.
-	 */
-	percpu_counter_add(&sbi->alloc_valid_block_count, (*count));
+	block_t	valid_block_count;
 
 	spin_lock(&sbi->stat_lock);
-	sbi->total_valid_block_count += (block_t)(*count);
-	avail_user_block_count = sbi->user_block_count - sbi->reserved_blocks;
-	if (unlikely(sbi->total_valid_block_count > avail_user_block_count)) {
-		diff = sbi->total_valid_block_count - avail_user_block_count;
-		*count -= diff;
-		release = diff;
-		sbi->total_valid_block_count = avail_user_block_count;
-		if (!*count) {
-			spin_unlock(&sbi->stat_lock);
-			percpu_counter_sub(&sbi->alloc_valid_block_count, diff);
-			goto enospc;
-		}
+	valid_block_count =
+		sbi->total_valid_block_count + (block_t)count;
+	if (unlikely(valid_block_count > sbi->user_block_count)) {
+		spin_unlock(&sbi->stat_lock);
+		return false;
 	}
+	inode->i_blocks += count;
+	sbi->total_valid_block_count = valid_block_count;
+	sbi->alloc_valid_block_count += (block_t)count;
 	spin_unlock(&sbi->stat_lock);
-
-	if (release)
-		dquot_release_reservation_block(inode, release);
-	f2fs_i_blocks_write(inode, *count, true, true);
-	return 0;
-
-enospc:
-	dquot_release_reservation_block(inode, release);
-	return -ENOSPC;
+	return true;
 }
 
 static inline void dec_valid_block_count(struct f2fs_sb_info *sbi,
 						struct inode *inode,
-						block_t count)
+						blkcnt_t count)
 {
-	blkcnt_t sectors = count << F2FS_LOG_SECTORS_PER_BLOCK;
-
 	spin_lock(&sbi->stat_lock);
 	f2fs_bug_on(sbi, sbi->total_valid_block_count < (block_t) count);
-	f2fs_bug_on(sbi, inode->i_blocks < sectors);
+	f2fs_bug_on(sbi, inode->i_blocks < count);
+	inode->i_blocks -= count;
 	sbi->total_valid_block_count -= (block_t)count;
 	spin_unlock(&sbi->stat_lock);
-	f2fs_i_blocks_write(inode, count, false, true);
 }
 
 static inline void inc_page_count(struct f2fs_sb_info *sbi, int count_type)
 {
 	atomic_inc(&sbi->nr_pages[count_type]);
-
-	if (count_type == F2FS_DIRTY_DATA || count_type == F2FS_INMEM_PAGES ||
-		count_type == F2FS_WB_CP_DATA || count_type == F2FS_WB_DATA)
-		return;
-
-	set_sbi_flag(sbi, SBI_IS_DIRTY);
+	F2FS_SET_SB_DIRT(sbi);
 }
 
 static inline void inode_inc_dirty_pages(struct inode *inode)
 {
 	atomic_inc(&F2FS_I(inode)->dirty_pages);
-	inc_page_count(F2FS_I_SB(inode), S_ISDIR(inode->i_mode) ?
-				F2FS_DIRTY_DENTS : F2FS_DIRTY_DATA);
+	if (S_ISDIR(inode->i_mode))
+		inc_page_count(F2FS_I_SB(inode), F2FS_DIRTY_DENTS);
 }
 
 static inline void dec_page_count(struct f2fs_sb_info *sbi, int count_type)
@@ -1575,16 +786,16 @@ static inline void dec_page_count(struct f2fs_sb_info *sbi, int count_type)
 
 static inline void inode_dec_dirty_pages(struct inode *inode)
 {
-	if (!S_ISDIR(inode->i_mode) && !S_ISREG(inode->i_mode) &&
-			!S_ISLNK(inode->i_mode))
+	if (!S_ISDIR(inode->i_mode) && !S_ISREG(inode->i_mode))
 		return;
 
 	atomic_dec(&F2FS_I(inode)->dirty_pages);
-	dec_page_count(F2FS_I_SB(inode), S_ISDIR(inode->i_mode) ?
-				F2FS_DIRTY_DENTS : F2FS_DIRTY_DATA);
+
+	if (S_ISDIR(inode->i_mode))
+		dec_page_count(F2FS_I_SB(inode), F2FS_DIRTY_DENTS);
 }
 
-static inline s64 get_pages(struct f2fs_sb_info *sbi, int count_type)
+static inline int get_pages(struct f2fs_sb_info *sbi, int count_type)
 {
 	return atomic_read(&sbi->nr_pages[count_type]);
 }
@@ -1596,11 +807,10 @@ static inline int get_dirty_pages(struct inode *inode)
 
 static inline int get_blocktype_secs(struct f2fs_sb_info *sbi, int block_type)
 {
-	unsigned int pages_per_sec = sbi->segs_per_sec * sbi->blocks_per_seg;
-	unsigned int segs = (get_pages(sbi, block_type) + pages_per_sec - 1) >>
-						sbi->log_blocks_per_seg;
-
-	return segs / sbi->segs_per_sec;
+	unsigned int pages_per_sec = sbi->segs_per_sec *
+					(1 << sbi->log_blocks_per_seg);
+	return ((get_pages(sbi, block_type) + pages_per_sec - 1)
+			>> sbi->log_blocks_per_seg) / sbi->segs_per_sec;
 }
 
 static inline block_t valid_user_blocks(struct f2fs_sb_info *sbi)
@@ -1608,11 +818,6 @@ static inline block_t valid_user_blocks(struct f2fs_sb_info *sbi)
 	return sbi->total_valid_block_count;
 }
 
-static inline block_t discard_blocks(struct f2fs_sb_info *sbi)
-{
-	return sbi->discard_blks;
-}
-
 static inline unsigned long __bitmap_size(struct f2fs_sb_info *sbi, int flag)
 {
 	struct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);
@@ -1626,17 +831,12 @@ static inline unsigned long __bitmap_size(struct f2fs_sb_info *sbi, int flag)
 	return 0;
 }
 
-static inline block_t __cp_payload(struct f2fs_sb_info *sbi)
-{
-	return le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_payload);
-}
-
 static inline void *__bitmap_ptr(struct f2fs_sb_info *sbi, int flag)
 {
 	struct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);
 	int offset;
 
-	if (__cp_payload(sbi) > 0) {
+	if (le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_payload) > 0) {
 		if (flag == NAT_BITMAP)
 			return &ckpt->sit_nat_version_bitmap;
 		else
@@ -1650,25 +850,20 @@ static inline void *__bitmap_ptr(struct f2fs_sb_info *sbi, int flag)
 
 static inline block_t __start_cp_addr(struct f2fs_sb_info *sbi)
 {
-	block_t start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);
-
-	if (sbi->cur_cp_pack == 2)
-		start_addr += sbi->blocks_per_seg;
-	return start_addr;
-}
+	block_t start_addr;
+	struct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);
+	unsigned long long ckpt_version = cur_cp_version(ckpt);
 
-static inline block_t __start_cp_next_addr(struct f2fs_sb_info *sbi)
-{
-	block_t start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);
+	start_addr = le32_to_cpu(F2FS_RAW_SUPER(sbi)->cp_blkaddr);
 
-	if (sbi->cur_cp_pack == 1)
+	/*
+	 * odd numbered checkpoint should at cp segment 0
+	 * and even segment must be at cp segment 1
+	 */
+	if (!(ckpt_version & 1))
 		start_addr += sbi->blocks_per_seg;
-	return start_addr;
-}
 
-static inline void __set_cp_next_pack(struct f2fs_sb_info *sbi)
-{
-	sbi->cur_cp_pack = (sbi->cur_cp_pack == 1) ? 2 : 1;
+	return start_addr;
 }
 
 static inline block_t __start_sum_addr(struct f2fs_sb_info *sbi)
@@ -1676,70 +871,51 @@ static inline block_t __start_sum_addr(struct f2fs_sb_info *sbi)
 	return le32_to_cpu(F2FS_CKPT(sbi)->cp_pack_start_sum);
 }
 
-static inline int inc_valid_node_count(struct f2fs_sb_info *sbi,
-					struct inode *inode, bool is_inode)
+static inline bool inc_valid_node_count(struct f2fs_sb_info *sbi,
+						struct inode *inode)
 {
 	block_t	valid_block_count;
 	unsigned int valid_node_count;
-	bool quota = inode && !is_inode;
-
-	if (quota) {
-		int ret = dquot_reserve_block(inode, 1);
-		if (ret)
-			return ret;
-	}
 
 	spin_lock(&sbi->stat_lock);
 
 	valid_block_count = sbi->total_valid_block_count + 1;
-	if (unlikely(valid_block_count + sbi->reserved_blocks >
-						sbi->user_block_count)) {
+	if (unlikely(valid_block_count > sbi->user_block_count)) {
 		spin_unlock(&sbi->stat_lock);
-		goto enospc;
+		return false;
 	}
 
 	valid_node_count = sbi->total_valid_node_count + 1;
 	if (unlikely(valid_node_count > sbi->total_node_count)) {
 		spin_unlock(&sbi->stat_lock);
-		goto enospc;
+		return false;
 	}
 
+	if (inode)
+		inode->i_blocks++;
+
+	sbi->alloc_valid_block_count++;
 	sbi->total_valid_node_count++;
 	sbi->total_valid_block_count++;
 	spin_unlock(&sbi->stat_lock);
 
-	if (inode) {
-		if (is_inode)
-			f2fs_mark_inode_dirty_sync(inode, true);
-		else
-			f2fs_i_blocks_write(inode, 1, true, true);
-	}
-
-	percpu_counter_inc(&sbi->alloc_valid_block_count);
-	return 0;
-
-enospc:
-	if (quota)
-		dquot_release_reservation_block(inode, 1);
-	return -ENOSPC;
+	return true;
 }
 
 static inline void dec_valid_node_count(struct f2fs_sb_info *sbi,
-					struct inode *inode, bool is_inode)
+						struct inode *inode)
 {
 	spin_lock(&sbi->stat_lock);
 
 	f2fs_bug_on(sbi, !sbi->total_valid_block_count);
 	f2fs_bug_on(sbi, !sbi->total_valid_node_count);
-	f2fs_bug_on(sbi, !is_inode && !inode->i_blocks);
+	f2fs_bug_on(sbi, !inode->i_blocks);
 
+	inode->i_blocks--;
 	sbi->total_valid_node_count--;
 	sbi->total_valid_block_count--;
 
 	spin_unlock(&sbi->stat_lock);
-
-	if (!is_inode)
-		f2fs_i_blocks_write(inode, 1, false, true);
 }
 
 static inline unsigned int valid_node_count(struct f2fs_sb_info *sbi)
@@ -1749,46 +925,23 @@ static inline unsigned int valid_node_count(struct f2fs_sb_info *sbi)
 
 static inline void inc_valid_inode_count(struct f2fs_sb_info *sbi)
 {
-	percpu_counter_inc(&sbi->total_valid_inode_count);
+	spin_lock(&sbi->stat_lock);
+	f2fs_bug_on(sbi, sbi->total_valid_inode_count == sbi->total_node_count);
+	sbi->total_valid_inode_count++;
+	spin_unlock(&sbi->stat_lock);
 }
 
 static inline void dec_valid_inode_count(struct f2fs_sb_info *sbi)
 {
-	percpu_counter_dec(&sbi->total_valid_inode_count);
-}
-
-static inline s64 valid_inode_count(struct f2fs_sb_info *sbi)
-{
-	return percpu_counter_sum_positive(&sbi->total_valid_inode_count);
-}
-
-static inline struct page *f2fs_grab_cache_page(struct address_space *mapping,
-						pgoff_t index, bool for_write)
-{
-#ifdef CONFIG_F2FS_FAULT_INJECTION
-	struct page *page = find_lock_page(mapping, index);
-
-	if (page)
-		return page;
-
-	if (time_to_inject(F2FS_M_SB(mapping), FAULT_PAGE_ALLOC)) {
-		f2fs_show_injection_info(FAULT_PAGE_ALLOC);
-		return NULL;
-	}
-#endif
-	if (!for_write)
-		return grab_cache_page(mapping, index);
-	return grab_cache_page_write_begin(mapping, index, AOP_FLAG_NOFS);
+	spin_lock(&sbi->stat_lock);
+	f2fs_bug_on(sbi, !sbi->total_valid_inode_count);
+	sbi->total_valid_inode_count--;
+	spin_unlock(&sbi->stat_lock);
 }
 
-static inline void f2fs_copy_page(struct page *src, struct page *dst)
+static inline unsigned int valid_inode_count(struct f2fs_sb_info *sbi)
 {
-	char *src_kaddr = kmap(src);
-	char *dst_kaddr = kmap(dst);
-
-	memcpy(dst_kaddr, src_kaddr, PAGE_SIZE);
-	kunmap(dst);
-	kunmap(src);
+	return sbi->total_valid_inode_count;
 }
 
 static inline void f2fs_put_page(struct page *page, int unlock)
@@ -1800,7 +953,7 @@ static inline void f2fs_put_page(struct page *page, int unlock)
 		f2fs_bug_on(F2FS_P_SB(page), !PageLocked(page));
 		unlock_page(page);
 	}
-	put_page(page);
+	page_cache_release(page);
 }
 
 static inline void f2fs_put_dnode(struct dnode_of_data *dn)
@@ -1823,29 +976,14 @@ static inline void *f2fs_kmem_cache_alloc(struct kmem_cache *cachep,
 						gfp_t flags)
 {
 	void *entry;
-
+retry:
 	entry = kmem_cache_alloc(cachep, flags);
-	if (!entry)
-		entry = kmem_cache_alloc(cachep, flags | __GFP_NOFAIL);
-	return entry;
-}
-
-static inline struct bio *f2fs_bio_alloc(int npages)
-{
-	struct bio *bio;
-
-	/* No failure on bio allocation */
-	bio = bio_alloc(GFP_NOIO, npages);
-	if (!bio)
-		bio = bio_alloc(GFP_NOIO | __GFP_NOFAIL, npages);
-	return bio;
-}
-
-static inline void f2fs_radix_tree_insert(struct radix_tree_root *root,
-				unsigned long index, void *item)
-{
-	while (radix_tree_insert(root, index, item))
+	if (!entry) {
 		cond_resched();
+		goto retry;
+	}
+
+	return entry;
 }
 
 #define RAW_IS_INODE(p)	((p)->footer.nid == (p)->footer.ino)
@@ -1853,7 +991,6 @@ static inline void f2fs_radix_tree_insert(struct radix_tree_root *root,
 static inline bool IS_INODE(struct page *page)
 {
 	struct f2fs_node *p = F2FS_NODE(page);
-
 	return RAW_IS_INODE(p);
 }
 
@@ -1867,7 +1004,6 @@ static inline block_t datablock_addr(struct page *node_page,
 {
 	struct f2fs_node *raw_node;
 	__le32 *addr_array;
-
 	raw_node = F2FS_NODE(node_page);
 	addr_array = blkaddr_in_node(raw_node);
 	return le32_to_cpu(addr_array[offset]);
@@ -1882,25 +1018,7 @@ static inline int f2fs_test_bit(unsigned int nr, char *addr)
 	return mask & *addr;
 }
 
-static inline void f2fs_set_bit(unsigned int nr, char *addr)
-{
-	int mask;
-
-	addr += (nr >> 3);
-	mask = 1 << (7 - (nr & 0x07));
-	*addr |= mask;
-}
-
-static inline void f2fs_clear_bit(unsigned int nr, char *addr)
-{
-	int mask;
-
-	addr += (nr >> 3);
-	mask = 1 << (7 - (nr & 0x07));
-	*addr &= ~mask;
-}
-
-static inline int f2fs_test_and_set_bit(unsigned int nr, char *addr)
+static inline int f2fs_set_bit(unsigned int nr, char *addr)
 {
 	int mask;
 	int ret;
@@ -1912,7 +1030,7 @@ static inline int f2fs_test_and_set_bit(unsigned int nr, char *addr)
 	return ret;
 }
 
-static inline int f2fs_test_and_clear_bit(unsigned int nr, char *addr)
+static inline int f2fs_clear_bit(unsigned int nr, char *addr)
 {
 	int mask;
 	int ret;
@@ -1924,187 +1042,86 @@ static inline int f2fs_test_and_clear_bit(unsigned int nr, char *addr)
 	return ret;
 }
 
-static inline void f2fs_change_bit(unsigned int nr, char *addr)
-{
-	int mask;
-
-	addr += (nr >> 3);
-	mask = 1 << (7 - (nr & 0x07));
-	*addr ^= mask;
-}
-
 /* used for f2fs_inode_info->flags */
 enum {
 	FI_NEW_INODE,		/* indicate newly allocated inode */
 	FI_DIRTY_INODE,		/* indicate inode is dirty or not */
-	FI_AUTO_RECOVER,	/* indicate inode is recoverable */
 	FI_DIRTY_DIR,		/* indicate directory has dirty pages */
 	FI_INC_LINK,		/* need to increment i_nlink */
 	FI_ACL_MODE,		/* indicate acl mode */
 	FI_NO_ALLOC,		/* should not allocate any blocks */
-	FI_FREE_NID,		/* free allocated nide */
+	FI_UPDATE_DIR,		/* should update inode block for consistency */
+	FI_DELAY_IPUT,		/* used for the recovery */
 	FI_NO_EXTENT,		/* not to use the extent cache */
 	FI_INLINE_XATTR,	/* used for inline xattr */
 	FI_INLINE_DATA,		/* used for inline data*/
-	FI_INLINE_DENTRY,	/* used for inline dentry */
 	FI_APPEND_WRITE,	/* inode has appended data */
 	FI_UPDATE_WRITE,	/* inode has in-place-update data */
 	FI_NEED_IPU,		/* used for ipu per file */
 	FI_ATOMIC_FILE,		/* indicate atomic file */
-	FI_ATOMIC_COMMIT,	/* indicate the state of atomical committing */
 	FI_VOLATILE_FILE,	/* indicate volatile file */
-	FI_FIRST_BLOCK_WRITTEN,	/* indicate #0 data block was written */
-	FI_DROP_CACHE,		/* drop dirty page cache */
-	FI_DATA_EXIST,		/* indicate data exists */
-	FI_INLINE_DOTS,		/* indicate inline dot dentries */
-	FI_DO_DEFRAG,		/* indicate defragment is running */
-	FI_DIRTY_FILE,		/* indicate regular/symlink has dirty pages */
-	FI_HOT_DATA,		/* indicate file is hot */
 };
 
-static inline void __mark_inode_dirty_flag(struct inode *inode,
-						int flag, bool set)
-{
-	switch (flag) {
-	case FI_INLINE_XATTR:
-	case FI_INLINE_DATA:
-	case FI_INLINE_DENTRY:
-		if (set)
-			return;
-	case FI_DATA_EXIST:
-	case FI_INLINE_DOTS:
-		f2fs_mark_inode_dirty_sync(inode, true);
-	}
-}
-
-static inline void set_inode_flag(struct inode *inode, int flag)
-{
-	if (!test_bit(flag, &F2FS_I(inode)->flags))
-		set_bit(flag, &F2FS_I(inode)->flags);
-	__mark_inode_dirty_flag(inode, flag, true);
-}
-
-static inline int is_inode_flag_set(struct inode *inode, int flag)
+static inline void set_inode_flag(struct f2fs_inode_info *fi, int flag)
 {
-	return test_bit(flag, &F2FS_I(inode)->flags);
+	if (!test_bit(flag, &fi->flags))
+		set_bit(flag, &fi->flags);
 }
 
-static inline void clear_inode_flag(struct inode *inode, int flag)
+static inline int is_inode_flag_set(struct f2fs_inode_info *fi, int flag)
 {
-	if (test_bit(flag, &F2FS_I(inode)->flags))
-		clear_bit(flag, &F2FS_I(inode)->flags);
-	__mark_inode_dirty_flag(inode, flag, false);
+	return test_bit(flag, &fi->flags);
 }
 
-static inline void set_acl_inode(struct inode *inode, umode_t mode)
+static inline void clear_inode_flag(struct f2fs_inode_info *fi, int flag)
 {
-	F2FS_I(inode)->i_acl_mode = mode;
-	set_inode_flag(inode, FI_ACL_MODE);
-	f2fs_mark_inode_dirty_sync(inode, false);
+	if (test_bit(flag, &fi->flags))
+		clear_bit(flag, &fi->flags);
 }
 
-static inline void f2fs_i_links_write(struct inode *inode, bool inc)
+static inline void set_acl_inode(struct f2fs_inode_info *fi, umode_t mode)
 {
-	if (inc)
-		inc_nlink(inode);
-	else
-		drop_nlink(inode);
-	f2fs_mark_inode_dirty_sync(inode, true);
+	fi->i_acl_mode = mode;
+	set_inode_flag(fi, FI_ACL_MODE);
 }
 
-static inline void f2fs_i_blocks_write(struct inode *inode,
-					block_t diff, bool add, bool claim)
+static inline int cond_clear_inode_flag(struct f2fs_inode_info *fi, int flag)
 {
-	bool clean = !is_inode_flag_set(inode, FI_DIRTY_INODE);
-	bool recover = is_inode_flag_set(inode, FI_AUTO_RECOVER);
-
-	/* add = 1, claim = 1 should be dquot_reserve_block in pair */
-	if (add) {
-		if (claim)
-			dquot_claim_block(inode, diff);
-		else
-			dquot_alloc_block_nofail(inode, diff);
-	} else {
-		dquot_free_block(inode, diff);
+	if (is_inode_flag_set(fi, FI_ACL_MODE)) {
+		clear_inode_flag(fi, FI_ACL_MODE);
+		return 1;
 	}
-
-	f2fs_mark_inode_dirty_sync(inode, true);
-	if (clean || recover)
-		set_inode_flag(inode, FI_AUTO_RECOVER);
-}
-
-static inline void f2fs_i_size_write(struct inode *inode, loff_t i_size)
-{
-	bool clean = !is_inode_flag_set(inode, FI_DIRTY_INODE);
-	bool recover = is_inode_flag_set(inode, FI_AUTO_RECOVER);
-
-	if (i_size_read(inode) == i_size)
-		return;
-
-	i_size_write(inode, i_size);
-	f2fs_mark_inode_dirty_sync(inode, true);
-	if (clean || recover)
-		set_inode_flag(inode, FI_AUTO_RECOVER);
-}
-
-static inline void f2fs_i_depth_write(struct inode *inode, unsigned int depth)
-{
-	F2FS_I(inode)->i_current_depth = depth;
-	f2fs_mark_inode_dirty_sync(inode, true);
-}
-
-static inline void f2fs_i_xnid_write(struct inode *inode, nid_t xnid)
-{
-	F2FS_I(inode)->i_xattr_nid = xnid;
-	f2fs_mark_inode_dirty_sync(inode, true);
-}
-
-static inline void f2fs_i_pino_write(struct inode *inode, nid_t pino)
-{
-	F2FS_I(inode)->i_pino = pino;
-	f2fs_mark_inode_dirty_sync(inode, true);
+	return 0;
 }
 
-static inline void get_inline_info(struct inode *inode, struct f2fs_inode *ri)
+static inline void get_inline_info(struct f2fs_inode_info *fi,
+					struct f2fs_inode *ri)
 {
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-
 	if (ri->i_inline & F2FS_INLINE_XATTR)
-		set_bit(FI_INLINE_XATTR, &fi->flags);
+		set_inode_flag(fi, FI_INLINE_XATTR);
 	if (ri->i_inline & F2FS_INLINE_DATA)
-		set_bit(FI_INLINE_DATA, &fi->flags);
-	if (ri->i_inline & F2FS_INLINE_DENTRY)
-		set_bit(FI_INLINE_DENTRY, &fi->flags);
-	if (ri->i_inline & F2FS_DATA_EXIST)
-		set_bit(FI_DATA_EXIST, &fi->flags);
-	if (ri->i_inline & F2FS_INLINE_DOTS)
-		set_bit(FI_INLINE_DOTS, &fi->flags);
+		set_inode_flag(fi, FI_INLINE_DATA);
 }
 
-static inline void set_raw_inline(struct inode *inode, struct f2fs_inode *ri)
+static inline void set_raw_inline(struct f2fs_inode_info *fi,
+					struct f2fs_inode *ri)
 {
 	ri->i_inline = 0;
 
-	if (is_inode_flag_set(inode, FI_INLINE_XATTR))
+	if (is_inode_flag_set(fi, FI_INLINE_XATTR))
 		ri->i_inline |= F2FS_INLINE_XATTR;
-	if (is_inode_flag_set(inode, FI_INLINE_DATA))
+	if (is_inode_flag_set(fi, FI_INLINE_DATA))
 		ri->i_inline |= F2FS_INLINE_DATA;
-	if (is_inode_flag_set(inode, FI_INLINE_DENTRY))
-		ri->i_inline |= F2FS_INLINE_DENTRY;
-	if (is_inode_flag_set(inode, FI_DATA_EXIST))
-		ri->i_inline |= F2FS_DATA_EXIST;
-	if (is_inode_flag_set(inode, FI_INLINE_DOTS))
-		ri->i_inline |= F2FS_INLINE_DOTS;
 }
 
 static inline int f2fs_has_inline_xattr(struct inode *inode)
 {
-	return is_inode_flag_set(inode, FI_INLINE_XATTR);
+	return is_inode_flag_set(F2FS_I(inode), FI_INLINE_XATTR);
 }
 
-static inline unsigned int addrs_per_inode(struct inode *inode)
+static inline unsigned int addrs_per_inode(struct f2fs_inode_info *fi)
 {
-	if (f2fs_has_inline_xattr(inode))
+	if (f2fs_has_inline_xattr(&fi->vfs_inode))
 		return DEF_ADDRS_PER_INODE - F2FS_INLINE_XATTR_ADDRS;
 	return DEF_ADDRS_PER_INODE;
 }
@@ -2112,7 +1129,6 @@ static inline unsigned int addrs_per_inode(struct inode *inode)
 static inline void *inline_xattr_addr(struct page *page)
 {
 	struct f2fs_inode *ri = F2FS_INODE(page);
-
 	return (void *)&(ri->i_addr[DEF_ADDRS_PER_INODE -
 					F2FS_INLINE_XATTR_ADDRS]);
 }
@@ -2127,97 +1143,25 @@ static inline int inline_xattr_size(struct inode *inode)
 
 static inline int f2fs_has_inline_data(struct inode *inode)
 {
-	return is_inode_flag_set(inode, FI_INLINE_DATA);
-}
-
-static inline int f2fs_exist_data(struct inode *inode)
-{
-	return is_inode_flag_set(inode, FI_DATA_EXIST);
-}
-
-static inline int f2fs_has_inline_dots(struct inode *inode)
-{
-	return is_inode_flag_set(inode, FI_INLINE_DOTS);
+	return is_inode_flag_set(F2FS_I(inode), FI_INLINE_DATA);
 }
 
 static inline bool f2fs_is_atomic_file(struct inode *inode)
 {
-	return is_inode_flag_set(inode, FI_ATOMIC_FILE);
-}
-
-static inline bool f2fs_is_commit_atomic_write(struct inode *inode)
-{
-	return is_inode_flag_set(inode, FI_ATOMIC_COMMIT);
+	return is_inode_flag_set(F2FS_I(inode), FI_ATOMIC_FILE);
 }
 
 static inline bool f2fs_is_volatile_file(struct inode *inode)
 {
-	return is_inode_flag_set(inode, FI_VOLATILE_FILE);
-}
-
-static inline bool f2fs_is_first_block_written(struct inode *inode)
-{
-	return is_inode_flag_set(inode, FI_FIRST_BLOCK_WRITTEN);
-}
-
-static inline bool f2fs_is_drop_cache(struct inode *inode)
-{
-	return is_inode_flag_set(inode, FI_DROP_CACHE);
+	return is_inode_flag_set(F2FS_I(inode), FI_VOLATILE_FILE);
 }
 
 static inline void *inline_data_addr(struct page *page)
 {
 	struct f2fs_inode *ri = F2FS_INODE(page);
-
 	return (void *)&(ri->i_addr[1]);
 }
 
-static inline int f2fs_has_inline_dentry(struct inode *inode)
-{
-	return is_inode_flag_set(inode, FI_INLINE_DENTRY);
-}
-
-static inline void f2fs_dentry_kunmap(struct inode *dir, struct page *page)
-{
-	if (!f2fs_has_inline_dentry(dir))
-		kunmap(page);
-}
-
-static inline int is_file(struct inode *inode, int type)
-{
-	return F2FS_I(inode)->i_advise & type;
-}
-
-static inline void set_file(struct inode *inode, int type)
-{
-	F2FS_I(inode)->i_advise |= type;
-	f2fs_mark_inode_dirty_sync(inode, true);
-}
-
-static inline void clear_file(struct inode *inode, int type)
-{
-	F2FS_I(inode)->i_advise &= ~type;
-	f2fs_mark_inode_dirty_sync(inode, true);
-}
-
-static inline bool f2fs_skip_inode_update(struct inode *inode, int dsync)
-{
-	if (dsync) {
-		struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-		bool ret;
-
-		spin_lock(&sbi->inode_lock[DIRTY_META]);
-		ret = list_empty(&F2FS_I(inode)->gdirty_list);
-		spin_unlock(&sbi->inode_lock[DIRTY_META]);
-		return ret;
-	}
-	if (!is_inode_flag_set(inode, FI_AUTO_RECOVER) ||
-			file_keep_isize(inode) ||
-			i_size_read(inode) & PAGE_MASK)
-		return false;
-	return F2FS_I(inode)->last_disk_size == i_size_read(inode);
-}
-
 static inline int f2fs_readonly(struct super_block *sb)
 {
 	return sb->s_flags & MS_RDONLY;
@@ -2225,92 +1169,50 @@ static inline int f2fs_readonly(struct super_block *sb)
 
 static inline bool f2fs_cp_error(struct f2fs_sb_info *sbi)
 {
-	return is_set_ckpt_flags(sbi, CP_ERROR_FLAG);
-}
-
-static inline bool is_dot_dotdot(const struct qstr *str)
-{
-	if (str->len == 1 && str->name[0] == '.')
-		return true;
-
-	if (str->len == 2 && str->name[0] == '.' && str->name[1] == '.')
-		return true;
-
-	return false;
-}
-
-static inline bool f2fs_may_extent_tree(struct inode *inode)
-{
-	if (!test_opt(F2FS_I_SB(inode), EXTENT_CACHE) ||
-			is_inode_flag_set(inode, FI_NO_EXTENT))
-		return false;
-
-	return S_ISREG(inode->i_mode);
+	return is_set_ckpt_flags(sbi->ckpt, CP_ERROR_FLAG);
 }
 
-static inline void *f2fs_kmalloc(struct f2fs_sb_info *sbi,
-					size_t size, gfp_t flags)
+static inline void f2fs_stop_checkpoint(struct f2fs_sb_info *sbi)
 {
-#ifdef CONFIG_F2FS_FAULT_INJECTION
-	if (time_to_inject(sbi, FAULT_KMALLOC)) {
-		f2fs_show_injection_info(FAULT_KMALLOC);
-		return NULL;
-	}
-#endif
-	return kmalloc(size, flags);
-}
-
-static inline void *kvmalloc(size_t size, gfp_t flags)
-{
-	void *ret;
-
-	ret = kmalloc(size, flags | __GFP_NOWARN);
-	if (!ret)
-		ret = __vmalloc(size, flags, PAGE_KERNEL);
-	return ret;
-}
-
-static inline void *kvzalloc(size_t size, gfp_t flags)
-{
-	void *ret;
-
-	ret = kzalloc(size, flags | __GFP_NOWARN);
-	if (!ret)
-		ret = __vmalloc(size, flags | __GFP_ZERO, PAGE_KERNEL);
-	return ret;
+	set_ckpt_flags(sbi->ckpt, CP_ERROR_FLAG);
+	sbi->sb->s_flags |= MS_RDONLY;
 }
 
 #define get_inode_mode(i) \
-	((is_inode_flag_set(i, FI_ACL_MODE)) ? \
+	((is_inode_flag_set(F2FS_I(i), FI_ACL_MODE)) ? \
 	 (F2FS_I(i)->i_acl_mode) : ((i)->i_mode))
 
+/* get offset of first page in next direct node */
+#define PGOFS_OF_NEXT_DNODE(pgofs, fi)				\
+	((pgofs < ADDRS_PER_INODE(fi)) ? ADDRS_PER_INODE(fi) :	\
+	(pgofs - ADDRS_PER_INODE(fi) + ADDRS_PER_BLOCK) /	\
+	ADDRS_PER_BLOCK * ADDRS_PER_BLOCK + ADDRS_PER_INODE(fi))
+
 /*
  * file.c
  */
-int f2fs_sync_file(struct file *file, loff_t start, loff_t end, int datasync);
-void truncate_data_blocks(struct dnode_of_data *dn);
-int truncate_blocks(struct inode *inode, u64 from, bool lock);
-int f2fs_truncate(struct inode *inode);
-int f2fs_getattr(struct vfsmount *mnt, struct dentry *dentry,
-			struct kstat *stat);
-int f2fs_setattr(struct dentry *dentry, struct iattr *attr);
-int truncate_hole(struct inode *inode, pgoff_t pg_start, pgoff_t pg_end);
-int truncate_data_blocks_range(struct dnode_of_data *dn, int count);
-long f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
-long f2fs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+int f2fs_sync_file(struct file *, loff_t, loff_t, int);
+void truncate_data_blocks(struct dnode_of_data *);
+int truncate_blocks(struct inode *, u64, bool);
+void f2fs_truncate(struct inode *);
+int f2fs_getattr(struct vfsmount *, struct dentry *, struct kstat *);
+int f2fs_setattr(struct dentry *, struct iattr *);
+int truncate_hole(struct inode *, pgoff_t, pgoff_t);
+int truncate_data_blocks_range(struct dnode_of_data *, int);
+long f2fs_ioctl(struct file *, unsigned int, unsigned long);
+long f2fs_compat_ioctl(struct file *, unsigned int, unsigned long);
 
 /*
  * inode.c
  */
-void f2fs_set_inode_flags(struct inode *inode);
-struct inode *f2fs_iget(struct super_block *sb, unsigned long ino);
-struct inode *f2fs_iget_retry(struct super_block *sb, unsigned long ino);
-int try_to_free_nats(struct f2fs_sb_info *sbi, int nr_shrink);
-int update_inode(struct inode *inode, struct page *node_page);
-int update_inode_page(struct inode *inode);
-int f2fs_write_inode(struct inode *inode, struct writeback_control *wbc);
-void f2fs_evict_inode(struct inode *inode);
-void handle_failed_inode(struct inode *inode);
+void f2fs_set_inode_flags(struct inode *);
+struct inode *f2fs_iget(struct super_block *, unsigned long);
+int try_to_free_nats(struct f2fs_sb_info *, int);
+void update_inode(struct inode *, struct page *);
+void update_inode_page(struct inode *);
+int f2fs_write_inode(struct inode *, struct writeback_control *);
+void f2fs_evict_inode(struct inode *);
+void handle_failed_inode(struct inode *);
 
 /*
  * namei.c
@@ -2320,68 +1222,36 @@ struct dentry *f2fs_get_parent(struct dentry *child);
 /*
  * dir.c
  */
-void set_de_type(struct f2fs_dir_entry *de, umode_t mode);
-unsigned char get_de_type(struct f2fs_dir_entry *de);
-struct f2fs_dir_entry *find_target_dentry(struct fscrypt_name *fname,
-			f2fs_hash_t namehash, int *max_slots,
-			struct f2fs_dentry_ptr *d);
-int f2fs_fill_dentries(struct dir_context *ctx, struct f2fs_dentry_ptr *d,
-			unsigned int start_pos, struct fscrypt_str *fstr);
-void do_make_empty_dir(struct inode *inode, struct inode *parent,
-			struct f2fs_dentry_ptr *d);
-struct page *init_inode_metadata(struct inode *inode, struct inode *dir,
-			const struct qstr *new_name,
-			const struct qstr *orig_name, struct page *dpage);
-void update_parent_metadata(struct inode *dir, struct inode *inode,
-			unsigned int current_depth);
-int room_for_filename(const void *bitmap, int slots, int max_slots);
-void f2fs_drop_nlink(struct inode *dir, struct inode *inode);
-struct f2fs_dir_entry *__f2fs_find_entry(struct inode *dir,
-			struct fscrypt_name *fname, struct page **res_page);
-struct f2fs_dir_entry *f2fs_find_entry(struct inode *dir,
-			const struct qstr *child, struct page **res_page);
-struct f2fs_dir_entry *f2fs_parent_dir(struct inode *dir, struct page **p);
-ino_t f2fs_inode_by_name(struct inode *dir, const struct qstr *qstr,
-			struct page **page);
-void f2fs_set_link(struct inode *dir, struct f2fs_dir_entry *de,
-			struct page *page, struct inode *inode);
-void f2fs_update_dentry(nid_t ino, umode_t mode, struct f2fs_dentry_ptr *d,
-			const struct qstr *name, f2fs_hash_t name_hash,
-			unsigned int bit_pos);
-int f2fs_add_regular_entry(struct inode *dir, const struct qstr *new_name,
-			const struct qstr *orig_name,
-			struct inode *inode, nid_t ino, umode_t mode);
-int __f2fs_do_add_link(struct inode *dir, struct fscrypt_name *fname,
-			struct inode *inode, nid_t ino, umode_t mode);
-int __f2fs_add_link(struct inode *dir, const struct qstr *name,
-			struct inode *inode, nid_t ino, umode_t mode);
-void f2fs_delete_entry(struct f2fs_dir_entry *dentry, struct page *page,
-			struct inode *dir, struct inode *inode);
-int f2fs_do_tmpfile(struct inode *inode, struct inode *dir);
-bool f2fs_empty_dir(struct inode *dir);
+struct f2fs_dir_entry *f2fs_find_entry(struct inode *, struct qstr *,
+							struct page **);
+struct f2fs_dir_entry *f2fs_parent_dir(struct inode *, struct page **);
+ino_t f2fs_inode_by_name(struct inode *, struct qstr *);
+void f2fs_set_link(struct inode *, struct f2fs_dir_entry *,
+				struct page *, struct inode *);
+int update_dent_inode(struct inode *, const struct qstr *);
+int __f2fs_add_link(struct inode *, const struct qstr *, struct inode *);
+void f2fs_delete_entry(struct f2fs_dir_entry *, struct page *, struct inode *);
+int f2fs_do_tmpfile(struct inode *, struct inode *);
+int f2fs_make_empty(struct inode *, struct inode *);
+bool f2fs_empty_dir(struct inode *);
 
 static inline int f2fs_add_link(struct dentry *dentry, struct inode *inode)
 {
-	return __f2fs_add_link(d_inode(dentry->d_parent), &dentry->d_name,
-				inode, inode->i_ino, inode->i_mode);
+	return __f2fs_add_link(dentry->d_parent->d_inode, &dentry->d_name,
+				inode);
 }
 
 /*
  * super.c
  */
-int f2fs_inode_dirtied(struct inode *inode, bool sync);
-void f2fs_inode_synced(struct inode *inode);
-int f2fs_commit_super(struct f2fs_sb_info *sbi, bool recover);
-int f2fs_sync_fs(struct super_block *sb, int sync);
+int f2fs_sync_fs(struct super_block *, int);
 extern __printf(3, 4)
-void f2fs_msg(struct super_block *sb, const char *level, const char *fmt, ...);
-int sanity_check_ckpt(struct f2fs_sb_info *sbi);
+void f2fs_msg(struct super_block *, const char *, const char *, ...);
 
 /*
  * hash.c
  */
-f2fs_hash_t f2fs_dentry_hash(const struct qstr *name_info,
-				struct fscrypt_name *fname);
+f2fs_hash_t f2fs_dentry_hash(const struct qstr *);
 
 /*
  * node.c
@@ -2389,187 +1259,136 @@ f2fs_hash_t f2fs_dentry_hash(const struct qstr *name_info,
 struct dnode_of_data;
 struct node_info;
 
-bool available_free_memory(struct f2fs_sb_info *sbi, int type);
-int need_dentry_mark(struct f2fs_sb_info *sbi, nid_t nid);
-bool is_checkpointed_node(struct f2fs_sb_info *sbi, nid_t nid);
-bool need_inode_block_update(struct f2fs_sb_info *sbi, nid_t ino);
-void get_node_info(struct f2fs_sb_info *sbi, nid_t nid, struct node_info *ni);
-pgoff_t get_next_page_offset(struct dnode_of_data *dn, pgoff_t pgofs);
-int get_dnode_of_data(struct dnode_of_data *dn, pgoff_t index, int mode);
-int truncate_inode_blocks(struct inode *inode, pgoff_t from);
-int truncate_xattr_node(struct inode *inode, struct page *page);
-int wait_on_node_pages_writeback(struct f2fs_sb_info *sbi, nid_t ino);
-int remove_inode_page(struct inode *inode);
-struct page *new_inode_page(struct inode *inode);
-struct page *new_node_page(struct dnode_of_data *dn,
-			unsigned int ofs, struct page *ipage);
-void ra_node_page(struct f2fs_sb_info *sbi, nid_t nid);
-struct page *get_node_page(struct f2fs_sb_info *sbi, pgoff_t nid);
-struct page *get_node_page_ra(struct page *parent, int start);
-void move_node_page(struct page *node_page, int gc_type);
-int fsync_node_pages(struct f2fs_sb_info *sbi, struct inode *inode,
-			struct writeback_control *wbc, bool atomic);
-int sync_node_pages(struct f2fs_sb_info *sbi, struct writeback_control *wbc);
-void build_free_nids(struct f2fs_sb_info *sbi, bool sync, bool mount);
-bool alloc_nid(struct f2fs_sb_info *sbi, nid_t *nid);
-void alloc_nid_done(struct f2fs_sb_info *sbi, nid_t nid);
-void alloc_nid_failed(struct f2fs_sb_info *sbi, nid_t nid);
-int try_to_free_nids(struct f2fs_sb_info *sbi, int nr_shrink);
-void recover_inline_xattr(struct inode *inode, struct page *page);
-int recover_xattr_data(struct inode *inode, struct page *page,
-			block_t blkaddr);
-int recover_inode_page(struct f2fs_sb_info *sbi, struct page *page);
-int restore_node_summary(struct f2fs_sb_info *sbi,
-			unsigned int segno, struct f2fs_summary_block *sum);
-void flush_nat_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc);
-int build_node_manager(struct f2fs_sb_info *sbi);
-void destroy_node_manager(struct f2fs_sb_info *sbi);
+bool available_free_memory(struct f2fs_sb_info *, int);
+bool is_checkpointed_node(struct f2fs_sb_info *, nid_t);
+bool has_fsynced_inode(struct f2fs_sb_info *, nid_t);
+bool need_inode_block_update(struct f2fs_sb_info *, nid_t);
+void get_node_info(struct f2fs_sb_info *, nid_t, struct node_info *);
+int get_dnode_of_data(struct dnode_of_data *, pgoff_t, int);
+int truncate_inode_blocks(struct inode *, pgoff_t);
+int truncate_xattr_node(struct inode *, struct page *);
+int wait_on_node_pages_writeback(struct f2fs_sb_info *, nid_t);
+void remove_inode_page(struct inode *);
+struct page *new_inode_page(struct inode *);
+struct page *new_node_page(struct dnode_of_data *, unsigned int, struct page *);
+void ra_node_page(struct f2fs_sb_info *, nid_t);
+struct page *get_node_page(struct f2fs_sb_info *, pgoff_t);
+struct page *get_node_page_ra(struct page *, int);
+void sync_inode_page(struct dnode_of_data *);
+int sync_node_pages(struct f2fs_sb_info *, nid_t, struct writeback_control *);
+bool alloc_nid(struct f2fs_sb_info *, nid_t *);
+void alloc_nid_done(struct f2fs_sb_info *, nid_t);
+void alloc_nid_failed(struct f2fs_sb_info *, nid_t);
+void recover_inline_xattr(struct inode *, struct page *);
+void recover_xattr_data(struct inode *, struct page *, block_t);
+int recover_inode_page(struct f2fs_sb_info *, struct page *);
+int restore_node_summary(struct f2fs_sb_info *, unsigned int,
+				struct f2fs_summary_block *);
+void flush_nat_entries(struct f2fs_sb_info *);
+int build_node_manager(struct f2fs_sb_info *);
+void destroy_node_manager(struct f2fs_sb_info *);
 int __init create_node_manager_caches(void);
 void destroy_node_manager_caches(void);
 
 /*
  * segment.c
  */
-void register_inmem_page(struct inode *inode, struct page *page);
-void drop_inmem_pages(struct inode *inode);
-void drop_inmem_page(struct inode *inode, struct page *page);
-int commit_inmem_pages(struct inode *inode);
-void f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need);
-void f2fs_balance_fs_bg(struct f2fs_sb_info *sbi);
-int f2fs_issue_flush(struct f2fs_sb_info *sbi);
-int create_flush_cmd_control(struct f2fs_sb_info *sbi);
-void destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free);
-void invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr);
-bool is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr);
-void refresh_sit_entry(struct f2fs_sb_info *sbi, block_t old, block_t new);
-void stop_discard_thread(struct f2fs_sb_info *sbi);
-void f2fs_wait_discard_bios(struct f2fs_sb_info *sbi);
-void clear_prefree_segments(struct f2fs_sb_info *sbi, struct cp_control *cpc);
-void release_discard_addrs(struct f2fs_sb_info *sbi);
-int npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra);
-void allocate_new_segments(struct f2fs_sb_info *sbi);
-int f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range);
-bool exist_trim_candidates(struct f2fs_sb_info *sbi, struct cp_control *cpc);
-struct page *get_sum_page(struct f2fs_sb_info *sbi, unsigned int segno);
-void update_meta_page(struct f2fs_sb_info *sbi, void *src, block_t blk_addr);
-void write_meta_page(struct f2fs_sb_info *sbi, struct page *page);
-void write_node_page(unsigned int nid, struct f2fs_io_info *fio);
-void write_data_page(struct dnode_of_data *dn, struct f2fs_io_info *fio);
-int rewrite_data_page(struct f2fs_io_info *fio);
-void __f2fs_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
-			block_t old_blkaddr, block_t new_blkaddr,
-			bool recover_curseg, bool recover_newaddr);
-void f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,
-			block_t old_addr, block_t new_addr,
-			unsigned char version, bool recover_curseg,
-			bool recover_newaddr);
-void allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,
-			block_t old_blkaddr, block_t *new_blkaddr,
-			struct f2fs_summary *sum, int type,
-			struct f2fs_io_info *fio, bool add_list);
-void f2fs_wait_on_page_writeback(struct page *page,
-			enum page_type type, bool ordered);
-void f2fs_wait_on_encrypted_page_writeback(struct f2fs_sb_info *sbi,
-			block_t blkaddr);
-void write_data_summaries(struct f2fs_sb_info *sbi, block_t start_blk);
-void write_node_summaries(struct f2fs_sb_info *sbi, block_t start_blk);
-int lookup_journal_in_cursum(struct f2fs_journal *journal, int type,
-			unsigned int val, int alloc);
-void flush_sit_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc);
-int build_segment_manager(struct f2fs_sb_info *sbi);
-void destroy_segment_manager(struct f2fs_sb_info *sbi);
+void register_inmem_page(struct inode *, struct page *);
+void commit_inmem_pages(struct inode *, bool);
+void f2fs_balance_fs(struct f2fs_sb_info *);
+void f2fs_balance_fs_bg(struct f2fs_sb_info *);
+int f2fs_issue_flush(struct f2fs_sb_info *);
+int create_flush_cmd_control(struct f2fs_sb_info *);
+void destroy_flush_cmd_control(struct f2fs_sb_info *);
+void invalidate_blocks(struct f2fs_sb_info *, block_t);
+void refresh_sit_entry(struct f2fs_sb_info *, block_t, block_t);
+void clear_prefree_segments(struct f2fs_sb_info *);
+void release_discard_addrs(struct f2fs_sb_info *);
+void discard_next_dnode(struct f2fs_sb_info *, block_t);
+int npages_for_summary_flush(struct f2fs_sb_info *);
+void allocate_new_segments(struct f2fs_sb_info *);
+int f2fs_trim_fs(struct f2fs_sb_info *, struct fstrim_range *);
+struct page *get_sum_page(struct f2fs_sb_info *, unsigned int);
+void write_meta_page(struct f2fs_sb_info *, struct page *);
+void write_node_page(struct f2fs_sb_info *, struct page *,
+		struct f2fs_io_info *, unsigned int, block_t, block_t *);
+void write_data_page(struct page *, struct dnode_of_data *, block_t *,
+					struct f2fs_io_info *);
+void rewrite_data_page(struct page *, block_t, struct f2fs_io_info *);
+void recover_data_page(struct f2fs_sb_info *, struct page *,
+				struct f2fs_summary *, block_t, block_t);
+void allocate_data_block(struct f2fs_sb_info *, struct page *,
+		block_t, block_t *, struct f2fs_summary *, int);
+void f2fs_wait_on_page_writeback(struct page *, enum page_type);
+void write_data_summaries(struct f2fs_sb_info *, block_t);
+void write_node_summaries(struct f2fs_sb_info *, block_t);
+int lookup_journal_in_cursum(struct f2fs_summary_block *,
+					int, unsigned int, int);
+void flush_sit_entries(struct f2fs_sb_info *, struct cp_control *);
+int build_segment_manager(struct f2fs_sb_info *);
+void destroy_segment_manager(struct f2fs_sb_info *);
 int __init create_segment_manager_caches(void);
 void destroy_segment_manager_caches(void);
 
 /*
  * checkpoint.c
  */
-void f2fs_stop_checkpoint(struct f2fs_sb_info *sbi, bool end_io);
-struct page *grab_meta_page(struct f2fs_sb_info *sbi, pgoff_t index);
-struct page *get_meta_page(struct f2fs_sb_info *sbi, pgoff_t index);
-struct page *get_tmp_page(struct f2fs_sb_info *sbi, pgoff_t index);
-bool is_valid_blkaddr(struct f2fs_sb_info *sbi, block_t blkaddr, int type);
-int ra_meta_pages(struct f2fs_sb_info *sbi, block_t start, int nrpages,
-			int type, bool sync);
-void ra_meta_pages_cond(struct f2fs_sb_info *sbi, pgoff_t index);
-long sync_meta_pages(struct f2fs_sb_info *sbi, enum page_type type,
-			long nr_to_write);
-void add_ino_entry(struct f2fs_sb_info *sbi, nid_t ino, int type);
-void remove_ino_entry(struct f2fs_sb_info *sbi, nid_t ino, int type);
-void release_ino_entry(struct f2fs_sb_info *sbi, bool all);
-bool exist_written_data(struct f2fs_sb_info *sbi, nid_t ino, int mode);
-int f2fs_sync_inode_meta(struct f2fs_sb_info *sbi);
-int acquire_orphan_inode(struct f2fs_sb_info *sbi);
-void release_orphan_inode(struct f2fs_sb_info *sbi);
-void add_orphan_inode(struct inode *inode);
-void remove_orphan_inode(struct f2fs_sb_info *sbi, nid_t ino);
-int recover_orphan_inodes(struct f2fs_sb_info *sbi);
-int get_valid_checkpoint(struct f2fs_sb_info *sbi);
-void update_dirty_page(struct inode *inode, struct page *page);
-void remove_dirty_inode(struct inode *inode);
-int sync_dirty_inodes(struct f2fs_sb_info *sbi, enum inode_type type);
-int write_checkpoint(struct f2fs_sb_info *sbi, struct cp_control *cpc);
-void init_ino_entry_info(struct f2fs_sb_info *sbi);
+struct page *grab_meta_page(struct f2fs_sb_info *, pgoff_t);
+struct page *get_meta_page(struct f2fs_sb_info *, pgoff_t);
+struct page *get_meta_page_ra(struct f2fs_sb_info *, pgoff_t);
+int ra_meta_pages(struct f2fs_sb_info *, block_t, int, int);
+long sync_meta_pages(struct f2fs_sb_info *, enum page_type, long);
+void add_dirty_inode(struct f2fs_sb_info *, nid_t, int type);
+void remove_dirty_inode(struct f2fs_sb_info *, nid_t, int type);
+void release_dirty_inode(struct f2fs_sb_info *);
+bool exist_written_data(struct f2fs_sb_info *, nid_t, int);
+int acquire_orphan_inode(struct f2fs_sb_info *);
+void release_orphan_inode(struct f2fs_sb_info *);
+void add_orphan_inode(struct f2fs_sb_info *, nid_t);
+void remove_orphan_inode(struct f2fs_sb_info *, nid_t);
+void recover_orphan_inodes(struct f2fs_sb_info *);
+int get_valid_checkpoint(struct f2fs_sb_info *);
+void update_dirty_page(struct inode *, struct page *);
+void add_dirty_dir_inode(struct inode *);
+void remove_dirty_dir_inode(struct inode *);
+void sync_dirty_dir_inodes(struct f2fs_sb_info *);
+void write_checkpoint(struct f2fs_sb_info *, struct cp_control *);
+void init_ino_entry_info(struct f2fs_sb_info *);
 int __init create_checkpoint_caches(void);
 void destroy_checkpoint_caches(void);
 
 /*
  * data.c
  */
-void f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type);
-void f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,
-				struct inode *inode, nid_t ino, pgoff_t idx,
-				enum page_type type);
-void f2fs_flush_merged_writes(struct f2fs_sb_info *sbi);
-int f2fs_submit_page_bio(struct f2fs_io_info *fio);
-int f2fs_submit_page_write(struct f2fs_io_info *fio);
-struct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,
-			block_t blk_addr, struct bio *bio);
-int f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr);
-void set_data_blkaddr(struct dnode_of_data *dn);
-void f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr);
-int reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count);
-int reserve_new_block(struct dnode_of_data *dn);
-int f2fs_get_block(struct dnode_of_data *dn, pgoff_t index);
-int f2fs_preallocate_blocks(struct inode *inode, loff_t pos,
-					size_t count, bool dio);
-int f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index);
-struct page *get_read_data_page(struct inode *inode, pgoff_t index,
-			int op_flags, bool for_write);
-struct page *find_data_page(struct inode *inode, pgoff_t index);
-struct page *get_lock_data_page(struct inode *inode, pgoff_t index,
-			bool for_write);
-struct page *get_new_data_page(struct inode *inode,
-			struct page *ipage, pgoff_t index, bool new_i_size);
-int do_write_data_page(struct f2fs_io_info *fio);
-int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map,
-			int create, int flag);
-int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
-			u64 start, u64 len);
-void f2fs_set_page_dirty_nobuffers(struct page *page);
-void f2fs_invalidate_page(struct page *page, unsigned int offset,
-			unsigned int length);
-int f2fs_release_page(struct page *page, gfp_t wait);
-#ifdef CONFIG_MIGRATION
-int f2fs_migrate_page(struct address_space *mapping, struct page *newpage,
-			struct page *page, enum migrate_mode mode);
-#endif
+void f2fs_submit_merged_bio(struct f2fs_sb_info *, enum page_type, int);
+int f2fs_submit_page_bio(struct f2fs_sb_info *, struct page *, block_t, int);
+void f2fs_submit_page_mbio(struct f2fs_sb_info *, struct page *, block_t,
+						struct f2fs_io_info *);
+int reserve_new_block(struct dnode_of_data *);
+int f2fs_reserve_block(struct dnode_of_data *, pgoff_t);
+void update_extent_cache(block_t, struct dnode_of_data *);
+struct page *find_data_page(struct inode *, pgoff_t, bool);
+struct page *get_lock_data_page(struct inode *, pgoff_t);
+struct page *get_new_data_page(struct inode *, struct page *, pgoff_t, bool);
+int do_write_data_page(struct page *, struct f2fs_io_info *);
+int f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *, u64, u64);
 
 /*
  * gc.c
  */
-int start_gc_thread(struct f2fs_sb_info *sbi);
-void stop_gc_thread(struct f2fs_sb_info *sbi);
-block_t start_bidx_of_node(unsigned int node_ofs, struct inode *inode);
-int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background,
-			unsigned int segno);
-void build_gc_manager(struct f2fs_sb_info *sbi);
+int start_gc_thread(struct f2fs_sb_info *);
+void stop_gc_thread(struct f2fs_sb_info *);
+block_t start_bidx_of_node(unsigned int, struct f2fs_inode_info *);
+int f2fs_gc(struct f2fs_sb_info *);
+void build_gc_manager(struct f2fs_sb_info *);
+int __init create_gc_caches(void);
+void destroy_gc_caches(void);
 
 /*
  * recovery.c
  */
-int recover_fsync_data(struct f2fs_sb_info *sbi, bool check_only);
-bool space_for_roll_forward(struct f2fs_sb_info *sbi);
+int recover_fsync_data(struct f2fs_sb_info *);
+bool space_for_roll_forward(struct f2fs_sb_info *);
 
 /*
  * debug.c
@@ -2580,39 +1399,26 @@ struct f2fs_stat_info {
 	struct f2fs_sb_info *sbi;
 	int all_area_segs, sit_area_segs, nat_area_segs, ssa_area_segs;
 	int main_area_segs, main_area_sections, main_area_zones;
-	unsigned long long hit_largest, hit_cached, hit_rbtree;
-	unsigned long long hit_total, total_ext;
-	int ext_tree, zombie_tree, ext_node;
-	int ndirty_node, ndirty_dent, ndirty_meta, ndirty_data, ndirty_imeta;
-	int inmem_pages;
-	unsigned int ndirty_dirs, ndirty_files, ndirty_all;
-	int nats, dirty_nats, sits, dirty_sits;
-	int free_nids, avail_nids, alloc_nids;
+	int hit_ext, total_ext;
+	int ndirty_node, ndirty_dent, ndirty_dirs, ndirty_meta;
+	int nats, sits, fnids;
 	int total_count, utilization;
-	int bg_gc, nr_wb_cp_data, nr_wb_data;
-	int nr_flushing, nr_flushed, nr_discarding, nr_discarded;
-	int nr_discard_cmd;
-	unsigned int undiscard_blks;
-	int inline_xattr, inline_inode, inline_dir, append, update, orphans;
-	int aw_cnt, max_aw_cnt, vw_cnt, max_vw_cnt;
-	unsigned int valid_count, valid_node_count, valid_inode_count, discard_blks;
+	int bg_gc, inline_inode;
+	unsigned int valid_count, valid_node_count, valid_inode_count;
 	unsigned int bimodal, avg_vblocks;
 	int util_free, util_valid, util_invalid;
 	int rsvd_segs, overp_segs;
 	int dirty_count, node_pages, meta_pages;
-	int prefree_count, call_count, cp_count, bg_cp_count;
+	int prefree_count, call_count, cp_count;
 	int tot_segs, node_segs, data_segs, free_segs, free_secs;
-	int bg_node_segs, bg_data_segs;
 	int tot_blks, data_blks, node_blks;
-	int bg_data_blks, bg_node_blks;
 	int curseg[NR_CURSEG_TYPE];
 	int cursec[NR_CURSEG_TYPE];
 	int curzone[NR_CURSEG_TYPE];
 
 	unsigned int segment_count[2];
 	unsigned int block_count[2];
-	unsigned int inplace_count;
-	unsigned long long base_mem, cache_mem, page_mem;
+	unsigned base_mem, cache_mem;
 };
 
 static inline struct f2fs_stat_info *F2FS_STAT(struct f2fs_sb_info *sbi)
@@ -2621,143 +1427,79 @@ static inline struct f2fs_stat_info *F2FS_STAT(struct f2fs_sb_info *sbi)
 }
 
 #define stat_inc_cp_count(si)		((si)->cp_count++)
-#define stat_inc_bg_cp_count(si)	((si)->bg_cp_count++)
 #define stat_inc_call_count(si)		((si)->call_count++)
 #define stat_inc_bggc_count(sbi)	((sbi)->bg_gc++)
-#define stat_inc_dirty_inode(sbi, type)	((sbi)->ndirty_inode[type]++)
-#define stat_dec_dirty_inode(sbi, type)	((sbi)->ndirty_inode[type]--)
-#define stat_inc_total_hit(sbi)		(atomic64_inc(&(sbi)->total_hit_ext))
-#define stat_inc_rbtree_node_hit(sbi)	(atomic64_inc(&(sbi)->read_hit_rbtree))
-#define stat_inc_largest_node_hit(sbi)	(atomic64_inc(&(sbi)->read_hit_largest))
-#define stat_inc_cached_node_hit(sbi)	(atomic64_inc(&(sbi)->read_hit_cached))
-#define stat_inc_inline_xattr(inode)					\
-	do {								\
-		if (f2fs_has_inline_xattr(inode))			\
-			(atomic_inc(&F2FS_I_SB(inode)->inline_xattr));	\
-	} while (0)
-#define stat_dec_inline_xattr(inode)					\
-	do {								\
-		if (f2fs_has_inline_xattr(inode))			\
-			(atomic_dec(&F2FS_I_SB(inode)->inline_xattr));	\
-	} while (0)
+#define stat_inc_dirty_dir(sbi)		((sbi)->n_dirty_dirs++)
+#define stat_dec_dirty_dir(sbi)		((sbi)->n_dirty_dirs--)
+#define stat_inc_total_hit(sb)		((F2FS_SB(sb))->total_hit_ext++)
+#define stat_inc_read_hit(sb)		((F2FS_SB(sb))->read_hit_ext++)
 #define stat_inc_inline_inode(inode)					\
 	do {								\
 		if (f2fs_has_inline_data(inode))			\
-			(atomic_inc(&F2FS_I_SB(inode)->inline_inode));	\
+			((F2FS_I_SB(inode))->inline_inode++);		\
 	} while (0)
 #define stat_dec_inline_inode(inode)					\
 	do {								\
 		if (f2fs_has_inline_data(inode))			\
-			(atomic_dec(&F2FS_I_SB(inode)->inline_inode));	\
-	} while (0)
-#define stat_inc_inline_dir(inode)					\
-	do {								\
-		if (f2fs_has_inline_dentry(inode))			\
-			(atomic_inc(&F2FS_I_SB(inode)->inline_dir));	\
-	} while (0)
-#define stat_dec_inline_dir(inode)					\
-	do {								\
-		if (f2fs_has_inline_dentry(inode))			\
-			(atomic_dec(&F2FS_I_SB(inode)->inline_dir));	\
+			((F2FS_I_SB(inode))->inline_inode--);		\
 	} while (0)
+
 #define stat_inc_seg_type(sbi, curseg)					\
 		((sbi)->segment_count[(curseg)->alloc_type]++)
 #define stat_inc_block_count(sbi, curseg)				\
 		((sbi)->block_count[(curseg)->alloc_type]++)
-#define stat_inc_inplace_blocks(sbi)					\
-		(atomic_inc(&(sbi)->inplace_count))
-#define stat_inc_atomic_write(inode)					\
-		(atomic_inc(&F2FS_I_SB(inode)->aw_cnt))
-#define stat_dec_atomic_write(inode)					\
-		(atomic_dec(&F2FS_I_SB(inode)->aw_cnt))
-#define stat_update_max_atomic_write(inode)				\
-	do {								\
-		int cur = atomic_read(&F2FS_I_SB(inode)->aw_cnt);	\
-		int max = atomic_read(&F2FS_I_SB(inode)->max_aw_cnt);	\
-		if (cur > max)						\
-			atomic_set(&F2FS_I_SB(inode)->max_aw_cnt, cur);	\
-	} while (0)
-#define stat_inc_volatile_write(inode)					\
-		(atomic_inc(&F2FS_I_SB(inode)->vw_cnt))
-#define stat_dec_volatile_write(inode)					\
-		(atomic_dec(&F2FS_I_SB(inode)->vw_cnt))
-#define stat_update_max_volatile_write(inode)				\
-	do {								\
-		int cur = atomic_read(&F2FS_I_SB(inode)->vw_cnt);	\
-		int max = atomic_read(&F2FS_I_SB(inode)->max_vw_cnt);	\
-		if (cur > max)						\
-			atomic_set(&F2FS_I_SB(inode)->max_vw_cnt, cur);	\
-	} while (0)
-#define stat_inc_seg_count(sbi, type, gc_type)				\
+
+#define stat_inc_seg_count(sbi, type)					\
 	do {								\
 		struct f2fs_stat_info *si = F2FS_STAT(sbi);		\
-		si->tot_segs++;						\
-		if ((type) == SUM_TYPE_DATA) {				\
+		(si)->tot_segs++;					\
+		if (type == SUM_TYPE_DATA)				\
 			si->data_segs++;				\
-			si->bg_data_segs += (gc_type == BG_GC) ? 1 : 0;	\
-		} else {						\
+		else							\
 			si->node_segs++;				\
-			si->bg_node_segs += (gc_type == BG_GC) ? 1 : 0;	\
-		}							\
 	} while (0)
 
 #define stat_inc_tot_blk_count(si, blks)				\
-	((si)->tot_blks += (blks))
+	(si->tot_blks += (blks))
 
-#define stat_inc_data_blk_count(sbi, blks, gc_type)			\
+#define stat_inc_data_blk_count(sbi, blks)				\
 	do {								\
 		struct f2fs_stat_info *si = F2FS_STAT(sbi);		\
 		stat_inc_tot_blk_count(si, blks);			\
 		si->data_blks += (blks);				\
-		si->bg_data_blks += ((gc_type) == BG_GC) ? (blks) : 0;	\
 	} while (0)
 
-#define stat_inc_node_blk_count(sbi, blks, gc_type)			\
+#define stat_inc_node_blk_count(sbi, blks)				\
 	do {								\
 		struct f2fs_stat_info *si = F2FS_STAT(sbi);		\
 		stat_inc_tot_blk_count(si, blks);			\
 		si->node_blks += (blks);				\
-		si->bg_node_blks += ((gc_type) == BG_GC) ? (blks) : 0;	\
 	} while (0)
 
-int f2fs_build_stats(struct f2fs_sb_info *sbi);
-void f2fs_destroy_stats(struct f2fs_sb_info *sbi);
-int __init f2fs_create_root_stats(void);
+int f2fs_build_stats(struct f2fs_sb_info *);
+void f2fs_destroy_stats(struct f2fs_sb_info *);
+void __init f2fs_create_root_stats(void);
 void f2fs_destroy_root_stats(void);
 #else
-#define stat_inc_cp_count(si)				do { } while (0)
-#define stat_inc_bg_cp_count(si)			do { } while (0)
-#define stat_inc_call_count(si)				do { } while (0)
-#define stat_inc_bggc_count(si)				do { } while (0)
-#define stat_inc_dirty_inode(sbi, type)			do { } while (0)
-#define stat_dec_dirty_inode(sbi, type)			do { } while (0)
-#define stat_inc_total_hit(sb)				do { } while (0)
-#define stat_inc_rbtree_node_hit(sb)			do { } while (0)
-#define stat_inc_largest_node_hit(sbi)			do { } while (0)
-#define stat_inc_cached_node_hit(sbi)			do { } while (0)
-#define stat_inc_inline_xattr(inode)			do { } while (0)
-#define stat_dec_inline_xattr(inode)			do { } while (0)
-#define stat_inc_inline_inode(inode)			do { } while (0)
-#define stat_dec_inline_inode(inode)			do { } while (0)
-#define stat_inc_inline_dir(inode)			do { } while (0)
-#define stat_dec_inline_dir(inode)			do { } while (0)
-#define stat_inc_atomic_write(inode)			do { } while (0)
-#define stat_dec_atomic_write(inode)			do { } while (0)
-#define stat_update_max_atomic_write(inode)		do { } while (0)
-#define stat_inc_volatile_write(inode)			do { } while (0)
-#define stat_dec_volatile_write(inode)			do { } while (0)
-#define stat_update_max_volatile_write(inode)		do { } while (0)
-#define stat_inc_seg_type(sbi, curseg)			do { } while (0)
-#define stat_inc_block_count(sbi, curseg)		do { } while (0)
-#define stat_inc_inplace_blocks(sbi)			do { } while (0)
-#define stat_inc_seg_count(sbi, type, gc_type)		do { } while (0)
-#define stat_inc_tot_blk_count(si, blks)		do { } while (0)
-#define stat_inc_data_blk_count(sbi, blks, gc_type)	do { } while (0)
-#define stat_inc_node_blk_count(sbi, blks, gc_type)	do { } while (0)
+#define stat_inc_cp_count(si)
+#define stat_inc_call_count(si)
+#define stat_inc_bggc_count(si)
+#define stat_inc_dirty_dir(sbi)
+#define stat_dec_dirty_dir(sbi)
+#define stat_inc_total_hit(sb)
+#define stat_inc_read_hit(sb)
+#define stat_inc_inline_inode(inode)
+#define stat_dec_inline_inode(inode)
+#define stat_inc_seg_type(sbi, curseg)
+#define stat_inc_block_count(sbi, curseg)
+#define stat_inc_seg_count(si, type)
+#define stat_inc_tot_blk_count(si, blks)
+#define stat_inc_data_blk_count(si, blks)
+#define stat_inc_node_blk_count(sbi, blks)
 
 static inline int f2fs_build_stats(struct f2fs_sb_info *sbi) { return 0; }
 static inline void f2fs_destroy_stats(struct f2fs_sb_info *sbi) { }
-static inline int __init f2fs_create_root_stats(void) { return 0; }
+static inline void __init f2fs_create_root_stats(void) { }
 static inline void f2fs_destroy_root_stats(void) { }
 #endif
 
@@ -2769,160 +1511,15 @@ extern const struct address_space_operations f2fs_node_aops;
 extern const struct address_space_operations f2fs_meta_aops;
 extern const struct inode_operations f2fs_dir_inode_operations;
 extern const struct inode_operations f2fs_symlink_inode_operations;
-extern const struct inode_operations f2fs_encrypted_symlink_inode_operations;
 extern const struct inode_operations f2fs_special_inode_operations;
-extern struct kmem_cache *inode_entry_slab;
 
 /*
  * inline.c
  */
-bool f2fs_may_inline_data(struct inode *inode);
-bool f2fs_may_inline_dentry(struct inode *inode);
-void read_inline_data(struct page *page, struct page *ipage);
-void truncate_inline_inode(struct inode *inode, struct page *ipage, u64 from);
-int f2fs_read_inline_data(struct inode *inode, struct page *page);
-int f2fs_convert_inline_page(struct dnode_of_data *dn, struct page *page);
-int f2fs_convert_inline_inode(struct inode *inode);
-int f2fs_write_inline_data(struct inode *inode, struct page *page);
-bool recover_inline_data(struct inode *inode, struct page *npage);
-struct f2fs_dir_entry *find_in_inline_dir(struct inode *dir,
-			struct fscrypt_name *fname, struct page **res_page);
-int make_empty_inline_dir(struct inode *inode, struct inode *parent,
-			struct page *ipage);
-int f2fs_add_inline_entry(struct inode *dir, const struct qstr *new_name,
-			const struct qstr *orig_name,
-			struct inode *inode, nid_t ino, umode_t mode);
-void f2fs_delete_inline_entry(struct f2fs_dir_entry *dentry, struct page *page,
-			struct inode *dir, struct inode *inode);
-bool f2fs_empty_inline_dir(struct inode *dir);
-int f2fs_read_inline_dir(struct file *file, struct dir_context *ctx,
-			struct fscrypt_str *fstr);
-int f2fs_inline_data_fiemap(struct inode *inode,
-			struct fiemap_extent_info *fieinfo,
-			__u64 start, __u64 len);
-
-/*
- * shrinker.c
- */
-unsigned long f2fs_shrink_count(struct shrinker *shrink,
-			struct shrink_control *sc);
-unsigned long f2fs_shrink_scan(struct shrinker *shrink,
-			struct shrink_control *sc);
-void f2fs_join_shrinker(struct f2fs_sb_info *sbi);
-void f2fs_leave_shrinker(struct f2fs_sb_info *sbi);
-
-/*
- * extent_cache.c
- */
-struct rb_entry *__lookup_rb_tree(struct rb_root *root,
-				struct rb_entry *cached_re, unsigned int ofs);
-struct rb_node **__lookup_rb_tree_for_insert(struct f2fs_sb_info *sbi,
-				struct rb_root *root, struct rb_node **parent,
-				unsigned int ofs);
-struct rb_entry *__lookup_rb_tree_ret(struct rb_root *root,
-		struct rb_entry *cached_re, unsigned int ofs,
-		struct rb_entry **prev_entry, struct rb_entry **next_entry,
-		struct rb_node ***insert_p, struct rb_node **insert_parent,
-		bool force);
-bool __check_rb_tree_consistence(struct f2fs_sb_info *sbi,
-						struct rb_root *root);
-unsigned int f2fs_shrink_extent_tree(struct f2fs_sb_info *sbi, int nr_shrink);
-bool f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext);
-void f2fs_drop_extent_tree(struct inode *inode);
-unsigned int f2fs_destroy_extent_node(struct inode *inode);
-void f2fs_destroy_extent_tree(struct inode *inode);
-bool f2fs_lookup_extent_cache(struct inode *inode, pgoff_t pgofs,
-			struct extent_info *ei);
-void f2fs_update_extent_cache(struct dnode_of_data *dn);
-void f2fs_update_extent_cache_range(struct dnode_of_data *dn,
-			pgoff_t fofs, block_t blkaddr, unsigned int len);
-void init_extent_cache_info(struct f2fs_sb_info *sbi);
-int __init create_extent_cache(void);
-void destroy_extent_cache(void);
-
-/*
- * sysfs.c
- */
-int __init f2fs_register_sysfs(void);
-void f2fs_unregister_sysfs(void);
-int f2fs_init_sysfs(struct f2fs_sb_info *sbi);
-void f2fs_exit_sysfs(struct f2fs_sb_info *sbi);
-
-/*
- * crypto support
- */
-static inline bool f2fs_encrypted_inode(struct inode *inode)
-{
-	return file_is_encrypt(inode);
-}
-
-static inline void f2fs_set_encrypted_inode(struct inode *inode)
-{
-#ifdef CONFIG_F2FS_FS_ENCRYPTION
-	file_set_encrypt(inode);
-#endif
-}
-
-static inline bool f2fs_bio_encrypted(struct bio *bio)
-{
-	return bio->bi_private != NULL;
-}
-
-static inline int f2fs_sb_has_crypto(struct super_block *sb)
-{
-	return F2FS_HAS_FEATURE(sb, F2FS_FEATURE_ENCRYPT);
-}
-
-static inline int f2fs_sb_mounted_blkzoned(struct super_block *sb)
-{
-	return F2FS_HAS_FEATURE(sb, F2FS_FEATURE_BLKZONED);
-}
-
-#ifdef CONFIG_BLK_DEV_ZONED
-static inline int get_blkz_type(struct f2fs_sb_info *sbi,
-			struct block_device *bdev, block_t blkaddr)
-{
-	unsigned int zno = blkaddr >> sbi->log_blocks_per_blkz;
-	int i;
-
-	for (i = 0; i < sbi->s_ndevs; i++)
-		if (FDEV(i).bdev == bdev)
-			return FDEV(i).blkz_type[zno];
-	return -EINVAL;
-}
-#endif
-
-static inline bool f2fs_discard_en(struct f2fs_sb_info *sbi)
-{
-	struct request_queue *q = bdev_get_queue(sbi->sb->s_bdev);
-
-	return blk_queue_discard(q) || f2fs_sb_mounted_blkzoned(sbi->sb);
-}
-
-static inline void set_opt_mode(struct f2fs_sb_info *sbi, unsigned int mt)
-{
-	clear_opt(sbi, ADAPTIVE);
-	clear_opt(sbi, LFS);
-
-	switch (mt) {
-	case F2FS_MOUNT_ADAPTIVE:
-		set_opt(sbi, ADAPTIVE);
-		break;
-	case F2FS_MOUNT_LFS:
-		set_opt(sbi, LFS);
-		break;
-	}
-}
-
-static inline bool f2fs_may_encrypt(struct inode *inode)
-{
-#ifdef CONFIG_F2FS_FS_ENCRYPTION
-	umode_t mode = inode->i_mode;
-
-	return (S_ISREG(mode) || S_ISDIR(mode) || S_ISLNK(mode));
-#else
-	return 0;
-#endif
-}
-
+bool f2fs_may_inline(struct inode *);
+int f2fs_read_inline_data(struct inode *, struct page *);
+int f2fs_convert_inline_data(struct inode *, pgoff_t, struct page *);
+int f2fs_write_inline_data(struct inode *, struct page *, unsigned int);
+void truncate_inline_data(struct inode *, u64);
+bool recover_inline_data(struct inode *, struct page *);
 #endif
diff --git a/kernel/msm-3.18/fs/jbd/checkpoint.c b/kernel/msm-3.18/fs/jbd/checkpoint.c
index 08c03044a..95debd71e 100644
--- a/kernel/msm-3.18/fs/jbd/checkpoint.c
+++ b/kernel/msm-3.18/fs/jbd/checkpoint.c
@@ -129,6 +129,8 @@ void __log_wait_for_space(journal_t *journal)
 		if (journal->j_flags & JFS_ABORT)
 			return;
 		spin_unlock(&journal->j_state_lock);
+		if (current->plug)
+			io_schedule();
 		mutex_lock(&journal->j_checkpoint_mutex);
 
 		/*
diff --git a/kernel/msm-3.18/fs/jbd2/checkpoint.c b/kernel/msm-3.18/fs/jbd2/checkpoint.c
index 8c44654ce..78c1545a3 100644
--- a/kernel/msm-3.18/fs/jbd2/checkpoint.c
+++ b/kernel/msm-3.18/fs/jbd2/checkpoint.c
@@ -116,6 +116,8 @@ void __jbd2_log_wait_for_space(journal_t *journal)
 	nblocks = jbd2_space_needed(journal);
 	while (jbd2_log_space_left(journal) < nblocks) {
 		write_unlock(&journal->j_state_lock);
+		if (current->plug)
+			io_schedule();
 		mutex_lock(&journal->j_checkpoint_mutex);
 
 		/*
diff --git a/kernel/msm-3.18/fs/namespace.c b/kernel/msm-3.18/fs/namespace.c
index c04a68d8d..cfc9f682b 100644
--- a/kernel/msm-3.18/fs/namespace.c
+++ b/kernel/msm-3.18/fs/namespace.c
@@ -14,6 +14,7 @@
 #include <linux/mnt_namespace.h>
 #include <linux/user_namespace.h>
 #include <linux/namei.h>
+#include <linux/delay.h>
 #include <linux/security.h>
 #include <linux/idr.h>
 #include <linux/init.h>		/* init_rootfs */
@@ -344,8 +345,11 @@ int __mnt_want_write(struct vfsmount *m)
 	 * incremented count after it has set MNT_WRITE_HOLD.
 	 */
 	smp_mb();
-	while (ACCESS_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)
-		cpu_relax();
+	while (ACCESS_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD) {
+		preempt_enable();
+		cpu_chill();
+		preempt_disable();
+	}
 	/*
 	 * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will
 	 * be set to match its requirements. So we must not load that until
diff --git a/kernel/msm-3.18/fs/ntfs/aops.c b/kernel/msm-3.18/fs/ntfs/aops.c
index 7521e11db..f0de4b6b8 100644
--- a/kernel/msm-3.18/fs/ntfs/aops.c
+++ b/kernel/msm-3.18/fs/ntfs/aops.c
@@ -107,8 +107,7 @@ static void ntfs_end_buffer_async_read(struct buffer_head *bh, int uptodate)
 				"0x%llx.", (unsigned long long)bh->b_blocknr);
 	}
 	first = page_buffers(page);
-	local_irq_save(flags);
-	bit_spin_lock(BH_Uptodate_Lock, &first->b_state);
+	flags = bh_uptodate_lock_irqsave(first);
 	clear_buffer_async_read(bh);
 	unlock_buffer(bh);
 	tmp = bh;
@@ -123,8 +122,7 @@ static void ntfs_end_buffer_async_read(struct buffer_head *bh, int uptodate)
 		}
 		tmp = tmp->b_this_page;
 	} while (tmp != bh);
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
+	bh_uptodate_unlock_irqrestore(first, flags);
 	/*
 	 * If none of the buffers had errors then we can set the page uptodate,
 	 * but we first have to perform the post read mst fixups, if the
@@ -145,13 +143,13 @@ static void ntfs_end_buffer_async_read(struct buffer_head *bh, int uptodate)
 		recs = PAGE_CACHE_SIZE / rec_size;
 		/* Should have been verified before we got here... */
 		BUG_ON(!recs);
-		local_irq_save(flags);
+		local_irq_save_nort(flags);
 		kaddr = kmap_atomic(page);
 		for (i = 0; i < recs; i++)
 			post_read_mst_fixup((NTFS_RECORD*)(kaddr +
 					i * rec_size), rec_size);
 		kunmap_atomic(kaddr);
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 		flush_dcache_page(page);
 		if (likely(page_uptodate && !PageError(page)))
 			SetPageUptodate(page);
@@ -159,9 +157,7 @@ static void ntfs_end_buffer_async_read(struct buffer_head *bh, int uptodate)
 	unlock_page(page);
 	return;
 still_busy:
-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
-	local_irq_restore(flags);
-	return;
+	bh_uptodate_unlock_irqrestore(first, flags);
 }
 
 /**
diff --git a/kernel/msm-3.18/fs/timerfd.c b/kernel/msm-3.18/fs/timerfd.c
index 31374ec8f..b6c9cc8b1 100644
--- a/kernel/msm-3.18/fs/timerfd.c
+++ b/kernel/msm-3.18/fs/timerfd.c
@@ -462,7 +462,10 @@ static int do_timerfd_settime(int ufd, int flags,
 				break;
 		}
 		spin_unlock_irq(&ctx->wqh.lock);
-		cpu_relax();
+		if (isalarm(ctx))
+			hrtimer_wait_for_timer(&ctx->t.alarm.timer);
+		else
+			hrtimer_wait_for_timer(&ctx->t.tmr);
 	}
 
 	/*
diff --git a/kernel/msm-3.18/fs/xfs/xfs_linux.h b/kernel/msm-3.18/fs/xfs/xfs_linux.h
index 6a51619d8..430e7987d 100644
--- a/kernel/msm-3.18/fs/xfs/xfs_linux.h
+++ b/kernel/msm-3.18/fs/xfs/xfs_linux.h
@@ -119,7 +119,7 @@ typedef __uint64_t __psunsigned_t;
 /*
  * Feature macros (disable/enable)
  */
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && !defined(CONFIG_PREEMPT_RT_FULL)
 #define HAVE_PERCPU_SB	/* per cpu superblock counters are a 2.6 feature */
 #else
 #undef  HAVE_PERCPU_SB	/* per cpu superblock counters are a 2.6 feature */
diff --git a/kernel/msm-3.18/include/acpi/platform/aclinux.h b/kernel/msm-3.18/include/acpi/platform/aclinux.h
index 1ba7c190c..21653ac47 100644
--- a/kernel/msm-3.18/include/acpi/platform/aclinux.h
+++ b/kernel/msm-3.18/include/acpi/platform/aclinux.h
@@ -123,6 +123,7 @@
 
 #define acpi_cache_t                        struct kmem_cache
 #define acpi_spinlock                       spinlock_t *
+#define acpi_raw_spinlock		raw_spinlock_t *
 #define acpi_cpu_flags                      unsigned long
 
 /* Use native linux version of acpi_os_allocate_zeroed */
@@ -141,6 +142,20 @@
 #define ACPI_USE_ALTERNATE_PROTOTYPE_acpi_os_get_thread_id
 #define ACPI_USE_ALTERNATE_PROTOTYPE_acpi_os_create_lock
 
+#define acpi_os_create_raw_lock(__handle)			\
+({								\
+	 raw_spinlock_t *lock = ACPI_ALLOCATE(sizeof(*lock));	\
+								\
+	 if (lock) {						\
+		*(__handle) = lock;				\
+		raw_spin_lock_init(*(__handle));		\
+	 }							\
+	 lock ? AE_OK : AE_NO_MEMORY;				\
+ })
+
+#define acpi_os_delete_raw_lock(__handle)	kfree(__handle)
+
+
 /*
  * OSL interfaces used by debugger/disassembler
  */
diff --git a/kernel/msm-3.18/include/asm-generic/bug.h b/kernel/msm-3.18/include/asm-generic/bug.h
index 630dd2372..850e4d993 100644
--- a/kernel/msm-3.18/include/asm-generic/bug.h
+++ b/kernel/msm-3.18/include/asm-generic/bug.h
@@ -206,6 +206,20 @@ extern void warn_slowpath_null(const char *file, const int line);
 # define WARN_ON_SMP(x)			({0;})
 #endif
 
+#ifdef CONFIG_PREEMPT_RT_BASE
+# define BUG_ON_RT(c)			BUG_ON(c)
+# define BUG_ON_NONRT(c)		do { } while (0)
+# define WARN_ON_RT(condition)		WARN_ON(condition)
+# define WARN_ON_NONRT(condition)	do { } while (0)
+# define WARN_ON_ONCE_NONRT(condition)	do { } while (0)
+#else
+# define BUG_ON_RT(c)			do { } while (0)
+# define BUG_ON_NONRT(c)		BUG_ON(c)
+# define WARN_ON_RT(condition)		do { } while (0)
+# define WARN_ON_NONRT(condition)	WARN_ON(condition)
+# define WARN_ON_ONCE_NONRT(condition)	WARN_ON_ONCE(condition)
+#endif
+
 #endif /* __ASSEMBLY__ */
 
 #endif
diff --git a/kernel/msm-3.18/include/asm-generic/preempt.h b/kernel/msm-3.18/include/asm-generic/preempt.h
index 54352f4dd..65759d8b0 100644
--- a/kernel/msm-3.18/include/asm-generic/preempt.h
+++ b/kernel/msm-3.18/include/asm-generic/preempt.h
@@ -7,10 +7,10 @@
 
 static __always_inline int preempt_count(void)
 {
-	return current_thread_info()->preempt_count;
+	return READ_ONCE(current_thread_info()->preempt_count);
 }
 
-static __always_inline int *preempt_count_ptr(void)
+static __always_inline volatile int *preempt_count_ptr(void)
 {
 	return &current_thread_info()->preempt_count;
 }
diff --git a/kernel/msm-3.18/include/linux/blk-mq.h b/kernel/msm-3.18/include/linux/blk-mq.h
index c9be15894..81a5ea3a7 100644
--- a/kernel/msm-3.18/include/linux/blk-mq.h
+++ b/kernel/msm-3.18/include/linux/blk-mq.h
@@ -169,6 +169,7 @@ struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
 
 struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
 struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);
+void __blk_mq_complete_request_remote_work(struct work_struct *work);
 
 void blk_mq_start_request(struct request *rq);
 void blk_mq_end_request(struct request *rq, int error);
diff --git a/kernel/msm-3.18/include/linux/blkdev.h b/kernel/msm-3.18/include/linux/blkdev.h
index 135643b35..7fc7d21da 100644
--- a/kernel/msm-3.18/include/linux/blkdev.h
+++ b/kernel/msm-3.18/include/linux/blkdev.h
@@ -101,6 +101,7 @@ struct request {
 	struct list_head queuelist;
 	union {
 		struct call_single_data csd;
+		struct work_struct work;
 		unsigned long fifo_time;
 	};
 
@@ -481,7 +482,7 @@ struct request_queue {
 	struct throtl_data *td;
 #endif
 	struct rcu_head		rcu_head;
-	wait_queue_head_t	mq_freeze_wq;
+	struct swait_head	mq_freeze_wq;
 	struct percpu_ref	mq_usage_counter;
 	struct list_head	all_q_node;
 
diff --git a/kernel/msm-3.18/include/linux/bottom_half.h b/kernel/msm-3.18/include/linux/bottom_half.h
index 8fdcb7831..585434224 100644
--- a/kernel/msm-3.18/include/linux/bottom_half.h
+++ b/kernel/msm-3.18/include/linux/bottom_half.h
@@ -3,6 +3,17 @@
 
 #include <linux/preempt.h>
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+
+extern void local_bh_disable(void);
+extern void _local_bh_enable(void);
+extern void local_bh_enable(void);
+extern void local_bh_enable_ip(unsigned long ip);
+extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
+extern void __local_bh_enable_ip(unsigned long ip, unsigned int cnt);
+
+#else
+
 #ifdef CONFIG_TRACE_IRQFLAGS
 extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
 #else
@@ -30,5 +41,6 @@ static inline void local_bh_enable(void)
 {
 	__local_bh_enable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET);
 }
+#endif
 
 #endif /* _LINUX_BH_H */
diff --git a/kernel/msm-3.18/include/linux/buffer_head.h b/kernel/msm-3.18/include/linux/buffer_head.h
index 73b45225a..ea72f5203 100644
--- a/kernel/msm-3.18/include/linux/buffer_head.h
+++ b/kernel/msm-3.18/include/linux/buffer_head.h
@@ -75,8 +75,52 @@ struct buffer_head {
 	struct address_space *b_assoc_map;	/* mapping this buffer is
 						   associated with */
 	atomic_t b_count;		/* users using this buffer_head */
+#ifdef CONFIG_PREEMPT_RT_BASE
+	spinlock_t b_uptodate_lock;
+#if defined(CONFIG_JBD) || defined(CONFIG_JBD_MODULE) || \
+	defined(CONFIG_JBD2) || defined(CONFIG_JBD2_MODULE)
+	spinlock_t b_state_lock;
+	spinlock_t b_journal_head_lock;
+#endif
+#endif
 };
 
+static inline unsigned long bh_uptodate_lock_irqsave(struct buffer_head *bh)
+{
+	unsigned long flags;
+
+#ifndef CONFIG_PREEMPT_RT_BASE
+	local_irq_save(flags);
+	bit_spin_lock(BH_Uptodate_Lock, &bh->b_state);
+#else
+	spin_lock_irqsave(&bh->b_uptodate_lock, flags);
+#endif
+	return flags;
+}
+
+static inline void
+bh_uptodate_unlock_irqrestore(struct buffer_head *bh, unsigned long flags)
+{
+#ifndef CONFIG_PREEMPT_RT_BASE
+	bit_spin_unlock(BH_Uptodate_Lock, &bh->b_state);
+	local_irq_restore(flags);
+#else
+	spin_unlock_irqrestore(&bh->b_uptodate_lock, flags);
+#endif
+}
+
+static inline void buffer_head_init_locks(struct buffer_head *bh)
+{
+#ifdef CONFIG_PREEMPT_RT_BASE
+	spin_lock_init(&bh->b_uptodate_lock);
+#if defined(CONFIG_JBD) || defined(CONFIG_JBD_MODULE) || \
+	defined(CONFIG_JBD2) || defined(CONFIG_JBD2_MODULE)
+	spin_lock_init(&bh->b_state_lock);
+	spin_lock_init(&bh->b_journal_head_lock);
+#endif
+#endif
+}
+
 /*
  * macro tricks to expand the set_buffer_foo(), clear_buffer_foo()
  * and buffer_foo() functions.
diff --git a/kernel/msm-3.18/include/linux/cgroup.h b/kernel/msm-3.18/include/linux/cgroup.h
index 5ba9719e9..ce7966cf3 100644
--- a/kernel/msm-3.18/include/linux/cgroup.h
+++ b/kernel/msm-3.18/include/linux/cgroup.h
@@ -22,6 +22,7 @@
 #include <linux/seq_file.h>
 #include <linux/kernfs.h>
 #include <linux/wait.h>
+#include <linux/work-simple.h>
 
 #ifdef CONFIG_CGROUPS
 
@@ -91,6 +92,7 @@ struct cgroup_subsys_state {
 	/* percpu_ref killing and RCU release */
 	struct rcu_head rcu_head;
 	struct work_struct destroy_work;
+	struct swork_event destroy_swork;
 };
 
 /* bits in struct cgroup_subsys_state flags field */
diff --git a/kernel/msm-3.18/include/linux/completion.h b/kernel/msm-3.18/include/linux/completion.h
index 5d5aaae3a..3fe8d14c9 100644
--- a/kernel/msm-3.18/include/linux/completion.h
+++ b/kernel/msm-3.18/include/linux/completion.h
@@ -7,8 +7,7 @@
  * Atomic wait-for-completion handler data structures.
  * See kernel/sched/completion.c for details.
  */
-
-#include <linux/wait.h>
+#include <linux/wait-simple.h>
 
 /*
  * struct completion - structure used to maintain state for a "completion"
@@ -24,11 +23,11 @@
  */
 struct completion {
 	unsigned int done;
-	wait_queue_head_t wait;
+	struct swait_head wait;
 };
 
 #define COMPLETION_INITIALIZER(work) \
-	{ 0, __WAIT_QUEUE_HEAD_INITIALIZER((work).wait) }
+	{ 0, SWAIT_HEAD_INITIALIZER((work).wait) }
 
 #define COMPLETION_INITIALIZER_ONSTACK(work) \
 	({ init_completion(&work); work; })
@@ -73,7 +72,7 @@ struct completion {
 static inline void init_completion(struct completion *x)
 {
 	x->done = 0;
-	init_waitqueue_head(&x->wait);
+	init_swait_head(&x->wait);
 }
 
 /**
diff --git a/kernel/msm-3.18/include/linux/cpu.h b/kernel/msm-3.18/include/linux/cpu.h
index fbca0f9bd..931e59fa4 100644
--- a/kernel/msm-3.18/include/linux/cpu.h
+++ b/kernel/msm-3.18/include/linux/cpu.h
@@ -236,6 +236,8 @@ extern bool try_get_online_cpus(void);
 extern void put_online_cpus(void);
 extern void cpu_hotplug_disable(void);
 extern void cpu_hotplug_enable(void);
+extern void pin_current_cpu(void);
+extern void unpin_current_cpu(void);
 #define hotcpu_notifier(fn, pri)	cpu_notifier(fn, pri)
 #define __hotcpu_notifier(fn, pri)	__cpu_notifier(fn, pri)
 #define register_hotcpu_notifier(nb)	register_cpu_notifier(nb)
@@ -254,6 +256,8 @@ static inline void cpu_hotplug_done(void) {}
 #define put_online_cpus()	do { } while (0)
 #define cpu_hotplug_disable()	do { } while (0)
 #define cpu_hotplug_enable()	do { } while (0)
+static inline void pin_current_cpu(void) { }
+static inline void unpin_current_cpu(void) { }
 #define hotcpu_notifier(fn, pri)	do { (void)(fn); } while (0)
 #define __hotcpu_notifier(fn, pri)	do { (void)(fn); } while (0)
 /* These aren't inline functions due to a GCC bug. */
diff --git a/kernel/msm-3.18/include/linux/delay.h b/kernel/msm-3.18/include/linux/delay.h
index a6ecb34cf..37caab306 100644
--- a/kernel/msm-3.18/include/linux/delay.h
+++ b/kernel/msm-3.18/include/linux/delay.h
@@ -52,4 +52,10 @@ static inline void ssleep(unsigned int seconds)
 	msleep(seconds * 1000);
 }
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+extern void cpu_chill(void);
+#else
+# define cpu_chill()	cpu_relax()
+#endif
+
 #endif /* defined(_LINUX_DELAY_H) */
diff --git a/kernel/msm-3.18/include/linux/ftrace.h b/kernel/msm-3.18/include/linux/ftrace.h
index dc008b9e9..fcfedc63f 100644
--- a/kernel/msm-3.18/include/linux/ftrace.h
+++ b/kernel/msm-3.18/include/linux/ftrace.h
@@ -649,6 +649,18 @@ static inline void __ftrace_enabled_restore(int enabled)
 #define CALLER_ADDR5 ((unsigned long)ftrace_return_address(5))
 #define CALLER_ADDR6 ((unsigned long)ftrace_return_address(6))
 
+static inline unsigned long get_lock_parent_ip(void)
+{
+	unsigned long addr = CALLER_ADDR0;
+
+	if (!in_lock_functions(addr))
+		return addr;
+	addr = CALLER_ADDR1;
+	if (!in_lock_functions(addr))
+		return addr;
+	return CALLER_ADDR2;
+}
+
 #ifdef CONFIG_IRQSOFF_TRACER
   extern void time_hardirqs_on(unsigned long a0, unsigned long a1);
   extern void time_hardirqs_off(unsigned long a0, unsigned long a1);
diff --git a/kernel/msm-3.18/include/linux/ftrace_event.h b/kernel/msm-3.18/include/linux/ftrace_event.h
index b3c28a61f..ecfb99a6a 100644
--- a/kernel/msm-3.18/include/linux/ftrace_event.h
+++ b/kernel/msm-3.18/include/linux/ftrace_event.h
@@ -66,6 +66,9 @@ struct trace_entry {
 	unsigned char		flags;
 	unsigned char		preempt_count;
 	int			pid;
+	unsigned short		migrate_disable;
+	unsigned short		padding;
+	unsigned char		preempt_lazy_count;
 };
 
 #define FTRACE_MAX_EVENT						\
diff --git a/kernel/msm-3.18/include/linux/highmem.h b/kernel/msm-3.18/include/linux/highmem.h
index bc4af51ce..737fe1c7e 100644
--- a/kernel/msm-3.18/include/linux/highmem.h
+++ b/kernel/msm-3.18/include/linux/highmem.h
@@ -7,6 +7,7 @@
 #include <linux/mm.h>
 #include <linux/uaccess.h>
 #include <linux/hardirq.h>
+#include <linux/sched.h>
 
 #include <asm/cacheflush.h>
 
@@ -92,32 +93,51 @@ static inline void __kunmap_atomic(void *addr)
 
 #if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 DECLARE_PER_CPU(int, __kmap_atomic_idx);
+#endif
 
 static inline int kmap_atomic_idx_push(void)
 {
+#ifndef CONFIG_PREEMPT_RT_FULL
 	int idx = __this_cpu_inc_return(__kmap_atomic_idx) - 1;
 
-#ifdef CONFIG_DEBUG_HIGHMEM
+# ifdef CONFIG_DEBUG_HIGHMEM
 	WARN_ON_ONCE(in_irq() && !irqs_disabled());
 	BUG_ON(idx >= KM_TYPE_NR);
-#endif
+# endif
 	return idx;
+#else
+	current->kmap_idx++;
+	BUG_ON(current->kmap_idx > KM_TYPE_NR);
+	return current->kmap_idx - 1;
+#endif
 }
 
 static inline int kmap_atomic_idx(void)
 {
+#ifndef CONFIG_PREEMPT_RT_FULL
 	return __this_cpu_read(__kmap_atomic_idx) - 1;
+#else
+	return current->kmap_idx - 1;
+#endif
 }
 
 static inline void kmap_atomic_idx_pop(void)
 {
-#ifdef CONFIG_DEBUG_HIGHMEM
+#ifndef CONFIG_PREEMPT_RT_FULL
+# ifdef CONFIG_DEBUG_HIGHMEM
 	int idx = __this_cpu_dec_return(__kmap_atomic_idx);
 
 	BUG_ON(idx < 0);
-#else
+# else
 	__this_cpu_dec(__kmap_atomic_idx);
+# endif
+#else
+	current->kmap_idx--;
+# ifdef CONFIG_DEBUG_HIGHMEM
+	BUG_ON(current->kmap_idx < 0);
+# endif
 #endif
 }
 
diff --git a/kernel/msm-3.18/include/linux/hrtimer.h b/kernel/msm-3.18/include/linux/hrtimer.h
index 05f6df1fd..64e1abb37 100644
--- a/kernel/msm-3.18/include/linux/hrtimer.h
+++ b/kernel/msm-3.18/include/linux/hrtimer.h
@@ -111,6 +111,11 @@ struct hrtimer {
 	enum hrtimer_restart		(*function)(struct hrtimer *);
 	struct hrtimer_clock_base	*base;
 	unsigned long			state;
+	struct list_head		cb_entry;
+	int				irqsafe;
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+	ktime_t				praecox;
+#endif
 #ifdef CONFIG_TIMER_STATS
 	int				start_pid;
 	void				*start_site;
@@ -147,6 +152,7 @@ struct hrtimer_clock_base {
 	int			index;
 	clockid_t		clockid;
 	struct timerqueue_head	active;
+	struct list_head	expired;
 	ktime_t			resolution;
 	ktime_t			(*get_time)(void);
 	ktime_t			softirq_time;
@@ -193,6 +199,9 @@ struct hrtimer_cpu_base {
 	unsigned long			nr_retries;
 	unsigned long			nr_hangs;
 	ktime_t				max_hang_time;
+#endif
+#ifdef CONFIG_PREEMPT_RT_BASE
+	wait_queue_head_t		wait;
 #endif
 	struct hrtimer_clock_base	clock_base[HRTIMER_MAX_CLOCK_BASES];
 };
@@ -381,6 +390,13 @@ static inline int hrtimer_restart(struct hrtimer *timer)
 	return hrtimer_start_expires(timer, HRTIMER_MODE_ABS);
 }
 
+/* Softirq preemption could deadlock timer removal */
+#ifdef CONFIG_PREEMPT_RT_BASE
+  extern void hrtimer_wait_for_timer(const struct hrtimer *timer);
+#else
+# define hrtimer_wait_for_timer(timer)	do { cpu_relax(); } while (0)
+#endif
+
 /* Query timers: */
 extern ktime_t hrtimer_get_remaining(const struct hrtimer *timer);
 extern int hrtimer_get_res(const clockid_t which_clock, struct timespec *tp);
diff --git a/kernel/msm-3.18/include/linux/idr.h b/kernel/msm-3.18/include/linux/idr.h
index 013fd9bc4..f62be0aec 100644
--- a/kernel/msm-3.18/include/linux/idr.h
+++ b/kernel/msm-3.18/include/linux/idr.h
@@ -95,10 +95,14 @@ bool idr_is_empty(struct idr *idp);
  * Each idr_preload() should be matched with an invocation of this
  * function.  See idr_preload() for details.
  */
+#ifdef CONFIG_PREEMPT_RT_FULL
+void idr_preload_end(void);
+#else
 static inline void idr_preload_end(void)
 {
 	preempt_enable();
 }
+#endif
 
 /**
  * idr_find - return pointer for given id
diff --git a/kernel/msm-3.18/include/linux/init_task.h b/kernel/msm-3.18/include/linux/init_task.h
index 16842b4e2..e90d1d6a9 100644
--- a/kernel/msm-3.18/include/linux/init_task.h
+++ b/kernel/msm-3.18/include/linux/init_task.h
@@ -147,9 +147,16 @@ extern struct task_group root_task_group;
 # define INIT_PERF_EVENTS(tsk)
 #endif
 
+#ifdef CONFIG_PREEMPT_RT_BASE
+# define INIT_TIMER_LIST		.posix_timer_list = NULL,
+#else
+# define INIT_TIMER_LIST
+#endif
+
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 # define INIT_VTIME(tsk)						\
-	.vtime_seqlock = __SEQLOCK_UNLOCKED(tsk.vtime_seqlock),	\
+	.vtime_lock = __RAW_SPIN_LOCK_UNLOCKED(tsk.vtime_lock),	\
+	.vtime_seq = SEQCNT_ZERO(tsk.vtime_seq),			\
 	.vtime_snap = 0,				\
 	.vtime_snap_whence = VTIME_SYS,
 #else
@@ -229,6 +236,7 @@ extern struct task_group root_task_group;
 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
 	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(tsk.pi_lock),	\
 	.timer_slack_ns = 50000, /* 50 usec default slack */		\
+	INIT_TIMER_LIST							\
 	.pids = {							\
 		[PIDTYPE_PID]  = INIT_PID_LINK(PIDTYPE_PID),		\
 		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),		\
diff --git a/kernel/msm-3.18/include/linux/interrupt.h b/kernel/msm-3.18/include/linux/interrupt.h
index 2357215b0..847f90f8f 100644
--- a/kernel/msm-3.18/include/linux/interrupt.h
+++ b/kernel/msm-3.18/include/linux/interrupt.h
@@ -57,6 +57,7 @@
  * IRQF_NO_THREAD - Interrupt cannot be threaded
  * IRQF_EARLY_RESUME - Resume IRQ early during syscore instead of at device
  *                resume time.
+ * IRQF_NO_SOFTIRQ_CALL - Do not process softirqs in the irq thread context (RT)
  */
 #define IRQF_DISABLED		0x00000020
 #define IRQF_SHARED		0x00000080
@@ -70,6 +71,7 @@
 #define IRQF_FORCE_RESUME	0x00008000
 #define IRQF_NO_THREAD		0x00010000
 #define IRQF_EARLY_RESUME	0x00020000
+#define IRQF_NO_SOFTIRQ_CALL	0x00080000
 
 #define IRQF_TIMER		(__IRQF_TIMER | IRQF_NO_SUSPEND | IRQF_NO_THREAD)
 
@@ -98,6 +100,7 @@ typedef irqreturn_t (*irq_handler_t)(int, void *);
  * @flags:	flags (see IRQF_* above)
  * @thread_fn:	interrupt handler function for threaded interrupts
  * @thread:	thread pointer for threaded interrupts
+ * @secondary:	pointer to secondary irqaction (force threading)
  * @thread_flags:	flags related to @thread
  * @thread_mask:	bitmask for keeping track of @thread activity
  * @dir:	pointer to the proc/irq/NN/name entry
@@ -109,6 +112,7 @@ struct irqaction {
 	struct irqaction	*next;
 	irq_handler_t		thread_fn;
 	struct task_struct	*thread;
+	struct irqaction	*secondary;
 	unsigned int		irq;
 	unsigned int		flags;
 	unsigned long		thread_flags;
@@ -180,7 +184,7 @@ extern void devm_free_irq(struct device *dev, unsigned int irq, void *dev_id);
 #ifdef CONFIG_LOCKDEP
 # define local_irq_enable_in_hardirq()	do { } while (0)
 #else
-# define local_irq_enable_in_hardirq()	local_irq_enable()
+# define local_irq_enable_in_hardirq()	local_irq_enable_nort()
 #endif
 
 extern void disable_irq_nosync(unsigned int irq);
@@ -210,6 +214,7 @@ struct irq_affinity_notify {
 	unsigned int irq;
 	struct kref kref;
 	struct work_struct work;
+	struct list_head list;
 	void (*notify)(struct irq_affinity_notify *, const cpumask_t *mask);
 	void (*release)(struct kref *ref);
 };
@@ -359,9 +364,13 @@ static inline int disable_irq_wake(unsigned int irq)
 
 
 #ifdef CONFIG_IRQ_FORCED_THREADING
+# ifndef CONFIG_PREEMPT_RT_BASE
 extern bool force_irqthreads;
+# else
+#  define force_irqthreads	(true)
+# endif
 #else
-#define force_irqthreads	(0)
+#define force_irqthreads	(false)
 #endif
 
 #ifndef __ARCH_SET_SOFTIRQ_PENDING
@@ -417,9 +426,10 @@ struct softirq_action
 	void	(*action)(struct softirq_action *);
 };
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 asmlinkage void do_softirq(void);
 asmlinkage void __do_softirq(void);
-
+static inline void thread_do_softirq(void) { do_softirq(); }
 #ifdef __ARCH_HAS_DO_SOFTIRQ
 void do_softirq_own_stack(void);
 #else
@@ -428,13 +438,25 @@ static inline void do_softirq_own_stack(void)
 	__do_softirq();
 }
 #endif
+#else
+extern void thread_do_softirq(void);
+#endif
 
 extern void open_softirq(int nr, void (*action)(struct softirq_action *));
 extern void softirq_init(void);
 extern void __raise_softirq_irqoff(unsigned int nr);
+#ifdef CONFIG_PREEMPT_RT_FULL
+extern void __raise_softirq_irqoff_ksoft(unsigned int nr);
+#else
+static inline void __raise_softirq_irqoff_ksoft(unsigned int nr)
+{
+	__raise_softirq_irqoff(nr);
+}
+#endif
 
 extern void raise_softirq_irqoff(unsigned int nr);
 extern void raise_softirq(unsigned int nr);
+extern void softirq_check_pending_idle(void);
 
 DECLARE_PER_CPU(struct task_struct *, ksoftirqd);
 
@@ -456,8 +478,9 @@ static inline struct task_struct *this_cpu_ksoftirqd(void)
      to be executed on some cpu at least once after this.
    * If the tasklet is already scheduled, but its execution is still not
      started, it will be executed only once.
-   * If this tasklet is already running on another CPU (or schedule is called
-     from tasklet itself), it is rescheduled for later.
+   * If this tasklet is already running on another CPU, it is rescheduled
+     for later.
+   * Schedule must not be called from the tasklet itself (a lockup occurs)
    * Tasklet is strictly serialized wrt itself, but not
      wrt another tasklets. If client needs some intertask synchronization,
      he makes it with spinlocks.
@@ -482,27 +505,36 @@ struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(1), func, data }
 enum
 {
 	TASKLET_STATE_SCHED,	/* Tasklet is scheduled for execution */
-	TASKLET_STATE_RUN	/* Tasklet is running (SMP only) */
+	TASKLET_STATE_RUN,	/* Tasklet is running (SMP only) */
+	TASKLET_STATE_PENDING	/* Tasklet is pending */
 };
 
-#ifdef CONFIG_SMP
+#define TASKLET_STATEF_SCHED	(1 << TASKLET_STATE_SCHED)
+#define TASKLET_STATEF_RUN	(1 << TASKLET_STATE_RUN)
+#define TASKLET_STATEF_PENDING	(1 << TASKLET_STATE_PENDING)
+
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT_FULL)
 static inline int tasklet_trylock(struct tasklet_struct *t)
 {
 	return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);
 }
 
+static inline int tasklet_tryunlock(struct tasklet_struct *t)
+{
+	return cmpxchg(&t->state, TASKLET_STATEF_RUN, 0) == TASKLET_STATEF_RUN;
+}
+
 static inline void tasklet_unlock(struct tasklet_struct *t)
 {
 	smp_mb__before_atomic();
 	clear_bit(TASKLET_STATE_RUN, &(t)->state);
 }
 
-static inline void tasklet_unlock_wait(struct tasklet_struct *t)
-{
-	while (test_bit(TASKLET_STATE_RUN, &(t)->state)) { barrier(); }
-}
+extern void tasklet_unlock_wait(struct tasklet_struct *t);
+
 #else
 #define tasklet_trylock(t) 1
+#define tasklet_tryunlock(t)	1
 #define tasklet_unlock_wait(t) do { } while (0)
 #define tasklet_unlock(t) do { } while (0)
 #endif
@@ -551,17 +583,8 @@ static inline void tasklet_disable(struct tasklet_struct *t)
 	smp_mb();
 }
 
-static inline void tasklet_enable(struct tasklet_struct *t)
-{
-	smp_mb__before_atomic();
-	atomic_dec(&t->count);
-}
-
-static inline void tasklet_hi_enable(struct tasklet_struct *t)
-{
-	smp_mb__before_atomic();
-	atomic_dec(&t->count);
-}
+extern void tasklet_enable(struct tasklet_struct *t);
+extern void tasklet_hi_enable(struct tasklet_struct *t);
 
 extern void tasklet_kill(struct tasklet_struct *t);
 extern void tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu);
@@ -593,6 +616,12 @@ void tasklet_hrtimer_cancel(struct tasklet_hrtimer *ttimer)
 	tasklet_kill(&ttimer->tasklet);
 }
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+extern void softirq_early_init(void);
+#else
+static inline void softirq_early_init(void) { }
+#endif
+
 /*
  * Autoprobing for irqs:
  *
diff --git a/kernel/msm-3.18/include/linux/irq.h b/kernel/msm-3.18/include/linux/irq.h
index 4f58d2973..02cc30aac 100644
--- a/kernel/msm-3.18/include/linux/irq.h
+++ b/kernel/msm-3.18/include/linux/irq.h
@@ -71,6 +71,7 @@ struct msi_msg;
  * IRQ_IS_POLLED		- Always polled by another interrupt. Exclude
  *				  it from the spurious interrupt detection
  *				  mechanism and from core side polling.
+ * IRQ_NO_SOFTIRQ_CALL		- No softirq processing in the irq thread context (RT)
  */
 enum {
 	IRQ_TYPE_NONE		= 0x00000000,
@@ -96,13 +97,14 @@ enum {
 	IRQ_NOTHREAD		= (1 << 16),
 	IRQ_PER_CPU_DEVID	= (1 << 17),
 	IRQ_IS_POLLED		= (1 << 18),
+	IRQ_NO_SOFTIRQ_CALL     = (1 << 19),
 };
 
 #define IRQF_MODIFY_MASK	\
 	(IRQ_TYPE_SENSE_MASK | IRQ_NOPROBE | IRQ_NOREQUEST | \
 	 IRQ_NOAUTOEN | IRQ_MOVE_PCNTXT | IRQ_LEVEL | IRQ_NO_BALANCING | \
 	 IRQ_PER_CPU | IRQ_NESTED_THREAD | IRQ_NOTHREAD | IRQ_PER_CPU_DEVID | \
-	 IRQ_IS_POLLED)
+	 IRQ_IS_POLLED | IRQ_NO_SOFTIRQ_CALL)
 
 #define IRQ_NO_BALANCING_MASK	(IRQ_PER_CPU | IRQ_NO_BALANCING)
 
diff --git a/kernel/msm-3.18/include/linux/irq_work.h b/kernel/msm-3.18/include/linux/irq_work.h
index bf3fe719c..af7ed9ad5 100644
--- a/kernel/msm-3.18/include/linux/irq_work.h
+++ b/kernel/msm-3.18/include/linux/irq_work.h
@@ -16,6 +16,7 @@
 #define IRQ_WORK_BUSY		2UL
 #define IRQ_WORK_FLAGS		3UL
 #define IRQ_WORK_LAZY		4UL /* Doesn't want IPI, wait for tick */
+#define IRQ_WORK_HARD_IRQ	8UL /* Run hard IRQ context, even on RT */
 
 struct irq_work {
 	unsigned long flags;
@@ -50,4 +51,10 @@ bool irq_work_needs_cpu(void);
 static inline bool irq_work_needs_cpu(void) { return false; }
 #endif
 
+#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_PREEMPT_RT_FULL)
+void irq_work_tick_soft(void);
+#else
+static inline void irq_work_tick_soft(void) { }
+#endif
+
 #endif /* _LINUX_IRQ_WORK_H */
diff --git a/kernel/msm-3.18/include/linux/irqdesc.h b/kernel/msm-3.18/include/linux/irqdesc.h
index 8fe3aafdc..375bec26e 100644
--- a/kernel/msm-3.18/include/linux/irqdesc.h
+++ b/kernel/msm-3.18/include/linux/irqdesc.h
@@ -63,6 +63,7 @@ struct irq_desc {
 	unsigned int		irqs_unhandled;
 	atomic_t		threads_handled;
 	int			threads_handled_last;
+	u64			random_ip;
 	raw_spinlock_t		lock;
 	struct cpumask		*percpu_enabled;
 #ifdef CONFIG_SMP
diff --git a/kernel/msm-3.18/include/linux/irqflags.h b/kernel/msm-3.18/include/linux/irqflags.h
index d176d658f..097782947 100644
--- a/kernel/msm-3.18/include/linux/irqflags.h
+++ b/kernel/msm-3.18/include/linux/irqflags.h
@@ -25,8 +25,6 @@
 # define trace_softirqs_enabled(p)	((p)->softirqs_enabled)
 # define trace_hardirq_enter()	do { current->hardirq_context++; } while (0)
 # define trace_hardirq_exit()	do { current->hardirq_context--; } while (0)
-# define lockdep_softirq_enter()	do { current->softirq_context++; } while (0)
-# define lockdep_softirq_exit()	do { current->softirq_context--; } while (0)
 # define INIT_TRACE_IRQFLAGS	.softirqs_enabled = 1,
 #else
 # define trace_hardirqs_on()		do { } while (0)
@@ -39,9 +37,15 @@
 # define trace_softirqs_enabled(p)	0
 # define trace_hardirq_enter()		do { } while (0)
 # define trace_hardirq_exit()		do { } while (0)
+# define INIT_TRACE_IRQFLAGS
+#endif
+
+#if defined(CONFIG_TRACE_IRQFLAGS) && !defined(CONFIG_PREEMPT_RT_FULL)
+# define lockdep_softirq_enter() do { current->softirq_context++; } while (0)
+# define lockdep_softirq_exit()	 do { current->softirq_context--; } while (0)
+#else
 # define lockdep_softirq_enter()	do { } while (0)
 # define lockdep_softirq_exit()		do { } while (0)
-# define INIT_TRACE_IRQFLAGS
 #endif
 
 #if defined(CONFIG_IRQSOFF_TRACER) || \
@@ -147,4 +151,23 @@
 
 #endif /* CONFIG_TRACE_IRQFLAGS_SUPPORT */
 
+/*
+ * local_irq* variants depending on RT/!RT
+ */
+#ifdef CONFIG_PREEMPT_RT_FULL
+# define local_irq_disable_nort()	do { } while (0)
+# define local_irq_enable_nort()	do { } while (0)
+# define local_irq_save_nort(flags)	local_save_flags(flags)
+# define local_irq_restore_nort(flags)	(void)(flags)
+# define local_irq_disable_rt()		local_irq_disable()
+# define local_irq_enable_rt()		local_irq_enable()
+#else
+# define local_irq_disable_nort()	local_irq_disable()
+# define local_irq_enable_nort()	local_irq_enable()
+# define local_irq_save_nort(flags)	local_irq_save(flags)
+# define local_irq_restore_nort(flags)	local_irq_restore(flags)
+# define local_irq_disable_rt()		do { } while (0)
+# define local_irq_enable_rt()		do { } while (0)
+#endif
+
 #endif
diff --git a/kernel/msm-3.18/include/linux/jbd_common.h b/kernel/msm-3.18/include/linux/jbd_common.h
index 3dc534323..a90a6f5ca 100644
--- a/kernel/msm-3.18/include/linux/jbd_common.h
+++ b/kernel/msm-3.18/include/linux/jbd_common.h
@@ -15,32 +15,56 @@ static inline struct journal_head *bh2jh(struct buffer_head *bh)
 
 static inline void jbd_lock_bh_state(struct buffer_head *bh)
 {
+#ifndef CONFIG_PREEMPT_RT_BASE
 	bit_spin_lock(BH_State, &bh->b_state);
+#else
+	spin_lock(&bh->b_state_lock);
+#endif
 }
 
 static inline int jbd_trylock_bh_state(struct buffer_head *bh)
 {
+#ifndef CONFIG_PREEMPT_RT_BASE
 	return bit_spin_trylock(BH_State, &bh->b_state);
+#else
+	return spin_trylock(&bh->b_state_lock);
+#endif
 }
 
 static inline int jbd_is_locked_bh_state(struct buffer_head *bh)
 {
+#ifndef CONFIG_PREEMPT_RT_BASE
 	return bit_spin_is_locked(BH_State, &bh->b_state);
+#else
+	return spin_is_locked(&bh->b_state_lock);
+#endif
 }
 
 static inline void jbd_unlock_bh_state(struct buffer_head *bh)
 {
+#ifndef CONFIG_PREEMPT_RT_BASE
 	bit_spin_unlock(BH_State, &bh->b_state);
+#else
+	spin_unlock(&bh->b_state_lock);
+#endif
 }
 
 static inline void jbd_lock_bh_journal_head(struct buffer_head *bh)
 {
+#ifndef CONFIG_PREEMPT_RT_BASE
 	bit_spin_lock(BH_JournalHead, &bh->b_state);
+#else
+	spin_lock(&bh->b_journal_head_lock);
+#endif
 }
 
 static inline void jbd_unlock_bh_journal_head(struct buffer_head *bh)
 {
+#ifndef CONFIG_PREEMPT_RT_BASE
 	bit_spin_unlock(BH_JournalHead, &bh->b_state);
+#else
+	spin_unlock(&bh->b_journal_head_lock);
+#endif
 }
 
 #endif
diff --git a/kernel/msm-3.18/include/linux/jump_label.h b/kernel/msm-3.18/include/linux/jump_label.h
index 98f923b6a..e17a47eae 100644
--- a/kernel/msm-3.18/include/linux/jump_label.h
+++ b/kernel/msm-3.18/include/linux/jump_label.h
@@ -55,7 +55,8 @@ extern bool static_key_initialized;
 				    "%s used before call to jump_label_init", \
 				    __func__)
 
-#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
+#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL) && \
+	!defined(CONFIG_PREEMPT_BASE)
 
 struct static_key {
 	atomic_t enabled;
diff --git a/kernel/msm-3.18/include/linux/kdb.h b/kernel/msm-3.18/include/linux/kdb.h
index 290db1269..d9ae75ee7 100644
--- a/kernel/msm-3.18/include/linux/kdb.h
+++ b/kernel/msm-3.18/include/linux/kdb.h
@@ -116,7 +116,7 @@ extern int kdb_trap_printk;
 extern __printf(1, 0) int vkdb_printf(const char *fmt, va_list args);
 extern __printf(1, 2) int kdb_printf(const char *, ...);
 typedef __printf(1, 2) int (*kdb_printf_t)(const char *, ...);
-
+#define in_kdb_printk() (kdb_trap_printk)
 extern void kdb_init(int level);
 
 /* Access to kdb specific polling devices */
@@ -151,6 +151,7 @@ extern int kdb_register_repeat(char *, kdb_func_t, char *, char *,
 extern int kdb_unregister(char *);
 #else /* ! CONFIG_KGDB_KDB */
 static inline __printf(1, 2) int kdb_printf(const char *fmt, ...) { return 0; }
+#define in_kdb_printk() (0)
 static inline void kdb_init(int level) {}
 static inline int kdb_register(char *cmd, kdb_func_t func, char *usage,
 			       char *help, short minlen) { return 0; }
diff --git a/kernel/msm-3.18/include/linux/kernel.h b/kernel/msm-3.18/include/linux/kernel.h
index 5c1fcdf04..2ac6814a7 100644
--- a/kernel/msm-3.18/include/linux/kernel.h
+++ b/kernel/msm-3.18/include/linux/kernel.h
@@ -463,6 +463,7 @@ extern enum system_states {
 	SYSTEM_HALT,
 	SYSTEM_POWER_OFF,
 	SYSTEM_RESTART,
+	SYSTEM_SUSPEND,
 } system_state;
 
 #define TAINT_PROPRIETARY_MODULE	0
diff --git a/kernel/msm-3.18/include/linux/kvm_host.h b/kernel/msm-3.18/include/linux/kvm_host.h
index aa75be9cb..caccd7758 100644
--- a/kernel/msm-3.18/include/linux/kvm_host.h
+++ b/kernel/msm-3.18/include/linux/kvm_host.h
@@ -245,7 +245,7 @@ struct kvm_vcpu {
 
 	int fpu_active;
 	int guest_fpu_loaded, guest_xcr0_loaded;
-	wait_queue_head_t wq;
+	struct swait_head wq;
 	struct pid *pid;
 	int sigset_active;
 	sigset_t sigset;
@@ -688,7 +688,7 @@ static inline bool kvm_arch_has_noncoherent_dma(struct kvm *kvm)
 }
 #endif
 
-static inline wait_queue_head_t *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
+static inline struct swait_head *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
 {
 #ifdef __KVM_HAVE_ARCH_WQP
 	return vcpu->arch.wqp;
diff --git a/kernel/msm-3.18/include/linux/lglock.h b/kernel/msm-3.18/include/linux/lglock.h
index 0081f000e..9603a1500 100644
--- a/kernel/msm-3.18/include/linux/lglock.h
+++ b/kernel/msm-3.18/include/linux/lglock.h
@@ -34,22 +34,39 @@
 #endif
 
 struct lglock {
+#ifndef CONFIG_PREEMPT_RT_FULL
 	arch_spinlock_t __percpu *lock;
+#else
+	struct rt_mutex __percpu *lock;
+#endif
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lock_class_key lock_key;
 	struct lockdep_map    lock_dep_map;
 #endif
 };
 
-#define DEFINE_LGLOCK(name)						\
+#ifndef CONFIG_PREEMPT_RT_FULL
+# define DEFINE_LGLOCK(name)						\
 	static DEFINE_PER_CPU(arch_spinlock_t, name ## _lock)		\
 	= __ARCH_SPIN_LOCK_UNLOCKED;					\
 	struct lglock name = { .lock = &name ## _lock }
 
-#define DEFINE_STATIC_LGLOCK(name)					\
+# define DEFINE_STATIC_LGLOCK(name)					\
 	static DEFINE_PER_CPU(arch_spinlock_t, name ## _lock)		\
 	= __ARCH_SPIN_LOCK_UNLOCKED;					\
 	static struct lglock name = { .lock = &name ## _lock }
+#else
+
+# define DEFINE_LGLOCK(name)						\
+	static DEFINE_PER_CPU(struct rt_mutex, name ## _lock)		\
+	= __RT_MUTEX_INITIALIZER( name ## _lock);			\
+	struct lglock name = { .lock = &name ## _lock }
+
+# define DEFINE_STATIC_LGLOCK(name)					\
+	static DEFINE_PER_CPU(struct rt_mutex, name ## _lock)		\
+	= __RT_MUTEX_INITIALIZER( name ## _lock);			\
+	static struct lglock name = { .lock = &name ## _lock }
+#endif
 
 void lg_lock_init(struct lglock *lg, char *name);
 void lg_local_lock(struct lglock *lg);
@@ -59,6 +76,12 @@ void lg_local_unlock_cpu(struct lglock *lg, int cpu);
 void lg_global_lock(struct lglock *lg);
 void lg_global_unlock(struct lglock *lg);
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+#define lg_global_trylock_relax(name)	lg_global_lock(name)
+#else
+void lg_global_trylock_relax(struct lglock *lg);
+#endif
+
 #else
 /* When !CONFIG_SMP, map lglock to spinlock */
 #define lglock spinlock
diff --git a/kernel/msm-3.18/include/linux/list_bl.h b/kernel/msm-3.18/include/linux/list_bl.h
index 2eb88556c..017d0f1c1 100644
--- a/kernel/msm-3.18/include/linux/list_bl.h
+++ b/kernel/msm-3.18/include/linux/list_bl.h
@@ -2,6 +2,7 @@
 #define _LINUX_LIST_BL_H
 
 #include <linux/list.h>
+#include <linux/spinlock.h>
 #include <linux/bit_spinlock.h>
 
 /*
@@ -32,13 +33,24 @@
 
 struct hlist_bl_head {
 	struct hlist_bl_node *first;
+#ifdef CONFIG_PREEMPT_RT_BASE
+	raw_spinlock_t lock;
+#endif
 };
 
 struct hlist_bl_node {
 	struct hlist_bl_node *next, **pprev;
 };
-#define INIT_HLIST_BL_HEAD(ptr) \
-	((ptr)->first = NULL)
+
+#ifdef CONFIG_PREEMPT_RT_BASE
+#define INIT_HLIST_BL_HEAD(h)		\
+do {					\
+	(h)->first = NULL;		\
+	raw_spin_lock_init(&(h)->lock);	\
+} while (0)
+#else
+#define INIT_HLIST_BL_HEAD(h) (h)->first = NULL
+#endif
 
 static inline void INIT_HLIST_BL_NODE(struct hlist_bl_node *h)
 {
@@ -117,12 +129,26 @@ static inline void hlist_bl_del_init(struct hlist_bl_node *n)
 
 static inline void hlist_bl_lock(struct hlist_bl_head *b)
 {
+#ifndef CONFIG_PREEMPT_RT_BASE
 	bit_spin_lock(0, (unsigned long *)b);
+#else
+	raw_spin_lock(&b->lock);
+#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
+	__set_bit(0, (unsigned long *)b);
+#endif
+#endif
 }
 
 static inline void hlist_bl_unlock(struct hlist_bl_head *b)
 {
+#ifndef CONFIG_PREEMPT_RT_BASE
 	__bit_spin_unlock(0, (unsigned long *)b);
+#else
+#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
+	__clear_bit(0, (unsigned long *)b);
+#endif
+	raw_spin_unlock(&b->lock);
+#endif
 }
 
 static inline bool hlist_bl_is_locked(struct hlist_bl_head *b)
diff --git a/kernel/msm-3.18/include/linux/locallock.h b/kernel/msm-3.18/include/linux/locallock.h
new file mode 100644
index 000000000..015271ff8
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/locallock.h
@@ -0,0 +1,276 @@
+#ifndef _LINUX_LOCALLOCK_H
+#define _LINUX_LOCALLOCK_H
+
+#include <linux/percpu.h>
+#include <linux/spinlock.h>
+
+#ifdef CONFIG_PREEMPT_RT_BASE
+
+#ifdef CONFIG_DEBUG_SPINLOCK
+# define LL_WARN(cond)	WARN_ON(cond)
+#else
+# define LL_WARN(cond)	do { } while (0)
+#endif
+
+/*
+ * per cpu lock based substitute for local_irq_*()
+ */
+struct local_irq_lock {
+	spinlock_t		lock;
+	struct task_struct	*owner;
+	int			nestcnt;
+	unsigned long		flags;
+};
+
+#define DEFINE_LOCAL_IRQ_LOCK(lvar)					\
+	DEFINE_PER_CPU(struct local_irq_lock, lvar) = {			\
+		.lock = __SPIN_LOCK_UNLOCKED((lvar).lock) }
+
+#define DECLARE_LOCAL_IRQ_LOCK(lvar)					\
+	DECLARE_PER_CPU(struct local_irq_lock, lvar)
+
+#define local_irq_lock_init(lvar)					\
+	do {								\
+		int __cpu;						\
+		for_each_possible_cpu(__cpu)				\
+			spin_lock_init(&per_cpu(lvar, __cpu).lock);	\
+	} while (0)
+
+/*
+ * spin_lock|trylock|unlock_local flavour that does not migrate disable
+ * used for __local_lock|trylock|unlock where get_local_var/put_local_var
+ * already takes care of the migrate_disable/enable
+ * for CONFIG_PREEMPT_BASE map to the normal spin_* calls.
+ */
+#ifdef CONFIG_PREEMPT_RT_FULL
+# define spin_lock_local(lock)			rt_spin_lock(lock)
+# define spin_trylock_local(lock)		rt_spin_trylock(lock)
+# define spin_unlock_local(lock)		rt_spin_unlock(lock)
+#else
+# define spin_lock_local(lock)			spin_lock(lock)
+# define spin_trylock_local(lock)		spin_trylock(lock)
+# define spin_unlock_local(lock)		spin_unlock(lock)
+#endif
+
+static inline void __local_lock(struct local_irq_lock *lv)
+{
+	if (lv->owner != current) {
+		spin_lock_local(&lv->lock);
+		LL_WARN(lv->owner);
+		LL_WARN(lv->nestcnt);
+		lv->owner = current;
+	}
+	lv->nestcnt++;
+}
+
+#define local_lock(lvar)					\
+	do { __local_lock(&get_local_var(lvar)); } while (0)
+
+#define local_lock_on(lvar, cpu)				\
+	do { __local_lock(&per_cpu(lvar, cpu)); } while (0)
+
+static inline int __local_trylock(struct local_irq_lock *lv)
+{
+	if (lv->owner != current && spin_trylock_local(&lv->lock)) {
+		LL_WARN(lv->owner);
+		LL_WARN(lv->nestcnt);
+		lv->owner = current;
+		lv->nestcnt = 1;
+		return 1;
+	}
+	return 0;
+}
+
+#define local_trylock(lvar)						\
+	({								\
+		int __locked;						\
+		__locked = __local_trylock(&get_local_var(lvar));	\
+		if (!__locked)						\
+			put_local_var(lvar);				\
+		__locked;						\
+	})
+
+static inline void __local_unlock(struct local_irq_lock *lv)
+{
+	LL_WARN(lv->nestcnt == 0);
+	LL_WARN(lv->owner != current);
+	if (--lv->nestcnt)
+		return;
+
+	lv->owner = NULL;
+	spin_unlock_local(&lv->lock);
+}
+
+#define local_unlock(lvar)					\
+	do {							\
+		__local_unlock(&__get_cpu_var(lvar));		\
+		put_local_var(lvar);				\
+	} while (0)
+
+#define local_unlock_on(lvar, cpu)                       \
+	do { __local_unlock(&per_cpu(lvar, cpu)); } while (0)
+
+static inline void __local_lock_irq(struct local_irq_lock *lv)
+{
+	spin_lock_irqsave(&lv->lock, lv->flags);
+	LL_WARN(lv->owner);
+	LL_WARN(lv->nestcnt);
+	lv->owner = current;
+	lv->nestcnt = 1;
+}
+
+#define local_lock_irq(lvar)						\
+	do { __local_lock_irq(&get_local_var(lvar)); } while (0)
+
+#define local_lock_irq_on(lvar, cpu)					\
+	do { __local_lock_irq(&per_cpu(lvar, cpu)); } while (0)
+
+static inline void __local_unlock_irq(struct local_irq_lock *lv)
+{
+	LL_WARN(!lv->nestcnt);
+	LL_WARN(lv->owner != current);
+	lv->owner = NULL;
+	lv->nestcnt = 0;
+	spin_unlock_irq(&lv->lock);
+}
+
+#define local_unlock_irq(lvar)						\
+	do {								\
+		__local_unlock_irq(&__get_cpu_var(lvar));		\
+		put_local_var(lvar);					\
+	} while (0)
+
+#define local_unlock_irq_on(lvar, cpu)					\
+	do {								\
+		__local_unlock_irq(&per_cpu(lvar, cpu));		\
+	} while (0)
+
+static inline int __local_lock_irqsave(struct local_irq_lock *lv)
+{
+	if (lv->owner != current) {
+		__local_lock_irq(lv);
+		return 0;
+	} else {
+		lv->nestcnt++;
+		return 1;
+	}
+}
+
+#define local_lock_irqsave(lvar, _flags)				\
+	do {								\
+		if (__local_lock_irqsave(&get_local_var(lvar)))		\
+			put_local_var(lvar);				\
+		_flags = __get_cpu_var(lvar).flags;			\
+	} while (0)
+
+#define local_lock_irqsave_on(lvar, _flags, cpu)			\
+	do {								\
+		__local_lock_irqsave(&per_cpu(lvar, cpu));		\
+		_flags = per_cpu(lvar, cpu).flags;			\
+	} while (0)
+
+static inline int __local_unlock_irqrestore(struct local_irq_lock *lv,
+					    unsigned long flags)
+{
+	LL_WARN(!lv->nestcnt);
+	LL_WARN(lv->owner != current);
+	if (--lv->nestcnt)
+		return 0;
+
+	lv->owner = NULL;
+	spin_unlock_irqrestore(&lv->lock, lv->flags);
+	return 1;
+}
+
+#define local_unlock_irqrestore(lvar, flags)				\
+	do {								\
+		if (__local_unlock_irqrestore(&__get_cpu_var(lvar), flags)) \
+			put_local_var(lvar);				\
+	} while (0)
+
+#define local_unlock_irqrestore_on(lvar, flags, cpu)			\
+	do {								\
+		__local_unlock_irqrestore(&per_cpu(lvar, cpu), flags);	\
+	} while (0)
+
+#define local_spin_trylock_irq(lvar, lock)				\
+	({								\
+		int __locked;						\
+		local_lock_irq(lvar);					\
+		__locked = spin_trylock(lock);				\
+		if (!__locked)						\
+			local_unlock_irq(lvar);				\
+		__locked;						\
+	})
+
+#define local_spin_lock_irq(lvar, lock)					\
+	do {								\
+		local_lock_irq(lvar);					\
+		spin_lock(lock);					\
+	} while (0)
+
+#define local_spin_unlock_irq(lvar, lock)				\
+	do {								\
+		spin_unlock(lock);					\
+		local_unlock_irq(lvar);					\
+	} while (0)
+
+#define local_spin_lock_irqsave(lvar, lock, flags)			\
+	do {								\
+		local_lock_irqsave(lvar, flags);			\
+		spin_lock(lock);					\
+	} while (0)
+
+#define local_spin_unlock_irqrestore(lvar, lock, flags)			\
+	do {								\
+		spin_unlock(lock);					\
+		local_unlock_irqrestore(lvar, flags);			\
+	} while (0)
+
+#define get_locked_var(lvar, var)					\
+	(*({								\
+		local_lock(lvar);					\
+		&__get_cpu_var(var);					\
+	}))
+
+#define put_locked_var(lvar, var)	local_unlock(lvar);
+
+#define local_lock_cpu(lvar)						\
+	({								\
+		local_lock(lvar);					\
+		smp_processor_id();					\
+	})
+
+#define local_unlock_cpu(lvar)			local_unlock(lvar)
+
+#else /* PREEMPT_RT_BASE */
+
+#define DEFINE_LOCAL_IRQ_LOCK(lvar)		__typeof__(const int) lvar
+#define DECLARE_LOCAL_IRQ_LOCK(lvar)		extern __typeof__(const int) lvar
+
+static inline void local_irq_lock_init(int lvar) { }
+
+#define local_lock(lvar)			preempt_disable()
+#define local_unlock(lvar)			preempt_enable()
+#define local_lock_irq(lvar)			local_irq_disable()
+#define local_unlock_irq(lvar)			local_irq_enable()
+#define local_lock_irqsave(lvar, flags)		local_irq_save(flags)
+#define local_unlock_irqrestore(lvar, flags)	local_irq_restore(flags)
+
+#define local_spin_trylock_irq(lvar, lock)	spin_trylock_irq(lock)
+#define local_spin_lock_irq(lvar, lock)		spin_lock_irq(lock)
+#define local_spin_unlock_irq(lvar, lock)	spin_unlock_irq(lock)
+#define local_spin_lock_irqsave(lvar, lock, flags)	\
+	spin_lock_irqsave(lock, flags)
+#define local_spin_unlock_irqrestore(lvar, lock, flags)	\
+	spin_unlock_irqrestore(lock, flags)
+
+#define get_locked_var(lvar, var)		get_cpu_var(var)
+#define put_locked_var(lvar, var)		put_cpu_var(var)
+
+#define local_lock_cpu(lvar)			get_cpu()
+#define local_unlock_cpu(lvar)			put_cpu()
+
+#endif
+
+#endif
diff --git a/kernel/msm-3.18/include/linux/mm_types.h b/kernel/msm-3.18/include/linux/mm_types.h
index 2e67166e3..a2547ac55 100644
--- a/kernel/msm-3.18/include/linux/mm_types.h
+++ b/kernel/msm-3.18/include/linux/mm_types.h
@@ -12,6 +12,7 @@
 #include <linux/completion.h>
 #include <linux/cpumask.h>
 #include <linux/page-debug-flags.h>
+#include <linux/rcupdate.h>
 #include <linux/uprobes.h>
 #include <linux/page-flags-layout.h>
 #include <asm/page.h>
@@ -472,7 +473,9 @@ struct mm_struct {
 #ifdef CONFIG_MSM_APP_SETTINGS
 	int app_setting;
 #endif
-
+#ifdef CONFIG_PREEMPT_RT_BASE
+	struct rcu_head delayed_drop;
+#endif
 };
 
 static inline void mm_init_cpumask(struct mm_struct *mm)
diff --git a/kernel/msm-3.18/include/linux/module.h b/kernel/msm-3.18/include/linux/module.h
index 30e2b3feb..ba0155bde 100644
--- a/kernel/msm-3.18/include/linux/module.h
+++ b/kernel/msm-3.18/include/linux/module.h
@@ -396,6 +396,7 @@ static inline int module_is_live(struct module *mod)
 struct module *__module_text_address(unsigned long addr);
 struct module *__module_address(unsigned long addr);
 bool is_module_address(unsigned long addr);
+bool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr);
 bool is_module_percpu_address(unsigned long addr);
 bool is_module_text_address(unsigned long addr);
 
@@ -553,6 +554,11 @@ static inline bool is_module_percpu_address(unsigned long addr)
 	return false;
 }
 
+static inline bool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr)
+{
+	return false;
+}
+
 static inline bool is_module_text_address(unsigned long addr)
 {
 	return false;
diff --git a/kernel/msm-3.18/include/linux/mutex.h b/kernel/msm-3.18/include/linux/mutex.h
index cc31498fc..a9143091a 100644
--- a/kernel/msm-3.18/include/linux/mutex.h
+++ b/kernel/msm-3.18/include/linux/mutex.h
@@ -19,6 +19,17 @@
 #include <asm/processor.h>
 #include <linux/osq_lock.h>
 
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define __DEP_MAP_MUTEX_INITIALIZER(lockname) \
+	, .dep_map = { .name = #lockname }
+#else
+# define __DEP_MAP_MUTEX_INITIALIZER(lockname)
+#endif
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+# include <linux/mutex_rt.h>
+#else
+
 /*
  * Simple, straightforward mutexes with strict semantics:
  *
@@ -100,13 +111,6 @@ do {							\
 static inline void mutex_destroy(struct mutex *lock) {}
 #endif
 
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define __DEP_MAP_MUTEX_INITIALIZER(lockname) \
-		, .dep_map = { .name = #lockname }
-#else
-# define __DEP_MAP_MUTEX_INITIALIZER(lockname)
-#endif
-
 #define __MUTEX_INITIALIZER(lockname) \
 		{ .count = ATOMIC_INIT(1) \
 		, .wait_lock = __SPIN_LOCK_UNLOCKED(lockname.wait_lock) \
@@ -174,6 +178,8 @@ extern int __must_check mutex_lock_killable(struct mutex *lock);
 extern int mutex_trylock(struct mutex *lock);
 extern void mutex_unlock(struct mutex *lock);
 
+#endif /* !PREEMPT_RT_FULL */
+
 extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);
 
 #endif /* __LINUX_MUTEX_H */
diff --git a/kernel/msm-3.18/include/linux/mutex_rt.h b/kernel/msm-3.18/include/linux/mutex_rt.h
new file mode 100644
index 000000000..e0284edec
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/mutex_rt.h
@@ -0,0 +1,89 @@
+#ifndef __LINUX_MUTEX_RT_H
+#define __LINUX_MUTEX_RT_H
+
+#ifndef __LINUX_MUTEX_H
+#error "Please include mutex.h"
+#endif
+
+#include <linux/rtmutex.h>
+
+/* FIXME: Just for __lockfunc */
+#include <linux/spinlock.h>
+
+struct mutex {
+	struct rt_mutex		lock;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define __MUTEX_INITIALIZER(mutexname)					\
+	{								\
+		.lock = __RT_MUTEX_INITIALIZER(mutexname.lock)		\
+		__DEP_MAP_MUTEX_INITIALIZER(mutexname)			\
+	}
+
+#define DEFINE_MUTEX(mutexname)						\
+	struct mutex mutexname = __MUTEX_INITIALIZER(mutexname)
+
+extern void __mutex_do_init(struct mutex *lock, const char *name, struct lock_class_key *key);
+extern void __lockfunc _mutex_lock(struct mutex *lock);
+extern int __lockfunc _mutex_lock_interruptible(struct mutex *lock);
+extern int __lockfunc _mutex_lock_killable(struct mutex *lock);
+extern void __lockfunc _mutex_lock_nested(struct mutex *lock, int subclass);
+extern void __lockfunc _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest_lock);
+extern int __lockfunc _mutex_lock_interruptible_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_lock_killable_nested(struct mutex *lock, int subclass);
+extern int __lockfunc _mutex_trylock(struct mutex *lock);
+extern void __lockfunc _mutex_unlock(struct mutex *lock);
+
+#define mutex_is_locked(l)		rt_mutex_is_locked(&(l)->lock)
+#define mutex_lock(l)			_mutex_lock(l)
+#define mutex_lock_interruptible(l)	_mutex_lock_interruptible(l)
+#define mutex_lock_killable(l)		_mutex_lock_killable(l)
+#define mutex_trylock(l)		_mutex_trylock(l)
+#define mutex_unlock(l)			_mutex_unlock(l)
+
+#ifdef CONFIG_DEBUG_MUTEXES
+#define mutex_destroy(l)		rt_mutex_destroy(&(l)->lock)
+#else
+static inline void mutex_destroy(struct mutex *lock) {}
+#endif
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define mutex_lock_nested(l, s)	_mutex_lock_nested(l, s)
+# define mutex_lock_interruptible_nested(l, s) \
+					_mutex_lock_interruptible_nested(l, s)
+# define mutex_lock_killable_nested(l, s) \
+					_mutex_lock_killable_nested(l, s)
+
+# define mutex_lock_nest_lock(lock, nest_lock)				\
+do {									\
+	typecheck(struct lockdep_map *, &(nest_lock)->dep_map);		\
+	_mutex_lock_nest_lock(lock, &(nest_lock)->dep_map);		\
+} while (0)
+
+#else
+# define mutex_lock_nested(l, s)	_mutex_lock(l)
+# define mutex_lock_interruptible_nested(l, s) \
+					_mutex_lock_interruptible(l)
+# define mutex_lock_killable_nested(l, s) \
+					_mutex_lock_killable(l)
+# define mutex_lock_nest_lock(lock, nest_lock) mutex_lock(lock)
+#endif
+
+# define mutex_init(mutex)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	rt_mutex_init(&(mutex)->lock);			\
+	__mutex_do_init((mutex), #mutex, &__key);	\
+} while (0)
+
+# define __mutex_init(mutex, name, key)			\
+do {							\
+	rt_mutex_init(&(mutex)->lock);			\
+	__mutex_do_init((mutex), name, key);		\
+} while (0)
+
+#endif
diff --git a/kernel/msm-3.18/include/linux/netdevice.h b/kernel/msm-3.18/include/linux/netdevice.h
index 766e08f9e..846dadb46 100644
--- a/kernel/msm-3.18/include/linux/netdevice.h
+++ b/kernel/msm-3.18/include/linux/netdevice.h
@@ -2157,11 +2157,20 @@ void netdev_freemem(struct net_device *dev);
 void synchronize_net(void);
 int init_dummy_netdev(struct net_device *dev);
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+static inline int dev_recursion_level(void)
+{
+	return current->xmit_recursion;
+}
+
+#else
+
 DECLARE_PER_CPU(int, xmit_recursion);
 static inline int dev_recursion_level(void)
 {
 	return this_cpu_read(xmit_recursion);
 }
+#endif
 
 struct net_device *dev_get_by_index(struct net *net, int ifindex);
 struct net_device *__dev_get_by_index(struct net *net, int ifindex);
@@ -2388,6 +2397,7 @@ struct softnet_data {
 	unsigned int		dropped;
 	struct sk_buff_head	input_pkt_queue;
 	struct napi_struct	backlog;
+	struct sk_buff_head	tofree_queue;
 
 #ifdef CONFIG_NET_FLOW_LIMIT
 	struct sd_flow_limit __rcu *flow_limit;
diff --git a/kernel/msm-3.18/include/linux/netfilter/x_tables.h b/kernel/msm-3.18/include/linux/netfilter/x_tables.h
index cc615e273..1a6ba6d7f 100644
--- a/kernel/msm-3.18/include/linux/netfilter/x_tables.h
+++ b/kernel/msm-3.18/include/linux/netfilter/x_tables.h
@@ -3,6 +3,7 @@
 
 
 #include <linux/netdevice.h>
+#include <linux/locallock.h>
 #include <uapi/linux/netfilter/x_tables.h>
 
 /**
@@ -293,6 +294,8 @@ void xt_free_table_info(struct xt_table_info *info);
  */
 DECLARE_PER_CPU(seqcount_t, xt_recseq);
 
+DECLARE_LOCAL_IRQ_LOCK(xt_write_lock);
+
 /**
  * xt_write_recseq_begin - start of a write section
  *
@@ -307,6 +310,9 @@ static inline unsigned int xt_write_recseq_begin(void)
 {
 	unsigned int addend;
 
+	/* RT protection */
+	local_lock(xt_write_lock);
+
 	/*
 	 * Low order bit of sequence is set if we already
 	 * called xt_write_recseq_begin().
@@ -337,6 +343,7 @@ static inline void xt_write_recseq_end(unsigned int addend)
 	/* this is kind of a write_seqcount_end(), but addend is 0 or 1 */
 	smp_wmb();
 	__this_cpu_add(xt_recseq.sequence, addend);
+	local_unlock(xt_write_lock);
 }
 
 /*
diff --git a/kernel/msm-3.18/include/linux/notifier.h b/kernel/msm-3.18/include/linux/notifier.h
index d14a4c362..2e4414a0c 100644
--- a/kernel/msm-3.18/include/linux/notifier.h
+++ b/kernel/msm-3.18/include/linux/notifier.h
@@ -6,7 +6,7 @@
  *
  *				Alan Cox <Alan.Cox@linux.org>
  */
- 
+
 #ifndef _LINUX_NOTIFIER_H
 #define _LINUX_NOTIFIER_H
 #include <linux/errno.h>
@@ -42,9 +42,7 @@
  * in srcu_notifier_call_chain(): no cache bounces and no memory barriers.
  * As compensation, srcu_notifier_chain_unregister() is rather expensive.
  * SRCU notifier chains should be used when the chain will be called very
- * often but notifier_blocks will seldom be removed.  Also, SRCU notifier
- * chains are slightly more difficult to use because they require special
- * runtime initialization.
+ * often but notifier_blocks will seldom be removed.
  */
 
 typedef	int (*notifier_fn_t)(struct notifier_block *nb,
@@ -88,7 +86,7 @@ struct srcu_notifier_head {
 		(name)->head = NULL;		\
 	} while (0)
 
-/* srcu_notifier_heads must be initialized and cleaned up dynamically */
+/* srcu_notifier_heads must be cleaned up dynamically */
 extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
 #define srcu_cleanup_notifier_head(name)	\
 		cleanup_srcu_struct(&(name)->srcu);
@@ -101,7 +99,13 @@ extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
 		.head = NULL }
 #define RAW_NOTIFIER_INIT(name)	{				\
 		.head = NULL }
-/* srcu_notifier_heads cannot be initialized statically */
+
+#define SRCU_NOTIFIER_INIT(name, pcpu)				\
+	{							\
+		.mutex = __MUTEX_INITIALIZER(name.mutex),	\
+		.head = NULL,					\
+		.srcu = __SRCU_STRUCT_INIT(name.srcu, pcpu),	\
+	}
 
 #define ATOMIC_NOTIFIER_HEAD(name)				\
 	struct atomic_notifier_head name =			\
@@ -113,6 +117,18 @@ extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
 	struct raw_notifier_head name =				\
 		RAW_NOTIFIER_INIT(name)
 
+#define _SRCU_NOTIFIER_HEAD(name, mod)				\
+	static DEFINE_PER_CPU(struct srcu_struct_array,		\
+			name##_head_srcu_array);		\
+	mod struct srcu_notifier_head name =			\
+			SRCU_NOTIFIER_INIT(name, name##_head_srcu_array)
+
+#define SRCU_NOTIFIER_HEAD(name)				\
+	_SRCU_NOTIFIER_HEAD(name, )
+
+#define SRCU_NOTIFIER_HEAD_STATIC(name)				\
+	_SRCU_NOTIFIER_HEAD(name, static)
+
 #ifdef __KERNEL__
 
 extern int atomic_notifier_chain_register(struct atomic_notifier_head *nh,
@@ -182,12 +198,12 @@ static inline int notifier_to_errno(int ret)
 
 /*
  *	Declared notifiers so far. I can imagine quite a few more chains
- *	over time (eg laptop power reset chains, reboot chain (to clean 
+ *	over time (eg laptop power reset chains, reboot chain (to clean
  *	device units up), device [un]mount chain, module load/unload chain,
- *	low memory chain, screenblank chain (for plug in modular screenblankers) 
+ *	low memory chain, screenblank chain (for plug in modular screenblankers)
  *	VC switch chains (for loadable kernel svgalib VC switch helpers) etc...
  */
- 
+
 /* CPU notfiers are defined in include/linux/cpu.h. */
 
 /* netdevice notifiers are defined in include/linux/netdevice.h */
diff --git a/kernel/msm-3.18/include/linux/percpu.h b/kernel/msm-3.18/include/linux/percpu.h
index a3aa63e47..a3a446c29 100644
--- a/kernel/msm-3.18/include/linux/percpu.h
+++ b/kernel/msm-3.18/include/linux/percpu.h
@@ -23,6 +23,35 @@
 	 PERCPU_MODULE_RESERVE)
 #endif
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+
+#define get_local_var(var) (*({		\
+	       migrate_disable();	\
+	       &__get_cpu_var(var);	}))
+
+#define put_local_var(var) do {	\
+	(void)&(var);		\
+	migrate_enable();	\
+} while (0)
+
+# define get_local_ptr(var) ({		\
+		migrate_disable();	\
+		this_cpu_ptr(var);	})
+
+# define put_local_ptr(var) do {	\
+	(void)(var);			\
+	migrate_enable();		\
+} while (0)
+
+#else
+
+#define get_local_var(var)	get_cpu_var(var)
+#define put_local_var(var)	put_cpu_var(var)
+#define get_local_ptr(var)	get_cpu_ptr(var)
+#define put_local_ptr(var)	put_cpu_ptr(var)
+
+#endif
+
 /* minimum unit size, also is the maximum supported allocation size */
 #define PCPU_MIN_UNIT_SIZE		PFN_ALIGN(32 << 10)
 
@@ -115,6 +144,7 @@ extern int __init pcpu_page_first_chunk(size_t reserved_size,
 #endif
 
 extern void __percpu *__alloc_reserved_percpu(size_t size, size_t align);
+extern bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr);
 extern bool is_kernel_percpu_address(unsigned long addr);
 
 #if !defined(CONFIG_SMP) || !defined(CONFIG_HAVE_SETUP_PER_CPU_AREA)
diff --git a/kernel/msm-3.18/include/linux/pid.h b/kernel/msm-3.18/include/linux/pid.h
index 23705a53a..2cc64b779 100644
--- a/kernel/msm-3.18/include/linux/pid.h
+++ b/kernel/msm-3.18/include/linux/pid.h
@@ -2,6 +2,7 @@
 #define _LINUX_PID_H
 
 #include <linux/rcupdate.h>
+#include <linux/atomic.h>
 
 enum pid_type
 {
diff --git a/kernel/msm-3.18/include/linux/preempt.h b/kernel/msm-3.18/include/linux/preempt.h
index de2389062..55d327eae 100644
--- a/kernel/msm-3.18/include/linux/preempt.h
+++ b/kernel/msm-3.18/include/linux/preempt.h
@@ -58,7 +58,11 @@
 #define HARDIRQ_OFFSET	(1UL << HARDIRQ_SHIFT)
 #define NMI_OFFSET	(1UL << NMI_SHIFT)
 
-#define SOFTIRQ_DISABLE_OFFSET	(2 * SOFTIRQ_OFFSET)
+#ifndef CONFIG_PREEMPT_RT_FULL
+# define SOFTIRQ_DISABLE_OFFSET        (2 * SOFTIRQ_OFFSET)
+#else
+# define SOFTIRQ_DISABLE_OFFSET        (0)
+#endif
 
 #define PREEMPT_ACTIVE_BITS	1
 #define PREEMPT_ACTIVE_SHIFT	(NMI_SHIFT + NMI_BITS)
@@ -68,6 +72,14 @@
 #define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
 #define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK \
 				 | NMI_MASK))
+#ifndef CONFIG_PREEMPT_RT_FULL
+# define softirq_count()       (preempt_count() & SOFTIRQ_MASK)
+# define in_serving_softirq()  (softirq_count() & SOFTIRQ_OFFSET)
+#else
+# define softirq_count()       (0UL)
+extern int in_serving_softirq(void);
+#endif
+
 
 /*
  * Are we doing bottom half or hardware interrupt processing?
@@ -78,7 +90,6 @@
 #define in_irq()		(hardirq_count())
 #define in_softirq()		(softirq_count())
 #define in_interrupt()		(irq_count())
-#define in_serving_softirq()	(softirq_count() & SOFTIRQ_OFFSET)
 
 /*
  * Are we in NMI context?
@@ -153,6 +164,20 @@ extern void preempt_count_sub(int val);
 #define preempt_count_inc() preempt_count_add(1)
 #define preempt_count_dec() preempt_count_sub(1)
 
+#ifdef CONFIG_PREEMPT_LAZY
+#define add_preempt_lazy_count(val)	do { preempt_lazy_count() += (val); } while (0)
+#define sub_preempt_lazy_count(val)	do { preempt_lazy_count() -= (val); } while (0)
+#define inc_preempt_lazy_count()	add_preempt_lazy_count(1)
+#define dec_preempt_lazy_count()	sub_preempt_lazy_count(1)
+#define preempt_lazy_count()		(current_thread_info()->preempt_lazy_count)
+#else
+#define add_preempt_lazy_count(val)	do { } while (0)
+#define sub_preempt_lazy_count(val)	do { } while (0)
+#define inc_preempt_lazy_count()	do { } while (0)
+#define dec_preempt_lazy_count()	do { } while (0)
+#define preempt_lazy_count()		(0)
+#endif
+
 #ifdef CONFIG_PREEMPT_COUNT
 
 #define preempt_disable() \
@@ -161,13 +186,25 @@ do { \
 	barrier(); \
 } while (0)
 
+#define preempt_lazy_disable() \
+do { \
+	inc_preempt_lazy_count(); \
+	barrier(); \
+} while (0)
+
 #define sched_preempt_enable_no_resched() \
 do { \
 	barrier(); \
 	preempt_count_dec(); \
 } while (0)
 
-#define preempt_enable_no_resched() sched_preempt_enable_no_resched()
+#ifdef CONFIG_PREEMPT_RT_BASE
+# define preempt_enable_no_resched() sched_preempt_enable_no_resched()
+# define preempt_check_resched_rt() preempt_check_resched()
+#else
+# define preempt_enable_no_resched() preempt_enable()
+# define preempt_check_resched_rt() barrier();
+#endif
 
 #ifdef CONFIG_PREEMPT
 #define preempt_enable() \
@@ -183,6 +220,13 @@ do { \
 		__preempt_schedule(); \
 } while (0)
 
+#define preempt_lazy_enable() \
+do { \
+	dec_preempt_lazy_count(); \
+	barrier(); \
+	preempt_check_resched(); \
+} while (0)
+
 #else
 #define preempt_enable() \
 do { \
@@ -241,6 +285,7 @@ do { \
 #define preempt_disable_notrace()		barrier()
 #define preempt_enable_no_resched_notrace()	barrier()
 #define preempt_enable_notrace()		barrier()
+#define preempt_check_resched_rt()		barrier()
 
 #endif /* CONFIG_PREEMPT_COUNT */
 
@@ -260,10 +305,31 @@ do { \
 } while (0)
 #define preempt_fold_need_resched() \
 do { \
-	if (tif_need_resched()) \
+	if (tif_need_resched_now()) \
 		set_preempt_need_resched(); \
 } while (0)
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+# define preempt_disable_rt()		preempt_disable()
+# define preempt_enable_rt()		preempt_enable()
+# define preempt_disable_nort()		barrier()
+# define preempt_enable_nort()		barrier()
+# ifdef CONFIG_SMP
+   extern void migrate_disable(void);
+   extern void migrate_enable(void);
+# else /* CONFIG_SMP */
+#  define migrate_disable()		barrier()
+#  define migrate_enable()		barrier()
+# endif /* CONFIG_SMP */
+#else
+# define preempt_disable_rt()		barrier()
+# define preempt_enable_rt()		barrier()
+# define preempt_disable_nort()		preempt_disable()
+# define preempt_enable_nort()		preempt_enable()
+# define migrate_disable()		preempt_disable()
+# define migrate_enable()		preempt_enable()
+#endif
+
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 
 struct preempt_notifier;
diff --git a/kernel/msm-3.18/include/linux/printk.h b/kernel/msm-3.18/include/linux/printk.h
index d78125f73..9fd2d6fa4 100644
--- a/kernel/msm-3.18/include/linux/printk.h
+++ b/kernel/msm-3.18/include/linux/printk.h
@@ -119,9 +119,11 @@ int no_printk(const char *fmt, ...)
 extern asmlinkage __printf(1, 2)
 void early_printk(const char *fmt, ...);
 void early_vprintk(const char *fmt, va_list ap);
+extern void printk_kill(void);
 #else
 static inline __printf(1, 2) __cold
 void early_printk(const char *s, ...) { }
+static inline void printk_kill(void) { }
 #endif
 
 #ifdef CONFIG_PRINTK
@@ -155,7 +157,6 @@ extern int __printk_ratelimit(const char *func);
 #define printk_ratelimit() __printk_ratelimit(__func__)
 extern bool printk_timed_ratelimit(unsigned long *caller_jiffies,
 				   unsigned int interval_msec);
-
 extern int printk_delay_msec;
 extern int dmesg_restrict;
 extern int kptr_restrict;
diff --git a/kernel/msm-3.18/include/linux/radix-tree.h b/kernel/msm-3.18/include/linux/radix-tree.h
index f9f3a4541..5334b7f74 100644
--- a/kernel/msm-3.18/include/linux/radix-tree.h
+++ b/kernel/msm-3.18/include/linux/radix-tree.h
@@ -283,6 +283,8 @@ unsigned int radix_tree_gang_lookup_slot(struct radix_tree_root *root,
 			unsigned long first_index, unsigned int max_items);
 int radix_tree_preload(gfp_t gfp_mask);
 int radix_tree_maybe_preload(gfp_t gfp_mask);
+void radix_tree_preload_end(void);
+
 void radix_tree_init(void);
 void *radix_tree_tag_set(struct radix_tree_root *root,
 			unsigned long index, unsigned int tag);
@@ -305,11 +307,6 @@ unsigned long radix_tree_range_tag_if_tagged(struct radix_tree_root *root,
 int radix_tree_tagged(struct radix_tree_root *root, unsigned int tag);
 unsigned long radix_tree_locate_item(struct radix_tree_root *root, void *item);
 
-static inline void radix_tree_preload_end(void)
-{
-	preempt_enable();
-}
-
 /**
  * struct radix_tree_iter - radix tree iterator state
  *
diff --git a/kernel/msm-3.18/include/linux/random.h b/kernel/msm-3.18/include/linux/random.h
index 0fe49a14d..c71b98cd6 100644
--- a/kernel/msm-3.18/include/linux/random.h
+++ b/kernel/msm-3.18/include/linux/random.h
@@ -11,7 +11,7 @@
 extern void add_device_randomness(const void *, unsigned int);
 extern void add_input_randomness(unsigned int type, unsigned int code,
 				 unsigned int value);
-extern void add_interrupt_randomness(int irq, int irq_flags);
+extern void add_interrupt_randomness(int irq, int irq_flags, __u64 ip);
 
 extern void get_random_bytes(void *buf, int nbytes);
 extern void get_random_bytes_arch(void *buf, int nbytes);
diff --git a/kernel/msm-3.18/include/linux/rcupdate.h b/kernel/msm-3.18/include/linux/rcupdate.h
index f19afd33b..175f46ec5 100644
--- a/kernel/msm-3.18/include/linux/rcupdate.h
+++ b/kernel/msm-3.18/include/linux/rcupdate.h
@@ -146,6 +146,9 @@ void call_rcu(struct rcu_head *head,
 
 #endif /* #else #ifdef CONFIG_PREEMPT_RCU */
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+#define call_rcu_bh	call_rcu
+#else
 /**
  * call_rcu_bh() - Queue an RCU for invocation after a quicker grace period.
  * @head: structure to be used for queueing the RCU updates.
@@ -169,6 +172,7 @@ void call_rcu(struct rcu_head *head,
  */
 void call_rcu_bh(struct rcu_head *head,
 		 void (*func)(struct rcu_head *head));
+#endif
 
 /**
  * call_rcu_sched() - Queue an RCU for invocation after sched grace period.
@@ -230,6 +234,11 @@ void synchronize_rcu(void);
  * types of kernel builds, the rcu_read_lock() nesting depth is unknowable.
  */
 #define rcu_preempt_depth() (current->rcu_read_lock_nesting)
+#ifndef CONFIG_PREEMPT_RT_FULL
+#define sched_rcu_preempt_depth()	rcu_preempt_depth()
+#else
+static inline int sched_rcu_preempt_depth(void) { return 0; }
+#endif
 
 #else /* #ifdef CONFIG_PREEMPT_RCU */
 
@@ -253,6 +262,8 @@ static inline int rcu_preempt_depth(void)
 	return 0;
 }
 
+#define sched_rcu_preempt_depth()	rcu_preempt_depth()
+
 #endif /* #else #ifdef CONFIG_PREEMPT_RCU */
 
 /* Internal to kernel */
@@ -429,7 +440,14 @@ extern struct lockdep_map rcu_callback_map;
 int debug_lockdep_rcu_enabled(void);
 
 int rcu_read_lock_held(void);
+#ifdef CONFIG_PREEMPT_RT_FULL
+static inline int rcu_read_lock_bh_held(void)
+{
+	return rcu_read_lock_held();
+}
+#else
 int rcu_read_lock_bh_held(void);
+#endif
 
 /**
  * rcu_read_lock_sched_held() - might we be in RCU-sched read-side critical section?
@@ -939,10 +957,14 @@ static inline void rcu_read_unlock(void)
 static inline void rcu_read_lock_bh(void)
 {
 	local_bh_disable();
+#ifdef CONFIG_PREEMPT_RT_FULL
+	rcu_read_lock();
+#else
 	__acquire(RCU_BH);
 	rcu_lock_acquire(&rcu_bh_lock_map);
 	rcu_lockdep_assert(rcu_is_watching(),
 			   "rcu_read_lock_bh() used illegally while idle");
+#endif
 }
 
 /*
@@ -952,10 +974,14 @@ static inline void rcu_read_lock_bh(void)
  */
 static inline void rcu_read_unlock_bh(void)
 {
+#ifdef CONFIG_PREEMPT_RT_FULL
+	rcu_read_unlock();
+#else
 	rcu_lockdep_assert(rcu_is_watching(),
 			   "rcu_read_unlock_bh() used illegally while idle");
 	rcu_lock_release(&rcu_bh_lock_map);
 	__release(RCU_BH);
+#endif
 	local_bh_enable();
 }
 
diff --git a/kernel/msm-3.18/include/linux/rcutree.h b/kernel/msm-3.18/include/linux/rcutree.h
index 3e2f5d432..b9899eff1 100644
--- a/kernel/msm-3.18/include/linux/rcutree.h
+++ b/kernel/msm-3.18/include/linux/rcutree.h
@@ -46,7 +46,11 @@ static inline void rcu_virt_note_context_switch(int cpu)
 	rcu_note_context_switch(cpu);
 }
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+# define synchronize_rcu_bh	synchronize_rcu
+#else
 void synchronize_rcu_bh(void);
+#endif
 void synchronize_sched_expedited(void);
 void synchronize_rcu_expedited(void);
 
@@ -74,7 +78,11 @@ static inline void synchronize_rcu_bh_expedited(void)
 }
 
 void rcu_barrier(void);
+#ifdef CONFIG_PREEMPT_RT_FULL
+# define rcu_barrier_bh                rcu_barrier
+#else
 void rcu_barrier_bh(void);
+#endif
 void rcu_barrier_sched(void);
 unsigned long get_state_synchronize_rcu(void);
 void cond_synchronize_rcu(unsigned long oldstate);
@@ -82,12 +90,10 @@ void cond_synchronize_rcu(unsigned long oldstate);
 extern unsigned long rcutorture_testseq;
 extern unsigned long rcutorture_vernum;
 long rcu_batches_completed(void);
-long rcu_batches_completed_bh(void);
 long rcu_batches_completed_sched(void);
 void show_rcu_gp_kthreads(void);
 
 void rcu_force_quiescent_state(void);
-void rcu_bh_force_quiescent_state(void);
 void rcu_sched_force_quiescent_state(void);
 
 void exit_rcu(void);
@@ -97,4 +103,12 @@ extern int rcu_scheduler_active __read_mostly;
 
 bool rcu_is_watching(void);
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+void rcu_bh_force_quiescent_state(void);
+long rcu_batches_completed_bh(void);
+#else
+# define rcu_bh_force_quiescent_state	rcu_force_quiescent_state
+# define rcu_batches_completed_bh	rcu_batches_completed
+#endif
+
 #endif /* __LINUX_RCUTREE_H */
diff --git a/kernel/msm-3.18/include/linux/rtmutex.h b/kernel/msm-3.18/include/linux/rtmutex.h
index 1abba5ce2..d5a04ea47 100644
--- a/kernel/msm-3.18/include/linux/rtmutex.h
+++ b/kernel/msm-3.18/include/linux/rtmutex.h
@@ -14,10 +14,14 @@
 
 #include <linux/linkage.h>
 #include <linux/rbtree.h>
-#include <linux/spinlock_types.h>
+#include <linux/spinlock_types_raw.h>
 
 extern int max_lock_depth; /* for sysctl */
 
+#ifdef CONFIG_DEBUG_MUTEXES
+#include <linux/debug_locks.h>
+#endif
+
 /**
  * The rt_mutex structure
  *
@@ -31,8 +35,8 @@ struct rt_mutex {
 	struct rb_root          waiters;
 	struct rb_node          *waiters_leftmost;
 	struct task_struct	*owner;
-#ifdef CONFIG_DEBUG_RT_MUTEXES
 	int			save_state;
+#ifdef CONFIG_DEBUG_RT_MUTEXES
 	const char 		*name, *file;
 	int			line;
 	void			*magic;
@@ -55,22 +59,33 @@ struct hrtimer_sleeper;
 # define rt_mutex_debug_check_no_locks_held(task)	do { } while (0)
 #endif
 
+# define rt_mutex_init(mutex)					\
+	do {							\
+		raw_spin_lock_init(&(mutex)->wait_lock);	\
+		__rt_mutex_init(mutex, #mutex);			\
+	} while (0)
+
 #ifdef CONFIG_DEBUG_RT_MUTEXES
 # define __DEBUG_RT_MUTEX_INITIALIZER(mutexname) \
 	, .name = #mutexname, .file = __FILE__, .line = __LINE__
-# define rt_mutex_init(mutex)			__rt_mutex_init(mutex, __func__)
  extern void rt_mutex_debug_task_free(struct task_struct *tsk);
 #else
 # define __DEBUG_RT_MUTEX_INITIALIZER(mutexname)
-# define rt_mutex_init(mutex)			__rt_mutex_init(mutex, NULL)
 # define rt_mutex_debug_task_free(t)			do { } while (0)
 #endif
 
-#define __RT_MUTEX_INITIALIZER(mutexname) \
-	{ .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
+#define __RT_MUTEX_INITIALIZER_PLAIN(mutexname) \
+	 .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
 	, .waiters = RB_ROOT \
 	, .owner = NULL \
-	__DEBUG_RT_MUTEX_INITIALIZER(mutexname)}
+	__DEBUG_RT_MUTEX_INITIALIZER(mutexname)
+
+#define __RT_MUTEX_INITIALIZER(mutexname) \
+	{ __RT_MUTEX_INITIALIZER_PLAIN(mutexname) }
+
+#define __RT_MUTEX_INITIALIZER_SAVE_STATE(mutexname) \
+	{ __RT_MUTEX_INITIALIZER_PLAIN(mutexname)    \
+	, .save_state = 1 }
 
 #define DEFINE_RT_MUTEX(mutexname) \
 	struct rt_mutex mutexname = __RT_MUTEX_INITIALIZER(mutexname)
@@ -91,6 +106,7 @@ extern void rt_mutex_destroy(struct rt_mutex *lock);
 
 extern void rt_mutex_lock(struct rt_mutex *lock);
 extern int rt_mutex_lock_interruptible(struct rt_mutex *lock);
+extern int rt_mutex_lock_killable(struct rt_mutex *lock);
 extern int rt_mutex_timed_lock(struct rt_mutex *lock,
 			       struct hrtimer_sleeper *timeout);
 
diff --git a/kernel/msm-3.18/include/linux/rwlock_rt.h b/kernel/msm-3.18/include/linux/rwlock_rt.h
new file mode 100644
index 000000000..49ed2d45d
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/rwlock_rt.h
@@ -0,0 +1,99 @@
+#ifndef __LINUX_RWLOCK_RT_H
+#define __LINUX_RWLOCK_RT_H
+
+#ifndef __LINUX_SPINLOCK_H
+#error Do not include directly. Use spinlock.h
+#endif
+
+#define rwlock_init(rwl)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	rt_mutex_init(&(rwl)->lock);			\
+	__rt_rwlock_init(rwl, #rwl, &__key);		\
+} while (0)
+
+extern void __lockfunc rt_write_lock(rwlock_t *rwlock);
+extern void __lockfunc rt_read_lock(rwlock_t *rwlock);
+extern int __lockfunc rt_write_trylock(rwlock_t *rwlock);
+extern int __lockfunc rt_write_trylock_irqsave(rwlock_t *trylock, unsigned long *flags);
+extern int __lockfunc rt_read_trylock(rwlock_t *rwlock);
+extern void __lockfunc rt_write_unlock(rwlock_t *rwlock);
+extern void __lockfunc rt_read_unlock(rwlock_t *rwlock);
+extern unsigned long __lockfunc rt_write_lock_irqsave(rwlock_t *rwlock);
+extern unsigned long __lockfunc rt_read_lock_irqsave(rwlock_t *rwlock);
+extern void __rt_rwlock_init(rwlock_t *rwlock, char *name, struct lock_class_key *key);
+
+#define read_trylock(lock)	__cond_lock(lock, rt_read_trylock(lock))
+#define write_trylock(lock)	__cond_lock(lock, rt_write_trylock(lock))
+
+#define write_trylock_irqsave(lock, flags)	\
+	__cond_lock(lock, rt_write_trylock_irqsave(lock, &flags))
+
+#define read_lock_irqsave(lock, flags)			\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		flags = rt_read_lock_irqsave(lock);	\
+	} while (0)
+
+#define write_lock_irqsave(lock, flags)			\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		flags = rt_write_lock_irqsave(lock);	\
+	} while (0)
+
+#define read_lock(lock)		rt_read_lock(lock)
+
+#define read_lock_bh(lock)				\
+	do {						\
+		local_bh_disable();			\
+		rt_read_lock(lock);			\
+	} while (0)
+
+#define read_lock_irq(lock)	read_lock(lock)
+
+#define write_lock(lock)	rt_write_lock(lock)
+
+#define write_lock_bh(lock)				\
+	do {						\
+		local_bh_disable();			\
+		rt_write_lock(lock);			\
+	} while (0)
+
+#define write_lock_irq(lock)	write_lock(lock)
+
+#define read_unlock(lock)	rt_read_unlock(lock)
+
+#define read_unlock_bh(lock)				\
+	do {						\
+		rt_read_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define read_unlock_irq(lock)	read_unlock(lock)
+
+#define write_unlock(lock)	rt_write_unlock(lock)
+
+#define write_unlock_bh(lock)				\
+	do {						\
+		rt_write_unlock(lock);			\
+		local_bh_enable();			\
+	} while (0)
+
+#define write_unlock_irq(lock)	write_unlock(lock)
+
+#define read_unlock_irqrestore(lock, flags)		\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		rt_read_unlock(lock);			\
+	} while (0)
+
+#define write_unlock_irqrestore(lock, flags) \
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		rt_write_unlock(lock);			\
+	} while (0)
+
+#endif
diff --git a/kernel/msm-3.18/include/linux/rwlock_types.h b/kernel/msm-3.18/include/linux/rwlock_types.h
index cc0072e93..d0da966ad 100644
--- a/kernel/msm-3.18/include/linux/rwlock_types.h
+++ b/kernel/msm-3.18/include/linux/rwlock_types.h
@@ -1,6 +1,10 @@
 #ifndef __LINUX_RWLOCK_TYPES_H
 #define __LINUX_RWLOCK_TYPES_H
 
+#if !defined(__LINUX_SPINLOCK_TYPES_H)
+# error "Do not include directly, include spinlock_types.h"
+#endif
+
 /*
  * include/linux/rwlock_types.h - generic rwlock type definitions
  *				  and initializers
@@ -43,6 +47,7 @@ typedef struct {
 				RW_DEP_MAP_INIT(lockname) }
 #endif
 
-#define DEFINE_RWLOCK(x)	rwlock_t x = __RW_LOCK_UNLOCKED(x)
+#define DEFINE_RWLOCK(name) \
+	rwlock_t name __cacheline_aligned_in_smp = __RW_LOCK_UNLOCKED(name)
 
 #endif /* __LINUX_RWLOCK_TYPES_H */
diff --git a/kernel/msm-3.18/include/linux/rwlock_types_rt.h b/kernel/msm-3.18/include/linux/rwlock_types_rt.h
new file mode 100644
index 000000000..b13832119
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/rwlock_types_rt.h
@@ -0,0 +1,33 @@
+#ifndef __LINUX_RWLOCK_TYPES_RT_H
+#define __LINUX_RWLOCK_TYPES_RT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+/*
+ * rwlocks - rtmutex which allows single reader recursion
+ */
+typedef struct {
+	struct rt_mutex		lock;
+	int			read_depth;
+	unsigned int		break_lock;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+} rwlock_t;
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define RW_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
+#else
+# define RW_DEP_MAP_INIT(lockname)
+#endif
+
+#define __RW_LOCK_UNLOCKED(name) \
+	{ .lock = __RT_MUTEX_INITIALIZER_SAVE_STATE(name.lock),	\
+	  RW_DEP_MAP_INIT(name) }
+
+#define DEFINE_RWLOCK(name) \
+	rwlock_t name __cacheline_aligned_in_smp = __RW_LOCK_UNLOCKED(name)
+
+#endif
diff --git a/kernel/msm-3.18/include/linux/rwsem.h b/kernel/msm-3.18/include/linux/rwsem.h
index 8f498cdde..2b2148431 100644
--- a/kernel/msm-3.18/include/linux/rwsem.h
+++ b/kernel/msm-3.18/include/linux/rwsem.h
@@ -18,6 +18,10 @@
 #include <linux/osq_lock.h>
 #endif
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+#include <linux/rwsem_rt.h>
+#else /* PREEMPT_RT_FULL */
+
 struct rw_semaphore;
 
 #ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
@@ -177,4 +181,6 @@ extern void up_read_non_owner(struct rw_semaphore *sem);
 # define up_read_non_owner(sem)			up_read(sem)
 #endif
 
+#endif /* !PREEMPT_RT_FULL */
+
 #endif /* _LINUX_RWSEM_H */
diff --git a/kernel/msm-3.18/include/linux/rwsem_rt.h b/kernel/msm-3.18/include/linux/rwsem_rt.h
new file mode 100644
index 000000000..924c2d274
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/rwsem_rt.h
@@ -0,0 +1,134 @@
+#ifndef _LINUX_RWSEM_RT_H
+#define _LINUX_RWSEM_RT_H
+
+#ifndef _LINUX_RWSEM_H
+#error "Include rwsem.h"
+#endif
+
+/*
+ * RW-semaphores are a spinlock plus a reader-depth count.
+ *
+ * Note that the semantics are different from the usual
+ * Linux rw-sems, in PREEMPT_RT mode we do not allow
+ * multiple readers to hold the lock at once, we only allow
+ * a read-lock owner to read-lock recursively. This is
+ * better for latency, makes the implementation inherently
+ * fair and makes it simpler as well.
+ */
+
+#include <linux/rtmutex.h>
+
+struct rw_semaphore {
+	struct rt_mutex		lock;
+	int			read_depth;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+};
+
+#define __RWSEM_INITIALIZER(name) \
+	{ .lock = __RT_MUTEX_INITIALIZER(name.lock), \
+	  RW_DEP_MAP_INIT(name) }
+
+#define DECLARE_RWSEM(lockname) \
+	struct rw_semaphore lockname = __RWSEM_INITIALIZER(lockname)
+
+extern void  __rt_rwsem_init(struct rw_semaphore *rwsem, const char *name,
+				     struct lock_class_key *key);
+
+#define __rt_init_rwsem(sem, name, key)			\
+	do {						\
+		rt_mutex_init(&(sem)->lock);		\
+		__rt_rwsem_init((sem), (name), (key));\
+	} while (0)
+
+#define __init_rwsem(sem, name, key) __rt_init_rwsem(sem, name, key)
+
+# define rt_init_rwsem(sem)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	__rt_init_rwsem((sem), #sem, &__key);		\
+} while (0)
+
+extern void  rt_down_write(struct rw_semaphore *rwsem);
+extern void rt_down_read_nested(struct rw_semaphore *rwsem, int subclass);
+extern void rt_down_write_nested(struct rw_semaphore *rwsem, int subclass);
+extern void rt_down_write_nested_lock(struct rw_semaphore *rwsem,
+		struct lockdep_map *nest);
+extern void  rt_down_read(struct rw_semaphore *rwsem);
+extern int  rt_down_write_trylock(struct rw_semaphore *rwsem);
+extern int  rt_down_read_trylock(struct rw_semaphore *rwsem);
+extern void  rt_up_read(struct rw_semaphore *rwsem);
+extern void  rt_up_write(struct rw_semaphore *rwsem);
+extern void  rt_downgrade_write(struct rw_semaphore *rwsem);
+
+#define init_rwsem(sem)		rt_init_rwsem(sem)
+#define rwsem_is_locked(s)	rt_mutex_is_locked(&(s)->lock)
+
+static inline int rwsem_is_contended(struct rw_semaphore *sem)
+{
+	/* rt_mutex_has_waiters() */
+	return !RB_EMPTY_ROOT(&sem->lock.waiters);
+}
+
+static inline void down_read(struct rw_semaphore *sem)
+{
+	rt_down_read(sem);
+}
+
+static inline int down_read_trylock(struct rw_semaphore *sem)
+{
+	return rt_down_read_trylock(sem);
+}
+
+static inline void down_write(struct rw_semaphore *sem)
+{
+	rt_down_write(sem);
+}
+
+static inline int down_write_trylock(struct rw_semaphore *sem)
+{
+	return rt_down_write_trylock(sem);
+}
+
+static inline void up_read(struct rw_semaphore *sem)
+{
+	rt_up_read(sem);
+}
+
+static inline void up_write(struct rw_semaphore *sem)
+{
+	rt_up_write(sem);
+}
+
+static inline void downgrade_write(struct rw_semaphore *sem)
+{
+	rt_downgrade_write(sem);
+}
+
+static inline void down_read_nested(struct rw_semaphore *sem, int subclass)
+{
+	return rt_down_read_nested(sem, subclass);
+}
+
+static inline void down_write_nested(struct rw_semaphore *sem, int subclass)
+{
+	rt_down_write_nested(sem, subclass);
+}
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+static inline void down_write_nest_lock(struct rw_semaphore *sem,
+		struct rw_semaphore *nest_lock)
+{
+	rt_down_write_nested_lock(sem, &nest_lock->dep_map);
+}
+
+#else
+
+static inline void down_write_nest_lock(struct rw_semaphore *sem,
+		struct rw_semaphore *nest_lock)
+{
+	rt_down_write_nested_lock(sem, NULL);
+}
+#endif
+#endif
diff --git a/kernel/msm-3.18/include/linux/sched.h b/kernel/msm-3.18/include/linux/sched.h
index 2a416e477..29ff40ddf 100644
--- a/kernel/msm-3.18/include/linux/sched.h
+++ b/kernel/msm-3.18/include/linux/sched.h
@@ -26,6 +26,7 @@ struct sched_param {
 #include <linux/nodemask.h>
 #include <linux/mm_types.h>
 #include <linux/preempt.h>
+#include <asm/kmap_types.h>
 
 #include <asm/page.h>
 #include <asm/ptrace.h>
@@ -56,6 +57,7 @@ struct sched_param {
 #include <linux/cred.h>
 #include <linux/llist.h>
 #include <linux/uidgid.h>
+#include <linux/hardirq.h>
 #include <linux/gfp.h>
 #include <linux/magic.h>
 
@@ -179,8 +181,6 @@ extern void sched_get_nr_running_avg(int *avg, int *iowait_avg, int *big_avg);
 extern void calc_global_load(unsigned long ticks);
 extern void update_cpu_load_nohz(void);
 
-extern unsigned long get_parent_ip(unsigned long addr);
-
 extern void dump_cpu_task(int cpu);
 
 struct seq_file;
@@ -238,10 +238,7 @@ extern char ___assert_task_state[1 - 2*!!(
 				 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
 				 __TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD)
 
-#define task_is_traced(task)	((task->state & __TASK_TRACED) != 0)
 #define task_is_stopped(task)	((task->state & __TASK_STOPPED) != 0)
-#define task_is_stopped_or_traced(task)	\
-			((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
 #define task_contributes_to_load(task)	\
 				((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
 				 (task->flags & PF_FROZEN) == 0)
@@ -891,6 +888,50 @@ enum cpu_idle_type {
 #define SCHED_CAPACITY_SHIFT	10
 #define SCHED_CAPACITY_SCALE	(1L << SCHED_CAPACITY_SHIFT)
 
+/*
+ * Wake-queues are lists of tasks with a pending wakeup, whose
+ * callers have already marked the task as woken internally,
+ * and can thus carry on. A common use case is being able to
+ * do the wakeups once the corresponding user lock as been
+ * released.
+ *
+ * We hold reference to each task in the list across the wakeup,
+ * thus guaranteeing that the memory is still valid by the time
+ * the actual wakeups are performed in wake_up_q().
+ *
+ * One per task suffices, because there's never a need for a task to be
+ * in two wake queues simultaneously; it is forbidden to abandon a task
+ * in a wake queue (a call to wake_up_q() _must_ follow), so if a task is
+ * already in a wake queue, the wakeup will happen soon and the second
+ * waker can just skip it.
+ *
+ * The WAKE_Q macro declares and initializes the list head.
+ * wake_up_q() does NOT reinitialize the list; it's expected to be
+ * called near the end of a function, where the fact that the queue is
+ * not used again will be easy to see by inspection.
+ *
+ * Note that this can cause spurious wakeups. schedule() callers
+ * must ensure the call is done inside a loop, confirming that the
+ * wakeup condition has in fact occurred.
+ */
+struct wake_q_node {
+	struct wake_q_node *next;
+};
+
+struct wake_q_head {
+	struct wake_q_node *first;
+	struct wake_q_node **lastp;
+};
+
+#define WAKE_Q_TAIL ((struct wake_q_node *) 0x01)
+
+#define WAKE_Q(name)					\
+	struct wake_q_head name = { WAKE_Q_TAIL, &name.first }
+
+extern void wake_q_add(struct wake_q_head *head,
+		       struct task_struct *task);
+extern void wake_up_q(struct wake_q_head *head);
+
 /*
  * sched-domains (multiprocessor balancing) declarations:
  */
@@ -1318,6 +1359,7 @@ enum perf_event_task_context {
 
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
+	volatile long saved_state;	/* saved state for "spinlock sleepers" */
 	void *stack;
 	atomic_t usage;
 	unsigned int flags;	/* per process flags, defined below */
@@ -1371,6 +1413,12 @@ struct task_struct {
 #endif
 
 	unsigned int policy;
+#ifdef CONFIG_PREEMPT_RT_FULL
+	int migrate_disable;
+# ifdef CONFIG_SCHED_DEBUG
+	int migrate_disable_atomic;
+# endif
+#endif
 	int nr_cpus_allowed;
 	cpumask_t cpus_allowed;
 
@@ -1474,7 +1522,8 @@ struct task_struct {
 	struct cputime prev_cputime;
 #endif
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-	seqlock_t vtime_seqlock;
+	raw_spinlock_t vtime_lock;
+	seqcount_t vtime_seq;
 	unsigned long long vtime_snap;
 	enum {
 		VTIME_SLEEPING = 0,
@@ -1490,6 +1539,9 @@ struct task_struct {
 
 	struct task_cputime cputime_expires;
 	struct list_head cpu_timers[3];
+#ifdef CONFIG_PREEMPT_RT_BASE
+	struct task_struct *posix_timer_list;
+#endif
 
 /* process credentials */
 	const struct cred __rcu *real_cred; /* objective and real subjective task
@@ -1522,10 +1574,15 @@ struct task_struct {
 /* signal handlers */
 	struct signal_struct *signal;
 	struct sighand_struct *sighand;
+	struct sigqueue *sigqueue_cache;
 
 	sigset_t blocked, real_blocked;
 	sigset_t saved_sigmask;	/* restored if set_restore_sigmask() was used */
 	struct sigpending pending;
+#ifdef CONFIG_PREEMPT_RT_FULL
+	/* TODO: move me into ->restart_block ? */
+	struct siginfo forced_info;
+#endif
 
 	unsigned long sas_ss_sp;
 	size_t sas_ss_size;
@@ -1551,6 +1608,8 @@ struct task_struct {
 	/* Protection of the PI data structures: */
 	raw_spinlock_t pi_lock;
 
+	struct wake_q_node wake_q;
+
 #ifdef CONFIG_RT_MUTEXES
 	/* PI waiters blocked on a rt_mutex held by this task */
 	struct rb_root pi_waiters;
@@ -1563,6 +1622,10 @@ struct task_struct {
 	/* mutex deadlock detection */
 	struct mutex_waiter *blocked_on;
 #endif
+#ifdef CONFIG_PREEMPT_RT_FULL
+	int xmit_recursion;
+	int pagefault_disabled;
+#endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	unsigned int irq_events;
 	unsigned long hardirq_enable_ip;
@@ -1753,6 +1816,12 @@ struct task_struct {
 	unsigned long trace;
 	/* bitmask and counter of trace recursion */
 	unsigned long trace_recursion;
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+	u64 preempt_timestamp_hist;
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+	long timer_offset;
+#endif
+#endif
 #endif /* CONFIG_TRACING */
 #ifdef CONFIG_KCOV
 	/* Coverage collection mode enabled for this task (0 if disabled). */
@@ -1781,11 +1850,19 @@ struct task_struct {
 	unsigned int	sequential_io;
 	unsigned int	sequential_io_avg;
 #endif
+#ifdef CONFIG_PREEMPT_RT_BASE
+	struct rcu_head put_rcu;
+	int softirq_nestcnt;
+	unsigned int softirqs_raised;
+#endif
+#ifdef CONFIG_PREEMPT_RT_FULL
+# if defined CONFIG_HIGHMEM || defined CONFIG_X86_32
+	int kmap_idx;
+	pte_t kmap_pte[KM_TYPE_NR];
+# endif
+#endif
 };
 
-/* Future-safe accessor for struct task_struct's cpus_allowed. */
-#define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
-
 #define TNF_MIGRATED	0x01
 #define TNF_NO_GROUP	0x02
 #define TNF_SHARED	0x04
@@ -1820,6 +1897,17 @@ static inline bool should_numa_migrate_memory(struct task_struct *p,
 }
 #endif
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+static inline bool cur_pf_disabled(void) { return current->pagefault_disabled; }
+#else
+static inline bool cur_pf_disabled(void) { return false; }
+#endif
+
+static inline bool pagefault_disabled(void)
+{
+	return in_atomic() || cur_pf_disabled();
+}
+
 static inline struct pid *task_pid(struct task_struct *task)
 {
 	return task->pids[PIDTYPE_PID].pid;
@@ -1973,6 +2061,15 @@ extern struct pid *cad_pid;
 extern void free_task(struct task_struct *tsk);
 #define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
 
+#ifdef CONFIG_PREEMPT_RT_BASE
+extern void __put_task_struct_cb(struct rcu_head *rhp);
+
+static inline void put_task_struct(struct task_struct *t)
+{
+	if (atomic_dec_and_test(&t->usage))
+		call_rcu(&t->put_rcu, __put_task_struct_cb);
+}
+#else
 extern void __put_task_struct(struct task_struct *t);
 
 static inline void put_task_struct(struct task_struct *t)
@@ -1980,6 +2077,7 @@ static inline void put_task_struct(struct task_struct *t)
 	if (atomic_dec_and_test(&t->usage))
 		__put_task_struct(t);
 }
+#endif
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 extern void task_cputime(struct task_struct *t,
@@ -2056,6 +2154,7 @@ static inline void sched_set_io_is_busy(int val) {};
 /*
  * Per process flags
  */
+#define PF_IN_SOFTIRQ	0x00000001	/* Task is serving softirq */
 #define PF_WAKE_UP_IDLE 0x00000002	/* try to wake up on an idle CPU */
 #define PF_EXITING	0x00000004	/* getting shut down */
 #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
@@ -2217,6 +2316,10 @@ extern void do_set_cpus_allowed(struct task_struct *p,
 
 extern int set_cpus_allowed_ptr(struct task_struct *p,
 				const struct cpumask *new_mask);
+int migrate_me(void);
+void tell_sched_cpu_down_begin(int cpu);
+void tell_sched_cpu_down_done(int cpu);
+
 #else
 static inline void do_set_cpus_allowed(struct task_struct *p,
 				      const struct cpumask *new_mask)
@@ -2229,6 +2332,9 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p,
 		return -EINVAL;
 	return 0;
 }
+static inline int migrate_me(void) { return 0; }
+static inline void tell_sched_cpu_down_begin(int cpu) { }
+static inline void tell_sched_cpu_down_done(int cpu) { }
 #endif
 
 extern int sched_set_wake_up_idle(struct task_struct *p, int wake_up_idle);
@@ -2511,6 +2617,7 @@ extern void xtime_update(unsigned long ticks);
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
 extern int wake_up_process_no_notif(struct task_struct *tsk);
+extern int wake_up_lock_sleeper(struct task_struct * tsk);
 extern void wake_up_new_task(struct task_struct *tsk);
 #ifdef CONFIG_SMP
  extern void kick_process(struct task_struct *tsk);
@@ -2632,12 +2739,24 @@ extern struct mm_struct * mm_alloc(void);
 
 /* mmdrop drops the mm and the page tables */
 extern void __mmdrop(struct mm_struct *);
+
 static inline void mmdrop(struct mm_struct * mm)
 {
 	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
 		__mmdrop(mm);
 }
 
+#ifdef CONFIG_PREEMPT_RT_BASE
+extern void __mmdrop_delayed(struct rcu_head *rhp);
+static inline void mmdrop_delayed(struct mm_struct *mm)
+{
+	if (atomic_dec_and_test(&mm->mm_count))
+		call_rcu(&mm->delayed_drop, __mmdrop_delayed);
+}
+#else
+# define mmdrop_delayed(mm)	mmdrop(mm)
+#endif
+
 /* mmput gets rid of the mappings and all user-space */
 extern int mmput(struct mm_struct *);
 /* Grab a reference to a task's mm, if it is not already going away */
@@ -2945,6 +3064,43 @@ static inline int test_tsk_need_resched(struct task_struct *tsk)
 	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+static inline void set_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	set_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY);
+}
+
+static inline void clear_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY);
+}
+
+static inline int test_tsk_need_resched_lazy(struct task_struct *tsk)
+{
+	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY));
+}
+
+static inline int need_resched_lazy(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
+}
+
+static inline int need_resched_now(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED);
+}
+
+#else
+static inline void clear_tsk_need_resched_lazy(struct task_struct *tsk) { }
+static inline int need_resched_lazy(void) { return 0; }
+
+static inline int need_resched_now(void)
+{
+	return test_thread_flag(TIF_NEED_RESCHED);
+}
+
+#endif
+
 static inline int restart_syscall(void)
 {
 	set_tsk_thread_flag(current, TIF_SIGPENDING);
@@ -2976,6 +3132,51 @@ static inline int signal_pending_state(long state, struct task_struct *p)
 	return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);
 }
 
+static inline bool __task_is_stopped_or_traced(struct task_struct *task)
+{
+	if (task->state & (__TASK_STOPPED | __TASK_TRACED))
+		return true;
+#ifdef CONFIG_PREEMPT_RT_FULL
+	if (task->saved_state & (__TASK_STOPPED | __TASK_TRACED))
+		return true;
+#endif
+	return false;
+}
+
+static inline bool task_is_stopped_or_traced(struct task_struct *task)
+{
+	bool traced_stopped;
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&task->pi_lock, flags);
+	traced_stopped = __task_is_stopped_or_traced(task);
+	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
+#else
+	traced_stopped = __task_is_stopped_or_traced(task);
+#endif
+	return traced_stopped;
+}
+
+static inline bool task_is_traced(struct task_struct *task)
+{
+	bool traced = false;
+
+	if (task->state & __TASK_TRACED)
+		return true;
+#ifdef CONFIG_PREEMPT_RT_FULL
+	/* in case the task is sleeping on tasklist_lock */
+	raw_spin_lock_irq(&task->pi_lock);
+	if (task->state & __TASK_TRACED)
+		traced = true;
+	else if (task->saved_state & __TASK_TRACED)
+		traced = true;
+	raw_spin_unlock_irq(&task->pi_lock);
+#endif
+	return traced;
+}
+
 /*
  * cond_resched() and cond_resched_lock(): latency reduction via
  * explicit rescheduling in places that are safe. The return
@@ -2992,17 +3193,27 @@ extern int _cond_resched(void);
 
 extern int __cond_resched_lock(spinlock_t *lock);
 
+#if defined(CONFIG_PREEMPT_COUNT) && !defined(CONFIG_PREEMPT_RT_FULL)
+#define PREEMPT_LOCK_OFFSET	PREEMPT_OFFSET
+#else
+#define PREEMPT_LOCK_OFFSET	0
+#endif
+
 #define cond_resched_lock(lock) ({				\
 	__might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);	\
 	__cond_resched_lock(lock);				\
 })
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 extern int __cond_resched_softirq(void);
 
 #define cond_resched_softirq() ({					\
 	__might_sleep(__FILE__, __LINE__, SOFTIRQ_DISABLE_OFFSET);	\
 	__cond_resched_softirq();					\
 })
+#else
+# define cond_resched_softirq()		cond_resched()
+#endif
 
 static inline void cond_resched_rcu(void)
 {
@@ -3178,6 +3389,26 @@ struct migration_notify_data {
 
 extern struct atomic_notifier_head load_alert_notifier_head;
 
+static inline int __migrate_disabled(struct task_struct *p)
+{
+#ifdef CONFIG_PREEMPT_RT_FULL
+	return p->migrate_disable;
+#else
+	return 0;
+#endif
+}
+
+/* Future-safe accessor for struct task_struct's cpus_allowed. */
+static inline const struct cpumask *tsk_cpus_allowed(struct task_struct *p)
+{
+#ifdef CONFIG_PREEMPT_RT_FULL
+	if (p->migrate_disable)
+		return cpumask_of(task_cpu(p));
+#endif
+
+	return &p->cpus_allowed;
+}
+
 extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
 extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 
diff --git a/kernel/msm-3.18/include/linux/seqlock.h b/kernel/msm-3.18/include/linux/seqlock.h
index f5df8f687..4acd0e2fb 100644
--- a/kernel/msm-3.18/include/linux/seqlock.h
+++ b/kernel/msm-3.18/include/linux/seqlock.h
@@ -219,20 +219,30 @@ static inline int read_seqcount_retry(const seqcount_t *s, unsigned start)
 	return __read_seqcount_retry(s, start);
 }
 
-
-
-static inline void raw_write_seqcount_begin(seqcount_t *s)
+static inline void __raw_write_seqcount_begin(seqcount_t *s)
 {
 	s->sequence++;
 	smp_wmb();
 }
 
-static inline void raw_write_seqcount_end(seqcount_t *s)
+static inline void raw_write_seqcount_begin(seqcount_t *s)
+{
+	preempt_disable_rt();
+	__raw_write_seqcount_begin(s);
+}
+
+static inline void __raw_write_seqcount_end(seqcount_t *s)
 {
 	smp_wmb();
 	s->sequence++;
 }
 
+static inline void raw_write_seqcount_end(seqcount_t *s)
+{
+	__raw_write_seqcount_end(s);
+	preempt_enable_rt();
+}
+
 /*
  * raw_write_seqcount_latch - redirect readers to even/odd copy
  * @s: pointer to seqcount_t
@@ -305,10 +315,32 @@ typedef struct {
 /*
  * Read side functions for starting and finalizing a read side section.
  */
+#ifndef CONFIG_PREEMPT_RT_FULL
 static inline unsigned read_seqbegin(const seqlock_t *sl)
 {
 	return read_seqcount_begin(&sl->seqcount);
 }
+#else
+/*
+ * Starvation safe read side for RT
+ */
+static inline unsigned read_seqbegin(seqlock_t *sl)
+{
+	unsigned ret;
+
+repeat:
+	ret = ACCESS_ONCE(sl->seqcount.sequence);
+	if (unlikely(ret & 1)) {
+		/*
+		 * Take the lock and let the writer proceed (i.e. evtl
+		 * boost it), otherwise we could loop here forever.
+		 */
+		spin_unlock_wait(&sl->lock);
+		goto repeat;
+	}
+	return ret;
+}
+#endif
 
 static inline unsigned read_seqretry(const seqlock_t *sl, unsigned start)
 {
@@ -323,36 +355,36 @@ static inline unsigned read_seqretry(const seqlock_t *sl, unsigned start)
 static inline void write_seqlock(seqlock_t *sl)
 {
 	spin_lock(&sl->lock);
-	write_seqcount_begin(&sl->seqcount);
+	__raw_write_seqcount_begin(&sl->seqcount);
 }
 
 static inline void write_sequnlock(seqlock_t *sl)
 {
-	write_seqcount_end(&sl->seqcount);
+	__raw_write_seqcount_end(&sl->seqcount);
 	spin_unlock(&sl->lock);
 }
 
 static inline void write_seqlock_bh(seqlock_t *sl)
 {
 	spin_lock_bh(&sl->lock);
-	write_seqcount_begin(&sl->seqcount);
+	__raw_write_seqcount_begin(&sl->seqcount);
 }
 
 static inline void write_sequnlock_bh(seqlock_t *sl)
 {
-	write_seqcount_end(&sl->seqcount);
+	__raw_write_seqcount_end(&sl->seqcount);
 	spin_unlock_bh(&sl->lock);
 }
 
 static inline void write_seqlock_irq(seqlock_t *sl)
 {
 	spin_lock_irq(&sl->lock);
-	write_seqcount_begin(&sl->seqcount);
+	__raw_write_seqcount_begin(&sl->seqcount);
 }
 
 static inline void write_sequnlock_irq(seqlock_t *sl)
 {
-	write_seqcount_end(&sl->seqcount);
+	__raw_write_seqcount_end(&sl->seqcount);
 	spin_unlock_irq(&sl->lock);
 }
 
@@ -361,7 +393,7 @@ static inline unsigned long __write_seqlock_irqsave(seqlock_t *sl)
 	unsigned long flags;
 
 	spin_lock_irqsave(&sl->lock, flags);
-	write_seqcount_begin(&sl->seqcount);
+	__raw_write_seqcount_begin(&sl->seqcount);
 	return flags;
 }
 
@@ -371,7 +403,7 @@ static inline unsigned long __write_seqlock_irqsave(seqlock_t *sl)
 static inline void
 write_sequnlock_irqrestore(seqlock_t *sl, unsigned long flags)
 {
-	write_seqcount_end(&sl->seqcount);
+	__raw_write_seqcount_end(&sl->seqcount);
 	spin_unlock_irqrestore(&sl->lock, flags);
 }
 
diff --git a/kernel/msm-3.18/include/linux/signal.h b/kernel/msm-3.18/include/linux/signal.h
index ab1e0392b..66215b20a 100644
--- a/kernel/msm-3.18/include/linux/signal.h
+++ b/kernel/msm-3.18/include/linux/signal.h
@@ -218,6 +218,7 @@ static inline void init_sigpending(struct sigpending *sig)
 }
 
 extern void flush_sigqueue(struct sigpending *queue);
+extern void flush_task_sigqueue(struct task_struct *tsk);
 
 /* Test if 'sig' is valid signal. Use this instead of testing _NSIG directly */
 static inline int valid_signal(unsigned long sig)
diff --git a/kernel/msm-3.18/include/linux/skbuff.h b/kernel/msm-3.18/include/linux/skbuff.h
index a8c990647..27dac6b65 100644
--- a/kernel/msm-3.18/include/linux/skbuff.h
+++ b/kernel/msm-3.18/include/linux/skbuff.h
@@ -172,6 +172,7 @@ struct sk_buff_head {
 
 	__u32		qlen;
 	spinlock_t	lock;
+	raw_spinlock_t	raw_lock;
 };
 
 struct sk_buff;
@@ -1330,6 +1331,12 @@ static inline void skb_queue_head_init(struct sk_buff_head *list)
 	__skb_queue_head_init(list);
 }
 
+static inline void skb_queue_head_init_raw(struct sk_buff_head *list)
+{
+	raw_spin_lock_init(&list->raw_lock);
+	__skb_queue_head_init(list);
+}
+
 static inline void skb_queue_head_init_class(struct sk_buff_head *list,
 		struct lock_class_key *class)
 {
diff --git a/kernel/msm-3.18/include/linux/smp.h b/kernel/msm-3.18/include/linux/smp.h
index 93dff5fff..42c3d2ca1 100644
--- a/kernel/msm-3.18/include/linux/smp.h
+++ b/kernel/msm-3.18/include/linux/smp.h
@@ -178,6 +178,9 @@ static inline void wake_up_all_idle_cpus(void) {  }
 #define get_cpu()		({ preempt_disable(); smp_processor_id(); })
 #define put_cpu()		preempt_enable()
 
+#define get_cpu_light()		({ migrate_disable(); smp_processor_id(); })
+#define put_cpu_light()		migrate_enable()
+
 /*
  * Callback to arch code if there's nosmp or maxcpus=0 on the
  * boot command line:
diff --git a/kernel/msm-3.18/include/linux/spinlock.h b/kernel/msm-3.18/include/linux/spinlock.h
index 262ba4ef9..98e802659 100644
--- a/kernel/msm-3.18/include/linux/spinlock.h
+++ b/kernel/msm-3.18/include/linux/spinlock.h
@@ -278,7 +278,11 @@ static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
 #define raw_spin_can_lock(lock)	(!raw_spin_is_locked(lock))
 
 /* Include rwlock functions */
-#include <linux/rwlock.h>
+#ifdef CONFIG_PREEMPT_RT_FULL
+# include <linux/rwlock_rt.h>
+#else
+# include <linux/rwlock.h>
+#endif
 
 /*
  * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:
@@ -289,6 +293,10 @@ static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
 # include <linux/spinlock_api_up.h>
 #endif
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+# include <linux/spinlock_rt.h>
+#else /* PREEMPT_RT_FULL */
+
 /*
  * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
  */
@@ -418,4 +426,6 @@ extern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);
 #define atomic_dec_and_lock(atomic, lock) \
 		__cond_lock(lock, _atomic_dec_and_lock(atomic, lock))
 
+#endif /* !PREEMPT_RT_FULL */
+
 #endif /* __LINUX_SPINLOCK_H */
diff --git a/kernel/msm-3.18/include/linux/spinlock_api_smp.h b/kernel/msm-3.18/include/linux/spinlock_api_smp.h
index 42dfab89e..29d99ae5a 100644
--- a/kernel/msm-3.18/include/linux/spinlock_api_smp.h
+++ b/kernel/msm-3.18/include/linux/spinlock_api_smp.h
@@ -187,6 +187,8 @@ static inline int __raw_spin_trylock_bh(raw_spinlock_t *lock)
 	return 0;
 }
 
-#include <linux/rwlock_api_smp.h>
+#ifndef CONFIG_PREEMPT_RT_FULL
+# include <linux/rwlock_api_smp.h>
+#endif
 
 #endif /* __LINUX_SPINLOCK_API_SMP_H */
diff --git a/kernel/msm-3.18/include/linux/spinlock_rt.h b/kernel/msm-3.18/include/linux/spinlock_rt.h
new file mode 100644
index 000000000..c0d1367d3
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/spinlock_rt.h
@@ -0,0 +1,167 @@
+#ifndef __LINUX_SPINLOCK_RT_H
+#define __LINUX_SPINLOCK_RT_H
+
+#ifndef __LINUX_SPINLOCK_H
+#error Do not include directly. Use spinlock.h
+#endif
+
+#include <linux/bug.h>
+
+extern void
+__rt_spin_lock_init(spinlock_t *lock, char *name, struct lock_class_key *key);
+
+#define spin_lock_init(slock)				\
+do {							\
+	static struct lock_class_key __key;		\
+							\
+	rt_mutex_init(&(slock)->lock);			\
+	__rt_spin_lock_init(slock, #slock, &__key);	\
+} while (0)
+
+extern void __lockfunc rt_spin_lock(spinlock_t *lock);
+extern unsigned long __lockfunc rt_spin_lock_trace_flags(spinlock_t *lock);
+extern void __lockfunc rt_spin_lock_nested(spinlock_t *lock, int subclass);
+extern void __lockfunc rt_spin_unlock(spinlock_t *lock);
+extern void __lockfunc rt_spin_unlock_after_trylock_in_irq(spinlock_t *lock);
+extern void __lockfunc rt_spin_unlock_wait(spinlock_t *lock);
+extern int __lockfunc rt_spin_trylock_irqsave(spinlock_t *lock, unsigned long *flags);
+extern int __lockfunc rt_spin_trylock_bh(spinlock_t *lock);
+extern int __lockfunc rt_spin_trylock(spinlock_t *lock);
+extern int atomic_dec_and_spin_lock(atomic_t *atomic, spinlock_t *lock);
+
+/*
+ * lockdep-less calls, for derived types like rwlock:
+ * (for trylock they can use rt_mutex_trylock() directly.
+ */
+extern void __lockfunc __rt_spin_lock(struct rt_mutex *lock);
+extern void __lockfunc __rt_spin_unlock(struct rt_mutex *lock);
+extern int __lockfunc __rt_spin_trylock(struct rt_mutex *lock);
+
+#define spin_lock(lock)				\
+	do {					\
+		migrate_disable();		\
+		rt_spin_lock(lock);		\
+	} while (0)
+
+#define spin_lock_bh(lock)			\
+	do {					\
+		local_bh_disable();		\
+		migrate_disable();		\
+		rt_spin_lock(lock);		\
+	} while (0)
+
+#define spin_lock_irq(lock)		spin_lock(lock)
+
+#define spin_do_trylock(lock)		__cond_lock(lock, rt_spin_trylock(lock))
+
+#define spin_trylock(lock)			\
+({						\
+	int __locked;				\
+	migrate_disable();			\
+	__locked = spin_do_trylock(lock);	\
+	if (!__locked)				\
+		migrate_enable();		\
+	__locked;				\
+})
+
+#ifdef CONFIG_LOCKDEP
+# define spin_lock_nested(lock, subclass)		\
+	do {						\
+		migrate_disable();			\
+		rt_spin_lock_nested(lock, subclass);	\
+	} while (0)
+
+# define spin_lock_irqsave_nested(lock, flags, subclass) \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		migrate_disable();			 \
+		rt_spin_lock_nested(lock, subclass);	 \
+	} while (0)
+#else
+# define spin_lock_nested(lock, subclass)	spin_lock(lock)
+
+# define spin_lock_irqsave_nested(lock, flags, subclass) \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		spin_lock(lock);			 \
+	} while (0)
+#endif
+
+#define spin_lock_irqsave(lock, flags)			 \
+	do {						 \
+		typecheck(unsigned long, flags);	 \
+		flags = 0;				 \
+		spin_lock(lock);			 \
+	} while (0)
+
+static inline unsigned long spin_lock_trace_flags(spinlock_t *lock)
+{
+	unsigned long flags = 0;
+#ifdef CONFIG_TRACE_IRQFLAGS
+	flags = rt_spin_lock_trace_flags(lock);
+#else
+	spin_lock(lock); /* lock_local */
+#endif
+	return flags;
+}
+
+/* FIXME: we need rt_spin_lock_nest_lock */
+#define spin_lock_nest_lock(lock, nest_lock) spin_lock_nested(lock, 0)
+
+#define spin_unlock(lock)				\
+	do {						\
+		rt_spin_unlock(lock);			\
+		migrate_enable();			\
+	} while (0)
+
+#define spin_unlock_bh(lock)				\
+	do {						\
+		rt_spin_unlock(lock);			\
+		migrate_enable();			\
+		local_bh_enable();			\
+	} while (0)
+
+#define spin_unlock_irq(lock)		spin_unlock(lock)
+
+#define spin_unlock_irqrestore(lock, flags)		\
+	do {						\
+		typecheck(unsigned long, flags);	\
+		(void) flags;				\
+		spin_unlock(lock);			\
+	} while (0)
+
+#define spin_trylock_bh(lock)	__cond_lock(lock, rt_spin_trylock_bh(lock))
+#define spin_trylock_irq(lock)	spin_trylock(lock)
+
+#define spin_trylock_irqsave(lock, flags)	\
+	rt_spin_trylock_irqsave(lock, &(flags))
+
+#define spin_unlock_wait(lock)		rt_spin_unlock_wait(lock)
+
+#ifdef CONFIG_GENERIC_LOCKBREAK
+# define spin_is_contended(lock)	((lock)->break_lock)
+#else
+# define spin_is_contended(lock)	(((void)(lock), 0))
+#endif
+
+static inline int spin_can_lock(spinlock_t *lock)
+{
+	return !rt_mutex_is_locked(&lock->lock);
+}
+
+static inline int spin_is_locked(spinlock_t *lock)
+{
+	return rt_mutex_is_locked(&lock->lock);
+}
+
+static inline void assert_spin_locked(spinlock_t *lock)
+{
+	BUG_ON(!spin_is_locked(lock));
+}
+
+#define atomic_dec_and_lock(atomic, lock) \
+	atomic_dec_and_spin_lock(atomic, lock)
+
+#endif
diff --git a/kernel/msm-3.18/include/linux/spinlock_types.h b/kernel/msm-3.18/include/linux/spinlock_types.h
index 73548eb13..10bac715e 100644
--- a/kernel/msm-3.18/include/linux/spinlock_types.h
+++ b/kernel/msm-3.18/include/linux/spinlock_types.h
@@ -9,80 +9,15 @@
  * Released under the General Public License (GPL).
  */
 
-#if defined(CONFIG_SMP)
-# include <asm/spinlock_types.h>
-#else
-# include <linux/spinlock_types_up.h>
-#endif
-
-#include <linux/lockdep.h>
-
-typedef struct raw_spinlock {
-	arch_spinlock_t raw_lock;
-#ifdef CONFIG_GENERIC_LOCKBREAK
-	unsigned int break_lock;
-#endif
-#ifdef CONFIG_DEBUG_SPINLOCK
-	unsigned int magic, owner_cpu;
-	void *owner;
-#endif
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-	struct lockdep_map dep_map;
-#endif
-} raw_spinlock_t;
-
-#define SPINLOCK_MAGIC		0xdead4ead
-
-#define SPINLOCK_OWNER_INIT	((void *)-1L)
-
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define SPIN_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
-#else
-# define SPIN_DEP_MAP_INIT(lockname)
-#endif
+#include <linux/spinlock_types_raw.h>
 
-#ifdef CONFIG_DEBUG_SPINLOCK
-# define SPIN_DEBUG_INIT(lockname)		\
-	.magic = SPINLOCK_MAGIC,		\
-	.owner_cpu = -1,			\
-	.owner = SPINLOCK_OWNER_INIT,
+#ifndef CONFIG_PREEMPT_RT_FULL
+# include <linux/spinlock_types_nort.h>
+# include <linux/rwlock_types.h>
 #else
-# define SPIN_DEBUG_INIT(lockname)
+# include <linux/rtmutex.h>
+# include <linux/spinlock_types_rt.h>
+# include <linux/rwlock_types_rt.h>
 #endif
 
-#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
-	{					\
-	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
-	SPIN_DEBUG_INIT(lockname)		\
-	SPIN_DEP_MAP_INIT(lockname) }
-
-#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
-	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)
-
-#define DEFINE_RAW_SPINLOCK(x)	raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)
-
-typedef struct spinlock {
-	union {
-		struct raw_spinlock rlock;
-
-#ifdef CONFIG_DEBUG_LOCK_ALLOC
-# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
-		struct {
-			u8 __padding[LOCK_PADSIZE];
-			struct lockdep_map dep_map;
-		};
-#endif
-	};
-} spinlock_t;
-
-#define __SPIN_LOCK_INITIALIZER(lockname) \
-	{ { .rlock = __RAW_SPIN_LOCK_INITIALIZER(lockname) } }
-
-#define __SPIN_LOCK_UNLOCKED(lockname) \
-	(spinlock_t ) __SPIN_LOCK_INITIALIZER(lockname)
-
-#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
-
-#include <linux/rwlock_types.h>
-
 #endif /* __LINUX_SPINLOCK_TYPES_H */
diff --git a/kernel/msm-3.18/include/linux/spinlock_types_nort.h b/kernel/msm-3.18/include/linux/spinlock_types_nort.h
new file mode 100644
index 000000000..f1dac1fb1
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/spinlock_types_nort.h
@@ -0,0 +1,33 @@
+#ifndef __LINUX_SPINLOCK_TYPES_NORT_H
+#define __LINUX_SPINLOCK_TYPES_NORT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+/*
+ * The non RT version maps spinlocks to raw_spinlocks
+ */
+typedef struct spinlock {
+	union {
+		struct raw_spinlock rlock;
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
+		struct {
+			u8 __padding[LOCK_PADSIZE];
+			struct lockdep_map dep_map;
+		};
+#endif
+	};
+} spinlock_t;
+
+#define __SPIN_LOCK_INITIALIZER(lockname) \
+	{ { .rlock = __RAW_SPIN_LOCK_INITIALIZER(lockname) } }
+
+#define __SPIN_LOCK_UNLOCKED(lockname) \
+	(spinlock_t ) __SPIN_LOCK_INITIALIZER(lockname)
+
+#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
+
+#endif
diff --git a/kernel/msm-3.18/include/linux/spinlock_types_raw.h b/kernel/msm-3.18/include/linux/spinlock_types_raw.h
new file mode 100644
index 000000000..edffc4d53
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/spinlock_types_raw.h
@@ -0,0 +1,56 @@
+#ifndef __LINUX_SPINLOCK_TYPES_RAW_H
+#define __LINUX_SPINLOCK_TYPES_RAW_H
+
+#if defined(CONFIG_SMP)
+# include <asm/spinlock_types.h>
+#else
+# include <linux/spinlock_types_up.h>
+#endif
+
+#include <linux/lockdep.h>
+
+typedef struct raw_spinlock {
+	arch_spinlock_t raw_lock;
+#ifdef CONFIG_GENERIC_LOCKBREAK
+	unsigned int break_lock;
+#endif
+#ifdef CONFIG_DEBUG_SPINLOCK
+	unsigned int magic, owner_cpu;
+	void *owner;
+#endif
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map dep_map;
+#endif
+} raw_spinlock_t;
+
+#define SPINLOCK_MAGIC		0xdead4ead
+
+#define SPINLOCK_OWNER_INIT	((void *)-1L)
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+# define SPIN_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
+#else
+# define SPIN_DEP_MAP_INIT(lockname)
+#endif
+
+#ifdef CONFIG_DEBUG_SPINLOCK
+# define SPIN_DEBUG_INIT(lockname)		\
+	.magic = SPINLOCK_MAGIC,		\
+	.owner_cpu = -1,			\
+	.owner = SPINLOCK_OWNER_INIT,
+#else
+# define SPIN_DEBUG_INIT(lockname)
+#endif
+
+#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
+	{					\
+	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
+	SPIN_DEBUG_INIT(lockname)		\
+	SPIN_DEP_MAP_INIT(lockname) }
+
+#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
+	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)
+
+#define DEFINE_RAW_SPINLOCK(x)	raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)
+
+#endif
diff --git a/kernel/msm-3.18/include/linux/spinlock_types_rt.h b/kernel/msm-3.18/include/linux/spinlock_types_rt.h
new file mode 100644
index 000000000..9fd431967
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/spinlock_types_rt.h
@@ -0,0 +1,51 @@
+#ifndef __LINUX_SPINLOCK_TYPES_RT_H
+#define __LINUX_SPINLOCK_TYPES_RT_H
+
+#ifndef __LINUX_SPINLOCK_TYPES_H
+#error "Do not include directly. Include spinlock_types.h instead"
+#endif
+
+#include <linux/cache.h>
+
+/*
+ * PREEMPT_RT: spinlocks - an RT mutex plus lock-break field:
+ */
+typedef struct spinlock {
+	struct rt_mutex		lock;
+	unsigned int		break_lock;
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	struct lockdep_map	dep_map;
+#endif
+} spinlock_t;
+
+#ifdef CONFIG_DEBUG_RT_MUTEXES
+# define __RT_SPIN_INITIALIZER(name) \
+	{ \
+	.wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock), \
+	.save_state = 1, \
+	.file = __FILE__, \
+	.line = __LINE__ , \
+	}
+#else
+# define __RT_SPIN_INITIALIZER(name) \
+	{								\
+	.wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),		\
+	.save_state = 1, \
+	}
+#endif
+
+/*
+.wait_list = PLIST_HEAD_INIT_RAW((name).lock.wait_list, (name).lock.wait_lock)
+*/
+
+#define __SPIN_LOCK_UNLOCKED(name)			\
+	{ .lock = __RT_SPIN_INITIALIZER(name.lock),		\
+	  SPIN_DEP_MAP_INIT(name) }
+
+#define __DEFINE_SPINLOCK(name) \
+	spinlock_t name = __SPIN_LOCK_UNLOCKED(name)
+
+#define DEFINE_SPINLOCK(name) \
+	spinlock_t name __cacheline_aligned_in_smp = __SPIN_LOCK_UNLOCKED(name)
+
+#endif
diff --git a/kernel/msm-3.18/include/linux/srcu.h b/kernel/msm-3.18/include/linux/srcu.h
index a2783cb5d..f12ffc644 100644
--- a/kernel/msm-3.18/include/linux/srcu.h
+++ b/kernel/msm-3.18/include/linux/srcu.h
@@ -84,10 +84,10 @@ int init_srcu_struct(struct srcu_struct *sp);
 
 void process_srcu(struct work_struct *work);
 
-#define __SRCU_STRUCT_INIT(name)					\
+#define __SRCU_STRUCT_INIT(name, pcpu_name)				\
 	{								\
 		.completed = -300,					\
-		.per_cpu_ref = &name##_srcu_array,			\
+		.per_cpu_ref = &pcpu_name,				\
 		.queue_lock = __SPIN_LOCK_UNLOCKED(name.queue_lock),	\
 		.running = false,					\
 		.batch_queue = RCU_BATCH_INIT(name.batch_queue),	\
@@ -104,11 +104,12 @@ void process_srcu(struct work_struct *work);
  */
 #define DEFINE_SRCU(name)						\
 	static DEFINE_PER_CPU(struct srcu_struct_array, name##_srcu_array);\
-	struct srcu_struct name = __SRCU_STRUCT_INIT(name);
+	struct srcu_struct name = __SRCU_STRUCT_INIT(name, name##_srcu_array);
 
 #define DEFINE_STATIC_SRCU(name)					\
 	static DEFINE_PER_CPU(struct srcu_struct_array, name##_srcu_array);\
-	static struct srcu_struct name = __SRCU_STRUCT_INIT(name);
+	static struct srcu_struct name = __SRCU_STRUCT_INIT(\
+		name, name##_srcu_array);
 
 /**
  * call_srcu() - Queue a callback for invocation after an SRCU grace period
diff --git a/kernel/msm-3.18/include/linux/swap.h b/kernel/msm-3.18/include/linux/swap.h
index 923e6948a..41e339c4b 100644
--- a/kernel/msm-3.18/include/linux/swap.h
+++ b/kernel/msm-3.18/include/linux/swap.h
@@ -11,6 +11,7 @@
 #include <linux/fs.h>
 #include <linux/atomic.h>
 #include <linux/page-flags.h>
+#include <linux/locallock.h>
 #include <asm/page.h>
 
 struct notifier_block;
@@ -264,7 +265,8 @@ struct swap_info_struct {
 void *workingset_eviction(struct address_space *mapping, struct page *page);
 bool workingset_refault(void *shadow);
 void workingset_activation(struct page *page);
-extern struct list_lru workingset_shadow_nodes;
+extern struct list_lru __workingset_shadow_nodes;
+DECLARE_LOCAL_IRQ_LOCK(workingset_shadow_lock);
 
 static inline unsigned int workingset_node_pages(struct radix_tree_node *node)
 {
diff --git a/kernel/msm-3.18/include/linux/sysctl.h b/kernel/msm-3.18/include/linux/sysctl.h
index b7361f831..22050201d 100644
--- a/kernel/msm-3.18/include/linux/sysctl.h
+++ b/kernel/msm-3.18/include/linux/sysctl.h
@@ -25,6 +25,7 @@
 #include <linux/rcupdate.h>
 #include <linux/wait.h>
 #include <linux/rbtree.h>
+#include <linux/atomic.h>
 #include <uapi/linux/sysctl.h>
 
 /* For the /proc/sys support */
diff --git a/kernel/msm-3.18/include/linux/thread_info.h b/kernel/msm-3.18/include/linux/thread_info.h
index 4cf895177..76feb5870 100644
--- a/kernel/msm-3.18/include/linux/thread_info.h
+++ b/kernel/msm-3.18/include/linux/thread_info.h
@@ -102,7 +102,17 @@ static inline int test_ti_thread_flag(struct thread_info *ti, int flag)
 #define test_thread_flag(flag) \
 	test_ti_thread_flag(current_thread_info(), flag)
 
-#define tif_need_resched() test_thread_flag(TIF_NEED_RESCHED)
+#ifdef CONFIG_PREEMPT_LAZY
+#define tif_need_resched()	(test_thread_flag(TIF_NEED_RESCHED) || \
+				 test_thread_flag(TIF_NEED_RESCHED_LAZY))
+#define tif_need_resched_now()	(test_thread_flag(TIF_NEED_RESCHED))
+#define tif_need_resched_lazy()	test_thread_flag(TIF_NEED_RESCHED_LAZY))
+
+#else
+#define tif_need_resched()	test_thread_flag(TIF_NEED_RESCHED)
+#define tif_need_resched_now()	test_thread_flag(TIF_NEED_RESCHED)
+#define tif_need_resched_lazy()	0
+#endif
 
 #if defined TIF_RESTORE_SIGMASK && !defined HAVE_SET_RESTORE_SIGMASK
 /*
diff --git a/kernel/msm-3.18/include/linux/timer.h b/kernel/msm-3.18/include/linux/timer.h
index 8d836cd1e..d7ebff766 100644
--- a/kernel/msm-3.18/include/linux/timer.h
+++ b/kernel/msm-3.18/include/linux/timer.h
@@ -256,7 +256,7 @@ extern void add_timer(struct timer_list *timer);
 
 extern int try_to_del_timer_sync(struct timer_list *timer);
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT_FULL)
   extern int del_timer_sync(struct timer_list *timer);
 #else
 # define del_timer_sync(t)		del_timer(t)
diff --git a/kernel/msm-3.18/include/linux/uaccess.h b/kernel/msm-3.18/include/linux/uaccess.h
index 5e38c6d4d..0bf5bc054 100644
--- a/kernel/msm-3.18/include/linux/uaccess.h
+++ b/kernel/msm-3.18/include/linux/uaccess.h
@@ -6,14 +6,9 @@
 
 /*
  * These routines enable/disable the pagefault handler in that
- * it will not take any locks and go straight to the fixup table.
- *
- * They have great resemblance to the preempt_disable/enable calls
- * and in fact they are identical; this is because currently there is
- * no other way to make the pagefault handlers do this. So we do
- * disable preemption but we don't necessarily care about that.
+ * it will not take any MM locks and go straight to the fixup table.
  */
-static inline void pagefault_disable(void)
+static inline void raw_pagefault_disable(void)
 {
 	preempt_count_inc();
 	/*
@@ -23,7 +18,7 @@ static inline void pagefault_disable(void)
 	barrier();
 }
 
-static inline void pagefault_enable(void)
+static inline void raw_pagefault_enable(void)
 {
 #ifndef CONFIG_PREEMPT
 	/*
@@ -37,6 +32,21 @@ static inline void pagefault_enable(void)
 #endif
 }
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+static inline void pagefault_disable(void)
+{
+	raw_pagefault_disable();
+}
+
+static inline void pagefault_enable(void)
+{
+	raw_pagefault_enable();
+}
+#else
+extern void pagefault_disable(void);
+extern void pagefault_enable(void);
+#endif
+
 #ifndef ARCH_HAS_NOCACHE_UACCESS
 
 static inline unsigned long __copy_from_user_inatomic_nocache(void *to,
@@ -76,9 +86,9 @@ static inline unsigned long __copy_from_user_nocache(void *to,
 		mm_segment_t old_fs = get_fs();		\
 							\
 		set_fs(KERNEL_DS);			\
-		pagefault_disable();			\
+		raw_pagefault_disable();		\
 		ret = __copy_from_user_inatomic(&(retval), (__force typeof(retval) __user *)(addr), sizeof(retval));		\
-		pagefault_enable();			\
+		raw_pagefault_enable();			\
 		set_fs(old_fs);				\
 		ret;					\
 	})
diff --git a/kernel/msm-3.18/include/linux/uprobes.h b/kernel/msm-3.18/include/linux/uprobes.h
index 60beb5dc7..f5a644c64 100644
--- a/kernel/msm-3.18/include/linux/uprobes.h
+++ b/kernel/msm-3.18/include/linux/uprobes.h
@@ -27,6 +27,7 @@
 #include <linux/errno.h>
 #include <linux/rbtree.h>
 #include <linux/types.h>
+#include <linux/wait.h>
 
 struct vm_area_struct;
 struct mm_struct;
diff --git a/kernel/msm-3.18/include/linux/vmstat.h b/kernel/msm-3.18/include/linux/vmstat.h
index e23520650..e2e3b8727 100644
--- a/kernel/msm-3.18/include/linux/vmstat.h
+++ b/kernel/msm-3.18/include/linux/vmstat.h
@@ -33,7 +33,9 @@ DECLARE_PER_CPU(struct vm_event_state, vm_event_states);
  */
 static inline void __count_vm_event(enum vm_event_item item)
 {
+	preempt_disable_rt();
 	raw_cpu_inc(vm_event_states.event[item]);
+	preempt_enable_rt();
 }
 
 static inline void count_vm_event(enum vm_event_item item)
@@ -43,7 +45,9 @@ static inline void count_vm_event(enum vm_event_item item)
 
 static inline void __count_vm_events(enum vm_event_item item, long delta)
 {
+	preempt_disable_rt();
 	raw_cpu_add(vm_event_states.event[item], delta);
+	preempt_enable_rt();
 }
 
 static inline void count_vm_events(enum vm_event_item item, long delta)
diff --git a/kernel/msm-3.18/include/linux/wait-simple.h b/kernel/msm-3.18/include/linux/wait-simple.h
new file mode 100644
index 000000000..f86bca2c4
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/wait-simple.h
@@ -0,0 +1,207 @@
+#ifndef _LINUX_WAIT_SIMPLE_H
+#define _LINUX_WAIT_SIMPLE_H
+
+#include <linux/spinlock.h>
+#include <linux/list.h>
+
+#include <asm/current.h>
+
+struct swaiter {
+	struct task_struct	*task;
+	struct list_head	node;
+};
+
+#define DEFINE_SWAITER(name)					\
+	struct swaiter name = {					\
+		.task	= current,				\
+		.node	= LIST_HEAD_INIT((name).node),		\
+	}
+
+struct swait_head {
+	raw_spinlock_t		lock;
+	struct list_head	list;
+};
+
+#define SWAIT_HEAD_INITIALIZER(name) {				\
+		.lock	= __RAW_SPIN_LOCK_UNLOCKED(name.lock),	\
+		.list	= LIST_HEAD_INIT((name).list),		\
+	}
+
+#define DEFINE_SWAIT_HEAD(name)					\
+	struct swait_head name = SWAIT_HEAD_INITIALIZER(name)
+
+extern void __init_swait_head(struct swait_head *h, struct lock_class_key *key);
+
+#define init_swait_head(swh)					\
+	do {							\
+		static struct lock_class_key __key;		\
+								\
+		__init_swait_head((swh), &__key);		\
+	} while (0)
+
+/*
+ * Waiter functions
+ */
+extern void swait_prepare_locked(struct swait_head *head, struct swaiter *w);
+extern void swait_prepare(struct swait_head *head, struct swaiter *w, int state);
+extern void swait_finish_locked(struct swait_head *head, struct swaiter *w);
+extern void swait_finish(struct swait_head *head, struct swaiter *w);
+
+/* Check whether a head has waiters enqueued */
+static inline bool swaitqueue_active(struct swait_head *h)
+{
+	/* Make sure the condition is visible before checking list_empty() */
+	smp_mb();
+	return !list_empty(&h->list);
+}
+
+/*
+ * Wakeup functions
+ */
+extern unsigned int __swait_wake(struct swait_head *head, unsigned int state, unsigned int num);
+extern unsigned int __swait_wake_locked(struct swait_head *head, unsigned int state, unsigned int num);
+
+#define swait_wake(head)			__swait_wake(head, TASK_NORMAL, 1)
+#define swait_wake_interruptible(head)		__swait_wake(head, TASK_INTERRUPTIBLE, 1)
+#define swait_wake_all(head)			__swait_wake(head, TASK_NORMAL, 0)
+#define swait_wake_all_interruptible(head)	__swait_wake(head, TASK_INTERRUPTIBLE, 0)
+
+/*
+ * Event API
+ */
+#define __swait_event(wq, condition)					\
+do {									\
+	DEFINE_SWAITER(__wait);						\
+									\
+	for (;;) {							\
+		swait_prepare(&wq, &__wait, TASK_UNINTERRUPTIBLE);	\
+		if (condition)						\
+			break;						\
+		schedule();						\
+	}								\
+	swait_finish(&wq, &__wait);					\
+} while (0)
+
+/**
+ * swait_event - sleep until a condition gets true
+ * @wq: the waitqueue to wait on
+ * @condition: a C expression for the event to wait for
+ *
+ * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
+ * @condition evaluates to true. The @condition is checked each time
+ * the waitqueue @wq is woken up.
+ *
+ * wake_up() has to be called after changing any variable that could
+ * change the result of the wait condition.
+ */
+#define swait_event(wq, condition)					\
+do {									\
+	if (condition)							\
+		break;							\
+	__swait_event(wq, condition);					\
+} while (0)
+
+#define __swait_event_interruptible(wq, condition, ret)			\
+do {									\
+	DEFINE_SWAITER(__wait);						\
+									\
+	for (;;) {							\
+		swait_prepare(&wq, &__wait, TASK_INTERRUPTIBLE);	\
+		if (condition)						\
+			break;						\
+		if (signal_pending(current)) {				\
+			ret = -ERESTARTSYS;				\
+			break;						\
+		}							\
+		schedule();						\
+	}								\
+	swait_finish(&wq, &__wait);					\
+} while (0)
+
+#define __swait_event_interruptible_timeout(wq, condition, ret)		\
+do {									\
+	DEFINE_SWAITER(__wait);						\
+									\
+	for (;;) {							\
+		swait_prepare(&wq, &__wait, TASK_INTERRUPTIBLE);	\
+		if (condition)						\
+			break;						\
+		if (signal_pending(current)) {				\
+			ret = -ERESTARTSYS;				\
+			break;						\
+		}							\
+		ret = schedule_timeout(ret);				\
+		if (!ret)						\
+			break;						\
+	}								\
+	swait_finish(&wq, &__wait);					\
+} while (0)
+
+/**
+ * swait_event_interruptible - sleep until a condition gets true
+ * @wq: the waitqueue to wait on
+ * @condition: a C expression for the event to wait for
+ *
+ * The process is put to sleep (TASK_INTERRUPTIBLE) until the
+ * @condition evaluates to true. The @condition is checked each time
+ * the waitqueue @wq is woken up.
+ *
+ * wake_up() has to be called after changing any variable that could
+ * change the result of the wait condition.
+ */
+#define swait_event_interruptible(wq, condition)			\
+({									\
+	int __ret = 0;							\
+	if (!(condition))						\
+		__swait_event_interruptible(wq, condition, __ret);	\
+	__ret;								\
+})
+
+#define swait_event_interruptible_timeout(wq, condition, timeout)	\
+({									\
+	int __ret = timeout;						\
+	if (!(condition))						\
+		__swait_event_interruptible_timeout(wq, condition, __ret);	\
+	__ret;								\
+})
+
+#define __swait_event_timeout(wq, condition, ret)			\
+do {									\
+	DEFINE_SWAITER(__wait);						\
+									\
+	for (;;) {							\
+		swait_prepare(&wq, &__wait, TASK_UNINTERRUPTIBLE);	\
+		if (condition)						\
+			break;						\
+		ret = schedule_timeout(ret);				\
+		if (!ret)						\
+			break;						\
+	}								\
+	swait_finish(&wq, &__wait);					\
+} while (0)
+
+/**
+ * swait_event_timeout - sleep until a condition gets true or a timeout elapses
+ * @wq: the waitqueue to wait on
+ * @condition: a C expression for the event to wait for
+ * @timeout: timeout, in jiffies
+ *
+ * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
+ * @condition evaluates to true. The @condition is checked each time
+ * the waitqueue @wq is woken up.
+ *
+ * wake_up() has to be called after changing any variable that could
+ * change the result of the wait condition.
+ *
+ * The function returns 0 if the @timeout elapsed, and the remaining
+ * jiffies if the condition evaluated to true before the timeout elapsed.
+ */
+#define swait_event_timeout(wq, condition, timeout)			\
+({									\
+	long __ret = timeout;						\
+	if (!(condition))						\
+		__swait_event_timeout(wq, condition, __ret);		\
+	__ret;								\
+})
+
+#endif
diff --git a/kernel/msm-3.18/include/linux/wait.h b/kernel/msm-3.18/include/linux/wait.h
index fc0e99395..5e04bf447 100644
--- a/kernel/msm-3.18/include/linux/wait.h
+++ b/kernel/msm-3.18/include/linux/wait.h
@@ -8,6 +8,7 @@
 #include <linux/spinlock.h>
 #include <asm/current.h>
 #include <uapi/linux/wait.h>
+#include <linux/atomic.h>
 
 typedef struct __wait_queue wait_queue_t;
 typedef int (*wait_queue_func_t)(wait_queue_t *wait, unsigned mode, int flags, void *key);
diff --git a/kernel/msm-3.18/include/linux/work-simple.h b/kernel/msm-3.18/include/linux/work-simple.h
new file mode 100644
index 000000000..f175fa9a6
--- /dev/null
+++ b/kernel/msm-3.18/include/linux/work-simple.h
@@ -0,0 +1,24 @@
+#ifndef _LINUX_SWORK_H
+#define _LINUX_SWORK_H
+
+#include <linux/list.h>
+
+struct swork_event {
+	struct list_head item;
+	unsigned long flags;
+	void (*func)(struct swork_event *);
+};
+
+static inline void INIT_SWORK(struct swork_event *event,
+			      void (*func)(struct swork_event *))
+{
+	event->flags = 0;
+	event->func = func;
+}
+
+bool swork_queue(struct swork_event *sev);
+
+int swork_get(void);
+void swork_put(void);
+
+#endif /* _LINUX_SWORK_H */
diff --git a/kernel/msm-3.18/include/net/dst.h b/kernel/msm-3.18/include/net/dst.h
index 182b812d4..74baade72 100644
--- a/kernel/msm-3.18/include/net/dst.h
+++ b/kernel/msm-3.18/include/net/dst.h
@@ -436,7 +436,7 @@ static inline void dst_confirm(struct dst_entry *dst)
 static inline int dst_neigh_output(struct dst_entry *dst, struct neighbour *n,
 				   struct sk_buff *skb)
 {
-	const struct hh_cache *hh;
+	struct hh_cache *hh;
 
 	if (dst->pending_confirm) {
 		unsigned long now = jiffies;
diff --git a/kernel/msm-3.18/include/net/neighbour.h b/kernel/msm-3.18/include/net/neighbour.h
index 746878b0b..34990d012 100644
--- a/kernel/msm-3.18/include/net/neighbour.h
+++ b/kernel/msm-3.18/include/net/neighbour.h
@@ -388,7 +388,7 @@ static inline int neigh_hh_bridge(struct hh_cache *hh, struct sk_buff *skb)
 }
 #endif
 
-static inline int neigh_hh_output(const struct hh_cache *hh, struct sk_buff *skb)
+static inline int neigh_hh_output(struct hh_cache *hh, struct sk_buff *skb)
 {
 	unsigned int seq;
 	int hh_len;
@@ -443,7 +443,7 @@ struct neighbour_cb {
 
 #define NEIGH_CB(skb)	((struct neighbour_cb *)(skb)->cb)
 
-static inline void neigh_ha_snapshot(char *dst, const struct neighbour *n,
+static inline void neigh_ha_snapshot(char *dst, struct neighbour *n,
 				     const struct net_device *dev)
 {
 	unsigned int seq;
diff --git a/kernel/msm-3.18/include/net/netns/ipv4.h b/kernel/msm-3.18/include/net/netns/ipv4.h
index 0ffef1a38..dddc3a717 100644
--- a/kernel/msm-3.18/include/net/netns/ipv4.h
+++ b/kernel/msm-3.18/include/net/netns/ipv4.h
@@ -67,6 +67,7 @@ struct netns_ipv4 {
 
 	int sysctl_icmp_echo_ignore_all;
 	int sysctl_icmp_echo_ignore_broadcasts;
+	int sysctl_icmp_echo_sysrq;
 	int sysctl_icmp_ignore_bogus_error_responses;
 	int sysctl_icmp_ratelimit;
 	int sysctl_icmp_ratemask;
diff --git a/kernel/msm-3.18/include/trace/events/hist.h b/kernel/msm-3.18/include/trace/events/hist.h
new file mode 100644
index 000000000..f7710de1b
--- /dev/null
+++ b/kernel/msm-3.18/include/trace/events/hist.h
@@ -0,0 +1,73 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM hist
+
+#if !defined(_TRACE_HIST_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_HIST_H
+
+#include "latency_hist.h"
+#include <linux/tracepoint.h>
+
+#if !defined(CONFIG_PREEMPT_OFF_HIST) && !defined(CONFIG_INTERRUPT_OFF_HIST)
+#define trace_preemptirqsoff_hist(a, b)
+#define trace_preemptirqsoff_hist_rcuidle(a, b)
+#else
+TRACE_EVENT(preemptirqsoff_hist,
+
+	TP_PROTO(int reason, int starthist),
+
+	TP_ARGS(reason, starthist),
+
+	TP_STRUCT__entry(
+		__field(int,	reason)
+		__field(int,	starthist)
+	),
+
+	TP_fast_assign(
+		__entry->reason		= reason;
+		__entry->starthist	= starthist;
+	),
+
+	TP_printk("reason=%s starthist=%s", getaction(__entry->reason),
+		  __entry->starthist ? "start" : "stop")
+);
+#endif
+
+#ifndef CONFIG_MISSED_TIMER_OFFSETS_HIST
+#define trace_hrtimer_interrupt(a, b, c, d)
+#else
+TRACE_EVENT(hrtimer_interrupt,
+
+	TP_PROTO(int cpu, long long offset, struct task_struct *curr,
+		struct task_struct *task),
+
+	TP_ARGS(cpu, offset, curr, task),
+
+	TP_STRUCT__entry(
+		__field(int,		cpu)
+		__field(long long,	offset)
+		__array(char,		ccomm,	TASK_COMM_LEN)
+		__field(int,		cprio)
+		__array(char,		tcomm,	TASK_COMM_LEN)
+		__field(int,		tprio)
+	),
+
+	TP_fast_assign(
+		__entry->cpu	= cpu;
+		__entry->offset	= offset;
+		memcpy(__entry->ccomm, curr->comm, TASK_COMM_LEN);
+		__entry->cprio  = curr->prio;
+		memcpy(__entry->tcomm, task != NULL ? task->comm : "<none>",
+			task != NULL ? TASK_COMM_LEN : 7);
+		__entry->tprio  = task != NULL ? task->prio : -1;
+	),
+
+	TP_printk("cpu=%d offset=%lld curr=%s[%d] thread=%s[%d]",
+		__entry->cpu, __entry->offset, __entry->ccomm,
+		__entry->cprio, __entry->tcomm, __entry->tprio)
+);
+#endif
+
+#endif /* _TRACE_HIST_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/kernel/msm-3.18/include/trace/events/latency_hist.h b/kernel/msm-3.18/include/trace/events/latency_hist.h
new file mode 100644
index 000000000..d3f2fbd56
--- /dev/null
+++ b/kernel/msm-3.18/include/trace/events/latency_hist.h
@@ -0,0 +1,29 @@
+#ifndef _LATENCY_HIST_H
+#define _LATENCY_HIST_H
+
+enum hist_action {
+	IRQS_ON,
+	PREEMPT_ON,
+	TRACE_STOP,
+	IRQS_OFF,
+	PREEMPT_OFF,
+	TRACE_START,
+};
+
+static char *actions[] = {
+	"IRQS_ON",
+	"PREEMPT_ON",
+	"TRACE_STOP",
+	"IRQS_OFF",
+	"PREEMPT_OFF",
+	"TRACE_START",
+};
+
+static inline char *getaction(int action)
+{
+	if (action >= 0 && action <= sizeof(actions)/sizeof(actions[0]))
+		return actions[action];
+	return "unknown";
+}
+
+#endif /* _LATENCY_HIST_H */
diff --git a/kernel/msm-3.18/include/trace/events/sched.h b/kernel/msm-3.18/include/trace/events/sched.h
index 6510169ac..401c67e1a 100644
--- a/kernel/msm-3.18/include/trace/events/sched.h
+++ b/kernel/msm-3.18/include/trace/events/sched.h
@@ -647,9 +647,9 @@ TRACE_EVENT(sched_freq_alert,
  */
 DECLARE_EVENT_CLASS(sched_wakeup_template,
 
-	TP_PROTO(struct task_struct *p, int success),
+	TP_PROTO(struct task_struct *p),
 
-	TP_ARGS(__perf_task(p), success),
+	TP_ARGS(__perf_task(p)),
 
 	TP_STRUCT__entry(
 		__array(	char,	comm,	TASK_COMM_LEN	)
@@ -663,25 +663,37 @@ DECLARE_EVENT_CLASS(sched_wakeup_template,
 		memcpy(__entry->comm, p->comm, TASK_COMM_LEN);
 		__entry->pid		= p->pid;
 		__entry->prio		= p->prio;
-		__entry->success	= success;
+		__entry->success	= 1; /* rudiment, kill when possible */
 		__entry->target_cpu	= task_cpu(p);
 	),
 
-	TP_printk("comm=%s pid=%d prio=%d success=%d target_cpu=%03d",
+	TP_printk("comm=%s pid=%d prio=%d target_cpu=%03d",
 		  __entry->comm, __entry->pid, __entry->prio,
-		  __entry->success, __entry->target_cpu)
+		  __entry->target_cpu)
 );
 
+/*
+ * Tracepoint called when waking a task; this tracepoint is guaranteed to be
+ * called from the waking context.
+ */
+DEFINE_EVENT(sched_wakeup_template, sched_waking,
+	     TP_PROTO(struct task_struct *p),
+	     TP_ARGS(p));
+
+/*
+ * Tracepoint called when the task is actually woken; p->state == TASK_RUNNNG.
+ * It it not always called from the waking context.
+ */
 DEFINE_EVENT(sched_wakeup_template, sched_wakeup,
-	     TP_PROTO(struct task_struct *p, int success),
-	     TP_ARGS(p, success));
+	     TP_PROTO(struct task_struct *p),
+	     TP_ARGS(p));
 
 /*
  * Tracepoint for waking up a new task:
  */
 DEFINE_EVENT(sched_wakeup_template, sched_wakeup_new,
-	     TP_PROTO(struct task_struct *p, int success),
-	     TP_ARGS(p, success));
+	     TP_PROTO(struct task_struct *p),
+	     TP_ARGS(p));
 
 #ifdef CREATE_TRACE_POINTS
 static inline long __trace_sched_switch_state(struct task_struct *p)
diff --git a/kernel/msm-3.18/init/Kconfig b/kernel/msm-3.18/init/Kconfig
index de0994b3f..fa2482387 100644
--- a/kernel/msm-3.18/init/Kconfig
+++ b/kernel/msm-3.18/init/Kconfig
@@ -644,7 +644,7 @@ config RCU_FANOUT_EXACT
 
 config RCU_FAST_NO_HZ
 	bool "Accelerate last non-dyntick-idle CPU's grace periods"
-	depends on NO_HZ_COMMON && SMP
+	depends on NO_HZ_COMMON && SMP && !PREEMPT_RT_FULL
 	default n
 	help
 	  This option causes RCU to attempt to accelerate grace periods in
@@ -668,7 +668,7 @@ config TREE_RCU_TRACE
 config RCU_BOOST
 	bool "Enable RCU priority boosting"
 	depends on RT_MUTEXES && PREEMPT_RCU
-	default n
+	default y if PREEMPT_RT_FULL
 	help
 	  This option boosts the priority of preempted RCU readers that
 	  block the current preemptible RCU grace period for too long.
@@ -1139,6 +1139,7 @@ config CFS_BANDWIDTH
 config RT_GROUP_SCHED
 	bool "Group scheduling for SCHED_RR/FIFO"
 	depends on CGROUP_SCHED
+	depends on !PREEMPT_RT_FULL
 	default n
 	help
 	  This feature lets you explicitly allocate real CPU bandwidth
@@ -1774,6 +1775,7 @@ choice
 config SLAB
 	bool "SLAB"
 	select HAVE_HARDENED_USERCOPY_ALLOCATOR
+	depends on !PREEMPT_RT_FULL
 	help
 	  The regular slab allocator that is established and known to work
 	  well in all environments. It organizes cache hot objects in
@@ -1793,6 +1795,7 @@ config SLUB
 config SLOB
 	depends on EXPERT
 	bool "SLOB (Simple Allocator)"
+	depends on !PREEMPT_RT_FULL
 	help
 	   SLOB replaces the stock allocator with a drastically simpler
 	   allocator. SLOB is generally more space efficient but
diff --git a/kernel/msm-3.18/init/Makefile b/kernel/msm-3.18/init/Makefile
index 243f61de2..955f13434 100644
--- a/kernel/msm-3.18/init/Makefile
+++ b/kernel/msm-3.18/init/Makefile
@@ -31,4 +31,4 @@ silent_chk_compile.h = :
 include/generated/compile.h: FORCE
 	@$($(quiet)chk_compile.h)
 	$(Q)$(CONFIG_SHELL) $(srctree)/scripts/mkcompile_h $@ \
-	"$(UTS_MACHINE)" "$(CONFIG_SMP)" "$(CONFIG_PREEMPT)" "$(CC) $(KBUILD_CFLAGS)"
+	"$(UTS_MACHINE)" "$(CONFIG_SMP)" "$(CONFIG_PREEMPT)" "$(CONFIG_PREEMPT_RT_FULL)" "$(CC) $(KBUILD_CFLAGS)"
diff --git a/kernel/msm-3.18/init/main.c b/kernel/msm-3.18/init/main.c
index ce781b1b9..47bff5b05 100644
--- a/kernel/msm-3.18/init/main.c
+++ b/kernel/msm-3.18/init/main.c
@@ -535,6 +535,7 @@ asmlinkage __visible void __init start_kernel(void)
 	setup_command_line(command_line);
 	setup_nr_cpu_ids();
 	setup_per_cpu_areas();
+	softirq_early_init();
 	smp_prepare_boot_cpu();	/* arch-specific boot-cpu hooks */
 
 	build_all_zonelists(NULL, NULL);
diff --git a/kernel/msm-3.18/ipc/mqueue.c b/kernel/msm-3.18/ipc/mqueue.c
index 31a913a30..0f2c62e9b 100644
--- a/kernel/msm-3.18/ipc/mqueue.c
+++ b/kernel/msm-3.18/ipc/mqueue.c
@@ -47,8 +47,7 @@
 #define RECV		1
 
 #define STATE_NONE	0
-#define STATE_PENDING	1
-#define STATE_READY	2
+#define STATE_READY	1
 
 struct posix_msg_tree_node {
 	struct rb_node		rb_node;
@@ -568,15 +567,12 @@ static int wq_sleep(struct mqueue_inode_info *info, int sr,
 	wq_add(info, sr, ewp);
 
 	for (;;) {
-		set_current_state(TASK_INTERRUPTIBLE);
+		__set_current_state(TASK_INTERRUPTIBLE);
 
 		spin_unlock(&info->lock);
 		time = schedule_hrtimeout_range_clock(timeout, 0,
 			HRTIMER_MODE_ABS, CLOCK_REALTIME);
 
-		while (ewp->state == STATE_PENDING)
-			cpu_relax();
-
 		if (ewp->state == STATE_READY) {
 			retval = 0;
 			goto out;
@@ -904,11 +900,15 @@ out_name:
  * list of waiting receivers. A sender checks that list before adding the new
  * message into the message array. If there is a waiting receiver, then it
  * bypasses the message array and directly hands the message over to the
- * receiver.
- * The receiver accepts the message and returns without grabbing the queue
- * spinlock. Therefore an intermediate STATE_PENDING state and memory barriers
- * are necessary. The same algorithm is used for sysv semaphores, see
- * ipc/sem.c for more details.
+ * receiver. The receiver accepts the message and returns without grabbing the
+ * queue spinlock:
+ *
+ * - Set pointer to message.
+ * - Queue the receiver task for later wakeup (without the info->lock).
+ * - Update its state to STATE_READY. Now the receiver can continue.
+ * - Wake up the process after the lock is dropped. Should the process wake up
+ *   before this wakeup (due to a timeout or a signal) it will either see
+ *   STATE_READY and continue or acquire the lock to check the state again.
  *
  * The same algorithm is used for senders.
  */
@@ -916,21 +916,34 @@ out_name:
 /* pipelined_send() - send a message directly to the task waiting in
  * sys_mq_timedreceive() (without inserting message into a queue).
  */
-static inline void pipelined_send(struct mqueue_inode_info *info,
+static inline void pipelined_send(struct wake_q_head *wake_q,
+				  struct mqueue_inode_info *info,
 				  struct msg_msg *message,
 				  struct ext_wait_queue *receiver)
 {
+	/*
+	 * Keep them in one critical section for PREEMPT_RT:
+	 */
+	preempt_disable_rt();
 	receiver->msg = message;
 	list_del(&receiver->list);
-	receiver->state = STATE_PENDING;
-	wake_up_process(receiver->task);
-	smp_wmb();
+	wake_q_add(wake_q, receiver->task);
+	/*
+	 * Rely on the implicit cmpxchg barrier from wake_q_add such
+	 * that we can ensure that updating receiver->state is the last
+	 * write operation: As once set, the receiver can continue,
+	 * and if we don't have the reference count from the wake_q,
+	 * yet, at that point we can later have a use-after-free
+	 * condition and bogus wakeup.
+	 */
 	receiver->state = STATE_READY;
+	preempt_enable_rt();
 }
 
 /* pipelined_receive() - if there is task waiting in sys_mq_timedsend()
  * gets its message and put to the queue (we have one free place for sure). */
-static inline void pipelined_receive(struct mqueue_inode_info *info)
+static inline void pipelined_receive(struct wake_q_head *wake_q,
+				     struct mqueue_inode_info *info)
 {
 	struct ext_wait_queue *sender = wq_get_first_waiter(info, SEND);
 
@@ -939,13 +952,16 @@ static inline void pipelined_receive(struct mqueue_inode_info *info)
 		wake_up_interruptible(&info->wait_q);
 		return;
 	}
-	if (msg_insert(sender->msg, info))
-		return;
-	list_del(&sender->list);
-	sender->state = STATE_PENDING;
-	wake_up_process(sender->task);
-	smp_wmb();
-	sender->state = STATE_READY;
+	/*
+	 * Keep them in one critical section for PREEMPT_RT:
+	 */
+	preempt_disable_rt();
+	if (!msg_insert(sender->msg, info)) {
+		list_del(&sender->list);
+		wake_q_add(wake_q, sender->task);
+		sender->state = STATE_READY;
+	}
+	preempt_enable_rt();
 }
 
 SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,
@@ -962,6 +978,7 @@ SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,
 	struct timespec ts;
 	struct posix_msg_tree_node *new_leaf = NULL;
 	int ret = 0;
+	WAKE_Q(wake_q);
 
 	if (u_abs_timeout) {
 		int res = prepare_timeout(u_abs_timeout, &expires, &ts);
@@ -1045,7 +1062,7 @@ SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,
 	} else {
 		receiver = wq_get_first_waiter(info, RECV);
 		if (receiver) {
-			pipelined_send(info, msg_ptr, receiver);
+			pipelined_send(&wake_q, info, msg_ptr, receiver);
 		} else {
 			/* adds message to the queue */
 			ret = msg_insert(msg_ptr, info);
@@ -1058,6 +1075,7 @@ SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,
 	}
 out_unlock:
 	spin_unlock(&info->lock);
+	wake_up_q(&wake_q);
 out_free:
 	if (ret)
 		free_msg(msg_ptr);
@@ -1144,14 +1162,17 @@ SYSCALL_DEFINE5(mq_timedreceive, mqd_t, mqdes, char __user *, u_msg_ptr,
 			msg_ptr = wait.msg;
 		}
 	} else {
+		WAKE_Q(wake_q);
+
 		msg_ptr = msg_get(info);
 
 		inode->i_atime = inode->i_mtime = inode->i_ctime =
 				CURRENT_TIME;
 
 		/* There is now free space in queue. */
-		pipelined_receive(info);
+		pipelined_receive(&wake_q, info);
 		spin_unlock(&info->lock);
+		wake_up_q(&wake_q);
 		ret = 0;
 	}
 	if (ret == 0) {
diff --git a/kernel/msm-3.18/ipc/msg.c b/kernel/msm-3.18/ipc/msg.c
index 02e72d3db..2381f5f20 100644
--- a/kernel/msm-3.18/ipc/msg.c
+++ b/kernel/msm-3.18/ipc/msg.c
@@ -188,6 +188,12 @@ static void expunge_all(struct msg_queue *msq, int res)
 	struct msg_receiver *msr, *t;
 
 	list_for_each_entry_safe(msr, t, &msq->q_receivers, r_list) {
+		/*
+		 * Make sure that the wakeup doesnt preempt
+		 * this CPU prematurely. (on PREEMPT_RT)
+		 */
+		preempt_disable_rt();
+
 		msr->r_msg = NULL; /* initialize expunge ordering */
 		wake_up_process(msr->r_tsk);
 		/*
@@ -198,6 +204,8 @@ static void expunge_all(struct msg_queue *msq, int res)
 		 */
 		smp_mb();
 		msr->r_msg = ERR_PTR(res);
+
+		preempt_enable_rt();
 	}
 }
 
@@ -574,6 +582,11 @@ static inline int pipelined_send(struct msg_queue *msq, struct msg_msg *msg)
 		if (testmsg(msg, msr->r_msgtype, msr->r_mode) &&
 		    !security_msg_queue_msgrcv(msq, msg, msr->r_tsk,
 					       msr->r_msgtype, msr->r_mode)) {
+			/*
+			 * Make sure that the wakeup doesnt preempt
+			 * this CPU prematurely. (on PREEMPT_RT)
+			 */
+			preempt_disable_rt();
 
 			list_del(&msr->r_list);
 			if (msr->r_maxsize < msg->m_ts) {
@@ -595,12 +608,13 @@ static inline int pipelined_send(struct msg_queue *msq, struct msg_msg *msg)
 				 */
 				smp_mb();
 				msr->r_msg = msg;
+				preempt_enable_rt();
 
 				return 1;
 			}
+			preempt_enable_rt();
 		}
 	}
-
 	return 0;
 }
 
diff --git a/kernel/msm-3.18/ipc/sem.c b/kernel/msm-3.18/ipc/sem.c
index e59935d0d..1be83c33b 100644
--- a/kernel/msm-3.18/ipc/sem.c
+++ b/kernel/msm-3.18/ipc/sem.c
@@ -689,6 +689,13 @@ undo:
 static void wake_up_sem_queue_prepare(struct list_head *pt,
 				struct sem_queue *q, int error)
 {
+#ifdef CONFIG_PREEMPT_RT_BASE
+	struct task_struct *p = q->sleeper;
+	get_task_struct(p);
+	q->status = error;
+	wake_up_process(p);
+	put_task_struct(p);
+#else
 	if (list_empty(pt)) {
 		/*
 		 * Hold preempt off so that we don't get preempted and have the
@@ -700,6 +707,7 @@ static void wake_up_sem_queue_prepare(struct list_head *pt,
 	q->pid = error;
 
 	list_add_tail(&q->list, pt);
+#endif
 }
 
 /**
@@ -713,6 +721,7 @@ static void wake_up_sem_queue_prepare(struct list_head *pt,
  */
 static void wake_up_sem_queue_do(struct list_head *pt)
 {
+#ifndef CONFIG_PREEMPT_RT_BASE
 	struct sem_queue *q, *t;
 	int did_something;
 
@@ -725,6 +734,7 @@ static void wake_up_sem_queue_do(struct list_head *pt)
 	}
 	if (did_something)
 		preempt_enable();
+#endif
 }
 
 static void unlink_queue(struct sem_array *sma, struct sem_queue *q)
diff --git a/kernel/msm-3.18/kernel/Kconfig.locks b/kernel/msm-3.18/kernel/Kconfig.locks
index 76768ee81..68212ae72 100644
--- a/kernel/msm-3.18/kernel/Kconfig.locks
+++ b/kernel/msm-3.18/kernel/Kconfig.locks
@@ -225,11 +225,11 @@ config ARCH_SUPPORTS_ATOMIC_RMW
 
 config MUTEX_SPIN_ON_OWNER
 	def_bool y
-	depends on SMP && !DEBUG_MUTEXES && ARCH_SUPPORTS_ATOMIC_RMW
+	depends on SMP && !DEBUG_MUTEXES && ARCH_SUPPORTS_ATOMIC_RMW && !PREEMPT_RT_FULL
 
 config RWSEM_SPIN_ON_OWNER
        def_bool y
-       depends on SMP && RWSEM_XCHGADD_ALGORITHM && ARCH_SUPPORTS_ATOMIC_RMW
+       depends on SMP && RWSEM_XCHGADD_ALGORITHM && ARCH_SUPPORTS_ATOMIC_RMW && !PREEMPT_RT_FULL
 
 config ARCH_USE_QUEUE_RWLOCK
 	bool
diff --git a/kernel/msm-3.18/kernel/Kconfig.preempt b/kernel/msm-3.18/kernel/Kconfig.preempt
index 3f9c97419..11dbe26a8 100644
--- a/kernel/msm-3.18/kernel/Kconfig.preempt
+++ b/kernel/msm-3.18/kernel/Kconfig.preempt
@@ -1,3 +1,16 @@
+config PREEMPT
+	bool
+	select PREEMPT_COUNT
+
+config PREEMPT_RT_BASE
+	bool
+	select PREEMPT
+
+config HAVE_PREEMPT_LAZY
+	bool
+
+config PREEMPT_LAZY
+	def_bool y if HAVE_PREEMPT_LAZY && PREEMPT_RT_FULL
 
 choice
 	prompt "Preemption Model"
@@ -33,9 +46,9 @@ config PREEMPT_VOLUNTARY
 
 	  Select this if you are building a kernel for a desktop system.
 
-config PREEMPT
+config PREEMPT__LL
 	bool "Preemptible Kernel (Low-Latency Desktop)"
-	select PREEMPT_COUNT
+	select PREEMPT
 	select UNINLINE_SPIN_UNLOCK if !ARCH_INLINE_SPIN_UNLOCK
 	help
 	  This option reduces the latency of the kernel by making
@@ -52,6 +65,22 @@ config PREEMPT
 	  embedded system with latency requirements in the milliseconds
 	  range.
 
+config PREEMPT_RTB
+	bool "Preemptible Kernel (Basic RT)"
+	select PREEMPT_RT_BASE
+	help
+	  This option is basically the same as (Low-Latency Desktop) but
+	  enables changes which are preliminary for the full preemptible
+	  RT kernel.
+
+config PREEMPT_RT_FULL
+	bool "Fully Preemptible Kernel (RT)"
+	depends on IRQ_FORCED_THREADING
+	select PREEMPT_RT_BASE
+	select PREEMPT_RCU
+	help
+	  All and everything
+
 endchoice
 
 config PREEMPT_COUNT
diff --git a/kernel/msm-3.18/kernel/cgroup.c b/kernel/msm-3.18/kernel/cgroup.c
index 4b1a9ca08..62f78d845 100644
--- a/kernel/msm-3.18/kernel/cgroup.c
+++ b/kernel/msm-3.18/kernel/cgroup.c
@@ -4427,10 +4427,10 @@ static void css_free_rcu_fn(struct rcu_head *rcu_head)
 	queue_work(cgroup_destroy_wq, &css->destroy_work);
 }
 
-static void css_release_work_fn(struct work_struct *work)
+static void css_release_work_fn(struct swork_event *sev)
 {
 	struct cgroup_subsys_state *css =
-		container_of(work, struct cgroup_subsys_state, destroy_work);
+		container_of(sev, struct cgroup_subsys_state, destroy_swork);
 	struct cgroup_subsys *ss = css->ss;
 	struct cgroup *cgrp = css->cgroup;
 
@@ -4467,8 +4467,8 @@ static void css_release(struct percpu_ref *ref)
 	struct cgroup_subsys_state *css =
 		container_of(ref, struct cgroup_subsys_state, refcnt);
 
-	INIT_WORK(&css->destroy_work, css_release_work_fn);
-	queue_work(cgroup_destroy_wq, &css->destroy_work);
+	INIT_SWORK(&css->destroy_swork, css_release_work_fn);
+	swork_queue(&css->destroy_swork);
 }
 
 static void init_and_link_css(struct cgroup_subsys_state *css,
@@ -5066,6 +5066,7 @@ static int __init cgroup_wq_init(void)
 	 */
 	cgroup_destroy_wq = alloc_workqueue("cgroup_destroy", 0, 1);
 	BUG_ON(!cgroup_destroy_wq);
+	BUG_ON(swork_get());
 
 	/*
 	 * Used to destroy pidlists and separate to serve as flush domain.
diff --git a/kernel/msm-3.18/kernel/cpu.c b/kernel/msm-3.18/kernel/cpu.c
index 8503e5eaa..5e27b640d 100644
--- a/kernel/msm-3.18/kernel/cpu.c
+++ b/kernel/msm-3.18/kernel/cpu.c
@@ -90,6 +90,289 @@ static struct {
 #define cpuhp_lock_acquire()      lock_map_acquire(&cpu_hotplug.dep_map)
 #define cpuhp_lock_release()      lock_map_release(&cpu_hotplug.dep_map)
 
+/**
+ * hotplug_pcp	- per cpu hotplug descriptor
+ * @unplug:	set when pin_current_cpu() needs to sync tasks
+ * @sync_tsk:	the task that waits for tasks to finish pinned sections
+ * @refcount:	counter of tasks in pinned sections
+ * @grab_lock:	set when the tasks entering pinned sections should wait
+ * @synced:	notifier for @sync_tsk to tell cpu_down it's finished
+ * @mutex:	the mutex to make tasks wait (used when @grab_lock is true)
+ * @mutex_init:	zero if the mutex hasn't been initialized yet.
+ *
+ * Although @unplug and @sync_tsk may point to the same task, the @unplug
+ * is used as a flag and still exists after @sync_tsk has exited and
+ * @sync_tsk set to NULL.
+ */
+struct hotplug_pcp {
+	struct task_struct *unplug;
+	struct task_struct *sync_tsk;
+	int refcount;
+	int grab_lock;
+	struct completion synced;
+	struct completion unplug_wait;
+#ifdef CONFIG_PREEMPT_RT_FULL
+	/*
+	 * Note, on PREEMPT_RT, the hotplug lock must save the state of
+	 * the task, otherwise the mutex will cause the task to fail
+	 * to sleep when required. (Because it's called from migrate_disable())
+	 *
+	 * The spinlock_t on PREEMPT_RT is a mutex that saves the task's
+	 * state.
+	 */
+	spinlock_t lock;
+#else
+	struct mutex mutex;
+#endif
+	int mutex_init;
+};
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+# define hotplug_lock(hp) rt_spin_lock(&(hp)->lock)
+# define hotplug_unlock(hp) rt_spin_unlock(&(hp)->lock)
+#else
+# define hotplug_lock(hp) mutex_lock(&(hp)->mutex)
+# define hotplug_unlock(hp) mutex_unlock(&(hp)->mutex)
+#endif
+
+static DEFINE_PER_CPU(struct hotplug_pcp, hotplug_pcp);
+
+/**
+ * pin_current_cpu - Prevent the current cpu from being unplugged
+ *
+ * Lightweight version of get_online_cpus() to prevent cpu from being
+ * unplugged when code runs in a migration disabled region.
+ *
+ * Must be called with preemption disabled (preempt_count = 1)!
+ */
+void pin_current_cpu(void)
+{
+	struct hotplug_pcp *hp;
+	int force = 0;
+
+retry:
+	hp = &__get_cpu_var(hotplug_pcp);
+
+	if (!hp->unplug || hp->refcount || force || preempt_count() > 1 ||
+	    hp->unplug == current) {
+		hp->refcount++;
+		return;
+	}
+	if (hp->grab_lock) {
+		preempt_enable();
+		hotplug_lock(hp);
+		hotplug_unlock(hp);
+	} else {
+		preempt_enable();
+		/*
+		 * Try to push this task off of this CPU.
+		 */
+		if (!migrate_me()) {
+			preempt_disable();
+			hp = &__get_cpu_var(hotplug_pcp);
+			if (!hp->grab_lock) {
+				/*
+				 * Just let it continue it's already pinned
+				 * or about to sleep.
+				 */
+				force = 1;
+				goto retry;
+			}
+			preempt_enable();
+		}
+	}
+	preempt_disable();
+	goto retry;
+}
+
+/**
+ * unpin_current_cpu - Allow unplug of current cpu
+ *
+ * Must be called with preemption or interrupts disabled!
+ */
+void unpin_current_cpu(void)
+{
+	struct hotplug_pcp *hp = &__get_cpu_var(hotplug_pcp);
+
+	WARN_ON(hp->refcount <= 0);
+
+	/* This is safe. sync_unplug_thread is pinned to this cpu */
+	if (!--hp->refcount && hp->unplug && hp->unplug != current)
+		wake_up_process(hp->unplug);
+}
+
+static void wait_for_pinned_cpus(struct hotplug_pcp *hp)
+{
+	set_current_state(TASK_UNINTERRUPTIBLE);
+	while (hp->refcount) {
+		schedule_preempt_disabled();
+		set_current_state(TASK_UNINTERRUPTIBLE);
+	}
+}
+
+static int sync_unplug_thread(void *data)
+{
+	struct hotplug_pcp *hp = data;
+
+	wait_for_completion(&hp->unplug_wait);
+	preempt_disable();
+	hp->unplug = current;
+	wait_for_pinned_cpus(hp);
+
+	/*
+	 * This thread will synchronize the cpu_down() with threads
+	 * that have pinned the CPU. When the pinned CPU count reaches
+	 * zero, we inform the cpu_down code to continue to the next step.
+	 */
+	set_current_state(TASK_UNINTERRUPTIBLE);
+	preempt_enable();
+	complete(&hp->synced);
+
+	/*
+	 * If all succeeds, the next step will need tasks to wait till
+	 * the CPU is offline before continuing. To do this, the grab_lock
+	 * is set and tasks going into pin_current_cpu() will block on the
+	 * mutex. But we still need to wait for those that are already in
+	 * pinned CPU sections. If the cpu_down() failed, the kthread_should_stop()
+	 * will kick this thread out.
+	 */
+	while (!hp->grab_lock && !kthread_should_stop()) {
+		schedule();
+		set_current_state(TASK_UNINTERRUPTIBLE);
+	}
+
+	/* Make sure grab_lock is seen before we see a stale completion */
+	smp_mb();
+
+	/*
+	 * Now just before cpu_down() enters stop machine, we need to make
+	 * sure all tasks that are in pinned CPU sections are out, and new
+	 * tasks will now grab the lock, keeping them from entering pinned
+	 * CPU sections.
+	 */
+	if (!kthread_should_stop()) {
+		preempt_disable();
+		wait_for_pinned_cpus(hp);
+		preempt_enable();
+		complete(&hp->synced);
+	}
+
+	set_current_state(TASK_UNINTERRUPTIBLE);
+	while (!kthread_should_stop()) {
+		schedule();
+		set_current_state(TASK_UNINTERRUPTIBLE);
+	}
+	set_current_state(TASK_RUNNING);
+
+	/*
+	 * Force this thread off this CPU as it's going down and
+	 * we don't want any more work on this CPU.
+	 */
+	current->flags &= ~PF_NO_SETAFFINITY;
+	set_cpus_allowed_ptr(current, cpu_present_mask);
+	migrate_me();
+	return 0;
+}
+
+static void __cpu_unplug_sync(struct hotplug_pcp *hp)
+{
+	wake_up_process(hp->sync_tsk);
+	wait_for_completion(&hp->synced);
+}
+
+static void __cpu_unplug_wait(unsigned int cpu)
+{
+	struct hotplug_pcp *hp = &per_cpu(hotplug_pcp, cpu);
+
+	complete(&hp->unplug_wait);
+	wait_for_completion(&hp->synced);
+}
+
+/*
+ * Start the sync_unplug_thread on the target cpu and wait for it to
+ * complete.
+ */
+static int cpu_unplug_begin(unsigned int cpu)
+{
+	struct hotplug_pcp *hp = &per_cpu(hotplug_pcp, cpu);
+	int err;
+
+	/* Protected by cpu_hotplug.lock */
+	if (!hp->mutex_init) {
+#ifdef CONFIG_PREEMPT_RT_FULL
+		spin_lock_init(&hp->lock);
+#else
+		mutex_init(&hp->mutex);
+#endif
+		hp->mutex_init = 1;
+	}
+
+	/* Inform the scheduler to migrate tasks off this CPU */
+	tell_sched_cpu_down_begin(cpu);
+
+	init_completion(&hp->synced);
+	init_completion(&hp->unplug_wait);
+
+	hp->sync_tsk = kthread_create(sync_unplug_thread, hp, "sync_unplug/%d", cpu);
+	if (IS_ERR(hp->sync_tsk)) {
+		err = PTR_ERR(hp->sync_tsk);
+		hp->sync_tsk = NULL;
+		return err;
+	}
+	kthread_bind(hp->sync_tsk, cpu);
+
+	/*
+	 * Wait for tasks to get out of the pinned sections,
+	 * it's still OK if new tasks enter. Some CPU notifiers will
+	 * wait for tasks that are going to enter these sections and
+	 * we must not have them block.
+	 */
+	wake_up_process(hp->sync_tsk);
+	return 0;
+}
+
+static void cpu_unplug_sync(unsigned int cpu)
+{
+	struct hotplug_pcp *hp = &per_cpu(hotplug_pcp, cpu);
+
+	init_completion(&hp->synced);
+	/* The completion needs to be initialzied before setting grab_lock */
+	smp_wmb();
+
+	/* Grab the mutex before setting grab_lock */
+	hotplug_lock(hp);
+	hp->grab_lock = 1;
+
+	/*
+	 * The CPU notifiers have been completed.
+	 * Wait for tasks to get out of pinned CPU sections and have new
+	 * tasks block until the CPU is completely down.
+	 */
+	__cpu_unplug_sync(hp);
+
+	/* All done with the sync thread */
+	kthread_stop(hp->sync_tsk);
+	hp->sync_tsk = NULL;
+}
+
+static void cpu_unplug_done(unsigned int cpu)
+{
+	struct hotplug_pcp *hp = &per_cpu(hotplug_pcp, cpu);
+
+	hp->unplug = NULL;
+	/* Let all tasks know cpu unplug is finished before cleaning up */
+	smp_wmb();
+
+	if (hp->sync_tsk)
+		kthread_stop(hp->sync_tsk);
+
+	if (hp->grab_lock) {
+		hotplug_unlock(hp);
+		/* protected by cpu_hotplug.lock */
+		hp->grab_lock = 0;
+	}
+	tell_sched_cpu_down_done(cpu);
+}
 
 void get_online_cpus(void)
 {
@@ -107,6 +390,7 @@ bool try_get_online_cpus(void)
 {
 	if (cpu_hotplug.active_writer == current)
 		return true;
+
 	if (!mutex_trylock(&cpu_hotplug.lock))
 		return false;
 	cpuhp_lock_acquire_tryread();
@@ -349,13 +633,15 @@ static int __ref take_cpu_down(void *_param)
 /* Requires cpu_add_remove_lock to be held */
 static int __ref _cpu_down(unsigned int cpu, int tasks_frozen)
 {
-	int err, nr_calls = 0;
+	int mycpu, err, nr_calls = 0;
 	void *hcpu = (void *)(long)cpu;
 	unsigned long mod = tasks_frozen ? CPU_TASKS_FROZEN : 0;
 	struct take_cpu_down_param tcd_param = {
 		.mod = mod,
 		.hcpu = hcpu,
 	};
+	cpumask_var_t cpumask;
+	cpumask_var_t cpumask_org;
 
 	if (num_online_cpus() == 1)
 		return -EBUSY;
@@ -363,7 +649,34 @@ static int __ref _cpu_down(unsigned int cpu, int tasks_frozen)
 	if (!cpu_online(cpu))
 		return -EINVAL;
 
+	/* Move the downtaker off the unplug cpu */
+	if (!alloc_cpumask_var(&cpumask, GFP_KERNEL))
+		return -ENOMEM;
+	if (!alloc_cpumask_var(&cpumask_org, GFP_KERNEL))  {
+		free_cpumask_var(cpumask);
+		return -ENOMEM;
+	}
+
+	cpumask_copy(cpumask_org, tsk_cpus_allowed(current));
+	cpumask_andnot(cpumask, cpu_online_mask, cpumask_of(cpu));
+	set_cpus_allowed_ptr(current, cpumask);
+	free_cpumask_var(cpumask);
+	migrate_disable();
+	mycpu = smp_processor_id();
+	if (mycpu == cpu) {
+		printk(KERN_ERR "Yuck! Still on unplug CPU\n!");
+		migrate_enable();
+		err = -EBUSY;
+		goto restore_cpus;
+	}
+	migrate_enable();
+
 	cpu_hotplug_begin();
+	err = cpu_unplug_begin(cpu);
+	if (err) {
+		printk("cpu_unplug_begin(%d) failed\n", cpu);
+		goto out_cancel;
+	}
 
 	err = __cpu_notify(CPU_DOWN_PREPARE | mod, hcpu, -1, &nr_calls);
 	if (err) {
@@ -373,8 +686,32 @@ static int __ref _cpu_down(unsigned int cpu, int tasks_frozen)
 			__func__, cpu);
 		goto out_release;
 	}
+
+	/*
+	 * By now we've cleared cpu_active_mask, wait for all preempt-disabled
+	 * and RCU users of this state to go away such that all new such users
+	 * will observe it.
+	 *
+	 * For CONFIG_PREEMPT we have preemptible RCU and its sync_rcu() might
+	 * not imply sync_sched(), so explicitly call both.
+	 *
+	 * Do sync before park smpboot threads to take care the rcu boost case.
+	 */
+#ifdef CONFIG_PREEMPT
+	synchronize_sched();
+#endif
+	synchronize_rcu();
+
+	__cpu_unplug_wait(cpu);
 	smpboot_park_threads(cpu);
 
+	/* Notifiers are done. Don't let any more tasks pin this CPU. */
+	cpu_unplug_sync(cpu);
+
+	/*
+	 * So now all preempt/rcu users must observe !cpu_active().
+	 */
+
 	err = __stop_machine(take_cpu_down, &tcd_param, cpumask_of(cpu));
 	if (err) {
 		/* CPU didn't die: tell everyone.  Can't complain. */
@@ -404,10 +741,15 @@ static int __ref _cpu_down(unsigned int cpu, int tasks_frozen)
 	check_for_tasks(cpu);
 
 out_release:
+	cpu_unplug_done(cpu);
+out_cancel:
 	cpu_hotplug_done();
 	trace_sched_cpu_hotplug(cpu, err, 0);
 	if (!err)
 		cpu_notify_nofail(CPU_POST_DEAD | mod, hcpu);
+restore_cpus:
+	set_cpus_allowed_ptr(current, cpumask_org);
+	free_cpumask_var(cpumask_org);
 	return err;
 }
 
diff --git a/kernel/msm-3.18/kernel/debug/kdb/kdb_io.c b/kernel/msm-3.18/kernel/debug/kdb/kdb_io.c
index 13a1e6b5c..2b023f2a9 100644
--- a/kernel/msm-3.18/kernel/debug/kdb/kdb_io.c
+++ b/kernel/msm-3.18/kernel/debug/kdb/kdb_io.c
@@ -562,7 +562,6 @@ int vkdb_printf(const char *fmt, va_list ap)
 	int linecount;
 	int colcount;
 	int logging, saved_loglevel = 0;
-	int saved_trap_printk;
 	int got_printf_lock = 0;
 	int retlen = 0;
 	int fnd, len;
@@ -573,8 +572,6 @@ int vkdb_printf(const char *fmt, va_list ap)
 	unsigned long uninitialized_var(flags);
 
 	preempt_disable();
-	saved_trap_printk = kdb_trap_printk;
-	kdb_trap_printk = 0;
 
 	/* Serialize kdb_printf if multiple cpus try to write at once.
 	 * But if any cpu goes recursive in kdb, just print the output,
@@ -841,7 +838,6 @@ kdb_print_out:
 	} else {
 		__release(kdb_printf_lock);
 	}
-	kdb_trap_printk = saved_trap_printk;
 	preempt_enable();
 	return retlen;
 }
@@ -851,9 +847,11 @@ int kdb_printf(const char *fmt, ...)
 	va_list ap;
 	int r;
 
+	kdb_trap_printk++;
 	va_start(ap, fmt);
 	r = vkdb_printf(fmt, ap);
 	va_end(ap);
+	kdb_trap_printk--;
 
 	return r;
 }
diff --git a/kernel/msm-3.18/kernel/events/core.c b/kernel/msm-3.18/kernel/events/core.c
index ce5b6e043..38e8242fd 100644
--- a/kernel/msm-3.18/kernel/events/core.c
+++ b/kernel/msm-3.18/kernel/events/core.c
@@ -6563,6 +6563,7 @@ static void perf_swevent_init_hrtimer(struct perf_event *event)
 
 	hrtimer_init(&hwc->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	hwc->hrtimer.function = perf_swevent_hrtimer;
+	hwc->hrtimer.irqsafe = 1;
 
 	/*
 	 * Since hrtimers have a fixed rate, we can do a static freq->period
diff --git a/kernel/msm-3.18/kernel/exit.c b/kernel/msm-3.18/kernel/exit.c
index 2e29c554c..48d43bd48 100644
--- a/kernel/msm-3.18/kernel/exit.c
+++ b/kernel/msm-3.18/kernel/exit.c
@@ -148,7 +148,7 @@ static void __exit_signal(struct task_struct *tsk)
 	 * Do this under ->siglock, we can race with another thread
 	 * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.
 	 */
-	flush_sigqueue(&tsk->pending);
+	flush_task_sigqueue(tsk);
 	tsk->sighand = NULL;
 	spin_unlock(&sighand->siglock);
 
diff --git a/kernel/msm-3.18/kernel/fork.c b/kernel/msm-3.18/kernel/fork.c
index 0843f7b5a..b9c0a469a 100644
--- a/kernel/msm-3.18/kernel/fork.c
+++ b/kernel/msm-3.18/kernel/fork.c
@@ -99,7 +99,7 @@ int max_threads;		/* tunable limit on nr_threads */
 
 DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 
-__cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
+DEFINE_RWLOCK(tasklist_lock);  /* outer */
 
 #ifdef CONFIG_PROVE_RCU
 int lockdep_tasklist_lock_is_held(void)
@@ -252,6 +252,9 @@ int task_free_unregister(struct notifier_block *n)
 }
 EXPORT_SYMBOL(task_free_unregister);
 
+#ifdef CONFIG_PREEMPT_RT_BASE
+static
+#endif
 void __put_task_struct(struct task_struct *tsk)
 {
 	WARN_ON(!tsk->exit_state);
@@ -268,7 +271,18 @@ void __put_task_struct(struct task_struct *tsk)
 	if (!profile_handoff_task(tsk))
 		free_task(tsk);
 }
+#ifndef CONFIG_PREEMPT_RT_BASE
 EXPORT_SYMBOL_GPL(__put_task_struct);
+#else
+void __put_task_struct_cb(struct rcu_head *rhp)
+{
+	struct task_struct *tsk = container_of(rhp, struct task_struct, put_rcu);
+
+	__put_task_struct(tsk);
+
+}
+EXPORT_SYMBOL_GPL(__put_task_struct_cb);
+#endif
 
 void __init __weak arch_task_cache_init(void) { }
 
@@ -370,6 +384,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 #endif
 	tsk->splice_pipe = NULL;
 	tsk->task_frag.page = NULL;
+	tsk->wake_q.next = NULL;
 
 	account_kernel_stack(ti, 1);
 
@@ -664,6 +679,19 @@ void __mmdrop(struct mm_struct *mm)
 }
 EXPORT_SYMBOL_GPL(__mmdrop);
 
+#ifdef CONFIG_PREEMPT_RT_BASE
+/*
+ * RCU callback for delayed mm drop. Not strictly rcu, but we don't
+ * want another facility to make this work.
+ */
+void __mmdrop_delayed(struct rcu_head *rhp)
+{
+	struct mm_struct *mm = container_of(rhp, struct mm_struct, delayed_drop);
+
+	__mmdrop(mm);
+}
+#endif
+
 /*
  * Decrement the use count and release all resources for an mm.
  */
@@ -1181,6 +1209,9 @@ static void rt_mutex_init_task(struct task_struct *p)
  */
 static void posix_cpu_timers_init(struct task_struct *tsk)
 {
+#ifdef CONFIG_PREEMPT_RT_BASE
+	tsk->posix_timer_list = NULL;
+#endif
 	tsk->cputime_expires.prof_exp = 0;
 	tsk->cputime_expires.virt_exp = 0;
 	tsk->cputime_expires.sched_exp = 0;
@@ -1308,6 +1339,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	spin_lock_init(&p->alloc_lock);
 
 	init_sigpending(&p->pending);
+	p->sigqueue_cache = NULL;
 
 	p->utime = p->stime = p->gtime = 0;
 	p->utimescaled = p->stimescaled = 0;
@@ -1316,7 +1348,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->prev_cputime.utime = p->prev_cputime.stime = 0;
 #endif
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-	seqlock_init(&p->vtime_seqlock);
+	raw_spin_lock_init(&p->vtime_lock);
+	seqcount_init(&p->vtime_seq);
 	p->vtime_snap = 0;
 	p->vtime_snap_whence = VTIME_SLEEPING;
 #endif
@@ -1367,6 +1400,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->hardirq_context = 0;
 	p->softirq_context = 0;
 #endif
+#ifdef CONFIG_PREEMPT_RT_FULL
+	p->pagefault_disabled = 0;
+#endif
 #ifdef CONFIG_LOCKDEP
 	p->lockdep_depth = 0; /* no locks held yet */
 	p->curr_chain_key = 0;
diff --git a/kernel/msm-3.18/kernel/futex.c b/kernel/msm-3.18/kernel/futex.c
index bd0069674..4c6f7c44c 100644
--- a/kernel/msm-3.18/kernel/futex.c
+++ b/kernel/msm-3.18/kernel/futex.c
@@ -738,7 +738,9 @@ void exit_pi_state_list(struct task_struct *curr)
 		 * task still owns the PI-state:
 		 */
 		if (head->next != next) {
+			raw_spin_unlock_irq(&curr->pi_lock);
 			spin_unlock(&hb->lock);
+			raw_spin_lock_irq(&curr->pi_lock);
 			continue;
 		}
 
@@ -1090,9 +1092,11 @@ static void __unqueue_futex(struct futex_q *q)
 
 /*
  * The hash bucket lock must be held when this is called.
- * Afterwards, the futex_q must not be accessed.
+ * Afterwards, the futex_q must not be accessed. Callers
+ * must ensure to later call wake_up_q() for the actual
+ * wakeups to occur.
  */
-static void wake_futex(struct futex_q *q)
+static void mark_wake_futex(struct wake_q_head *wake_q, struct futex_q *q)
 {
 	struct task_struct *p = q->task;
 
@@ -1100,14 +1104,10 @@ static void wake_futex(struct futex_q *q)
 		return;
 
 	/*
-	 * We set q->lock_ptr = NULL _before_ we wake up the task. If
-	 * a non-futex wake up happens on another CPU then the task
-	 * might exit and p would dereference a non-existing task
-	 * struct. Prevent this by holding a reference on p across the
-	 * wake up.
+	 * Queue the task for later wakeup for after we've released
+	 * the hb->lock. wake_q_add() grabs reference to p.
 	 */
-	get_task_struct(p);
-
+	wake_q_add(wake_q, p);
 	__unqueue_futex(q);
 	/*
 	 * The waiting task can free the futex_q as soon as
@@ -1117,9 +1117,6 @@ static void wake_futex(struct futex_q *q)
 	 */
 	smp_wmb();
 	q->lock_ptr = NULL;
-
-	wake_up_state(p, TASK_NORMAL);
-	put_task_struct(p);
 }
 
 static int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_q *this)
@@ -1227,6 +1224,7 @@ futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)
 	struct futex_q *this, *next;
 	union futex_key key = FUTEX_KEY_INIT;
 	int ret;
+	WAKE_Q(wake_q);
 
 	if (!bitset)
 		return -EINVAL;
@@ -1254,13 +1252,14 @@ futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)
 			if (!(this->bitset & bitset))
 				continue;
 
-			wake_futex(this);
+			mark_wake_futex(&wake_q, this);
 			if (++ret >= nr_wake)
 				break;
 		}
 	}
 
 	spin_unlock(&hb->lock);
+	wake_up_q(&wake_q);
 out_put_key:
 	put_futex_key(&key);
 out:
@@ -1279,6 +1278,7 @@ futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,
 	struct futex_hash_bucket *hb1, *hb2;
 	struct futex_q *this, *next;
 	int ret, op_ret;
+	WAKE_Q(wake_q);
 
 retry:
 	ret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);
@@ -1330,7 +1330,7 @@ retry_private:
 				ret = -EINVAL;
 				goto out_unlock;
 			}
-			wake_futex(this);
+			mark_wake_futex(&wake_q, this);
 			if (++ret >= nr_wake)
 				break;
 		}
@@ -1344,7 +1344,7 @@ retry_private:
 					ret = -EINVAL;
 					goto out_unlock;
 				}
-				wake_futex(this);
+				mark_wake_futex(&wake_q, this);
 				if (++op_ret >= nr_wake2)
 					break;
 			}
@@ -1354,6 +1354,7 @@ retry_private:
 
 out_unlock:
 	double_unlock_hb(hb1, hb2);
+	wake_up_q(&wake_q);
 out_put_keys:
 	put_futex_key(&key2);
 out_put_key1:
@@ -1513,6 +1514,7 @@ static int futex_requeue(u32 __user *uaddr1, unsigned int flags,
 	struct futex_pi_state *pi_state = NULL;
 	struct futex_hash_bucket *hb1, *hb2;
 	struct futex_q *this, *next;
+	WAKE_Q(wake_q);
 
 	if (requeue_pi) {
 		/*
@@ -1689,7 +1691,7 @@ retry_private:
 		 * woken by futex_unlock_pi().
 		 */
 		if (++task_count <= nr_wake && !requeue_pi) {
-			wake_futex(this);
+			mark_wake_futex(&wake_q, this);
 			continue;
 		}
 
@@ -1715,6 +1717,16 @@ retry_private:
 				requeue_pi_wake_futex(this, &key2, hb2);
 				drop_count++;
 				continue;
+			} else if (ret == -EAGAIN) {
+				/*
+				 * Waiter was woken by timeout or
+				 * signal and has set pi_blocked_on to
+				 * PI_WAKEUP_INPROGRESS before we
+				 * tried to enqueue it on the rtmutex.
+				 */
+				this->pi_state = NULL;
+				free_pi_state(pi_state);
+				continue;
 			} else if (ret) {
 				/* -EDEADLK */
 				this->pi_state = NULL;
@@ -1729,6 +1741,7 @@ retry_private:
 out_unlock:
 	free_pi_state(pi_state);
 	double_unlock_hb(hb1, hb2);
+	wake_up_q(&wake_q);
 	hb_waiters_dec(hb2);
 
 	/*
@@ -2567,7 +2580,7 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 {
 	struct hrtimer_sleeper timeout, *to = NULL;
 	struct rt_mutex_waiter rt_waiter;
-	struct futex_hash_bucket *hb;
+	struct futex_hash_bucket *hb, *hb2;
 	union futex_key key2 = FUTEX_KEY_INIT;
 	struct futex_q q = futex_q_init;
 	int res, ret;
@@ -2592,10 +2605,7 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 	 * The waiter is allocated on our stack, manipulated by the requeue
 	 * code while we sleep on uaddr.
 	 */
-	debug_rt_mutex_init_waiter(&rt_waiter);
-	RB_CLEAR_NODE(&rt_waiter.pi_tree_entry);
-	RB_CLEAR_NODE(&rt_waiter.tree_entry);
-	rt_waiter.task = NULL;
+	rt_mutex_init_waiter(&rt_waiter, false);
 
 	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);
 	if (unlikely(ret != 0))
@@ -2626,20 +2636,55 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 	/* Queue the futex_q, drop the hb lock, wait for wakeup. */
 	futex_wait_queue_me(hb, &q, to);
 
-	spin_lock(&hb->lock);
-	ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
-	spin_unlock(&hb->lock);
-	if (ret)
-		goto out_put_keys;
+	/*
+	 * On RT we must avoid races with requeue and trying to block
+	 * on two mutexes (hb->lock and uaddr2's rtmutex) by
+	 * serializing access to pi_blocked_on with pi_lock.
+	 */
+	raw_spin_lock_irq(&current->pi_lock);
+	if (current->pi_blocked_on) {
+		/*
+		 * We have been requeued or are in the process of
+		 * being requeued.
+		 */
+		raw_spin_unlock_irq(&current->pi_lock);
+	} else {
+		/*
+		 * Setting pi_blocked_on to PI_WAKEUP_INPROGRESS
+		 * prevents a concurrent requeue from moving us to the
+		 * uaddr2 rtmutex. After that we can safely acquire
+		 * (and possibly block on) hb->lock.
+		 */
+		current->pi_blocked_on = PI_WAKEUP_INPROGRESS;
+		raw_spin_unlock_irq(&current->pi_lock);
+
+		spin_lock(&hb->lock);
+
+		/*
+		 * Clean up pi_blocked_on. We might leak it otherwise
+		 * when we succeeded with the hb->lock in the fast
+		 * path.
+		 */
+		raw_spin_lock_irq(&current->pi_lock);
+		current->pi_blocked_on = NULL;
+		raw_spin_unlock_irq(&current->pi_lock);
+
+		ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
+		spin_unlock(&hb->lock);
+		if (ret)
+			goto out_put_keys;
+	}
 
 	/*
-	 * In order for us to be here, we know our q.key == key2, and since
-	 * we took the hb->lock above, we also know that futex_requeue() has
-	 * completed and we no longer have to concern ourselves with a wakeup
-	 * race with the atomic proxy lock acquisition by the requeue code. The
-	 * futex_requeue dropped our key1 reference and incremented our key2
-	 * reference count.
+	 * In order to be here, we have either been requeued, are in
+	 * the process of being requeued, or requeue successfully
+	 * acquired uaddr2 on our behalf.  If pi_blocked_on was
+	 * non-null above, we may be racing with a requeue.  Do not
+	 * rely on q->lock_ptr to be hb2->lock until after blocking on
+	 * hb->lock or hb2->lock. The futex_requeue dropped our key1
+	 * reference and incremented our key2 reference count.
 	 */
+	hb2 = hash_futex(&key2);
 
 	/* Check if the requeue code acquired the second futex for us. */
 	if (!q.rt_waiter) {
@@ -2648,7 +2693,8 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 		 * did a lock-steal - fix up the PI-state in that case.
 		 */
 		if (q.pi_state && (q.pi_state->owner != current)) {
-			spin_lock(q.lock_ptr);
+			spin_lock(&hb2->lock);
+			BUG_ON(&hb2->lock != q.lock_ptr);
 			ret = fixup_pi_state_owner(uaddr2, &q, current);
 			if (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current)
 				rt_mutex_unlock(&q.pi_state->pi_mutex);
@@ -2657,7 +2703,7 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 			 * the requeue_pi() code acquired for us.
 			 */
 			free_pi_state(q.pi_state);
-			spin_unlock(q.lock_ptr);
+			spin_unlock(&hb2->lock);
 		}
 	} else {
 		struct rt_mutex *pi_mutex;
@@ -2672,7 +2718,8 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
 		ret = rt_mutex_finish_proxy_lock(pi_mutex, to, &rt_waiter);
 		debug_rt_mutex_free_waiter(&rt_waiter);
 
-		spin_lock(q.lock_ptr);
+		spin_lock(&hb2->lock);
+		BUG_ON(&hb2->lock != q.lock_ptr);
 		/*
 		 * Fixup the pi_state owner and possibly acquire the lock if we
 		 * haven't already.
diff --git a/kernel/msm-3.18/kernel/irq/handle.c b/kernel/msm-3.18/kernel/irq/handle.c
index 85a7a76cb..0beb0c733 100644
--- a/kernel/msm-3.18/kernel/irq/handle.c
+++ b/kernel/msm-3.18/kernel/irq/handle.c
@@ -134,6 +134,8 @@ void __irq_wake_thread(struct irq_desc *desc, struct irqaction *action)
 irqreturn_t
 handle_irq_event_percpu(struct irq_desc *desc, struct irqaction *action)
 {
+	struct pt_regs *regs = get_irq_regs();
+	u64 ip = regs ? instruction_pointer(regs) : 0;
 	irqreturn_t retval = IRQ_NONE;
 	unsigned int flags = 0, irq = desc->irq_data.irq;
 
@@ -174,7 +176,11 @@ handle_irq_event_percpu(struct irq_desc *desc, struct irqaction *action)
 		action = action->next;
 	} while (action);
 
-	add_interrupt_randomness(irq, flags);
+#ifndef CONFIG_PREEMPT_RT_FULL
+	add_interrupt_randomness(irq, flags, ip);
+#else
+	desc->random_ip = ip;
+#endif
 
 	if (!noirqdebug)
 		note_interrupt(irq, desc, retval);
diff --git a/kernel/msm-3.18/kernel/irq/manage.c b/kernel/msm-3.18/kernel/irq/manage.c
index 73feccf66..9168a2317 100644
--- a/kernel/msm-3.18/kernel/irq/manage.c
+++ b/kernel/msm-3.18/kernel/irq/manage.c
@@ -22,6 +22,7 @@
 #include "internals.h"
 
 #ifdef CONFIG_IRQ_FORCED_THREADING
+# ifndef CONFIG_PREEMPT_RT_BASE
 __read_mostly bool force_irqthreads;
 
 static int __init setup_forced_irqthreads(char *arg)
@@ -30,6 +31,7 @@ static int __init setup_forced_irqthreads(char *arg)
 	return 0;
 }
 early_param("threadirqs", setup_forced_irqthreads);
+# endif
 #endif
 
 static void __synchronize_hardirq(struct irq_desc *desc)
@@ -173,6 +175,62 @@ static inline void
 irq_get_pending(struct cpumask *mask, struct irq_desc *desc) { }
 #endif
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+static void _irq_affinity_notify(struct irq_affinity_notify *notify);
+static struct task_struct *set_affinity_helper;
+static LIST_HEAD(affinity_list);
+static DEFINE_RAW_SPINLOCK(affinity_list_lock);
+
+static int set_affinity_thread(void *unused)
+{
+	while (1) {
+		struct irq_affinity_notify *notify;
+		int empty;
+
+		set_current_state(TASK_INTERRUPTIBLE);
+
+		raw_spin_lock_irq(&affinity_list_lock);
+		empty = list_empty(&affinity_list);
+		raw_spin_unlock_irq(&affinity_list_lock);
+
+		if (empty)
+			schedule();
+		if (kthread_should_stop())
+			break;
+		set_current_state(TASK_RUNNING);
+try_next:
+		notify = NULL;
+
+		raw_spin_lock_irq(&affinity_list_lock);
+		if (!list_empty(&affinity_list)) {
+			notify = list_first_entry(&affinity_list,
+					struct irq_affinity_notify, list);
+			list_del_init(&notify->list);
+		}
+		raw_spin_unlock_irq(&affinity_list_lock);
+
+		if (!notify)
+			continue;
+		_irq_affinity_notify(notify);
+		goto try_next;
+	}
+	return 0;
+}
+
+static void init_helper_thread(void)
+{
+	if (set_affinity_helper)
+		return;
+	set_affinity_helper = kthread_run(set_affinity_thread, NULL,
+			"affinity-cb");
+	WARN_ON(IS_ERR(set_affinity_helper));
+}
+#else
+
+static inline void init_helper_thread(void) { }
+
+#endif
+
 int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
 			bool force)
 {
@@ -212,7 +270,17 @@ int irq_set_affinity_locked(struct irq_data *data, const struct cpumask *mask,
 
 	if (desc->affinity_notify) {
 		kref_get(&desc->affinity_notify->kref);
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+		raw_spin_lock(&affinity_list_lock);
+		if (list_empty(&desc->affinity_notify->list))
+			list_add_tail(&affinity_list,
+					&desc->affinity_notify->list);
+		raw_spin_unlock(&affinity_list_lock);
+		wake_up_process(set_affinity_helper);
+#else
 		schedule_work(&desc->affinity_notify->work);
+#endif
 	}
 	irqd_set(data, IRQD_AFFINITY_SET);
 
@@ -250,10 +318,8 @@ int irq_set_affinity_hint(unsigned int irq, const struct cpumask *m)
 }
 EXPORT_SYMBOL_GPL(irq_set_affinity_hint);
 
-static void irq_affinity_notify(struct work_struct *work)
+static void _irq_affinity_notify(struct irq_affinity_notify *notify)
 {
-	struct irq_affinity_notify *notify =
-		container_of(work, struct irq_affinity_notify, work);
 	struct irq_desc *desc = irq_to_desc(notify->irq);
 	cpumask_var_t cpumask;
 	unsigned long flags;
@@ -275,6 +341,13 @@ out:
 	kref_put(&notify->kref, notify->release);
 }
 
+static void irq_affinity_notify(struct work_struct *work)
+{
+	struct irq_affinity_notify *notify =
+		container_of(work, struct irq_affinity_notify, work);
+	_irq_affinity_notify(notify);
+}
+
 /**
  *	irq_set_affinity_notifier - control notification of IRQ affinity changes
  *	@irq:		Interrupt for which to enable/disable notification
@@ -304,6 +377,8 @@ irq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify)
 		notify->irq = irq;
 		kref_init(&notify->kref);
 		INIT_WORK(&notify->work, irq_affinity_notify);
+		INIT_LIST_HEAD(&notify->list);
+		init_helper_thread();
 	}
 
 	raw_spin_lock_irqsave(&desc->lock, flags);
@@ -694,6 +769,12 @@ static irqreturn_t irq_nested_primary_handler(int irq, void *dev_id)
 	return IRQ_NONE;
 }
 
+static irqreturn_t irq_forced_secondary_handler(int irq, void *dev_id)
+{
+	WARN(1, "Secondary action handler called for irq %d\n", irq);
+	return IRQ_NONE;
+}
+
 static int irq_wait_for_interrupt(struct irqaction *action)
 {
 	set_current_state(TASK_INTERRUPTIBLE);
@@ -720,7 +801,8 @@ static int irq_wait_for_interrupt(struct irqaction *action)
 static void irq_finalize_oneshot(struct irq_desc *desc,
 				 struct irqaction *action)
 {
-	if (!(desc->istate & IRQS_ONESHOT))
+	if (!(desc->istate & IRQS_ONESHOT) ||
+	    action->handler == irq_forced_secondary_handler)
 		return;
 again:
 	chip_bus_lock(desc);
@@ -822,7 +904,15 @@ irq_forced_thread_fn(struct irq_desc *desc, struct irqaction *action)
 	local_bh_disable();
 	ret = action->thread_fn(action->irq, action->dev_id);
 	irq_finalize_oneshot(desc, action);
-	local_bh_enable();
+	/*
+	 * Interrupts which have real time requirements can be set up
+	 * to avoid softirq processing in the thread handler. This is
+	 * safe as these interrupts do not raise soft interrupts.
+	 */
+	if (irq_settings_no_softirq_call(desc))
+		_local_bh_enable();
+	else
+		local_bh_enable();
 	return ret;
 }
 
@@ -874,6 +964,18 @@ static void irq_thread_dtor(struct callback_head *unused)
 	irq_finalize_oneshot(desc, action);
 }
 
+static void irq_wake_secondary(struct irq_desc *desc, struct irqaction *action)
+{
+	struct irqaction *secondary = action->secondary;
+
+	if (WARN_ON_ONCE(!secondary))
+		return;
+
+	raw_spin_lock_irq(&desc->lock);
+	__irq_wake_thread(desc, secondary);
+	raw_spin_unlock_irq(&desc->lock);
+}
+
 /*
  * Interrupt handler thread
  */
@@ -904,7 +1006,15 @@ static int irq_thread(void *data)
 		action_ret = handler_fn(desc, action);
 		if (action_ret == IRQ_HANDLED)
 			atomic_inc(&desc->threads_handled);
-
+		if (action_ret == IRQ_WAKE_THREAD)
+			irq_wake_secondary(desc, action);
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+		migrate_disable();
+		add_interrupt_randomness(action->irq, 0,
+				 desc->random_ip ^ (unsigned long) action);
+		migrate_enable();
+#endif
 		wake_threads_waitq(desc);
 	}
 
@@ -948,20 +1058,36 @@ void irq_wake_thread(unsigned int irq, void *dev_id)
 }
 EXPORT_SYMBOL_GPL(irq_wake_thread);
 
-static void irq_setup_forced_threading(struct irqaction *new)
+static int irq_setup_forced_threading(struct irqaction *new)
 {
 	if (!force_irqthreads)
-		return;
+		return 0;
 	if (new->flags & (IRQF_NO_THREAD | IRQF_PERCPU | IRQF_ONESHOT))
-		return;
+		return 0;
 
 	new->flags |= IRQF_ONESHOT;
 
-	if (!new->thread_fn) {
-		set_bit(IRQTF_FORCED_THREAD, &new->thread_flags);
-		new->thread_fn = new->handler;
-		new->handler = irq_default_primary_handler;
+	/*
+	 * Handle the case where we have a real primary handler and a
+	 * thread handler. We force thread them as well by creating a
+	 * secondary action.
+	 */
+	if (new->handler != irq_default_primary_handler && new->thread_fn) {
+		/* Allocate the secondary action */
+		new->secondary = kzalloc(sizeof(struct irqaction), GFP_KERNEL);
+		if (!new->secondary)
+			return -ENOMEM;
+		new->secondary->handler = irq_forced_secondary_handler;
+		new->secondary->thread_fn = new->thread_fn;
+		new->secondary->dev_id = new->dev_id;
+		new->secondary->irq = new->irq;
+		new->secondary->name = new->name;
 	}
+	/* Deal with the primary handler */
+	set_bit(IRQTF_FORCED_THREAD, &new->thread_flags);
+	new->thread_fn = new->handler;
+	new->handler = irq_default_primary_handler;
+	return 0;
 }
 
 static int irq_request_resources(struct irq_desc *desc)
@@ -981,6 +1107,48 @@ static void irq_release_resources(struct irq_desc *desc)
 		c->irq_release_resources(d);
 }
 
+static int
+setup_irq_thread(struct irqaction *new, unsigned int irq, bool secondary)
+{
+	struct task_struct *t;
+	struct sched_param param = {
+		.sched_priority = MAX_USER_RT_PRIO/2,
+	};
+
+	if (!secondary) {
+		t = kthread_create(irq_thread, new, "irq/%d-%s", irq,
+				   new->name);
+	} else {
+		t = kthread_create(irq_thread, new, "irq/%d-s-%s", irq,
+				   new->name);
+		param.sched_priority += 1;
+	}
+
+	if (IS_ERR(t))
+		return PTR_ERR(t);
+
+	sched_setscheduler_nocheck(t, SCHED_FIFO, &param);
+
+	/*
+	 * We keep the reference to the task struct even if
+	 * the thread dies to avoid that the interrupt code
+	 * references an already freed task_struct.
+	 */
+	get_task_struct(t);
+	new->thread = t;
+	/*
+	 * Tell the thread to set its affinity. This is
+	 * important for shared interrupt handlers as we do
+	 * not invoke setup_affinity() for the secondary
+	 * handlers as everything is already set up. Even for
+	 * interrupts marked with IRQF_NO_BALANCE this is
+	 * correct as we want the thread to move to the cpu(s)
+	 * on which the requesting code placed the interrupt.
+	 */
+	set_bit(IRQTF_AFFINITY, &new->thread_flags);
+	return 0;
+}
+
 /*
  * Internal function to register an irqaction - typically used to
  * allocate special interrupts that are part of the architecture.
@@ -1001,6 +1169,8 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 	if (!try_module_get(desc->owner))
 		return -ENODEV;
 
+	new->irq = irq;
+
 	/*
 	 * Check whether the interrupt nests into another interrupt
 	 * thread.
@@ -1018,8 +1188,11 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 		 */
 		new->handler = irq_nested_primary_handler;
 	} else {
-		if (irq_settings_can_thread(desc))
-			irq_setup_forced_threading(new);
+		if (irq_settings_can_thread(desc)) {
+			ret = irq_setup_forced_threading(new);
+			if (ret)
+				goto out_mput;
+		}
 	}
 
 	/*
@@ -1028,37 +1201,14 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 	 * thread.
 	 */
 	if (new->thread_fn && !nested) {
-		struct task_struct *t;
-		static const struct sched_param param = {
-			.sched_priority = MAX_USER_RT_PRIO/2,
-		};
-
-		t = kthread_create(irq_thread, new, "irq/%d-%s", irq,
-				   new->name);
-		if (IS_ERR(t)) {
-			ret = PTR_ERR(t);
+		ret = setup_irq_thread(new, irq, false);
+		if (ret)
 			goto out_mput;
+		if (new->secondary) {
+			ret = setup_irq_thread(new->secondary, irq, true);
+			if (ret)
+				goto out_thread;
 		}
-
-		sched_setscheduler_nocheck(t, SCHED_FIFO, &param);
-
-		/*
-		 * We keep the reference to the task struct even if
-		 * the thread dies to avoid that the interrupt code
-		 * references an already freed task_struct.
-		 */
-		get_task_struct(t);
-		new->thread = t;
-		/*
-		 * Tell the thread to set its affinity. This is
-		 * important for shared interrupt handlers as we do
-		 * not invoke setup_affinity() for the secondary
-		 * handlers as everything is already set up. Even for
-		 * interrupts marked with IRQF_NO_BALANCE this is
-		 * correct as we want the thread to move to the cpu(s)
-		 * on which the requesting code placed the interrupt.
-		 */
-		set_bit(IRQTF_AFFINITY, &new->thread_flags);
 	}
 
 	if (!alloc_cpumask_var(&mask, GFP_KERNEL)) {
@@ -1220,6 +1370,9 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 			irqd_set(&desc->irq_data, IRQD_NO_BALANCING);
 		}
 
+		if (new->flags & IRQF_NO_SOFTIRQ_CALL)
+			irq_settings_set_no_softirq_call(desc);
+
 		/* Set default affinity mask once everything is setup */
 		setup_affinity(irq, desc, mask);
 
@@ -1233,7 +1386,6 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 				   irq, nmsk, omsk);
 	}
 
-	new->irq = irq;
 	*old_ptr = new;
 
 	irq_pm_install_action(desc, new);
@@ -1259,6 +1411,8 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 	 */
 	if (new->thread)
 		wake_up_process(new->thread);
+	if (new->secondary)
+		wake_up_process(new->secondary->thread);
 
 	register_irq_proc(irq, desc);
 	new->dir = NULL;
@@ -1289,6 +1443,13 @@ out_thread:
 		kthread_stop(t);
 		put_task_struct(t);
 	}
+	if (new->secondary && new->secondary->thread) {
+		struct task_struct *t = new->secondary->thread;
+
+		new->secondary->thread = NULL;
+		kthread_stop(t);
+		put_task_struct(t);
+	}
 out_mput:
 	module_put(desc->owner);
 	return ret;
@@ -1396,9 +1557,14 @@ static struct irqaction *__free_irq(unsigned int irq, void *dev_id)
 	if (action->thread) {
 		kthread_stop(action->thread);
 		put_task_struct(action->thread);
+		if (action->secondary && action->secondary->thread) {
+			kthread_stop(action->secondary->thread);
+			put_task_struct(action->secondary->thread);
+		}
 	}
 
 	module_put(desc->owner);
+	kfree(action->secondary);
 	return action;
 }
 
@@ -1537,8 +1703,10 @@ int request_threaded_irq(unsigned int irq, irq_handler_t handler,
 	retval = __setup_irq(irq, desc, action);
 	chip_bus_sync_unlock(desc);
 
-	if (retval)
+	if (retval) {
+		kfree(action->secondary);
 		kfree(action);
+	}
 
 #ifdef CONFIG_DEBUG_SHIRQ_FIXME
 	if (!retval && (irqflags & IRQF_SHARED)) {
diff --git a/kernel/msm-3.18/kernel/irq/settings.h b/kernel/msm-3.18/kernel/irq/settings.h
index 3320b84cc..34b803b89 100644
--- a/kernel/msm-3.18/kernel/irq/settings.h
+++ b/kernel/msm-3.18/kernel/irq/settings.h
@@ -15,6 +15,7 @@ enum {
 	_IRQ_NESTED_THREAD	= IRQ_NESTED_THREAD,
 	_IRQ_PER_CPU_DEVID	= IRQ_PER_CPU_DEVID,
 	_IRQ_IS_POLLED		= IRQ_IS_POLLED,
+	_IRQ_NO_SOFTIRQ_CALL	= IRQ_NO_SOFTIRQ_CALL,
 	_IRQF_MODIFY_MASK	= IRQF_MODIFY_MASK,
 };
 
@@ -28,6 +29,7 @@ enum {
 #define IRQ_NESTED_THREAD	GOT_YOU_MORON
 #define IRQ_PER_CPU_DEVID	GOT_YOU_MORON
 #define IRQ_IS_POLLED		GOT_YOU_MORON
+#define IRQ_NO_SOFTIRQ_CALL	GOT_YOU_MORON
 #undef IRQF_MODIFY_MASK
 #define IRQF_MODIFY_MASK	GOT_YOU_MORON
 
@@ -38,6 +40,16 @@ irq_settings_clr_and_set(struct irq_desc *desc, u32 clr, u32 set)
 	desc->status_use_accessors |= (set & _IRQF_MODIFY_MASK);
 }
 
+static inline bool irq_settings_no_softirq_call(struct irq_desc *desc)
+{
+	return desc->status_use_accessors & _IRQ_NO_SOFTIRQ_CALL;
+}
+
+static inline void irq_settings_set_no_softirq_call(struct irq_desc *desc)
+{
+	desc->status_use_accessors |= _IRQ_NO_SOFTIRQ_CALL;
+}
+
 static inline bool irq_settings_is_per_cpu(struct irq_desc *desc)
 {
 	return desc->status_use_accessors & _IRQ_PER_CPU;
diff --git a/kernel/msm-3.18/kernel/irq/spurious.c b/kernel/msm-3.18/kernel/irq/spurious.c
index e2514b0e4..903a69c45 100644
--- a/kernel/msm-3.18/kernel/irq/spurious.c
+++ b/kernel/msm-3.18/kernel/irq/spurious.c
@@ -444,6 +444,10 @@ MODULE_PARM_DESC(noirqdebug, "Disable irq lockup detection when true");
 
 static int __init irqfixup_setup(char *str)
 {
+#ifdef CONFIG_PREEMPT_RT_BASE
+	pr_warn("irqfixup boot option not supported w/ CONFIG_PREEMPT_RT_BASE\n");
+	return 1;
+#endif
 	irqfixup = 1;
 	printk(KERN_WARNING "Misrouted IRQ fixup support enabled.\n");
 	printk(KERN_WARNING "This may impact system performance.\n");
@@ -456,6 +460,10 @@ module_param(irqfixup, int, 0644);
 
 static int __init irqpoll_setup(char *str)
 {
+#ifdef CONFIG_PREEMPT_RT_BASE
+	pr_warn("irqpoll boot option not supported w/ CONFIG_PREEMPT_RT_BASE\n");
+	return 1;
+#endif
 	irqfixup = 2;
 	printk(KERN_WARNING "Misrouted IRQ fixup and polling support "
 				"enabled\n");
diff --git a/kernel/msm-3.18/kernel/irq_work.c b/kernel/msm-3.18/kernel/irq_work.c
index 3ab904848..3d5a476b5 100644
--- a/kernel/msm-3.18/kernel/irq_work.c
+++ b/kernel/msm-3.18/kernel/irq_work.c
@@ -17,6 +17,7 @@
 #include <linux/cpu.h>
 #include <linux/notifier.h>
 #include <linux/smp.h>
+#include <linux/interrupt.h>
 #include <asm/processor.h>
 
 
@@ -65,6 +66,8 @@ void __weak arch_irq_work_raise(void)
  */
 bool irq_work_queue_on(struct irq_work *work, int cpu)
 {
+	struct llist_head *list;
+
 	/* All work should have been flushed before going offline */
 	WARN_ON_ONCE(cpu_is_offline(cpu));
 
@@ -75,7 +78,12 @@ bool irq_work_queue_on(struct irq_work *work, int cpu)
 	if (!irq_work_claim(work))
 		return false;
 
-	if (llist_add(&work->llnode, &per_cpu(raised_list, cpu)))
+	if (IS_ENABLED(CONFIG_PREEMPT_RT_FULL) && !(work->flags & IRQ_WORK_HARD_IRQ))
+		list = &per_cpu(lazy_list, cpu);
+	else
+		list = &per_cpu(raised_list, cpu);
+
+	if (llist_add(&work->llnode, list))
 		arch_send_call_function_single_ipi(cpu);
 
 	return true;
@@ -86,6 +94,9 @@ EXPORT_SYMBOL_GPL(irq_work_queue_on);
 /* Enqueue the irq work @work on the current CPU */
 bool irq_work_queue(struct irq_work *work)
 {
+	struct llist_head *list;
+	bool lazy_work, realtime = IS_ENABLED(CONFIG_PREEMPT_RT_FULL);
+
 	/* Only queue if not already pending */
 	if (!irq_work_claim(work))
 		return false;
@@ -93,13 +104,15 @@ bool irq_work_queue(struct irq_work *work)
 	/* Queue the entry and raise the IPI if needed. */
 	preempt_disable();
 
-	/* If the work is "lazy", handle it from next tick if any */
-	if (work->flags & IRQ_WORK_LAZY) {
-		if (llist_add(&work->llnode, this_cpu_ptr(&lazy_list)) &&
-		    tick_nohz_tick_stopped())
-			arch_irq_work_raise();
-	} else {
-		if (llist_add(&work->llnode, this_cpu_ptr(&raised_list)))
+	lazy_work = work->flags & IRQ_WORK_LAZY;
+
+	if (lazy_work || (realtime && !(work->flags & IRQ_WORK_HARD_IRQ)))
+		list = this_cpu_ptr(&lazy_list);
+	else
+		list = this_cpu_ptr(&raised_list);
+
+	if (llist_add(&work->llnode, list)) {
+		if (!lazy_work || tick_nohz_tick_stopped())
 			arch_irq_work_raise();
 	}
 
@@ -116,9 +129,8 @@ bool irq_work_needs_cpu(void)
 	raised = this_cpu_ptr(&raised_list);
 	lazy = this_cpu_ptr(&lazy_list);
 
-	if (llist_empty(raised) || arch_irq_work_has_interrupt())
-		if (llist_empty(lazy))
-			return false;
+	if (llist_empty(raised) && llist_empty(lazy))
+		return false;
 
 	/* All work should have been flushed before going offline */
 	WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
@@ -132,7 +144,7 @@ static void irq_work_run_list(struct llist_head *list)
 	struct irq_work *work;
 	struct llist_node *llnode;
 
-	BUG_ON(!irqs_disabled());
+	BUG_ON(!IS_ENABLED(CONFIG_PREEMPT_RT_FULL) && !irqs_disabled());
 
 	if (llist_empty(list))
 		return;
@@ -169,18 +181,36 @@ static void irq_work_run_list(struct llist_head *list)
 void irq_work_run(void)
 {
 	irq_work_run_list(this_cpu_ptr(&raised_list));
-	irq_work_run_list(this_cpu_ptr(&lazy_list));
+	if (IS_ENABLED(CONFIG_PREEMPT_RT_FULL)) {
+		/*
+		 * NOTE: we raise softirq via IPI for safety,
+		 * and execute in irq_work_tick() to move the
+		 * overhead from hard to soft irq context.
+		 */
+		if (!llist_empty(this_cpu_ptr(&lazy_list)))
+			raise_softirq(TIMER_SOFTIRQ);
+	} else
+		irq_work_run_list(this_cpu_ptr(&lazy_list));
 }
 EXPORT_SYMBOL_GPL(irq_work_run);
 
 void irq_work_tick(void)
 {
-	struct llist_head *raised = &__get_cpu_var(raised_list);
+	struct llist_head *raised = this_cpu_ptr(&raised_list);
 
 	if (!llist_empty(raised) && !arch_irq_work_has_interrupt())
 		irq_work_run_list(raised);
-	irq_work_run_list(&__get_cpu_var(lazy_list));
+
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT_FULL))
+		irq_work_run_list(this_cpu_ptr(&lazy_list));
+}
+
+#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_PREEMPT_RT_FULL)
+void irq_work_tick_soft(void)
+{
+	irq_work_run_list(this_cpu_ptr(&lazy_list));
 }
+#endif
 
 /*
  * Synchronize against the irq_work @entry, ensures the entry is not
diff --git a/kernel/msm-3.18/kernel/ksysfs.c b/kernel/msm-3.18/kernel/ksysfs.c
index 6683ccef9..d6fc8eeaa 100644
--- a/kernel/msm-3.18/kernel/ksysfs.c
+++ b/kernel/msm-3.18/kernel/ksysfs.c
@@ -136,6 +136,15 @@ KERNEL_ATTR_RO(vmcoreinfo);
 
 #endif /* CONFIG_KEXEC */
 
+#if defined(CONFIG_PREEMPT_RT_FULL)
+static ssize_t  realtime_show(struct kobject *kobj,
+			      struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", 1);
+}
+KERNEL_ATTR_RO(realtime);
+#endif
+
 /* whether file capabilities are enabled */
 static ssize_t fscaps_show(struct kobject *kobj,
 				  struct kobj_attribute *attr, char *buf)
@@ -203,6 +212,9 @@ static struct attribute * kernel_attrs[] = {
 	&vmcoreinfo_attr.attr,
 #endif
 	&rcu_expedited_attr.attr,
+#ifdef CONFIG_PREEMPT_RT_FULL
+	&realtime_attr.attr,
+#endif
 	NULL
 };
 
diff --git a/kernel/msm-3.18/kernel/locking/Makefile b/kernel/msm-3.18/kernel/locking/Makefile
index dc968422e..712e82b47 100644
--- a/kernel/msm-3.18/kernel/locking/Makefile
+++ b/kernel/msm-3.18/kernel/locking/Makefile
@@ -2,7 +2,7 @@
 # and is generally not a function of system call inputs.
 KCOV_INSTRUMENT		:= n
 
-obj-y += mutex.o semaphore.o rwsem.o mcs_spinlock.o
+obj-y += semaphore.o mcs_spinlock.o
 
 ifdef CONFIG_FUNCTION_TRACER
 CFLAGS_REMOVE_lockdep.o = -pg
@@ -11,7 +11,11 @@ CFLAGS_REMOVE_mutex-debug.o = -pg
 CFLAGS_REMOVE_rtmutex-debug.o = -pg
 endif
 
+ifneq ($(CONFIG_PREEMPT_RT_FULL),y)
+obj-y += mutex.o
 obj-$(CONFIG_DEBUG_MUTEXES) += mutex-debug.o
+obj-y += rwsem.o
+endif
 obj-$(CONFIG_LOCKDEP) += lockdep.o
 ifeq ($(CONFIG_PROC_FS),y)
 obj-$(CONFIG_LOCKDEP) += lockdep_proc.o
@@ -24,8 +28,11 @@ obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
 obj-$(CONFIG_RT_MUTEX_TESTER) += rtmutex-tester.o
 obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock.o
 obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock_debug.o
+ifneq ($(CONFIG_PREEMPT_RT_FULL),y)
 obj-$(CONFIG_RWSEM_GENERIC_SPINLOCK) += rwsem-spinlock.o
 obj-$(CONFIG_RWSEM_XCHGADD_ALGORITHM) += rwsem-xadd.o
+endif
 obj-$(CONFIG_PERCPU_RWSEM) += percpu-rwsem.o
+obj-$(CONFIG_PREEMPT_RT_FULL) += rt.o
 obj-$(CONFIG_QUEUE_RWLOCK) += qrwlock.o
 obj-$(CONFIG_LOCK_TORTURE_TEST) += locktorture.o
diff --git a/kernel/msm-3.18/kernel/locking/lglock.c b/kernel/msm-3.18/kernel/locking/lglock.c
index 86ae2aebf..9397974b1 100644
--- a/kernel/msm-3.18/kernel/locking/lglock.c
+++ b/kernel/msm-3.18/kernel/locking/lglock.c
@@ -4,6 +4,15 @@
 #include <linux/cpu.h>
 #include <linux/string.h>
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+# define lg_lock_ptr		arch_spinlock_t
+# define lg_do_lock(l)		arch_spin_lock(l)
+# define lg_do_unlock(l)	arch_spin_unlock(l)
+#else
+# define lg_lock_ptr		struct rt_mutex
+# define lg_do_lock(l)		__rt_spin_lock(l)
+# define lg_do_unlock(l)	__rt_spin_unlock(l)
+#endif
 /*
  * Note there is no uninit, so lglocks cannot be defined in
  * modules (but it's fine to use them from there)
@@ -12,51 +21,60 @@
 
 void lg_lock_init(struct lglock *lg, char *name)
 {
+#ifdef CONFIG_PREEMPT_RT_FULL
+	int i;
+
+	for_each_possible_cpu(i) {
+		struct rt_mutex *lock = per_cpu_ptr(lg->lock, i);
+
+		rt_mutex_init(lock);
+	}
+#endif
 	LOCKDEP_INIT_MAP(&lg->lock_dep_map, name, &lg->lock_key, 0);
 }
 EXPORT_SYMBOL(lg_lock_init);
 
 void lg_local_lock(struct lglock *lg)
 {
-	arch_spinlock_t *lock;
+	lg_lock_ptr *lock;
 
-	preempt_disable();
+	migrate_disable();
 	lock_acquire_shared(&lg->lock_dep_map, 0, 0, NULL, _RET_IP_);
 	lock = this_cpu_ptr(lg->lock);
-	arch_spin_lock(lock);
+	lg_do_lock(lock);
 }
 EXPORT_SYMBOL(lg_local_lock);
 
 void lg_local_unlock(struct lglock *lg)
 {
-	arch_spinlock_t *lock;
+	lg_lock_ptr *lock;
 
 	lock_release(&lg->lock_dep_map, 1, _RET_IP_);
 	lock = this_cpu_ptr(lg->lock);
-	arch_spin_unlock(lock);
-	preempt_enable();
+	lg_do_unlock(lock);
+	migrate_enable();
 }
 EXPORT_SYMBOL(lg_local_unlock);
 
 void lg_local_lock_cpu(struct lglock *lg, int cpu)
 {
-	arch_spinlock_t *lock;
+	lg_lock_ptr *lock;
 
-	preempt_disable();
+	preempt_disable_nort();
 	lock_acquire_shared(&lg->lock_dep_map, 0, 0, NULL, _RET_IP_);
 	lock = per_cpu_ptr(lg->lock, cpu);
-	arch_spin_lock(lock);
+	lg_do_lock(lock);
 }
 EXPORT_SYMBOL(lg_local_lock_cpu);
 
 void lg_local_unlock_cpu(struct lglock *lg, int cpu)
 {
-	arch_spinlock_t *lock;
+	lg_lock_ptr *lock;
 
 	lock_release(&lg->lock_dep_map, 1, _RET_IP_);
 	lock = per_cpu_ptr(lg->lock, cpu);
-	arch_spin_unlock(lock);
-	preempt_enable();
+	lg_do_unlock(lock);
+	preempt_enable_nort();
 }
 EXPORT_SYMBOL(lg_local_unlock_cpu);
 
@@ -64,12 +82,12 @@ void lg_global_lock(struct lglock *lg)
 {
 	int i;
 
-	preempt_disable();
+	preempt_disable_nort();
 	lock_acquire_exclusive(&lg->lock_dep_map, 0, 0, NULL, _RET_IP_);
 	for_each_possible_cpu(i) {
-		arch_spinlock_t *lock;
+		lg_lock_ptr *lock;
 		lock = per_cpu_ptr(lg->lock, i);
-		arch_spin_lock(lock);
+		lg_do_lock(lock);
 	}
 }
 EXPORT_SYMBOL(lg_global_lock);
@@ -80,10 +98,35 @@ void lg_global_unlock(struct lglock *lg)
 
 	lock_release(&lg->lock_dep_map, 1, _RET_IP_);
 	for_each_possible_cpu(i) {
-		arch_spinlock_t *lock;
+		lg_lock_ptr *lock;
 		lock = per_cpu_ptr(lg->lock, i);
-		arch_spin_unlock(lock);
+		lg_do_unlock(lock);
 	}
-	preempt_enable();
+	preempt_enable_nort();
 }
 EXPORT_SYMBOL(lg_global_unlock);
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+/*
+ * HACK: If you use this, you get to keep the pieces.
+ * Used in queue_stop_cpus_work() when stop machinery
+ * is called from inactive CPU, so we can't schedule.
+ */
+# define lg_do_trylock_relax(l)			\
+	do {					\
+		while (!__rt_spin_trylock(l))	\
+			cpu_relax();		\
+	} while (0)
+
+void lg_global_trylock_relax(struct lglock *lg)
+{
+	int i;
+
+	lock_acquire_exclusive(&lg->lock_dep_map, 0, 0, NULL, _RET_IP_);
+	for_each_possible_cpu(i) {
+		lg_lock_ptr *lock;
+		lock = per_cpu_ptr(lg->lock, i);
+		lg_do_trylock_relax(lock);
+	}
+}
+#endif
diff --git a/kernel/msm-3.18/kernel/locking/lockdep.c b/kernel/msm-3.18/kernel/locking/lockdep.c
index 88d0d4420..d4a4d8ea5 100644
--- a/kernel/msm-3.18/kernel/locking/lockdep.c
+++ b/kernel/msm-3.18/kernel/locking/lockdep.c
@@ -654,6 +654,7 @@ look_up_lock_class(struct lockdep_map *lock, unsigned int subclass)
 	struct lockdep_subclass_key *key;
 	struct list_head *hash_head;
 	struct lock_class *class;
+	bool is_static = false;
 
 #ifdef CONFIG_DEBUG_LOCKDEP
 	/*
@@ -681,10 +682,23 @@ look_up_lock_class(struct lockdep_map *lock, unsigned int subclass)
 
 	/*
 	 * Static locks do not have their class-keys yet - for them the key
-	 * is the lock object itself:
+	 * is the lock object itself. If the lock is in the per cpu area,
+	 * the canonical address of the lock (per cpu offset removed) is
+	 * used.
 	 */
-	if (unlikely(!lock->key))
-		lock->key = (void *)lock;
+	if (unlikely(!lock->key)) {
+		unsigned long can_addr, addr = (unsigned long)lock;
+
+		if (__is_kernel_percpu_address(addr, &can_addr))
+			lock->key = (void *)can_addr;
+		else if (__is_module_percpu_address(addr, &can_addr))
+			lock->key = (void *)can_addr;
+		else if (static_obj(lock))
+			lock->key = (void *)lock;
+		else
+			return ERR_PTR(-EINVAL);
+		is_static = true;
+	}
 
 	/*
 	 * NOTE: the class-key must be unique. For dynamic locks, a static
@@ -714,7 +728,7 @@ look_up_lock_class(struct lockdep_map *lock, unsigned int subclass)
 		}
 	}
 
-	return NULL;
+	return is_static || static_obj(lock->key) ? NULL : ERR_PTR(-EINVAL);
 }
 
 /*
@@ -731,13 +745,13 @@ register_lock_class(struct lockdep_map *lock, unsigned int subclass, int force)
 	unsigned long flags;
 
 	class = look_up_lock_class(lock, subclass);
-	if (likely(class))
+	if (likely(!IS_ERR_OR_NULL(class)))
 		goto out_set_class_cache;
 
 	/*
 	 * Debug-check: all keys must be persistent!
- 	 */
-	if (!static_obj(lock->key)) {
+	 */
+	if (IS_ERR(class)) {
 		debug_locks_off();
 		printk("INFO: trying to register non-static key.\n");
 		printk("the code is fine but needs lockdep annotation.\n");
@@ -3276,7 +3290,7 @@ static int match_held_lock(struct held_lock *hlock, struct lockdep_map *lock)
 		 * Clearly if the lock hasn't been acquired _ever_, we're not
 		 * holding it either, so report failure.
 		 */
-		if (!class)
+		if (IS_ERR_OR_NULL(class))
 			return 0;
 
 		/*
@@ -3542,6 +3556,7 @@ static void check_flags(unsigned long flags)
 		}
 	}
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 	/*
 	 * We dont accurately track softirq state in e.g.
 	 * hardirq contexts (such as on 4KSTACKS), so only
@@ -3556,6 +3571,7 @@ static void check_flags(unsigned long flags)
 			DEBUG_LOCKS_WARN_ON(!current->softirqs_enabled);
 		}
 	}
+#endif
 
 	if (!debug_locks)
 		print_irqtrace_events(current);
@@ -3936,7 +3952,7 @@ void lockdep_reset_lock(struct lockdep_map *lock)
 		 * If the class exists we look it up and zap it:
 		 */
 		class = look_up_lock_class(lock, j);
-		if (class)
+		if (!IS_ERR_OR_NULL(class))
 			zap_class(class);
 	}
 	/*
diff --git a/kernel/msm-3.18/kernel/locking/locktorture.c b/kernel/msm-3.18/kernel/locking/locktorture.c
index ec8cce259..aa60d919e 100644
--- a/kernel/msm-3.18/kernel/locking/locktorture.c
+++ b/kernel/msm-3.18/kernel/locking/locktorture.c
@@ -24,7 +24,6 @@
 #include <linux/module.h>
 #include <linux/kthread.h>
 #include <linux/spinlock.h>
-#include <linux/rwlock.h>
 #include <linux/mutex.h>
 #include <linux/rwsem.h>
 #include <linux/smp.h>
diff --git a/kernel/msm-3.18/kernel/locking/percpu-rwsem.c b/kernel/msm-3.18/kernel/locking/percpu-rwsem.c
index 652a8ee8e..2db0f42d5 100644
--- a/kernel/msm-3.18/kernel/locking/percpu-rwsem.c
+++ b/kernel/msm-3.18/kernel/locking/percpu-rwsem.c
@@ -84,8 +84,12 @@ void percpu_down_read(struct percpu_rw_semaphore *brw)
 
 	down_read(&brw->rw_sem);
 	atomic_inc(&brw->slow_read_ctr);
+#ifdef CONFIG_PREEMPT_RT_FULL
+	up_read(&brw->rw_sem);
+#else
 	/* avoid up_read()->rwsem_release() */
 	__up_read(&brw->rw_sem);
+#endif
 }
 
 void percpu_up_read(struct percpu_rw_semaphore *brw)
diff --git a/kernel/msm-3.18/kernel/locking/rt.c b/kernel/msm-3.18/kernel/locking/rt.c
new file mode 100644
index 000000000..73c55089f
--- /dev/null
+++ b/kernel/msm-3.18/kernel/locking/rt.c
@@ -0,0 +1,456 @@
+/*
+ * kernel/rt.c
+ *
+ * Real-Time Preemption Support
+ *
+ * started by Ingo Molnar:
+ *
+ *  Copyright (C) 2004-2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
+ *
+ * historic credit for proving that Linux spinlocks can be implemented via
+ * RT-aware mutexes goes to many people: The Pmutex project (Dirk Grambow
+ * and others) who prototyped it on 2.4 and did lots of comparative
+ * research and analysis; TimeSys, for proving that you can implement a
+ * fully preemptible kernel via the use of IRQ threading and mutexes;
+ * Bill Huey for persuasively arguing on lkml that the mutex model is the
+ * right one; and to MontaVista, who ported pmutexes to 2.6.
+ *
+ * This code is a from-scratch implementation and is not based on pmutexes,
+ * but the idea of converting spinlocks to mutexes is used here too.
+ *
+ * lock debugging, locking tree, deadlock detection:
+ *
+ *  Copyright (C) 2004, LynuxWorks, Inc., Igor Manyilov, Bill Huey
+ *  Released under the General Public License (GPL).
+ *
+ * Includes portions of the generic R/W semaphore implementation from:
+ *
+ *  Copyright (c) 2001   David Howells (dhowells@redhat.com).
+ *  - Derived partially from idea by Andrea Arcangeli <andrea@suse.de>
+ *  - Derived also from comments by Linus
+ *
+ * Pending ownership of locks and ownership stealing:
+ *
+ *  Copyright (C) 2005, Kihon Technologies Inc., Steven Rostedt
+ *
+ *   (also by Steven Rostedt)
+ *    - Converted single pi_lock to individual task locks.
+ *
+ * By Esben Nielsen:
+ *    Doing priority inheritance with help of the scheduler.
+ *
+ *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
+ *  - major rework based on Esben Nielsens initial patch
+ *  - replaced thread_info references by task_struct refs
+ *  - removed task->pending_owner dependency
+ *  - BKL drop/reacquire for semaphore style locks to avoid deadlocks
+ *    in the scheduler return path as discussed with Steven Rostedt
+ *
+ *  Copyright (C) 2006, Kihon Technologies Inc.
+ *    Steven Rostedt <rostedt@goodmis.org>
+ *  - debugged and patched Thomas Gleixner's rework.
+ *  - added back the cmpxchg to the rework.
+ *  - turned atomic require back on for SMP.
+ */
+
+#include <linux/spinlock.h>
+#include <linux/rtmutex.h>
+#include <linux/sched.h>
+#include <linux/delay.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/syscalls.h>
+#include <linux/interrupt.h>
+#include <linux/plist.h>
+#include <linux/fs.h>
+#include <linux/futex.h>
+#include <linux/hrtimer.h>
+
+#include "rtmutex_common.h"
+
+/*
+ * struct mutex functions
+ */
+void __mutex_do_init(struct mutex *mutex, const char *name,
+		     struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)mutex, sizeof(*mutex));
+	lockdep_init_map(&mutex->dep_map, name, key, 0);
+#endif
+	mutex->lock.save_state = 0;
+}
+EXPORT_SYMBOL(__mutex_do_init);
+
+void __lockfunc _mutex_lock(struct mutex *lock)
+{
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	rt_mutex_lock(&lock->lock);
+}
+EXPORT_SYMBOL(_mutex_lock);
+
+int __lockfunc _mutex_lock_interruptible(struct mutex *lock)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	ret = rt_mutex_lock_interruptible(&lock->lock);
+	if (ret)
+		mutex_release(&lock->dep_map, 1, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_interruptible);
+
+int __lockfunc _mutex_lock_killable(struct mutex *lock)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+	ret = rt_mutex_lock_killable(&lock->lock);
+	if (ret)
+		mutex_release(&lock->dep_map, 1, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_killable);
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+void __lockfunc _mutex_lock_nested(struct mutex *lock, int subclass)
+{
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	rt_mutex_lock(&lock->lock);
+}
+EXPORT_SYMBOL(_mutex_lock_nested);
+
+void __lockfunc _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest)
+{
+	mutex_acquire_nest(&lock->dep_map, 0, 0, nest, _RET_IP_);
+	rt_mutex_lock(&lock->lock);
+}
+EXPORT_SYMBOL(_mutex_lock_nest_lock);
+
+int __lockfunc _mutex_lock_interruptible_nested(struct mutex *lock, int subclass)
+{
+	int ret;
+
+	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
+	ret = rt_mutex_lock_interruptible(&lock->lock);
+	if (ret)
+		mutex_release(&lock->dep_map, 1, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_interruptible_nested);
+
+int __lockfunc _mutex_lock_killable_nested(struct mutex *lock, int subclass)
+{
+	int ret;
+
+	mutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
+	ret = rt_mutex_lock_killable(&lock->lock);
+	if (ret)
+		mutex_release(&lock->dep_map, 1, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_lock_killable_nested);
+#endif
+
+int __lockfunc _mutex_trylock(struct mutex *lock)
+{
+	int ret = rt_mutex_trylock(&lock->lock);
+
+	if (ret)
+		mutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+
+	return ret;
+}
+EXPORT_SYMBOL(_mutex_trylock);
+
+void __lockfunc _mutex_unlock(struct mutex *lock)
+{
+	mutex_release(&lock->dep_map, 1, _RET_IP_);
+	rt_mutex_unlock(&lock->lock);
+}
+EXPORT_SYMBOL(_mutex_unlock);
+
+/*
+ * rwlock_t functions
+ */
+int __lockfunc rt_write_trylock(rwlock_t *rwlock)
+{
+	int ret;
+
+	migrate_disable();
+	ret = rt_mutex_trylock(&rwlock->lock);
+	if (ret)
+		rwlock_acquire(&rwlock->dep_map, 0, 1, _RET_IP_);
+	else
+		migrate_enable();
+
+	return ret;
+}
+EXPORT_SYMBOL(rt_write_trylock);
+
+int __lockfunc rt_write_trylock_irqsave(rwlock_t *rwlock, unsigned long *flags)
+{
+	int ret;
+
+	*flags = 0;
+	ret = rt_write_trylock(rwlock);
+	return ret;
+}
+EXPORT_SYMBOL(rt_write_trylock_irqsave);
+
+int __lockfunc rt_read_trylock(rwlock_t *rwlock)
+{
+	struct rt_mutex *lock = &rwlock->lock;
+	int ret = 1;
+
+	/*
+	 * recursive read locks succeed when current owns the lock,
+	 * but not when read_depth == 0 which means that the lock is
+	 * write locked.
+	 */
+	if (rt_mutex_owner(lock) != current) {
+		migrate_disable();
+		ret = rt_mutex_trylock(lock);
+		if (ret)
+			rwlock_acquire(&rwlock->dep_map, 0, 1, _RET_IP_);
+		else
+			migrate_enable();
+
+	} else if (!rwlock->read_depth) {
+		ret = 0;
+	}
+
+	if (ret)
+		rwlock->read_depth++;
+
+	return ret;
+}
+EXPORT_SYMBOL(rt_read_trylock);
+
+void __lockfunc rt_write_lock(rwlock_t *rwlock)
+{
+	rwlock_acquire(&rwlock->dep_map, 0, 0, _RET_IP_);
+	migrate_disable();
+	__rt_spin_lock(&rwlock->lock);
+}
+EXPORT_SYMBOL(rt_write_lock);
+
+void __lockfunc rt_read_lock(rwlock_t *rwlock)
+{
+	struct rt_mutex *lock = &rwlock->lock;
+
+
+	/*
+	 * recursive read locks succeed when current owns the lock
+	 */
+	if (rt_mutex_owner(lock) != current) {
+		migrate_disable();
+		rwlock_acquire(&rwlock->dep_map, 0, 0, _RET_IP_);
+		__rt_spin_lock(lock);
+	}
+	rwlock->read_depth++;
+}
+
+EXPORT_SYMBOL(rt_read_lock);
+
+void __lockfunc rt_write_unlock(rwlock_t *rwlock)
+{
+	/* NOTE: we always pass in '1' for nested, for simplicity */
+	rwlock_release(&rwlock->dep_map, 1, _RET_IP_);
+	__rt_spin_unlock(&rwlock->lock);
+	migrate_enable();
+}
+EXPORT_SYMBOL(rt_write_unlock);
+
+void __lockfunc rt_read_unlock(rwlock_t *rwlock)
+{
+	/* Release the lock only when read_depth is down to 0 */
+	if (--rwlock->read_depth == 0) {
+		rwlock_release(&rwlock->dep_map, 1, _RET_IP_);
+		__rt_spin_unlock(&rwlock->lock);
+		migrate_enable();
+	}
+}
+EXPORT_SYMBOL(rt_read_unlock);
+
+unsigned long __lockfunc rt_write_lock_irqsave(rwlock_t *rwlock)
+{
+	rt_write_lock(rwlock);
+
+	return 0;
+}
+EXPORT_SYMBOL(rt_write_lock_irqsave);
+
+unsigned long __lockfunc rt_read_lock_irqsave(rwlock_t *rwlock)
+{
+	rt_read_lock(rwlock);
+
+	return 0;
+}
+EXPORT_SYMBOL(rt_read_lock_irqsave);
+
+void __rt_rwlock_init(rwlock_t *rwlock, char *name, struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)rwlock, sizeof(*rwlock));
+	lockdep_init_map(&rwlock->dep_map, name, key, 0);
+#endif
+	rwlock->lock.save_state = 1;
+	rwlock->read_depth = 0;
+}
+EXPORT_SYMBOL(__rt_rwlock_init);
+
+/*
+ * rw_semaphores
+ */
+
+void  rt_up_write(struct rw_semaphore *rwsem)
+{
+	rwsem_release(&rwsem->dep_map, 1, _RET_IP_);
+	rt_mutex_unlock(&rwsem->lock);
+}
+EXPORT_SYMBOL(rt_up_write);
+
+void  rt_up_read(struct rw_semaphore *rwsem)
+{
+	rwsem_release(&rwsem->dep_map, 1, _RET_IP_);
+	if (--rwsem->read_depth == 0)
+		rt_mutex_unlock(&rwsem->lock);
+}
+EXPORT_SYMBOL(rt_up_read);
+
+/*
+ * downgrade a write lock into a read lock
+ * - just wake up any readers at the front of the queue
+ */
+void  rt_downgrade_write(struct rw_semaphore *rwsem)
+{
+	BUG_ON(rt_mutex_owner(&rwsem->lock) != current);
+	rwsem->read_depth = 1;
+}
+EXPORT_SYMBOL(rt_downgrade_write);
+
+int  rt_down_write_trylock(struct rw_semaphore *rwsem)
+{
+	int ret = rt_mutex_trylock(&rwsem->lock);
+
+	if (ret)
+		rwsem_acquire(&rwsem->dep_map, 0, 1, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(rt_down_write_trylock);
+
+void  rt_down_write(struct rw_semaphore *rwsem)
+{
+	rwsem_acquire(&rwsem->dep_map, 0, 0, _RET_IP_);
+	rt_mutex_lock(&rwsem->lock);
+}
+EXPORT_SYMBOL(rt_down_write);
+
+void  rt_down_write_nested(struct rw_semaphore *rwsem, int subclass)
+{
+	rwsem_acquire(&rwsem->dep_map, subclass, 0, _RET_IP_);
+	rt_mutex_lock(&rwsem->lock);
+}
+EXPORT_SYMBOL(rt_down_write_nested);
+
+void rt_down_write_nested_lock(struct rw_semaphore *rwsem,
+			       struct lockdep_map *nest)
+{
+	rwsem_acquire_nest(&rwsem->dep_map, 0, 0, nest, _RET_IP_);
+	rt_mutex_lock(&rwsem->lock);
+}
+EXPORT_SYMBOL(rt_down_write_nested_lock);
+
+int  rt_down_read_trylock(struct rw_semaphore *rwsem)
+{
+	struct rt_mutex *lock = &rwsem->lock;
+	int ret = 1;
+
+	/*
+	 * recursive read locks succeed when current owns the rwsem,
+	 * but not when read_depth == 0 which means that the rwsem is
+	 * write locked.
+	 */
+	if (rt_mutex_owner(lock) != current)
+		ret = rt_mutex_trylock(&rwsem->lock);
+	else if (!rwsem->read_depth)
+		ret = 0;
+
+	if (ret) {
+		rwsem->read_depth++;
+		rwsem_acquire(&rwsem->dep_map, 0, 1, _RET_IP_);
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_down_read_trylock);
+
+static void __rt_down_read(struct rw_semaphore *rwsem, int subclass)
+{
+	struct rt_mutex *lock = &rwsem->lock;
+
+	rwsem_acquire_read(&rwsem->dep_map, subclass, 0, _RET_IP_);
+
+	if (rt_mutex_owner(lock) != current)
+		rt_mutex_lock(&rwsem->lock);
+	rwsem->read_depth++;
+}
+
+void  rt_down_read(struct rw_semaphore *rwsem)
+{
+	__rt_down_read(rwsem, 0);
+}
+EXPORT_SYMBOL(rt_down_read);
+
+void  rt_down_read_nested(struct rw_semaphore *rwsem, int subclass)
+{
+	__rt_down_read(rwsem, subclass);
+}
+EXPORT_SYMBOL(rt_down_read_nested);
+
+void  __rt_rwsem_init(struct rw_semaphore *rwsem, const char *name,
+			      struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)rwsem, sizeof(*rwsem));
+	lockdep_init_map(&rwsem->dep_map, name, key, 0);
+#endif
+	rwsem->read_depth = 0;
+	rwsem->lock.save_state = 0;
+}
+EXPORT_SYMBOL(__rt_rwsem_init);
+
+/**
+ * atomic_dec_and_mutex_lock - return holding mutex if we dec to 0
+ * @cnt: the atomic which we are to dec
+ * @lock: the mutex to return holding if we dec to 0
+ *
+ * return true and hold lock if we dec to 0, return false otherwise
+ */
+int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)
+{
+	/* dec if we can't possibly hit 0 */
+	if (atomic_add_unless(cnt, -1, 1))
+		return 0;
+	/* we might hit 0, so take the lock */
+	mutex_lock(lock);
+	if (!atomic_dec_and_test(cnt)) {
+		/* when we actually did the dec, we didn't hit 0 */
+		mutex_unlock(lock);
+		return 0;
+	}
+	/* we hit 0, and we hold the lock */
+	return 1;
+}
+EXPORT_SYMBOL(atomic_dec_and_mutex_lock);
diff --git a/kernel/msm-3.18/kernel/locking/rtmutex.c b/kernel/msm-3.18/kernel/locking/rtmutex.c
index 1929ee229..8d950b452 100644
--- a/kernel/msm-3.18/kernel/locking/rtmutex.c
+++ b/kernel/msm-3.18/kernel/locking/rtmutex.c
@@ -7,6 +7,11 @@
  *  Copyright (C) 2005-2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>
  *  Copyright (C) 2005 Kihon Technologies Inc., Steven Rostedt
  *  Copyright (C) 2006 Esben Nielsen
+ *  Adaptive Spinlocks:
+ *  Copyright (C) 2008 Novell, Inc., Gregory Haskins, Sven Dietrich,
+ *				     and Peter Morreale,
+ * Adaptive Spinlocks simplification:
+ *  Copyright (C) 2008 Red Hat, Inc., Steven Rostedt <srostedt@redhat.com>
  *
  *  See Documentation/locking/rt-mutex-design.txt for details.
  */
@@ -16,6 +21,7 @@
 #include <linux/sched/rt.h>
 #include <linux/sched/deadline.h>
 #include <linux/timer.h>
+#include <linux/ww_mutex.h>
 
 #include "rtmutex_common.h"
 
@@ -69,6 +75,12 @@ static void fixup_rt_mutex_waiters(struct rt_mutex *lock)
 		clear_rt_mutex_waiters(lock);
 }
 
+static int rt_mutex_real_waiter(struct rt_mutex_waiter *waiter)
+{
+	return waiter && waiter != PI_WAKEUP_INPROGRESS &&
+		waiter != PI_REQUEUE_INPROGRESS;
+}
+
 /*
  * We can speed up the acquire/release, if the architecture
  * supports cmpxchg and if there's no debugging state to be set up
@@ -335,6 +347,14 @@ static bool rt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,
 	return debug_rt_mutex_detect_deadlock(waiter, chwalk);
 }
 
+static void rt_mutex_wake_waiter(struct rt_mutex_waiter *waiter)
+{
+	if (waiter->savestate)
+		wake_up_lock_sleeper(waiter->task);
+	else
+		wake_up_process(waiter->task);
+}
+
 /*
  * Max number of times we'll walk the boosting chain:
  */
@@ -342,7 +362,8 @@ int max_lock_depth = 1024;
 
 static inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)
 {
-	return p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;
+	return rt_mutex_real_waiter(p->pi_blocked_on) ?
+		p->pi_blocked_on->lock : NULL;
 }
 
 /*
@@ -479,7 +500,7 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
 	 * reached or the state of the chain has changed while we
 	 * dropped the locks.
 	 */
-	if (!waiter)
+	if (!rt_mutex_real_waiter(waiter))
 		goto out_unlock_pi;
 
 	/*
@@ -641,13 +662,16 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
 	 * follow here. This is the end of the chain we are walking.
 	 */
 	if (!rt_mutex_owner(lock)) {
+		struct rt_mutex_waiter *lock_top_waiter;
+
 		/*
 		 * If the requeue [7] above changed the top waiter,
 		 * then we need to wake the new top waiter up to try
 		 * to get the lock.
 		 */
-		if (prerequeue_top_waiter != rt_mutex_top_waiter(lock))
-			wake_up_process(rt_mutex_top_waiter(lock)->task);
+		lock_top_waiter = rt_mutex_top_waiter(lock);
+		if (prerequeue_top_waiter != lock_top_waiter)
+			rt_mutex_wake_waiter(lock_top_waiter);
 		raw_spin_unlock(&lock->wait_lock);
 		return 0;
 	}
@@ -740,6 +764,25 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
 	return ret;
 }
 
+
+#define STEAL_NORMAL  0
+#define STEAL_LATERAL 1
+
+/*
+ * Note that RT tasks are excluded from lateral-steals to prevent the
+ * introduction of an unbounded latency
+ */
+static inline int lock_is_stealable(struct task_struct *task,
+				    struct task_struct *pendowner, int mode)
+{
+    if (mode == STEAL_NORMAL || rt_task(task)) {
+	    if (task->prio >= pendowner->prio)
+		    return 0;
+    } else if (task->prio > pendowner->prio)
+	    return 0;
+    return 1;
+}
+
 /*
  * Try to take an rt-mutex
  *
@@ -750,8 +793,9 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
  * @waiter: The waiter that is queued to the lock's wait list if the
  *	    callsite called task_blocked_on_lock(), otherwise NULL
  */
-static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
-				struct rt_mutex_waiter *waiter)
+static int __try_to_take_rt_mutex(struct rt_mutex *lock,
+				  struct task_struct *task,
+				  struct rt_mutex_waiter *waiter, int mode)
 {
 	unsigned long flags;
 
@@ -790,8 +834,10 @@ static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
 		 * If waiter is not the highest priority waiter of
 		 * @lock, give up.
 		 */
-		if (waiter != rt_mutex_top_waiter(lock))
+		if (waiter != rt_mutex_top_waiter(lock)) {
+			/* XXX lock_is_stealable() ? */
 			return 0;
+		}
 
 		/*
 		 * We can acquire the lock. Remove the waiter from the
@@ -809,14 +855,10 @@ static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
 		 * not need to be dequeued.
 		 */
 		if (rt_mutex_has_waiters(lock)) {
-			/*
-			 * If @task->prio is greater than or equal to
-			 * the top waiter priority (kernel view),
-			 * @task lost.
-			 */
-			if (task->prio >= rt_mutex_top_waiter(lock)->prio)
-				return 0;
+			struct task_struct *pown = rt_mutex_top_waiter(lock)->task;
 
+			if (task != pown && !lock_is_stealable(task, pown, mode))
+				return 0;
 			/*
 			 * The current top waiter stays enqueued. We
 			 * don't have to change anything in the lock
@@ -865,6 +907,369 @@ takeit:
 	return 1;
 }
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+/*
+ * preemptible spin_lock functions:
+ */
+static inline void rt_spin_lock_fastlock(struct rt_mutex *lock,
+					 void  (*slowfn)(struct rt_mutex *lock))
+{
+	might_sleep();
+
+	if (likely(rt_mutex_cmpxchg(lock, NULL, current)))
+		rt_mutex_deadlock_account_lock(lock, current);
+	else
+		slowfn(lock);
+}
+
+static inline void rt_spin_lock_fastunlock(struct rt_mutex *lock,
+					   void  (*slowfn)(struct rt_mutex *lock))
+{
+	if (likely(rt_mutex_cmpxchg(lock, current, NULL)))
+		rt_mutex_deadlock_account_unlock(current);
+	else
+		slowfn(lock);
+}
+#ifdef CONFIG_SMP
+/*
+ * Note that owner is a speculative pointer and dereferencing relies
+ * on rcu_read_lock() and the check against the lock owner.
+ */
+static int adaptive_wait(struct rt_mutex *lock,
+			 struct task_struct *owner)
+{
+	int res = 0;
+
+	rcu_read_lock();
+	for (;;) {
+		if (owner != rt_mutex_owner(lock))
+			break;
+		/*
+		 * Ensure that owner->on_cpu is dereferenced _after_
+		 * checking the above to be valid.
+		 */
+		barrier();
+		if (!owner->on_cpu) {
+			res = 1;
+			break;
+		}
+		cpu_relax();
+	}
+	rcu_read_unlock();
+	return res;
+}
+#else
+static int adaptive_wait(struct rt_mutex *lock,
+			 struct task_struct *orig_owner)
+{
+	return 1;
+}
+#endif
+
+# define pi_lock(lock)		raw_spin_lock_irq(lock)
+# define pi_unlock(lock)	raw_spin_unlock_irq(lock)
+
+static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
+				   struct rt_mutex_waiter *waiter,
+				   struct task_struct *task,
+				   enum rtmutex_chainwalk chwalk);
+/*
+ * Slow path lock function spin_lock style: this variant is very
+ * careful not to miss any non-lock wakeups.
+ *
+ * We store the current state under p->pi_lock in p->saved_state and
+ * the try_to_wake_up() code handles this accordingly.
+ */
+static void  noinline __sched rt_spin_lock_slowlock(struct rt_mutex *lock)
+{
+	struct task_struct *lock_owner, *self = current;
+	struct rt_mutex_waiter waiter, *top_waiter;
+	int ret;
+
+	rt_mutex_init_waiter(&waiter, true);
+
+	raw_spin_lock(&lock->wait_lock);
+
+	if (__try_to_take_rt_mutex(lock, self, NULL, STEAL_LATERAL)) {
+		raw_spin_unlock(&lock->wait_lock);
+		return;
+	}
+
+	BUG_ON(rt_mutex_owner(lock) == self);
+
+	/*
+	 * We save whatever state the task is in and we'll restore it
+	 * after acquiring the lock taking real wakeups into account
+	 * as well. We are serialized via pi_lock against wakeups. See
+	 * try_to_wake_up().
+	 */
+	pi_lock(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state(TASK_UNINTERRUPTIBLE);
+	pi_unlock(&self->pi_lock);
+
+	ret = task_blocks_on_rt_mutex(lock, &waiter, self, RT_MUTEX_MIN_CHAINWALK);
+	BUG_ON(ret);
+
+	for (;;) {
+		/* Try to acquire the lock again. */
+		if (__try_to_take_rt_mutex(lock, self, &waiter, STEAL_LATERAL))
+			break;
+
+		top_waiter = rt_mutex_top_waiter(lock);
+		lock_owner = rt_mutex_owner(lock);
+
+		raw_spin_unlock(&lock->wait_lock);
+
+		debug_rt_mutex_print_deadlock(&waiter);
+
+		if (top_waiter != &waiter || adaptive_wait(lock, lock_owner))
+			schedule_rt_mutex(lock);
+
+		raw_spin_lock(&lock->wait_lock);
+
+		pi_lock(&self->pi_lock);
+		__set_current_state(TASK_UNINTERRUPTIBLE);
+		pi_unlock(&self->pi_lock);
+	}
+
+	/*
+	 * Restore the task state to current->saved_state. We set it
+	 * to the original state above and the try_to_wake_up() code
+	 * has possibly updated it when a real (non-rtmutex) wakeup
+	 * happened while we were blocked. Clear saved_state so
+	 * try_to_wakeup() does not get confused.
+	 */
+	pi_lock(&self->pi_lock);
+	__set_current_state(self->saved_state);
+	self->saved_state = TASK_RUNNING;
+	pi_unlock(&self->pi_lock);
+
+	/*
+	 * try_to_take_rt_mutex() sets the waiter bit
+	 * unconditionally. We might have to fix that up:
+	 */
+	fixup_rt_mutex_waiters(lock);
+
+	BUG_ON(rt_mutex_has_waiters(lock) && &waiter == rt_mutex_top_waiter(lock));
+	BUG_ON(!RB_EMPTY_NODE(&waiter.tree_entry));
+
+	raw_spin_unlock(&lock->wait_lock);
+
+	debug_rt_mutex_free_waiter(&waiter);
+}
+
+static void wakeup_next_waiter(struct rt_mutex *lock);
+/*
+ * Slow path to release a rt_mutex spin_lock style
+ */
+static void __sched __rt_spin_lock_slowunlock(struct rt_mutex *lock)
+{
+	debug_rt_mutex_unlock(lock);
+
+	rt_mutex_deadlock_account_unlock(current);
+
+	if (!rt_mutex_has_waiters(lock)) {
+		lock->owner = NULL;
+		raw_spin_unlock(&lock->wait_lock);
+		return;
+	}
+
+	wakeup_next_waiter(lock);
+
+	raw_spin_unlock(&lock->wait_lock);
+
+	/* Undo pi boosting.when necessary */
+	rt_mutex_adjust_prio(current);
+}
+
+static void  noinline __sched rt_spin_lock_slowunlock(struct rt_mutex *lock)
+{
+	raw_spin_lock(&lock->wait_lock);
+	__rt_spin_lock_slowunlock(lock);
+}
+
+static void  noinline __sched rt_spin_lock_slowunlock_hirq(struct rt_mutex *lock)
+{
+	int ret;
+
+	do {
+		ret = raw_spin_trylock(&lock->wait_lock);
+	} while (!ret);
+
+	__rt_spin_lock_slowunlock(lock);
+}
+
+void __lockfunc rt_spin_lock(spinlock_t *lock)
+{
+	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
+	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
+}
+EXPORT_SYMBOL(rt_spin_lock);
+
+void __lockfunc __rt_spin_lock(struct rt_mutex *lock)
+{
+	rt_spin_lock_fastlock(lock, rt_spin_lock_slowlock);
+}
+EXPORT_SYMBOL(__rt_spin_lock);
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+void __lockfunc rt_spin_lock_nested(spinlock_t *lock, int subclass)
+{
+	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
+	spin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
+}
+EXPORT_SYMBOL(rt_spin_lock_nested);
+#endif
+
+void __lockfunc rt_spin_unlock(spinlock_t *lock)
+{
+	/* NOTE: we always pass in '1' for nested, for simplicity */
+	spin_release(&lock->dep_map, 1, _RET_IP_);
+	rt_spin_lock_fastunlock(&lock->lock, rt_spin_lock_slowunlock);
+}
+EXPORT_SYMBOL(rt_spin_unlock);
+
+void __lockfunc rt_spin_unlock_after_trylock_in_irq(spinlock_t *lock)
+{
+	/* NOTE: we always pass in '1' for nested, for simplicity */
+	spin_release(&lock->dep_map, 1, _RET_IP_);
+	rt_spin_lock_fastunlock(&lock->lock, rt_spin_lock_slowunlock_hirq);
+}
+
+void __lockfunc __rt_spin_unlock(struct rt_mutex *lock)
+{
+	rt_spin_lock_fastunlock(lock, rt_spin_lock_slowunlock);
+}
+EXPORT_SYMBOL(__rt_spin_unlock);
+
+/*
+ * Wait for the lock to get unlocked: instead of polling for an unlock
+ * (like raw spinlocks do), we lock and unlock, to force the kernel to
+ * schedule if there's contention:
+ */
+void __lockfunc rt_spin_unlock_wait(spinlock_t *lock)
+{
+	spin_lock(lock);
+	spin_unlock(lock);
+}
+EXPORT_SYMBOL(rt_spin_unlock_wait);
+
+int __lockfunc __rt_spin_trylock(struct rt_mutex *lock)
+{
+	return rt_mutex_trylock(lock);
+}
+
+int __lockfunc rt_spin_trylock(spinlock_t *lock)
+{
+	int ret = rt_mutex_trylock(&lock->lock);
+
+	if (ret)
+		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+	return ret;
+}
+EXPORT_SYMBOL(rt_spin_trylock);
+
+int __lockfunc rt_spin_trylock_bh(spinlock_t *lock)
+{
+	int ret;
+
+	local_bh_disable();
+	ret = rt_mutex_trylock(&lock->lock);
+	if (ret) {
+		migrate_disable();
+		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+	} else
+		local_bh_enable();
+	return ret;
+}
+EXPORT_SYMBOL(rt_spin_trylock_bh);
+
+int __lockfunc rt_spin_trylock_irqsave(spinlock_t *lock, unsigned long *flags)
+{
+	int ret;
+
+	*flags = 0;
+	ret = rt_mutex_trylock(&lock->lock);
+	if (ret) {
+		migrate_disable();
+		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+	}
+	return ret;
+}
+EXPORT_SYMBOL(rt_spin_trylock_irqsave);
+
+int atomic_dec_and_spin_lock(atomic_t *atomic, spinlock_t *lock)
+{
+	/* Subtract 1 from counter unless that drops it to 0 (ie. it was 1) */
+	if (atomic_add_unless(atomic, -1, 1))
+		return 0;
+	migrate_disable();
+	rt_spin_lock(lock);
+	if (atomic_dec_and_test(atomic))
+		return 1;
+	rt_spin_unlock(lock);
+	migrate_enable();
+	return 0;
+}
+EXPORT_SYMBOL(atomic_dec_and_spin_lock);
+
+	void
+__rt_spin_lock_init(spinlock_t *lock, char *name, struct lock_class_key *key)
+{
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+	/*
+	 * Make sure we are not reinitializing a held lock:
+	 */
+	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
+	lockdep_init_map(&lock->dep_map, name, key, 0);
+#endif
+}
+EXPORT_SYMBOL(__rt_spin_lock_init);
+
+#endif /* PREEMPT_RT_FULL */
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+	static inline int __sched
+__mutex_lock_check_stamp(struct rt_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base.lock);
+	struct ww_acquire_ctx *hold_ctx = ACCESS_ONCE(ww->ctx);
+
+	if (!hold_ctx)
+		return 0;
+
+	if (unlikely(ctx == hold_ctx))
+		return -EALREADY;
+
+	if (ctx->stamp - hold_ctx->stamp <= LONG_MAX &&
+	    (ctx->stamp != hold_ctx->stamp || ctx > hold_ctx)) {
+#ifdef CONFIG_DEBUG_MUTEXES
+		DEBUG_LOCKS_WARN_ON(ctx->contending_lock);
+		ctx->contending_lock = ww;
+#endif
+		return -EDEADLK;
+	}
+
+	return 0;
+}
+#else
+	static inline int __sched
+__mutex_lock_check_stamp(struct rt_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+	BUG();
+	return 0;
+}
+
+#endif
+
+static inline int
+try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
+		     struct rt_mutex_waiter *waiter)
+{
+	return __try_to_take_rt_mutex(lock, task, waiter, STEAL_NORMAL);
+}
+
 /*
  * Task blocks on lock.
  *
@@ -896,6 +1301,23 @@ static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
 		return -EDEADLK;
 
 	raw_spin_lock_irqsave(&task->pi_lock, flags);
+
+	/*
+	 * In the case of futex requeue PI, this will be a proxy
+	 * lock. The task will wake unaware that it is enqueueed on
+	 * this lock. Avoid blocking on two locks and corrupting
+	 * pi_blocked_on via the PI_WAKEUP_INPROGRESS
+	 * flag. futex_wait_requeue_pi() sets this when it wakes up
+	 * before requeue (due to a signal or timeout). Do not enqueue
+	 * the task if PI_WAKEUP_INPROGRESS is set.
+	 */
+	if (task != current && task->pi_blocked_on == PI_WAKEUP_INPROGRESS) {
+		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
+		return -EAGAIN;
+	}
+
+	BUG_ON(rt_mutex_real_waiter(task->pi_blocked_on));
+
 	__rt_mutex_adjust_prio(task);
 	waiter->task = task;
 	waiter->lock = lock;
@@ -919,7 +1341,7 @@ static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
 		rt_mutex_enqueue_pi(owner, waiter);
 
 		__rt_mutex_adjust_prio(owner);
-		if (owner->pi_blocked_on)
+		if (rt_mutex_real_waiter(owner->pi_blocked_on))
 			chain_walk = 1;
 	} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {
 		chain_walk = 1;
@@ -996,7 +1418,7 @@ static void wakeup_next_waiter(struct rt_mutex *lock)
 	 * long as we hold lock->wait_lock. The waiter task needs to
 	 * acquire it in order to dequeue the waiter.
 	 */
-	wake_up_process(waiter->task);
+	rt_mutex_wake_waiter(waiter);
 }
 
 /*
@@ -1010,7 +1432,7 @@ static void remove_waiter(struct rt_mutex *lock,
 {
 	bool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));
 	struct task_struct *owner = rt_mutex_owner(lock);
-	struct rt_mutex *next_lock;
+	struct rt_mutex *next_lock = NULL;
 	unsigned long flags;
 
 	raw_spin_lock_irqsave(&current->pi_lock, flags);
@@ -1035,7 +1457,8 @@ static void remove_waiter(struct rt_mutex *lock,
 	__rt_mutex_adjust_prio(owner);
 
 	/* Store the lock on which owner is blocked or NULL */
-	next_lock = task_blocked_on_lock(owner);
+	if (rt_mutex_real_waiter(owner->pi_blocked_on))
+		next_lock = task_blocked_on_lock(owner);
 
 	raw_spin_unlock_irqrestore(&owner->pi_lock, flags);
 
@@ -1071,17 +1494,17 @@ void rt_mutex_adjust_pi(struct task_struct *task)
 	raw_spin_lock_irqsave(&task->pi_lock, flags);
 
 	waiter = task->pi_blocked_on;
-	if (!waiter || (waiter->prio == task->prio &&
+	if (!rt_mutex_real_waiter(waiter) || (waiter->prio == task->prio &&
 			!dl_prio(task->prio))) {
 		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 		return;
 	}
 	next_lock = waiter->lock;
-	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 
 	/* gets dropped in rt_mutex_adjust_prio_chain()! */
 	get_task_struct(task);
 
+	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 	rt_mutex_adjust_prio_chain(task, RT_MUTEX_MIN_CHAINWALK, NULL,
 				   next_lock, NULL, task);
 }
@@ -1099,7 +1522,8 @@ void rt_mutex_adjust_pi(struct task_struct *task)
 static int __sched
 __rt_mutex_slowlock(struct rt_mutex *lock, int state,
 		    struct hrtimer_sleeper *timeout,
-		    struct rt_mutex_waiter *waiter)
+		    struct rt_mutex_waiter *waiter,
+		    struct ww_acquire_ctx *ww_ctx)
 {
 	int ret = 0;
 
@@ -1122,6 +1546,12 @@ __rt_mutex_slowlock(struct rt_mutex *lock, int state,
 				break;
 		}
 
+		if (ww_ctx && ww_ctx->acquired > 0) {
+			ret = __mutex_lock_check_stamp(lock, ww_ctx);
+			if (ret)
+				break;
+		}
+
 		raw_spin_unlock(&lock->wait_lock);
 
 		debug_rt_mutex_print_deadlock(waiter);
@@ -1155,25 +1585,102 @@ static void rt_mutex_handle_deadlock(int res, int detect_deadlock,
 	}
 }
 
+static __always_inline void ww_mutex_lock_acquired(struct ww_mutex *ww,
+						   struct ww_acquire_ctx *ww_ctx)
+{
+#ifdef CONFIG_DEBUG_MUTEXES
+	/*
+	 * If this WARN_ON triggers, you used ww_mutex_lock to acquire,
+	 * but released with a normal mutex_unlock in this call.
+	 *
+	 * This should never happen, always use ww_mutex_unlock.
+	 */
+	DEBUG_LOCKS_WARN_ON(ww->ctx);
+
+	/*
+	 * Not quite done after calling ww_acquire_done() ?
+	 */
+	DEBUG_LOCKS_WARN_ON(ww_ctx->done_acquire);
+
+	if (ww_ctx->contending_lock) {
+		/*
+		 * After -EDEADLK you tried to
+		 * acquire a different ww_mutex? Bad!
+		 */
+		DEBUG_LOCKS_WARN_ON(ww_ctx->contending_lock != ww);
+
+		/*
+		 * You called ww_mutex_lock after receiving -EDEADLK,
+		 * but 'forgot' to unlock everything else first?
+		 */
+		DEBUG_LOCKS_WARN_ON(ww_ctx->acquired > 0);
+		ww_ctx->contending_lock = NULL;
+	}
+
+	/*
+	 * Naughty, using a different class will lead to undefined behavior!
+	 */
+	DEBUG_LOCKS_WARN_ON(ww_ctx->ww_class != ww->ww_class);
+#endif
+	ww_ctx->acquired++;
+}
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+static void ww_mutex_account_lock(struct rt_mutex *lock,
+				  struct ww_acquire_ctx *ww_ctx)
+{
+	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base.lock);
+	struct rt_mutex_waiter *waiter, *n;
+
+	/*
+	 * This branch gets optimized out for the common case,
+	 * and is only important for ww_mutex_lock.
+	 */
+	ww_mutex_lock_acquired(ww, ww_ctx);
+	ww->ctx = ww_ctx;
+
+	/*
+	 * Give any possible sleeping processes the chance to wake up,
+	 * so they can recheck if they have to back off.
+	 */
+	rbtree_postorder_for_each_entry_safe(waiter, n, &lock->waiters,
+					     tree_entry) {
+		/* XXX debug rt mutex waiter wakeup */
+
+		BUG_ON(waiter->lock != lock);
+		rt_mutex_wake_waiter(waiter);
+	}
+}
+
+#else
+
+static void ww_mutex_account_lock(struct rt_mutex *lock,
+				  struct ww_acquire_ctx *ww_ctx)
+{
+	BUG();
+}
+#endif
+
 /*
  * Slow path lock function:
  */
 static int __sched
 rt_mutex_slowlock(struct rt_mutex *lock, int state,
 		  struct hrtimer_sleeper *timeout,
-		  enum rtmutex_chainwalk chwalk)
+		  enum rtmutex_chainwalk chwalk,
+		  struct ww_acquire_ctx *ww_ctx)
 {
 	struct rt_mutex_waiter waiter;
 	int ret = 0;
 
-	debug_rt_mutex_init_waiter(&waiter);
-	RB_CLEAR_NODE(&waiter.pi_tree_entry);
-	RB_CLEAR_NODE(&waiter.tree_entry);
+	rt_mutex_init_waiter(&waiter, false);
 
 	raw_spin_lock(&lock->wait_lock);
 
 	/* Try to acquire the lock again: */
 	if (try_to_take_rt_mutex(lock, current, NULL)) {
+		if (ww_ctx)
+			ww_mutex_account_lock(lock, ww_ctx);
 		raw_spin_unlock(&lock->wait_lock);
 		return 0;
 	}
@@ -1190,14 +1697,23 @@ rt_mutex_slowlock(struct rt_mutex *lock, int state,
 	ret = task_blocks_on_rt_mutex(lock, &waiter, current, chwalk);
 
 	if (likely(!ret))
-		ret = __rt_mutex_slowlock(lock, state, timeout, &waiter);
+		ret = __rt_mutex_slowlock(lock, state, timeout, &waiter, ww_ctx);
+	else if (ww_ctx) {
+		/* ww_mutex received EDEADLK, let it become EALREADY */
+		ret = __mutex_lock_check_stamp(lock, ww_ctx);
+		BUG_ON(!ret);
+	}
 
 	set_current_state(TASK_RUNNING);
 
 	if (unlikely(ret)) {
 		if (rt_mutex_has_waiters(lock))
 			remove_waiter(lock, &waiter);
-		rt_mutex_handle_deadlock(ret, chwalk, &waiter);
+		/* ww_mutex want to report EDEADLK/EALREADY, let them */
+		if (!ww_ctx)
+			rt_mutex_handle_deadlock(ret, chwalk, &waiter);
+	} else if (ww_ctx) {
+		ww_mutex_account_lock(lock, ww_ctx);
 	}
 
 	/*
@@ -1236,7 +1752,8 @@ static inline int rt_mutex_slowtrylock(struct rt_mutex *lock)
 	 * The mutex has currently no owner. Lock the wait lock and
 	 * try to acquire the lock.
 	 */
-	raw_spin_lock(&lock->wait_lock);
+	if (!raw_spin_trylock(&lock->wait_lock))
+		return 0;
 
 	ret = try_to_take_rt_mutex(lock, current, NULL);
 
@@ -1322,31 +1839,36 @@ rt_mutex_slowunlock(struct rt_mutex *lock)
  */
 static inline int
 rt_mutex_fastlock(struct rt_mutex *lock, int state,
+		  struct ww_acquire_ctx *ww_ctx,
 		  int (*slowfn)(struct rt_mutex *lock, int state,
 				struct hrtimer_sleeper *timeout,
-				enum rtmutex_chainwalk chwalk))
+				enum rtmutex_chainwalk chwalk,
+				struct ww_acquire_ctx *ww_ctx))
 {
 	if (likely(rt_mutex_cmpxchg(lock, NULL, current))) {
 		rt_mutex_deadlock_account_lock(lock, current);
 		return 0;
 	} else
-		return slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);
+		return slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK,
+			      ww_ctx);
 }
 
 static inline int
 rt_mutex_timed_fastlock(struct rt_mutex *lock, int state,
 			struct hrtimer_sleeper *timeout,
 			enum rtmutex_chainwalk chwalk,
+			struct ww_acquire_ctx *ww_ctx,
 			int (*slowfn)(struct rt_mutex *lock, int state,
 				      struct hrtimer_sleeper *timeout,
-				      enum rtmutex_chainwalk chwalk))
+				      enum rtmutex_chainwalk chwalk,
+				      struct ww_acquire_ctx *ww_ctx))
 {
 	if (chwalk == RT_MUTEX_MIN_CHAINWALK &&
 	    likely(rt_mutex_cmpxchg(lock, NULL, current))) {
 		rt_mutex_deadlock_account_lock(lock, current);
 		return 0;
 	} else
-		return slowfn(lock, state, timeout, chwalk);
+		return slowfn(lock, state, timeout, chwalk, ww_ctx);
 }
 
 static inline int
@@ -1379,7 +1901,7 @@ void __sched rt_mutex_lock(struct rt_mutex *lock)
 {
 	might_sleep();
 
-	rt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);
+	rt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, NULL, rt_mutex_slowlock);
 }
 EXPORT_SYMBOL_GPL(rt_mutex_lock);
 
@@ -1396,7 +1918,7 @@ int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)
 {
 	might_sleep();
 
-	return rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);
+	return rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, NULL, rt_mutex_slowlock);
 }
 EXPORT_SYMBOL_GPL(rt_mutex_lock_interruptible);
 
@@ -1409,10 +1931,29 @@ int rt_mutex_timed_futex_lock(struct rt_mutex *lock,
 	might_sleep();
 
 	return rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,
-				       RT_MUTEX_FULL_CHAINWALK,
+				       RT_MUTEX_FULL_CHAINWALK, NULL,
 				       rt_mutex_slowlock);
 }
 
+/**
+ * rt_mutex_lock_killable - lock a rt_mutex killable
+ *
+ * @lock:              the rt_mutex to be locked
+ * @detect_deadlock:   deadlock detection on/off
+ *
+ * Returns:
+ *  0          on success
+ * -EINTR      when interrupted by a signal
+ * -EDEADLK    when the lock would deadlock (when deadlock detection is on)
+ */
+int __sched rt_mutex_lock_killable(struct rt_mutex *lock)
+{
+	might_sleep();
+
+	return rt_mutex_fastlock(lock, TASK_KILLABLE, NULL, rt_mutex_slowlock);
+}
+EXPORT_SYMBOL_GPL(rt_mutex_lock_killable);
+
 /**
  * rt_mutex_timed_lock - lock a rt_mutex interruptible
  *			the timeout structure is provided
@@ -1433,6 +1974,7 @@ rt_mutex_timed_lock(struct rt_mutex *lock, struct hrtimer_sleeper *timeout)
 
 	return rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,
 				       RT_MUTEX_MIN_CHAINWALK,
+				       NULL,
 				       rt_mutex_slowlock);
 }
 EXPORT_SYMBOL_GPL(rt_mutex_timed_lock);
@@ -1491,13 +2033,12 @@ EXPORT_SYMBOL_GPL(rt_mutex_destroy);
 void __rt_mutex_init(struct rt_mutex *lock, const char *name)
 {
 	lock->owner = NULL;
-	raw_spin_lock_init(&lock->wait_lock);
 	lock->waiters = RB_ROOT;
 	lock->waiters_leftmost = NULL;
 
 	debug_rt_mutex_init(lock, name);
 }
-EXPORT_SYMBOL_GPL(__rt_mutex_init);
+EXPORT_SYMBOL(__rt_mutex_init);
 
 /**
  * rt_mutex_init_proxy_locked - initialize and lock a rt_mutex on behalf of a
@@ -1512,7 +2053,7 @@ EXPORT_SYMBOL_GPL(__rt_mutex_init);
 void rt_mutex_init_proxy_locked(struct rt_mutex *lock,
 				struct task_struct *proxy_owner)
 {
-	__rt_mutex_init(lock, NULL);
+	rt_mutex_init(lock);
 	debug_rt_mutex_proxy_lock(lock, proxy_owner);
 	rt_mutex_set_owner(lock, proxy_owner);
 	rt_mutex_deadlock_account_lock(lock, proxy_owner);
@@ -1560,6 +2101,35 @@ int rt_mutex_start_proxy_lock(struct rt_mutex *lock,
 		return 1;
 	}
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+	/*
+	 * In PREEMPT_RT there's an added race.
+	 * If the task, that we are about to requeue, times out,
+	 * it can set the PI_WAKEUP_INPROGRESS. This tells the requeue
+	 * to skip this task. But right after the task sets
+	 * its pi_blocked_on to PI_WAKEUP_INPROGRESS it can then
+	 * block on the spin_lock(&hb->lock), which in RT is an rtmutex.
+	 * This will replace the PI_WAKEUP_INPROGRESS with the actual
+	 * lock that it blocks on. We *must not* place this task
+	 * on this proxy lock in that case.
+	 *
+	 * To prevent this race, we first take the task's pi_lock
+	 * and check if it has updated its pi_blocked_on. If it has,
+	 * we assume that it woke up and we return -EAGAIN.
+	 * Otherwise, we set the task's pi_blocked_on to
+	 * PI_REQUEUE_INPROGRESS, so that if the task is waking up
+	 * it will know that we are in the process of requeuing it.
+	 */
+	raw_spin_lock_irq(&task->pi_lock);
+	if (task->pi_blocked_on) {
+		raw_spin_unlock_irq(&task->pi_lock);
+		raw_spin_unlock(&lock->wait_lock);
+		return -EAGAIN;
+	}
+	task->pi_blocked_on = PI_REQUEUE_INPROGRESS;
+	raw_spin_unlock_irq(&task->pi_lock);
+#endif
+
 	/* We enforce deadlock detection for futexes */
 	ret = task_blocks_on_rt_mutex(lock, waiter, task,
 				      RT_MUTEX_FULL_CHAINWALK);
@@ -1574,7 +2144,7 @@ int rt_mutex_start_proxy_lock(struct rt_mutex *lock,
 		ret = 0;
 	}
 
-	if (unlikely(ret))
+	if (ret && rt_mutex_has_waiters(lock))
 		remove_waiter(lock, waiter);
 
 	raw_spin_unlock(&lock->wait_lock);
@@ -1629,7 +2199,7 @@ int rt_mutex_finish_proxy_lock(struct rt_mutex *lock,
 
 	set_current_state(TASK_INTERRUPTIBLE);
 
-	ret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter);
+	ret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter, NULL);
 
 	set_current_state(TASK_RUNNING);
 
@@ -1646,3 +2216,89 @@ int rt_mutex_finish_proxy_lock(struct rt_mutex *lock,
 
 	return ret;
 }
+
+static inline int
+ww_mutex_deadlock_injection(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
+{
+#ifdef CONFIG_DEBUG_WW_MUTEX_SLOWPATH
+	unsigned tmp;
+
+	if (ctx->deadlock_inject_countdown-- == 0) {
+		tmp = ctx->deadlock_inject_interval;
+		if (tmp > UINT_MAX/4)
+			tmp = UINT_MAX;
+		else
+			tmp = tmp*2 + tmp + tmp/2;
+
+		ctx->deadlock_inject_interval = tmp;
+		ctx->deadlock_inject_countdown = tmp;
+		ctx->contending_lock = lock;
+
+		ww_mutex_unlock(lock);
+
+		return -EDEADLK;
+	}
+#endif
+
+	return 0;
+}
+
+#ifdef CONFIG_PREEMPT_RT_FULL
+int __sched
+__ww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx)
+{
+	int ret;
+
+	might_sleep();
+
+	mutex_acquire_nest(&lock->base.dep_map, 0, 0, &ww_ctx->dep_map, _RET_IP_);
+	ret = rt_mutex_slowlock(&lock->base.lock, TASK_INTERRUPTIBLE, NULL, 0, ww_ctx);
+	if (ret)
+		mutex_release(&lock->base.dep_map, 1, _RET_IP_);
+	else if (!ret && ww_ctx->acquired > 1)
+		return ww_mutex_deadlock_injection(lock, ww_ctx);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(__ww_mutex_lock_interruptible);
+
+int __sched
+__ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx)
+{
+	int ret;
+
+	might_sleep();
+
+	mutex_acquire_nest(&lock->base.dep_map, 0, 0, &ww_ctx->dep_map, _RET_IP_);
+	ret = rt_mutex_slowlock(&lock->base.lock, TASK_UNINTERRUPTIBLE, NULL, 0, ww_ctx);
+	if (ret)
+		mutex_release(&lock->base.dep_map, 1, _RET_IP_);
+	else if (!ret && ww_ctx->acquired > 1)
+		return ww_mutex_deadlock_injection(lock, ww_ctx);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(__ww_mutex_lock);
+
+void __sched ww_mutex_unlock(struct ww_mutex *lock)
+{
+	int nest = !!lock->ctx;
+
+	/*
+	 * The unlocking fastpath is the 0->1 transition from 'locked'
+	 * into 'unlocked' state:
+	 */
+	if (nest) {
+#ifdef CONFIG_DEBUG_MUTEXES
+		DEBUG_LOCKS_WARN_ON(!lock->ctx->acquired);
+#endif
+		if (lock->ctx->acquired > 0)
+			lock->ctx->acquired--;
+		lock->ctx = NULL;
+	}
+
+	mutex_release(&lock->base.dep_map, nest, _RET_IP_);
+	rt_mutex_unlock(&lock->base.lock);
+}
+EXPORT_SYMBOL(ww_mutex_unlock);
+#endif
diff --git a/kernel/msm-3.18/kernel/locking/rtmutex_common.h b/kernel/msm-3.18/kernel/locking/rtmutex_common.h
index 855212501..c6dcda5e5 100644
--- a/kernel/msm-3.18/kernel/locking/rtmutex_common.h
+++ b/kernel/msm-3.18/kernel/locking/rtmutex_common.h
@@ -49,6 +49,7 @@ struct rt_mutex_waiter {
 	struct rb_node          pi_tree_entry;
 	struct task_struct	*task;
 	struct rt_mutex		*lock;
+	bool			savestate;
 #ifdef CONFIG_DEBUG_RT_MUTEXES
 	unsigned long		ip;
 	struct pid		*deadlock_task_pid;
@@ -119,6 +120,9 @@ enum rtmutex_chainwalk {
 /*
  * PI-futex support (proxy locking functions, etc.):
  */
+#define PI_WAKEUP_INPROGRESS	((struct rt_mutex_waiter *) 1)
+#define PI_REQUEUE_INPROGRESS	((struct rt_mutex_waiter *) 2)
+
 extern struct task_struct *rt_mutex_next_owner(struct rt_mutex *lock);
 extern void rt_mutex_init_proxy_locked(struct rt_mutex *lock,
 				       struct task_struct *proxy_owner);
@@ -138,4 +142,14 @@ extern int rt_mutex_timed_futex_lock(struct rt_mutex *l, struct hrtimer_sleeper
 # include "rtmutex.h"
 #endif
 
+static inline void
+rt_mutex_init_waiter(struct rt_mutex_waiter *waiter, bool savestate)
+{
+	debug_rt_mutex_init_waiter(waiter);
+	waiter->task = NULL;
+	waiter->savestate = savestate;
+	RB_CLEAR_NODE(&waiter->pi_tree_entry);
+	RB_CLEAR_NODE(&waiter->tree_entry);
+}
+
 #endif
diff --git a/kernel/msm-3.18/kernel/locking/spinlock.c b/kernel/msm-3.18/kernel/locking/spinlock.c
index 4b082b5ca..5c76166f8 100644
--- a/kernel/msm-3.18/kernel/locking/spinlock.c
+++ b/kernel/msm-3.18/kernel/locking/spinlock.c
@@ -124,8 +124,11 @@ void __lockfunc __raw_##op##_lock_bh(locktype##_t *lock)		\
  *         __[spin|read|write]_lock_bh()
  */
 BUILD_LOCK_OPS(spin, raw_spinlock);
+
+#ifndef CONFIG_PREEMPT_RT_FULL
 BUILD_LOCK_OPS(read, rwlock);
 BUILD_LOCK_OPS(write, rwlock);
+#endif
 
 #endif
 
@@ -209,6 +212,8 @@ void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)
 EXPORT_SYMBOL(_raw_spin_unlock_bh);
 #endif
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+
 #ifndef CONFIG_INLINE_READ_TRYLOCK
 int __lockfunc _raw_read_trylock(rwlock_t *lock)
 {
@@ -353,6 +358,8 @@ void __lockfunc _raw_write_unlock_bh(rwlock_t *lock)
 EXPORT_SYMBOL(_raw_write_unlock_bh);
 #endif
 
+#endif /* !PREEMPT_RT_FULL */
+
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 
 void __lockfunc _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)
diff --git a/kernel/msm-3.18/kernel/locking/spinlock_debug.c b/kernel/msm-3.18/kernel/locking/spinlock_debug.c
index d381f559e..ea31b93c7 100644
--- a/kernel/msm-3.18/kernel/locking/spinlock_debug.c
+++ b/kernel/msm-3.18/kernel/locking/spinlock_debug.c
@@ -33,6 +33,7 @@ void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,
 
 EXPORT_SYMBOL(__raw_spin_lock_init);
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 void __rwlock_init(rwlock_t *lock, const char *name,
 		   struct lock_class_key *key)
 {
@@ -50,6 +51,7 @@ void __rwlock_init(rwlock_t *lock, const char *name,
 }
 
 EXPORT_SYMBOL(__rwlock_init);
+#endif
 
 static void spin_dump(raw_spinlock_t *lock, const char *msg)
 {
@@ -166,6 +168,7 @@ void do_raw_spin_unlock(raw_spinlock_t *lock)
 	arch_spin_unlock(&lock->raw_lock);
 }
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 static void rwlock_bug(rwlock_t *lock, const char *msg)
 {
 	if (!debug_locks_off())
@@ -312,3 +315,5 @@ void do_raw_write_unlock(rwlock_t *lock)
 	debug_write_unlock(lock);
 	arch_write_unlock(&lock->raw_lock);
 }
+
+#endif
diff --git a/kernel/msm-3.18/kernel/module.c b/kernel/msm-3.18/kernel/module.c
index 38390f5c9..5bb1270d5 100644
--- a/kernel/msm-3.18/kernel/module.c
+++ b/kernel/msm-3.18/kernel/module.c
@@ -526,16 +526,7 @@ static void percpu_modcopy(struct module *mod,
 		memcpy(per_cpu_ptr(mod->percpu, cpu), from, size);
 }
 
-/**
- * is_module_percpu_address - test whether address is from module static percpu
- * @addr: address to test
- *
- * Test whether @addr belongs to module static percpu area.
- *
- * RETURNS:
- * %true if @addr is from module static percpu area
- */
-bool is_module_percpu_address(unsigned long addr)
+bool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr)
 {
 	struct module *mod;
 	unsigned int cpu;
@@ -549,9 +540,11 @@ bool is_module_percpu_address(unsigned long addr)
 			continue;
 		for_each_possible_cpu(cpu) {
 			void *start = per_cpu_ptr(mod->percpu, cpu);
+			void *va = (void *)addr;
 
-			if ((void *)addr >= start &&
-			    (void *)addr < start + mod->percpu_size) {
+			if (va >= start && va < start + mod->percpu_size) {
+				if (can_addr)
+					*can_addr = (unsigned long) (va - start);
 				preempt_enable();
 				return true;
 			}
@@ -562,6 +555,20 @@ bool is_module_percpu_address(unsigned long addr)
 	return false;
 }
 
+/**
+ * is_module_percpu_address - test whether address is from module static percpu
+ * @addr: address to test
+ *
+ * Test whether @addr belongs to module static percpu area.
+ *
+ * RETURNS:
+ * %true if @addr is from module static percpu area
+ */
+bool is_module_percpu_address(unsigned long addr)
+{
+	return __is_module_percpu_address(addr, NULL);
+}
+
 #else /* ... !CONFIG_SMP */
 
 static inline void __percpu *mod_percpu(struct module *mod)
@@ -593,6 +600,11 @@ bool is_module_percpu_address(unsigned long addr)
 	return false;
 }
 
+bool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr)
+{
+	return false;
+}
+
 #endif /* CONFIG_SMP */
 
 #define MODINFO_ATTR(field)	\
diff --git a/kernel/msm-3.18/kernel/panic.c b/kernel/msm-3.18/kernel/panic.c
index 49229478e..b8197166b 100644
--- a/kernel/msm-3.18/kernel/panic.c
+++ b/kernel/msm-3.18/kernel/panic.c
@@ -403,9 +403,11 @@ static u64 oops_id;
 
 static int init_oops_id(void)
 {
+#ifndef CONFIG_PREEMPT_RT_FULL
 	if (!oops_id)
 		get_random_bytes(&oops_id, sizeof(oops_id));
 	else
+#endif
 		oops_id++;
 
 	return 0;
diff --git a/kernel/msm-3.18/kernel/power/hibernate.c b/kernel/msm-3.18/kernel/power/hibernate.c
index 090020728..83108ba3e 100644
--- a/kernel/msm-3.18/kernel/power/hibernate.c
+++ b/kernel/msm-3.18/kernel/power/hibernate.c
@@ -287,6 +287,8 @@ static int create_image(int platform_mode)
 
 	local_irq_disable();
 
+	system_state = SYSTEM_SUSPEND;
+
 	error = syscore_suspend();
 	if (error) {
 		printk(KERN_ERR "PM: Some system devices failed to power down, "
@@ -316,6 +318,7 @@ static int create_image(int platform_mode)
 	syscore_resume();
 
  Enable_irqs:
+	system_state = SYSTEM_RUNNING;
 	local_irq_enable();
 
  Enable_cpus:
@@ -439,6 +442,7 @@ static int resume_target_kernel(bool platform_mode)
 		goto Enable_cpus;
 
 	local_irq_disable();
+	system_state = SYSTEM_SUSPEND;
 
 	error = syscore_suspend();
 	if (error)
@@ -472,6 +476,7 @@ static int resume_target_kernel(bool platform_mode)
 	syscore_resume();
 
  Enable_irqs:
+	system_state = SYSTEM_RUNNING;
 	local_irq_enable();
 
  Enable_cpus:
@@ -557,6 +562,7 @@ int hibernation_platform_enter(void)
 		goto Platform_finish;
 
 	local_irq_disable();
+	system_state = SYSTEM_SUSPEND;
 	syscore_suspend();
 	if (pm_wakeup_pending()) {
 		error = -EAGAIN;
@@ -569,6 +575,7 @@ int hibernation_platform_enter(void)
 
  Power_up:
 	syscore_resume();
+	system_state = SYSTEM_RUNNING;
 	local_irq_enable();
 	enable_nonboot_cpus();
 
diff --git a/kernel/msm-3.18/kernel/power/suspend.c b/kernel/msm-3.18/kernel/power/suspend.c
index c3291f374..1f360982d 100644
--- a/kernel/msm-3.18/kernel/power/suspend.c
+++ b/kernel/msm-3.18/kernel/power/suspend.c
@@ -362,6 +362,8 @@ static int suspend_enter(suspend_state_t state, bool *wakeup)
 	arch_suspend_disable_irqs();
 	BUG_ON(!irqs_disabled());
 
+	system_state = SYSTEM_SUSPEND;
+
 	error = syscore_suspend();
 	if (!error) {
 		*wakeup = pm_wakeup_pending();
@@ -383,6 +385,8 @@ static int suspend_enter(suspend_state_t state, bool *wakeup)
 		syscore_resume();
 	}
 
+	system_state = SYSTEM_RUNNING;
+
 	arch_suspend_enable_irqs();
 	BUG_ON(irqs_disabled());
 
diff --git a/kernel/msm-3.18/kernel/printk/printk.c b/kernel/msm-3.18/kernel/printk/printk.c
index 0f4c08484..6cab350f7 100644
--- a/kernel/msm-3.18/kernel/printk/printk.c
+++ b/kernel/msm-3.18/kernel/printk/printk.c
@@ -1181,6 +1181,7 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
 {
 	char *text;
 	int len = 0;
+	int attempts = 0;
 
 	text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
 	if (!text)
@@ -1192,7 +1193,14 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
 		u64 seq;
 		u32 idx;
 		enum log_flags prev;
-
+		int num_msg;
+try_again:
+		attempts++;
+		if (attempts > 10) {
+			len = -EBUSY;
+			goto out;
+		}
+		num_msg = 0;
 		if (clear_seq < log_first_seq) {
 			/* messages are gone, move to first available one */
 			clear_seq = log_first_seq;
@@ -1213,6 +1221,14 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
 			prev = msg->flags;
 			idx = log_next(idx);
 			seq++;
+			num_msg++;
+			if (num_msg > 5) {
+				num_msg = 0;
+				raw_spin_unlock_irq(&logbuf_lock);
+				raw_spin_lock_irq(&logbuf_lock);
+				if (clear_seq < log_first_seq)
+					goto try_again;
+			}
 		}
 
 		/* move first record forward until length fits into the buffer */
@@ -1226,6 +1242,14 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
 			prev = msg->flags;
 			idx = log_next(idx);
 			seq++;
+			num_msg++;
+			if (num_msg > 5) {
+				num_msg = 0;
+				raw_spin_unlock_irq(&logbuf_lock);
+				raw_spin_lock_irq(&logbuf_lock);
+				if (clear_seq < log_first_seq)
+					goto try_again;
+			}
 		}
 
 		/* last message fitting into this dump */
@@ -1266,6 +1290,7 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
 		clear_seq = log_next_seq;
 		clear_idx = log_next_idx;
 	}
+out:
 	raw_spin_unlock_irq(&logbuf_lock);
 
 	kfree(text);
@@ -1419,6 +1444,12 @@ static void call_console_drivers(int level, const char *text, size_t len)
 	if (!console_drivers)
 		return;
 
+	if (IS_ENABLED(CONFIG_PREEMPT_RT_BASE)) {
+		if (in_irq() || in_nmi())
+			return;
+	}
+
+	migrate_disable();
 	for_each_console(con) {
 		if (exclusive_console && con != exclusive_console)
 			continue;
@@ -1431,6 +1462,7 @@ static void call_console_drivers(int level, const char *text, size_t len)
 			continue;
 		con->write(con, text, len);
 	}
+	migrate_enable();
 }
 
 /*
@@ -1491,6 +1523,15 @@ static inline int can_use_console(unsigned int cpu)
 static int console_trylock_for_printk(void)
 {
 	unsigned int cpu = smp_processor_id();
+#ifdef CONFIG_PREEMPT_RT_FULL
+	int lock = !early_boot_irqs_disabled && (preempt_count() == 0) &&
+		!irqs_disabled();
+#else
+	int lock = 1;
+#endif
+
+	if (!lock)
+		return 0;
 
 	if (!console_trylock())
 		return 0;
@@ -1625,6 +1666,62 @@ static size_t cont_print_text(char *text, size_t size)
 	return textlen;
 }
 
+#ifdef CONFIG_EARLY_PRINTK
+struct console *early_console;
+
+void early_vprintk(const char *fmt, va_list ap)
+{
+	if (early_console) {
+		char buf[512];
+		int n = vscnprintf(buf, sizeof(buf), fmt, ap);
+
+		early_console->write(early_console, buf, n);
+	}
+}
+
+asmlinkage void early_printk(const char *fmt, ...)
+{
+	va_list ap;
+
+	va_start(ap, fmt);
+	early_vprintk(fmt, ap);
+	va_end(ap);
+}
+
+/*
+ * This is independent of any log levels - a global
+ * kill switch that turns off all of printk.
+ *
+ * Used by the NMI watchdog if early-printk is enabled.
+ */
+static bool __read_mostly printk_killswitch;
+
+static int __init force_early_printk_setup(char *str)
+{
+	printk_killswitch = true;
+	return 0;
+}
+early_param("force_early_printk", force_early_printk_setup);
+
+void printk_kill(void)
+{
+	printk_killswitch = true;
+}
+
+static int forced_early_printk(const char *fmt, va_list ap)
+{
+	if (!printk_killswitch)
+		return 0;
+	early_vprintk(fmt, ap);
+	return 1;
+}
+#else
+static inline int forced_early_printk(const char *fmt, va_list ap)
+{
+	return 0;
+}
+#endif
+
 asmlinkage int vprintk_emit(int facility, int level,
 			    const char *dict, size_t dictlen,
 			    const char *fmt, va_list args)
@@ -1641,6 +1738,13 @@ asmlinkage int vprintk_emit(int facility, int level,
 	/* cpu currently holding logbuf_lock in this function */
 	static volatile unsigned int logbuf_cpu = UINT_MAX;
 
+	/*
+	 * Fall back to early_printk if a debugging subsystem has
+	 * killed printk output
+	 */
+	if (unlikely(forced_early_printk(fmt, args)))
+		return 1;
+
 	if (level == SCHED_MESSAGE_LOGLEVEL) {
 		level = -1;
 		in_sched = true;
@@ -1785,8 +1889,7 @@ asmlinkage int vprintk_emit(int facility, int level,
 		 * console_sem which would prevent anyone from printing to
 		 * console
 		 */
-		preempt_disable();
-
+		migrate_disable();
 		/*
 		 * Try to acquire and then immediately release the console
 		 * semaphore.  The release will print out buffers and wake up
@@ -1794,7 +1897,7 @@ asmlinkage int vprintk_emit(int facility, int level,
 		 */
 		if (console_trylock_for_printk())
 			console_unlock();
-		preempt_enable();
+		migrate_enable();
 		lockdep_on();
 	}
 
@@ -1894,29 +1997,6 @@ static size_t cont_print_text(char *text, size_t size) { return 0; }
 
 #endif /* CONFIG_PRINTK */
 
-#ifdef CONFIG_EARLY_PRINTK
-struct console *early_console;
-
-void early_vprintk(const char *fmt, va_list ap)
-{
-	if (early_console) {
-		char buf[512];
-		int n = vscnprintf(buf, sizeof(buf), fmt, ap);
-
-		early_console->write(early_console, buf, n);
-	}
-}
-
-asmlinkage __visible void early_printk(const char *fmt, ...)
-{
-	va_list ap;
-
-	va_start(ap, fmt);
-	early_vprintk(fmt, ap);
-	va_end(ap);
-}
-#endif
-
 static int __add_preferred_console(char *name, int idx, char *options,
 				   char *brl_options)
 {
@@ -2176,11 +2256,16 @@ static void console_cont_flush(char *text, size_t size)
 		goto out;
 
 	len = cont_print_text(text, size);
+#ifndef CONFIG_PREEMPT_RT_FULL
 	raw_spin_unlock(&logbuf_lock);
 	stop_critical_timings();
 	call_console_drivers(cont.level, text, len);
 	start_critical_timings();
 	local_irq_restore(flags);
+#else
+	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
+	call_console_drivers(cont.level, text, len);
+#endif
 	return;
 out:
 	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
@@ -2279,13 +2364,17 @@ skip:
 		console_idx = log_next(console_idx);
 		console_seq++;
 		console_prev = msg->flags;
+#ifdef CONFIG_PREEMPT_RT_FULL
+		raw_spin_unlock_irqrestore(&logbuf_lock, flags);
+		call_console_drivers(level, text, len);
+#else
 		raw_spin_unlock(&logbuf_lock);
 
 		stop_critical_timings();	/* don't trace print latency */
 		call_console_drivers(level, text, len);
 		start_critical_timings();
 		local_irq_restore(flags);
-
+#endif
 		if (do_cond_resched)
 			cond_resched();
 	}
@@ -2337,6 +2426,11 @@ void console_unblank(void)
 {
 	struct console *c;
 
+	if (IS_ENABLED(CONFIG_PREEMPT_RT_BASE)) {
+		if (in_irq() || in_nmi())
+			return;
+	}
+
 	/*
 	 * console_unblank can no longer be called in interrupt context unless
 	 * oops_in_progress is set to 1..
diff --git a/kernel/msm-3.18/kernel/ptrace.c b/kernel/msm-3.18/kernel/ptrace.c
index 8d2c10714..622ca5279 100644
--- a/kernel/msm-3.18/kernel/ptrace.c
+++ b/kernel/msm-3.18/kernel/ptrace.c
@@ -129,7 +129,14 @@ static bool ptrace_freeze_traced(struct task_struct *task)
 
 	spin_lock_irq(&task->sighand->siglock);
 	if (task_is_traced(task) && !__fatal_signal_pending(task)) {
-		task->state = __TASK_TRACED;
+		unsigned long flags;
+
+		raw_spin_lock_irqsave(&task->pi_lock, flags);
+		if (task->state & __TASK_TRACED)
+			task->state = __TASK_TRACED;
+		else
+			task->saved_state = __TASK_TRACED;
+		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
 		ret = true;
 	}
 	spin_unlock_irq(&task->sighand->siglock);
diff --git a/kernel/msm-3.18/kernel/rcu/rcutorture.c b/kernel/msm-3.18/kernel/rcu/rcutorture.c
index 240fa9094..ff0f2d5d6 100644
--- a/kernel/msm-3.18/kernel/rcu/rcutorture.c
+++ b/kernel/msm-3.18/kernel/rcu/rcutorture.c
@@ -392,6 +392,7 @@ static struct rcu_torture_ops rcu_ops = {
 	.name		= "rcu"
 };
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 /*
  * Definitions for rcu_bh torture testing.
  */
@@ -435,6 +436,12 @@ static struct rcu_torture_ops rcu_bh_ops = {
 	.name		= "rcu_bh"
 };
 
+#else
+static struct rcu_torture_ops rcu_bh_ops = {
+	.ttype		= INVALID_RCU_FLAVOR,
+};
+#endif
+
 /*
  * Don't even think about trying any of these in real life!!!
  * The names includes "busted", and they really means it!
diff --git a/kernel/msm-3.18/kernel/rcu/tiny.c b/kernel/msm-3.18/kernel/rcu/tiny.c
index da80f2f73..afc35f0ae 100644
--- a/kernel/msm-3.18/kernel/rcu/tiny.c
+++ b/kernel/msm-3.18/kernel/rcu/tiny.c
@@ -375,6 +375,7 @@ void call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
 }
 EXPORT_SYMBOL_GPL(call_rcu_sched);
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 /*
  * Post an RCU bottom-half callback to be invoked after any subsequent
  * quiescent state.
@@ -384,6 +385,7 @@ void call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
 	__call_rcu(head, func, &rcu_bh_ctrlblk);
 }
 EXPORT_SYMBOL_GPL(call_rcu_bh);
+#endif
 
 void rcu_init(void)
 {
diff --git a/kernel/msm-3.18/kernel/rcu/tree.c b/kernel/msm-3.18/kernel/rcu/tree.c
index c8832a409..68d106260 100644
--- a/kernel/msm-3.18/kernel/rcu/tree.c
+++ b/kernel/msm-3.18/kernel/rcu/tree.c
@@ -56,6 +56,11 @@
 #include <linux/random.h>
 #include <linux/ftrace_event.h>
 #include <linux/suspend.h>
+#include <linux/delay.h>
+#include <linux/gfp.h>
+#include <linux/oom.h>
+#include <linux/smpboot.h>
+#include "../time/tick-internal.h"
 
 #include "tree.h"
 #include "rcu.h"
@@ -152,8 +157,6 @@ EXPORT_SYMBOL_GPL(rcu_scheduler_active);
  */
 static int rcu_scheduler_fully_active __read_mostly;
 
-#ifdef CONFIG_RCU_BOOST
-
 /*
  * Control variables for per-CPU and per-rcu_node kthreads.  These
  * handle all flavors of RCU.
@@ -163,8 +166,6 @@ DEFINE_PER_CPU(unsigned int, rcu_cpu_kthread_status);
 DEFINE_PER_CPU(unsigned int, rcu_cpu_kthread_loops);
 DEFINE_PER_CPU(char, rcu_cpu_has_work);
 
-#endif /* #ifdef CONFIG_RCU_BOOST */
-
 static void rcu_boost_kthread_setaffinity(struct rcu_node *rnp, int outgoingcpu);
 static void invoke_rcu_core(void);
 static void invoke_rcu_callbacks(struct rcu_state *rsp, struct rcu_data *rdp);
@@ -207,6 +208,19 @@ void rcu_sched_qs(void)
 	}
 }
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+static void rcu_preempt_qs(void);
+
+void rcu_bh_qs(void)
+{
+	unsigned long flags;
+
+	/* Callers to this function, rcu_preempt_qs(), must disable irqs. */
+	local_irq_save(flags);
+	rcu_preempt_qs();
+	local_irq_restore(flags);
+}
+#else
 void rcu_bh_qs(void)
 {
 	if (!__this_cpu_read(rcu_bh_data.passed_quiesce)) {
@@ -216,6 +230,7 @@ void rcu_bh_qs(void)
 		__this_cpu_write(rcu_bh_data.passed_quiesce, 1);
 	}
 }
+#endif
 
 static DEFINE_PER_CPU(int, rcu_sched_qs_mask);
 
@@ -336,6 +351,7 @@ long rcu_batches_completed_sched(void)
 }
 EXPORT_SYMBOL_GPL(rcu_batches_completed_sched);
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 /*
  * Return the number of RCU BH batches processed thus far for debug & stats.
  */
@@ -363,6 +379,13 @@ void rcu_bh_force_quiescent_state(void)
 }
 EXPORT_SYMBOL_GPL(rcu_bh_force_quiescent_state);
 
+#else
+void rcu_force_quiescent_state(void)
+{
+}
+EXPORT_SYMBOL_GPL(rcu_force_quiescent_state);
+#endif
+
 /*
  * Show the state of the grace-period kthreads.
  */
@@ -1411,7 +1434,7 @@ static void rcu_gp_kthread_wake(struct rcu_state *rsp)
 	    !ACCESS_ONCE(rsp->gp_flags) ||
 	    !rsp->gp_kthread)
 		return;
-	wake_up(&rsp->gp_wq);
+	swait_wake(&rsp->gp_wq);
 }
 
 /*
@@ -1793,7 +1816,7 @@ static int __noreturn rcu_gp_kthread(void *arg)
 					       ACCESS_ONCE(rsp->gpnum),
 					       TPS("reqwait"));
 			rsp->gp_state = RCU_GP_WAIT_GPS;
-			wait_event_interruptible(rsp->gp_wq,
+			swait_event_interruptible(rsp->gp_wq,
 						 ACCESS_ONCE(rsp->gp_flags) &
 						 RCU_GP_FLAG_INIT);
 			/* Locking provides needed memory barrier. */
@@ -1821,7 +1844,7 @@ static int __noreturn rcu_gp_kthread(void *arg)
 					       ACCESS_ONCE(rsp->gpnum),
 					       TPS("fqswait"));
 			rsp->gp_state = RCU_GP_WAIT_FQS;
-			ret = wait_event_interruptible_timeout(rsp->gp_wq,
+			ret = swait_event_interruptible_timeout(rsp->gp_wq,
 					((gf = ACCESS_ONCE(rsp->gp_flags)) &
 					 RCU_GP_FLAG_FQS) ||
 					(!ACCESS_ONCE(rnp->qsmask) &&
@@ -2565,16 +2588,14 @@ __rcu_process_callbacks(struct rcu_state *rsp)
 /*
  * Do RCU core processing for the current CPU.
  */
-static void rcu_process_callbacks(struct softirq_action *unused)
+static void rcu_process_callbacks(void)
 {
 	struct rcu_state *rsp;
 
 	if (cpu_is_offline(smp_processor_id()))
 		return;
-	trace_rcu_utilization(TPS("Start RCU core"));
 	for_each_rcu_flavor(rsp)
 		__rcu_process_callbacks(rsp);
-	trace_rcu_utilization(TPS("End RCU core"));
 }
 
 /*
@@ -2588,18 +2609,105 @@ static void invoke_rcu_callbacks(struct rcu_state *rsp, struct rcu_data *rdp)
 {
 	if (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))
 		return;
-	if (likely(!rsp->boost)) {
-		rcu_do_batch(rsp, rdp);
+	rcu_do_batch(rsp, rdp);
+}
+
+static void rcu_wake_cond(struct task_struct *t, int status)
+{
+	/*
+	 * If the thread is yielding, only wake it when this
+	 * is invoked from idle
+	 */
+	if (t && (status != RCU_KTHREAD_YIELDING || is_idle_task(current)))
+		wake_up_process(t);
+}
+
+/*
+ * Wake up this CPU's rcuc kthread to do RCU core processing.
+ */
+static void invoke_rcu_core(void)
+{
+	unsigned long flags;
+	struct task_struct *t;
+
+	if (!cpu_online(smp_processor_id()))
 		return;
+	local_irq_save(flags);
+	__this_cpu_write(rcu_cpu_has_work, 1);
+	t = __this_cpu_read(rcu_cpu_kthread_task);
+	if (t != NULL && current != t)
+		rcu_wake_cond(t, __this_cpu_read(rcu_cpu_kthread_status));
+	local_irq_restore(flags);
+}
+
+static void rcu_cpu_kthread_park(unsigned int cpu)
+{
+	per_cpu(rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;
+}
+
+static int rcu_cpu_kthread_should_run(unsigned int cpu)
+{
+	return __this_cpu_read(rcu_cpu_has_work);
+}
+
+/*
+ * Per-CPU kernel thread that invokes RCU callbacks.  This replaces the
+ * RCU softirq used in flavors and configurations of RCU that do not
+ * support RCU priority boosting.
+ */
+static void rcu_cpu_kthread(unsigned int cpu)
+{
+	unsigned int *statusp = &__get_cpu_var(rcu_cpu_kthread_status);
+	char work, *workp = &__get_cpu_var(rcu_cpu_has_work);
+	int spincnt;
+
+	for (spincnt = 0; spincnt < 10; spincnt++) {
+		trace_rcu_utilization(TPS("Start CPU kthread@rcu_wait"));
+		local_bh_disable();
+		*statusp = RCU_KTHREAD_RUNNING;
+		this_cpu_inc(rcu_cpu_kthread_loops);
+		local_irq_disable();
+		work = *workp;
+		*workp = 0;
+		local_irq_enable();
+		if (work)
+			rcu_process_callbacks();
+		local_bh_enable();
+		if (*workp == 0) {
+			trace_rcu_utilization(TPS("End CPU kthread@rcu_wait"));
+			*statusp = RCU_KTHREAD_WAITING;
+			return;
+		}
 	}
-	invoke_rcu_callbacks_kthread();
+	*statusp = RCU_KTHREAD_YIELDING;
+	trace_rcu_utilization(TPS("Start CPU kthread@rcu_yield"));
+	schedule_timeout_interruptible(2);
+	trace_rcu_utilization(TPS("End CPU kthread@rcu_yield"));
+	*statusp = RCU_KTHREAD_WAITING;
 }
 
-static void invoke_rcu_core(void)
+static struct smp_hotplug_thread rcu_cpu_thread_spec = {
+	.store			= &rcu_cpu_kthread_task,
+	.thread_should_run	= rcu_cpu_kthread_should_run,
+	.thread_fn		= rcu_cpu_kthread,
+	.thread_comm		= "rcuc/%u",
+	.setup			= rcu_cpu_kthread_setup,
+	.park			= rcu_cpu_kthread_park,
+};
+
+/*
+ * Spawn per-CPU RCU core processing kthreads.
+ */
+static int __init rcu_spawn_core_kthreads(void)
 {
-	if (cpu_online(smp_processor_id()))
-		raise_softirq(RCU_SOFTIRQ);
+	int cpu;
+
+	for_each_possible_cpu(cpu)
+		per_cpu(rcu_cpu_has_work, cpu) = 0;
+	BUG_ON(smpboot_register_percpu_thread(&rcu_cpu_thread_spec));
+	return 0;
 }
+early_initcall(rcu_spawn_core_kthreads);
 
 /*
  * Handle any core-RCU processing required by a call_rcu() invocation.
@@ -2734,6 +2842,7 @@ void call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
 }
 EXPORT_SYMBOL_GPL(call_rcu_sched);
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 /*
  * Queue an RCU callback for invocation after a quicker grace period.
  */
@@ -2742,6 +2851,7 @@ void call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
 	__call_rcu(head, func, &rcu_bh_state, -1, 0);
 }
 EXPORT_SYMBOL_GPL(call_rcu_bh);
+#endif
 
 /*
  * Queue an RCU callback for lazy invocation after a grace period.
@@ -2833,6 +2943,7 @@ void synchronize_sched(void)
 }
 EXPORT_SYMBOL_GPL(synchronize_sched);
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 /**
  * synchronize_rcu_bh - wait until an rcu_bh grace period has elapsed.
  *
@@ -2859,6 +2970,7 @@ void synchronize_rcu_bh(void)
 		wait_rcu_gp(call_rcu_bh);
 }
 EXPORT_SYMBOL_GPL(synchronize_rcu_bh);
+#endif
 
 /**
  * get_state_synchronize_rcu - Snapshot current RCU state
@@ -3330,6 +3442,7 @@ static void _rcu_barrier(struct rcu_state *rsp)
 	mutex_unlock(&rsp->barrier_mutex);
 }
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 /**
  * rcu_barrier_bh - Wait until all in-flight call_rcu_bh() callbacks complete.
  */
@@ -3338,6 +3451,7 @@ void rcu_barrier_bh(void)
 	_rcu_barrier(&rcu_bh_state);
 }
 EXPORT_SYMBOL_GPL(rcu_barrier_bh);
+#endif
 
 /**
  * rcu_barrier_sched - Wait for in-flight call_rcu_sched() callbacks.
@@ -3651,7 +3765,7 @@ static void __init rcu_init_one(struct rcu_state *rsp,
 	}
 
 	rsp->rda = rda;
-	init_waitqueue_head(&rsp->gp_wq);
+	init_swait_head(&rsp->gp_wq);
 	rnp = rsp->level[rcu_num_lvls - 1];
 	for_each_possible_cpu(i) {
 		while (i > rnp->grphi)
@@ -3748,7 +3862,6 @@ void __init rcu_init(void)
 	rcu_init_one(&rcu_bh_state, &rcu_bh_data);
 	rcu_init_one(&rcu_sched_state, &rcu_sched_data);
 	__rcu_init_preempt();
-	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);
 
 	/*
 	 * We don't need protection against CPU-hotplug here because
diff --git a/kernel/msm-3.18/kernel/rcu/tree.h b/kernel/msm-3.18/kernel/rcu/tree.h
index 32eac9ac4..90afb5cda 100644
--- a/kernel/msm-3.18/kernel/rcu/tree.h
+++ b/kernel/msm-3.18/kernel/rcu/tree.h
@@ -28,6 +28,7 @@
 #include <linux/cpumask.h>
 #include <linux/seqlock.h>
 #include <linux/irq_work.h>
+#include <linux/wait-simple.h>
 
 /*
  * Define shape of hierarchy based on NR_CPUS, CONFIG_RCU_FANOUT, and
@@ -208,7 +209,7 @@ struct rcu_node {
 				/*  This can happen due to race conditions. */
 #endif /* #ifdef CONFIG_RCU_BOOST */
 #ifdef CONFIG_RCU_NOCB_CPU
-	wait_queue_head_t nocb_gp_wq[2];
+	struct swait_head nocb_gp_wq[2];
 				/* Place for rcu_nocb_kthread() to wait GP. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 	int need_future_gp[2];
@@ -348,7 +349,7 @@ struct rcu_data {
 	atomic_long_t nocb_follower_count_lazy; /*  (approximate). */
 	int nocb_p_count;		/* # CBs being invoked by kthread */
 	int nocb_p_count_lazy;		/*  (approximate). */
-	wait_queue_head_t nocb_wq;	/* For nocb kthreads to sleep on. */
+	struct swait_head nocb_wq;	/* For nocb kthreads to sleep on. */
 	struct task_struct *nocb_kthread;
 	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
 
@@ -439,7 +440,7 @@ struct rcu_state {
 	unsigned long gpnum;			/* Current gp number. */
 	unsigned long completed;		/* # of last completed gp. */
 	struct task_struct *gp_kthread;		/* Task for grace periods. */
-	wait_queue_head_t gp_wq;		/* Where GP task waits. */
+	struct swait_head gp_wq;		/* Where GP task waits. */
 	short gp_flags;				/* Commands for GP task. */
 	short gp_state;				/* GP kthread sleep state. */
 
@@ -570,10 +571,9 @@ static void rcu_report_exp_rnp(struct rcu_state *rsp, struct rcu_node *rnp,
 static void __init __rcu_init_preempt(void);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
 static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);
-static void invoke_rcu_callbacks_kthread(void);
 static bool rcu_is_callbacks_kthread(void);
+static void rcu_cpu_kthread_setup(unsigned int cpu);
 #ifdef CONFIG_RCU_BOOST
-static void rcu_preempt_do_callbacks(void);
 static int rcu_spawn_one_boost_kthread(struct rcu_state *rsp,
 						 struct rcu_node *rnp);
 #endif /* #ifdef CONFIG_RCU_BOOST */
diff --git a/kernel/msm-3.18/kernel/rcu/tree_plugin.h b/kernel/msm-3.18/kernel/rcu/tree_plugin.h
index de929b633..9f43957cd 100644
--- a/kernel/msm-3.18/kernel/rcu/tree_plugin.h
+++ b/kernel/msm-3.18/kernel/rcu/tree_plugin.h
@@ -24,12 +24,6 @@
  *	   Paul E. McKenney <paulmck@linux.vnet.ibm.com>
  */
 
-#include <linux/delay.h>
-#include <linux/gfp.h>
-#include <linux/oom.h>
-#include <linux/smpboot.h>
-#include "../time/tick-internal.h"
-
 #define RCU_KTHREAD_PRIO 1
 
 #ifdef CONFIG_RCU_BOOST
@@ -335,7 +329,7 @@ void rcu_read_unlock_special(struct task_struct *t)
 	}
 
 	/* Hardware IRQ handlers cannot block, complain if they get here. */
-	if (WARN_ON_ONCE(in_irq() || in_serving_softirq())) {
+	if (WARN_ON_ONCE(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_OFFSET))) {
 		local_irq_restore(flags);
 		return;
 	}
@@ -633,15 +627,6 @@ static void rcu_preempt_check_callbacks(int cpu)
 		t->rcu_read_unlock_special.b.need_qs = true;
 }
 
-#ifdef CONFIG_RCU_BOOST
-
-static void rcu_preempt_do_callbacks(void)
-{
-	rcu_do_batch(&rcu_preempt_state, this_cpu_ptr(&rcu_preempt_data));
-}
-
-#endif /* #ifdef CONFIG_RCU_BOOST */
-
 /*
  * Queue a preemptible-RCU callback for invocation after a grace period.
  */
@@ -1070,6 +1055,19 @@ void exit_rcu(void)
 
 #endif /* #else #ifdef CONFIG_TREE_PREEMPT_RCU */
 
+/*
+ * If boosting, set rcuc kthreads to realtime priority.
+ */
+static void rcu_cpu_kthread_setup(unsigned int cpu)
+{
+#ifdef CONFIG_RCU_BOOST
+	struct sched_param sp;
+
+	sp.sched_priority = RCU_KTHREAD_PRIO;
+	sched_setscheduler_nocheck(current, SCHED_FIFO, &sp);
+#endif /* #ifdef CONFIG_RCU_BOOST */
+}
+
 #ifdef CONFIG_RCU_BOOST
 
 #include "../locking/rtmutex_common.h"
@@ -1101,16 +1099,6 @@ static void rcu_initiate_boost_trace(struct rcu_node *rnp)
 
 #endif /* #else #ifdef CONFIG_RCU_TRACE */
 
-static void rcu_wake_cond(struct task_struct *t, int status)
-{
-	/*
-	 * If the thread is yielding, only wake it when this
-	 * is invoked from idle
-	 */
-	if (status != RCU_KTHREAD_YIELDING || is_idle_task(current))
-		wake_up_process(t);
-}
-
 /*
  * Carry out RCU priority boosting on the task indicated by ->exp_tasks
  * or ->boost_tasks, advancing the pointer to the next task in the
@@ -1254,23 +1242,6 @@ static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags)
 	}
 }
 
-/*
- * Wake up the per-CPU kthread to invoke RCU callbacks.
- */
-static void invoke_rcu_callbacks_kthread(void)
-{
-	unsigned long flags;
-
-	local_irq_save(flags);
-	__this_cpu_write(rcu_cpu_has_work, 1);
-	if (__this_cpu_read(rcu_cpu_kthread_task) != NULL &&
-	    current != __this_cpu_read(rcu_cpu_kthread_task)) {
-		rcu_wake_cond(__this_cpu_read(rcu_cpu_kthread_task),
-			      __this_cpu_read(rcu_cpu_kthread_status));
-	}
-	local_irq_restore(flags);
-}
-
 /*
  * Is the current CPU running the RCU-callbacks kthread?
  * Caller must have preemption disabled.
@@ -1326,67 +1297,6 @@ static int rcu_spawn_one_boost_kthread(struct rcu_state *rsp,
 	return 0;
 }
 
-static void rcu_kthread_do_work(void)
-{
-	rcu_do_batch(&rcu_sched_state, this_cpu_ptr(&rcu_sched_data));
-	rcu_do_batch(&rcu_bh_state, this_cpu_ptr(&rcu_bh_data));
-	rcu_preempt_do_callbacks();
-}
-
-static void rcu_cpu_kthread_setup(unsigned int cpu)
-{
-	struct sched_param sp;
-
-	sp.sched_priority = RCU_KTHREAD_PRIO;
-	sched_setscheduler_nocheck(current, SCHED_FIFO, &sp);
-}
-
-static void rcu_cpu_kthread_park(unsigned int cpu)
-{
-	per_cpu(rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;
-}
-
-static int rcu_cpu_kthread_should_run(unsigned int cpu)
-{
-	return __this_cpu_read(rcu_cpu_has_work);
-}
-
-/*
- * Per-CPU kernel thread that invokes RCU callbacks.  This replaces the
- * RCU softirq used in flavors and configurations of RCU that do not
- * support RCU priority boosting.
- */
-static void rcu_cpu_kthread(unsigned int cpu)
-{
-	unsigned int *statusp = this_cpu_ptr(&rcu_cpu_kthread_status);
-	char work, *workp = this_cpu_ptr(&rcu_cpu_has_work);
-	int spincnt;
-
-	for (spincnt = 0; spincnt < 10; spincnt++) {
-		trace_rcu_utilization(TPS("Start CPU kthread@rcu_wait"));
-		local_bh_disable();
-		*statusp = RCU_KTHREAD_RUNNING;
-		this_cpu_inc(rcu_cpu_kthread_loops);
-		local_irq_disable();
-		work = *workp;
-		*workp = 0;
-		local_irq_enable();
-		if (work)
-			rcu_kthread_do_work();
-		local_bh_enable();
-		if (*workp == 0) {
-			trace_rcu_utilization(TPS("End CPU kthread@rcu_wait"));
-			*statusp = RCU_KTHREAD_WAITING;
-			return;
-		}
-	}
-	*statusp = RCU_KTHREAD_YIELDING;
-	trace_rcu_utilization(TPS("Start CPU kthread@rcu_yield"));
-	schedule_timeout_interruptible(2);
-	trace_rcu_utilization(TPS("End CPU kthread@rcu_yield"));
-	*statusp = RCU_KTHREAD_WAITING;
-}
-
 /*
  * Set the per-rcu_node kthread's affinity to cover all CPUs that are
  * served by the rcu_node in question.  The CPU hotplug lock is still
@@ -1420,26 +1330,13 @@ static void rcu_boost_kthread_setaffinity(struct rcu_node *rnp, int outgoingcpu)
 	free_cpumask_var(cm);
 }
 
-static struct smp_hotplug_thread rcu_cpu_thread_spec = {
-	.store			= &rcu_cpu_kthread_task,
-	.thread_should_run	= rcu_cpu_kthread_should_run,
-	.thread_fn		= rcu_cpu_kthread,
-	.thread_comm		= "rcuc/%u",
-	.setup			= rcu_cpu_kthread_setup,
-	.park			= rcu_cpu_kthread_park,
-};
-
 /*
  * Spawn boost kthreads -- called as soon as the scheduler is running.
  */
 static void __init rcu_spawn_boost_kthreads(void)
 {
 	struct rcu_node *rnp;
-	int cpu;
 
-	for_each_possible_cpu(cpu)
-		per_cpu(rcu_cpu_has_work, cpu) = 0;
-	BUG_ON(smpboot_register_percpu_thread(&rcu_cpu_thread_spec));
 	rnp = rcu_get_root(rcu_state_p);
 	(void)rcu_spawn_one_boost_kthread(rcu_state_p, rnp);
 	if (NUM_RCU_NODES > 1) {
@@ -1466,11 +1363,6 @@ static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags)
 	raw_spin_unlock_irqrestore(&rnp->lock, flags);
 }
 
-static void invoke_rcu_callbacks_kthread(void)
-{
-	WARN_ON_ONCE(1);
-}
-
 static bool rcu_is_callbacks_kthread(void)
 {
 	return false;
@@ -1494,7 +1386,7 @@ static void rcu_prepare_kthreads(int cpu)
 
 #endif /* #else #ifdef CONFIG_RCU_BOOST */
 
-#if !defined(CONFIG_RCU_FAST_NO_HZ)
+#if !defined(CONFIG_RCU_FAST_NO_HZ) || defined(CONFIG_PREEMPT_RT_FULL)
 
 /*
  * Check to see if any future RCU-related work will need to be done
@@ -1519,7 +1411,9 @@ static void rcu_prepare_for_idle_init(int cpu)
 {
 }
 #endif /* #ifndef CONFIG_RCU_NOCB_CPU_ALL */
+#endif /* !defined(CONFIG_RCU_FAST_NO_HZ) || defined(CONFIG_PREEMPT_RT_FULL) */
 
+#if !defined(CONFIG_RCU_FAST_NO_HZ)
 /*
  * Because we do not have RCU_FAST_NO_HZ, don't bother cleaning up
  * after it.
@@ -1628,6 +1522,7 @@ static bool rcu_preempt_cpu_has_nonlazy_callbacks(int cpu)
 
 #endif /* else #ifdef CONFIG_TREE_PREEMPT_RCU */
 
+
 /*
  * Does any flavor of RCU have non-lazy callbacks on the specified CPU?
  */
@@ -1638,6 +1533,9 @@ static bool rcu_cpu_has_nonlazy_callbacks(int cpu)
 	       rcu_preempt_cpu_has_nonlazy_callbacks(cpu);
 }
 #endif
+
+#ifndef CONFIG_PREEMPT_RT_FULL
+
 /*
  * Allow the CPU to enter dyntick-idle mode if either: (1) There are no
  * callbacks on this CPU, (2) this CPU has not yet attempted to enter
@@ -1683,7 +1581,7 @@ int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
 	return 0;
 }
 #endif /* #ifndef CONFIG_RCU_NOCB_CPU_ALL */
-
+#endif /* #ifndef CONFIG_PREEMPT_RT_FULL */
 /*
  * Handler for smp_call_function_single().  The only point of this
  * handler is to wake the CPU up, so the handler does only tracing.
@@ -2167,7 +2065,7 @@ early_param("rcu_nocb_poll", parse_rcu_nocb_poll);
  */
 static void rcu_nocb_gp_cleanup(struct rcu_state *rsp, struct rcu_node *rnp)
 {
-	wake_up_all(&rnp->nocb_gp_wq[rnp->completed & 0x1]);
+	swait_wake_all(&rnp->nocb_gp_wq[rnp->completed & 0x1]);
 }
 
 /*
@@ -2185,8 +2083,8 @@ static void rcu_nocb_gp_set(struct rcu_node *rnp, int nrq)
 
 static void rcu_init_one_nocb(struct rcu_node *rnp)
 {
-	init_waitqueue_head(&rnp->nocb_gp_wq[0]);
-	init_waitqueue_head(&rnp->nocb_gp_wq[1]);
+	init_swait_head(&rnp->nocb_gp_wq[0]);
+	init_swait_head(&rnp->nocb_gp_wq[1]);
 }
 
 #ifndef CONFIG_RCU_NOCB_CPU_ALL
@@ -2211,7 +2109,7 @@ static void wake_nocb_leader(struct rcu_data *rdp, bool force)
 	if (ACCESS_ONCE(rdp_leader->nocb_leader_sleep) || force) {
 		/* Prior smp_mb__after_atomic() orders against prior enqueue. */
 		ACCESS_ONCE(rdp_leader->nocb_leader_sleep) = false;
-		wake_up(&rdp_leader->nocb_wq);
+		swait_wake(&rdp_leader->nocb_wq);
 	}
 }
 
@@ -2404,7 +2302,7 @@ static void rcu_nocb_wait_gp(struct rcu_data *rdp)
 	 */
 	trace_rcu_future_gp(rnp, rdp, c, TPS("StartWait"));
 	for (;;) {
-		wait_event_interruptible(
+		swait_event_interruptible(
 			rnp->nocb_gp_wq[c & 0x1],
 			(d = ULONG_CMP_GE(ACCESS_ONCE(rnp->completed), c)));
 		if (likely(d))
@@ -2432,7 +2330,7 @@ wait_again:
 	/* Wait for callbacks to appear. */
 	if (!rcu_nocb_poll) {
 		trace_rcu_nocb_wake(my_rdp->rsp->name, my_rdp->cpu, "Sleep");
-		wait_event_interruptible(my_rdp->nocb_wq,
+		swait_event_interruptible(my_rdp->nocb_wq,
 				!ACCESS_ONCE(my_rdp->nocb_leader_sleep));
 		/* Memory barrier handled by smp_mb() calls below and repoll. */
 	} else if (firsttime) {
@@ -2513,7 +2411,7 @@ wait_again:
 			 * List was empty, wake up the follower.
 			 * Memory barriers supplied by atomic_long_add().
 			 */
-			wake_up(&rdp->nocb_wq);
+			swait_wake(&rdp->nocb_wq);
 		}
 	}
 
@@ -2534,7 +2432,7 @@ static void nocb_follower_wait(struct rcu_data *rdp)
 		if (!rcu_nocb_poll) {
 			trace_rcu_nocb_wake(rdp->rsp->name, rdp->cpu,
 					    "FollowerSleep");
-			wait_event_interruptible(rdp->nocb_wq,
+			swait_event_interruptible(rdp->nocb_wq,
 						 ACCESS_ONCE(rdp->nocb_follower_head));
 		} else if (firsttime) {
 			/* Don't drown trace log with "Poll"! */
@@ -2705,7 +2603,7 @@ void __init rcu_init_nohz(void)
 static void __init rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp)
 {
 	rdp->nocb_tail = &rdp->nocb_head;
-	init_waitqueue_head(&rdp->nocb_wq);
+	init_swait_head(&rdp->nocb_wq);
 	rdp->nocb_follower_tail = &rdp->nocb_follower_head;
 }
 
diff --git a/kernel/msm-3.18/kernel/rcu/update.c b/kernel/msm-3.18/kernel/rcu/update.c
index 3ef8ba586..d80edcf92 100644
--- a/kernel/msm-3.18/kernel/rcu/update.c
+++ b/kernel/msm-3.18/kernel/rcu/update.c
@@ -170,6 +170,7 @@ int rcu_read_lock_held(void)
 }
 EXPORT_SYMBOL_GPL(rcu_read_lock_held);
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 /**
  * rcu_read_lock_bh_held() - might we be in RCU-bh read-side critical section?
  *
@@ -196,6 +197,7 @@ int rcu_read_lock_bh_held(void)
 	return in_softirq() || irqs_disabled();
 }
 EXPORT_SYMBOL_GPL(rcu_read_lock_bh_held);
+#endif
 
 #endif /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
 
diff --git a/kernel/msm-3.18/kernel/relay.c b/kernel/msm-3.18/kernel/relay.c
index 5a56d3c8d..822e7dab6 100644
--- a/kernel/msm-3.18/kernel/relay.c
+++ b/kernel/msm-3.18/kernel/relay.c
@@ -339,6 +339,10 @@ static void wakeup_readers(unsigned long data)
 {
 	struct rchan_buf *buf = (struct rchan_buf *)data;
 	wake_up_interruptible(&buf->read_wait);
+	/*
+	 * Stupid polling for now:
+	 */
+	mod_timer(&buf->timer, jiffies + 1);
 }
 
 /**
@@ -356,6 +360,7 @@ static void __relay_reset(struct rchan_buf *buf, unsigned int init)
 		init_waitqueue_head(&buf->read_wait);
 		kref_init(&buf->kref);
 		setup_timer(&buf->timer, wakeup_readers, (unsigned long)buf);
+		mod_timer(&buf->timer, jiffies + 1);
 	} else
 		del_timer_sync(&buf->timer);
 
@@ -739,15 +744,6 @@ size_t relay_switch_subbuf(struct rchan_buf *buf, size_t length)
 		else
 			buf->early_bytes += buf->chan->subbuf_size -
 					    buf->padding[old_subbuf];
-		smp_mb();
-		if (waitqueue_active(&buf->read_wait))
-			/*
-			 * Calling wake_up_interruptible() from here
-			 * will deadlock if we happen to be logging
-			 * from the scheduler (trying to re-grab
-			 * rq->lock), so defer it.
-			 */
-			mod_timer(&buf->timer, jiffies + 1);
 	}
 
 	old = buf->data;
diff --git a/kernel/msm-3.18/kernel/res_counter.c b/kernel/msm-3.18/kernel/res_counter.c
index e791130f8..75715182b 100644
--- a/kernel/msm-3.18/kernel/res_counter.c
+++ b/kernel/msm-3.18/kernel/res_counter.c
@@ -59,7 +59,7 @@ static int __res_counter_charge(struct res_counter *counter, unsigned long val,
 
 	r = ret = 0;
 	*limit_fail_at = NULL;
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	for (c = counter; c != NULL; c = c->parent) {
 		spin_lock(&c->lock);
 		r = res_counter_charge_locked(c, val, force);
@@ -79,7 +79,7 @@ static int __res_counter_charge(struct res_counter *counter, unsigned long val,
 			spin_unlock(&u->lock);
 		}
 	}
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 
 	return ret;
 }
@@ -104,7 +104,7 @@ u64 res_counter_uncharge_until(struct res_counter *counter,
 	struct res_counter *c;
 	u64 ret = 0;
 
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	for (c = counter; c != top; c = c->parent) {
 		u64 r;
 		spin_lock(&c->lock);
@@ -113,7 +113,7 @@ u64 res_counter_uncharge_until(struct res_counter *counter,
 			ret = r;
 		spin_unlock(&c->lock);
 	}
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 	return ret;
 }
 
diff --git a/kernel/msm-3.18/kernel/sched/Makefile b/kernel/msm-3.18/kernel/sched/Makefile
index 15cb264f1..e1002be6f 100644
--- a/kernel/msm-3.18/kernel/sched/Makefile
+++ b/kernel/msm-3.18/kernel/sched/Makefile
@@ -18,8 +18,9 @@ obj-y += core.o fair.o rt.o
 endif
 
 obj-y += proc.o clock.o cputime.o
-obj-y += idle_task.o deadline.o stop_task.o
-obj-y += wait.o completion.o idle.o sched_avg.o
+obj-y += idle_task.o deadline.o stop_task.o fair.o
+obj-y += wait.o  wait-simple.o work-simple.o completion.o idle.o sched_avg.o
+
 obj-$(CONFIG_SMP) += cpupri.o cpudeadline.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
diff --git a/kernel/msm-3.18/kernel/sched/completion.c b/kernel/msm-3.18/kernel/sched/completion.c
index a63f4dc27..e529e5fce 100644
--- a/kernel/msm-3.18/kernel/sched/completion.c
+++ b/kernel/msm-3.18/kernel/sched/completion.c
@@ -30,10 +30,10 @@ void complete(struct completion *x)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
 	x->done++;
-	__wake_up_locked(&x->wait, TASK_NORMAL, 1);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	__swait_wake_locked(&x->wait, TASK_NORMAL, 1);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 }
 EXPORT_SYMBOL(complete);
 
@@ -50,10 +50,10 @@ void complete_all(struct completion *x)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
 	x->done += UINT_MAX/2;
-	__wake_up_locked(&x->wait, TASK_NORMAL, 0);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	__swait_wake_locked(&x->wait, TASK_NORMAL, 0);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 }
 EXPORT_SYMBOL(complete_all);
 
@@ -62,20 +62,20 @@ do_wait_for_common(struct completion *x,
 		   long (*action)(long), long timeout, int state)
 {
 	if (!x->done) {
-		DECLARE_WAITQUEUE(wait, current);
+		DEFINE_SWAITER(wait);
 
-		__add_wait_queue_tail_exclusive(&x->wait, &wait);
+		swait_prepare_locked(&x->wait, &wait);
 		do {
 			if (signal_pending_state(state, current)) {
 				timeout = -ERESTARTSYS;
 				break;
 			}
 			__set_current_state(state);
-			spin_unlock_irq(&x->wait.lock);
+			raw_spin_unlock_irq(&x->wait.lock);
 			timeout = action(timeout);
-			spin_lock_irq(&x->wait.lock);
+			raw_spin_lock_irq(&x->wait.lock);
 		} while (!x->done && timeout);
-		__remove_wait_queue(&x->wait, &wait);
+		swait_finish_locked(&x->wait, &wait);
 		if (!x->done)
 			return timeout;
 	}
@@ -89,9 +89,9 @@ __wait_for_common(struct completion *x,
 {
 	might_sleep();
 
-	spin_lock_irq(&x->wait.lock);
+	raw_spin_lock_irq(&x->wait.lock);
 	timeout = do_wait_for_common(x, action, timeout, state);
-	spin_unlock_irq(&x->wait.lock);
+	raw_spin_unlock_irq(&x->wait.lock);
 	return timeout;
 }
 
@@ -267,12 +267,12 @@ bool try_wait_for_completion(struct completion *x)
 	unsigned long flags;
 	int ret = 1;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
 	if (!x->done)
 		ret = 0;
 	else
 		x->done--;
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL(try_wait_for_completion);
@@ -290,10 +290,10 @@ bool completion_done(struct completion *x)
 	unsigned long flags;
 	int ret = 1;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
 	if (!x->done)
 		ret = 0;
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL(completion_done);
diff --git a/kernel/msm-3.18/kernel/sched/core.c b/kernel/msm-3.18/kernel/sched/core.c
index 69a351fc3..057d72c78 100644
--- a/kernel/msm-3.18/kernel/sched/core.c
+++ b/kernel/msm-3.18/kernel/sched/core.c
@@ -296,7 +296,11 @@ late_initcall(sched_init_debug);
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
+#ifndef CONFIG_PREEMPT_RT_FULL
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
+#else
+const_debug unsigned int sysctl_sched_nr_migrate = 8;
+#endif
 
 /*
  * period over which we average the RT time consumption, measured
@@ -532,6 +536,7 @@ static void init_rq_hrtick(struct rq *rq)
 
 	hrtimer_init(&rq->hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	rq->hrtick_timer.function = hrtick;
+	rq->hrtick_timer.irqsafe = 1;
 }
 #else	/* CONFIG_SCHED_HRTICK */
 static inline void hrtick_clear(struct rq *rq)
@@ -612,6 +617,52 @@ static bool set_nr_if_polling(struct task_struct *p)
 #endif
 #endif
 
+void wake_q_add(struct wake_q_head *head, struct task_struct *task)
+{
+	struct wake_q_node *node = &task->wake_q;
+
+	/*
+	 * Atomically grab the task, if ->wake_q is !nil already it means
+	 * its already queued (either by us or someone else) and will get the
+	 * wakeup due to that.
+	 *
+	 * This cmpxchg() implies a full barrier, which pairs with the write
+	 * barrier implied by the wakeup in wake_up_list().
+	 */
+	if (cmpxchg(&node->next, NULL, WAKE_Q_TAIL))
+		return;
+
+	get_task_struct(task);
+
+	/*
+	 * The head is context local, there can be no concurrency.
+	 */
+	*head->lastp = node;
+	head->lastp = &node->next;
+}
+
+void wake_up_q(struct wake_q_head *head)
+{
+	struct wake_q_node *node = head->first;
+
+	while (node != WAKE_Q_TAIL) {
+		struct task_struct *task;
+
+		task = container_of(node, struct task_struct, wake_q);
+		BUG_ON(!task);
+		/* task can safely be re-inserted now */
+		node = node->next;
+		task->wake_q.next = NULL;
+
+		/*
+		 * wake_up_process() implies a wmb() to pair with the queueing
+		 * in wake_q_add() so as not to miss wakeups.
+		 */
+		wake_up_process(task);
+		put_task_struct(task);
+	}
+}
+
 /*
  * resched_curr - mark rq's current task 'to be rescheduled now'.
  *
@@ -643,6 +694,38 @@ void resched_curr(struct rq *rq)
 		trace_sched_wake_idle_without_ipi(cpu);
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+void resched_curr_lazy(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	int cpu;
+
+	if (!sched_feat(PREEMPT_LAZY)) {
+		resched_curr(rq);
+		return;
+	}
+
+	lockdep_assert_held(&rq->lock);
+
+	if (test_tsk_need_resched(curr))
+		return;
+
+	if (test_tsk_need_resched_lazy(curr))
+		return;
+
+	set_tsk_need_resched_lazy(curr);
+
+	cpu = cpu_of(rq);
+	if (cpu == smp_processor_id())
+		return;
+
+	/* NEED_RESCHED_LAZY must be visible before we test polling */
+	smp_mb();
+	if (!tsk_is_polling(curr))
+		smp_send_reschedule(cpu);
+}
+#endif
+
 void resched_cpu(int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);
@@ -666,12 +749,14 @@ void resched_cpu(int cpu)
  */
 int get_nohz_timer_target(int pinned)
 {
-	int cpu = smp_processor_id();
+	int cpu;
 	int i;
 	struct sched_domain *sd;
 
+	preempt_disable_rt();
+	cpu = smp_processor_id();
 	if (pinned || !get_sysctl_timer_migration() || !idle_cpu(cpu))
-		return cpu;
+		goto preempt_en_rt;
 
 	rcu_read_lock();
 	for_each_domain(cpu, sd) {
@@ -684,6 +769,8 @@ int get_nohz_timer_target(int pinned)
 	}
 unlock:
 	rcu_read_unlock();
+preempt_en_rt:
+	preempt_enable_rt();
 	return cpu;
 }
 /*
@@ -761,14 +848,29 @@ static inline bool got_nohz_idle_kick(void)
 #endif /* CONFIG_NO_HZ_COMMON */
 
 #ifdef CONFIG_NO_HZ_FULL
+
+static int ksoftirqd_running(void)
+{
+	struct task_struct *softirqd;
+
+	if (!IS_ENABLED(CONFIG_PREEMPT_RT_FULL))
+		return 0;
+	softirqd = this_cpu_ksoftirqd();
+	if (softirqd && softirqd->on_rq)
+		return 1;
+	return 0;
+}
+
 bool sched_can_stop_tick(void)
 {
 	/*
 	 * More than one running task need preemption.
 	 * nr_running update is assumed to be visible
 	 * after IPI is sent from wakers.
+	 *
+	 * NOTE, RT: if ksoftirqd is awake, subtract it.
 	 */
-	if (this_rq()->nr_running > 1)
+	if (this_rq()->nr_running - ksoftirqd_running() > 1)
 		return false;
 
 	return true;
@@ -4499,6 +4601,18 @@ struct migration_arg {
 
 static int migration_cpu_stop(void *data);
 
+static bool check_task_state(struct task_struct *p, long match_state)
+{
+	bool match = false;
+
+	raw_spin_lock_irq(&p->pi_lock);
+	if (p->state == match_state || p->saved_state == match_state)
+		match = true;
+	raw_spin_unlock_irq(&p->pi_lock);
+
+	return match;
+}
+
 /*
  * wait_task_inactive - wait for a thread to unschedule.
  *
@@ -4543,7 +4657,7 @@ unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 		 * is actually now running somewhere else!
 		 */
 		while (task_running(rq, p)) {
-			if (match_state && unlikely(p->state != match_state))
+			if (match_state && !check_task_state(p, match_state))
 				return 0;
 			cpu_relax();
 		}
@@ -4558,7 +4672,8 @@ unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 		running = task_running(rq, p);
 		queued = task_on_rq_queued(p);
 		ncsw = 0;
-		if (!match_state || p->state == match_state)
+		if (!match_state || p->state == match_state ||
+		    p->saved_state == match_state)
 			ncsw = p->nvcsw | LONG_MIN; /* sets MSB */
 		task_rq_unlock(rq, p, &flags);
 
@@ -4783,10 +4898,6 @@ static inline void ttwu_activate(struct rq *rq, struct task_struct *p, int en_fl
 {
 	activate_task(rq, p, en_flags);
 	p->on_rq = TASK_ON_RQ_QUEUED;
-
-	/* if a worker is waking up, notify workqueue */
-	if (p->flags & PF_WQ_WORKER)
-		wq_worker_waking_up(p, cpu_of(rq));
 }
 
 /*
@@ -4796,9 +4907,9 @@ static void
 ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)
 {
 	check_preempt_curr(rq, p, wake_flags);
-	trace_sched_wakeup(p, true);
-
 	p->state = TASK_RUNNING;
+	trace_sched_wakeup(p);
+
 #ifdef CONFIG_SMP
 	if (p->sched_class->task_woken)
 		p->sched_class->task_woken(rq, p);
@@ -5027,8 +5138,29 @@ try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	src_cpu = cpu = task_cpu(p);
 
-	if (!(p->state & state))
+	if (!(p->state & state)) {
+		/*
+		 * The task might be running due to a spinlock sleeper
+		 * wakeup. Check the saved state and set it to running
+		 * if the wakeup condition is true.
+		 */
+		if (!(wake_flags & WF_LOCK_SLEEPER)) {
+			if (p->saved_state & state) {
+				p->saved_state = TASK_RUNNING;
+				success = 1;
+			}
+		}
 		goto out;
+	}
+
+	/*
+	 * If this is a regular wakeup, then we can unconditionally
+	 * clear the saved state of a "lock sleeper".
+	 */
+	if (!(wake_flags & WF_LOCK_SLEEPER))
+		p->saved_state = TASK_RUNNING;
+
+	trace_sched_waking(p);
 
 	success = 1; /* we're going to change ->state */
 
@@ -5122,53 +5254,6 @@ out:
 	return success;
 }
 
-/**
- * try_to_wake_up_local - try to wake up a local task with rq lock held
- * @p: the thread to be awakened
- *
- * Put @p on the run-queue if it's not already there. The caller must
- * ensure that this_rq() is locked, @p is bound to this_rq() and not
- * the current task.
- */
-static void try_to_wake_up_local(struct task_struct *p)
-{
-	struct rq *rq = task_rq(p);
-
-	if (rq != this_rq() || p == current) {
-		printk_deferred("%s: Failed to wakeup task %d (%s), rq = %p,"
-				" this_rq = %p, p = %p, current = %p\n",
-			__func__, task_pid_nr(p), p->comm, rq,
-			this_rq(), p, current);
-		return;
-	}
-
-	lockdep_assert_held(&rq->lock);
-
-	if (!raw_spin_trylock(&p->pi_lock)) {
-		raw_spin_unlock(&rq->lock);
-		raw_spin_lock(&p->pi_lock);
-		raw_spin_lock(&rq->lock);
-	}
-
-	if (!(p->state & TASK_NORMAL))
-		goto out;
-
-	if (!task_on_rq_queued(p)) {
-		u64 wallclock = sched_ktime_clock();
-
-		update_task_ravg(rq->curr, rq, TASK_UPDATE, wallclock, 0);
-		update_task_ravg(p, rq, TASK_WAKE, wallclock, 0);
-		ttwu_activate(rq, p, ENQUEUE_WAKEUP);
-		set_task_last_wake(p, wallclock);
-	}
-
-	ttwu_do_wakeup(rq, p, 0);
-	ttwu_stat(p, smp_processor_id(), 0);
-out:
-	raw_spin_unlock(&p->pi_lock);
-	/* Todo : Send cpufreq notifier */
-}
-
 /**
  * wake_up_process - Wake up a specific process
  * @p: The process to be woken up.
@@ -5183,7 +5268,7 @@ out:
  */
 int wake_up_process(struct task_struct *p)
 {
-	WARN_ON(task_is_stopped_or_traced(p));
+	WARN_ON(__task_is_stopped_or_traced(p));
 	return try_to_wake_up(p, TASK_NORMAL, 0);
 }
 EXPORT_SYMBOL(wake_up_process);
@@ -5208,6 +5293,17 @@ int wake_up_process_no_notif(struct task_struct *p)
 }
 EXPORT_SYMBOL(wake_up_process_no_notif);
 
+ * wake_up_lock_sleeper - Wake up a specific process blocked on a "sleeping lock"
+ * @p: The process to be woken up.
+ *
+ * Same as wake_up_process() above, but wake_flags=WF_LOCK_SLEEPER to indicate
+ * the nature of the wakeup.
+ */
+int wake_up_lock_sleeper(struct task_struct *p)
+{
+	return try_to_wake_up(p, TASK_ALL, WF_LOCK_SLEEPER);
+}
+
 int wake_up_state(struct task_struct *p, unsigned int state)
 {
 	return try_to_wake_up(p, state, 0);
@@ -5403,6 +5499,9 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	p->on_cpu = 0;
 #endif
 	init_task_preempt_count(p);
+#ifdef CONFIG_HAVE_PREEMPT_LAZY
+	task_thread_info(p)->preempt_lazy_count = 0;
+#endif
 #ifdef CONFIG_SMP
 	plist_node_init(&p->pushable_tasks, MAX_PRIO);
 	RB_CLEAR_NODE(&p->pushable_dl_tasks);
@@ -5557,7 +5656,7 @@ void wake_up_new_task(struct task_struct *p)
 	mark_task_starting(p);
 	activate_task(rq, p, 0);
 	p->on_rq = TASK_ON_RQ_QUEUED;
-	trace_sched_wakeup_new(p, true);
+	trace_sched_wakeup_new(p);
 	check_preempt_curr(rq, p, WF_FORK);
 #ifdef CONFIG_SMP
 	if (p->sched_class->task_woken)
@@ -5697,8 +5796,12 @@ static void finish_task_switch(struct rq *rq, struct task_struct *prev)
 	finish_arch_post_lock_switch();
 
 	fire_sched_in_preempt_notifiers(current);
+	/*
+	 * We use mmdrop_delayed() here so we don't have to do the
+	 * full __mmdrop() when we are the last user.
+	 */
 	if (mm)
-		mmdrop(mm);
+		mmdrop_delayed(mm);
 	if (unlikely(prev_state == TASK_DEAD)) {
 		if (prev->sched_class->task_dead)
 			prev->sched_class->task_dead(prev);
@@ -6073,16 +6176,6 @@ u64 scheduler_tick_max_deferment(void)
 }
 #endif
 
-notrace unsigned long get_parent_ip(unsigned long addr)
-{
-	if (in_lock_functions(addr)) {
-		addr = CALLER_ADDR2;
-		if (in_lock_functions(addr))
-			addr = CALLER_ADDR3;
-	}
-	return addr;
-}
-
 #if defined(CONFIG_PREEMPT) && (defined(CONFIG_DEBUG_PREEMPT) || \
 				defined(CONFIG_PREEMPT_TRACER))
 
@@ -6104,7 +6197,7 @@ void preempt_count_add(int val)
 				PREEMPT_MASK - 10);
 #endif
 	if (preempt_count() == val) {
-		unsigned long ip = get_parent_ip(CALLER_ADDR1);
+		unsigned long ip = get_lock_parent_ip();
 #ifdef CONFIG_DEBUG_PREEMPT
 		current->preempt_disable_ip = ip;
 #endif
@@ -6131,7 +6224,7 @@ void preempt_count_sub(int val)
 #endif
 
 	if (preempt_count() == val)
-		trace_preempt_on(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));
+		trace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());
 	__preempt_count_sub(val);
 }
 EXPORT_SYMBOL(preempt_count_sub);
@@ -6191,6 +6284,133 @@ static inline void schedule_debug(struct task_struct *prev)
 	schedstat_inc(this_rq(), sched_count);
 }
 
+#if defined(CONFIG_PREEMPT_RT_FULL) && defined(CONFIG_SMP)
+#define MIGRATE_DISABLE_SET_AFFIN	(1<<30) /* Can't make a negative */
+#define migrate_disabled_updated(p)	((p)->migrate_disable & MIGRATE_DISABLE_SET_AFFIN)
+#define migrate_disable_count(p)	((p)->migrate_disable & ~MIGRATE_DISABLE_SET_AFFIN)
+
+static inline void update_migrate_disable(struct task_struct *p)
+{
+	const struct cpumask *mask;
+
+	if (likely(!p->migrate_disable))
+		return;
+
+	/* Did we already update affinity? */
+	if (unlikely(migrate_disabled_updated(p)))
+		return;
+
+	/*
+	 * Since this is always current we can get away with only locking
+	 * rq->lock, the ->cpus_allowed value can normally only be changed
+	 * while holding both p->pi_lock and rq->lock, but seeing that this
+	 * is current, we cannot actually be waking up, so all code that
+	 * relies on serialization against p->pi_lock is out of scope.
+	 *
+	 * Having rq->lock serializes us against things like
+	 * set_cpus_allowed_ptr() that can still happen concurrently.
+	 */
+	mask = tsk_cpus_allowed(p);
+
+	if (p->sched_class->set_cpus_allowed)
+		p->sched_class->set_cpus_allowed(p, mask);
+	/* mask==cpumask_of(task_cpu(p)) which has a cpumask_weight==1 */
+	p->nr_cpus_allowed = 1;
+
+	/* Let migrate_enable know to fix things back up */
+	p->migrate_disable |= MIGRATE_DISABLE_SET_AFFIN;
+}
+
+void migrate_disable(void)
+{
+	struct task_struct *p = current;
+
+	if (in_atomic() || irqs_disabled()) {
+#ifdef CONFIG_SCHED_DEBUG
+		p->migrate_disable_atomic++;
+#endif
+		return;
+	}
+
+#ifdef CONFIG_SCHED_DEBUG
+	if (unlikely(p->migrate_disable_atomic)) {
+		tracing_off();
+		WARN_ON_ONCE(1);
+	}
+#endif
+
+	if (p->migrate_disable) {
+		p->migrate_disable++;
+		return;
+	}
+
+	preempt_disable();
+	preempt_lazy_disable();
+	pin_current_cpu();
+	p->migrate_disable = 1;
+	preempt_enable();
+}
+EXPORT_SYMBOL(migrate_disable);
+
+void migrate_enable(void)
+{
+	struct task_struct *p = current;
+	const struct cpumask *mask;
+	unsigned long flags;
+	struct rq *rq;
+
+	if (in_atomic() || irqs_disabled()) {
+#ifdef CONFIG_SCHED_DEBUG
+		p->migrate_disable_atomic--;
+#endif
+		return;
+	}
+
+#ifdef CONFIG_SCHED_DEBUG
+	if (unlikely(p->migrate_disable_atomic)) {
+		tracing_off();
+		WARN_ON_ONCE(1);
+	}
+#endif
+	WARN_ON_ONCE(p->migrate_disable <= 0);
+
+	if (migrate_disable_count(p) > 1) {
+		p->migrate_disable--;
+		return;
+	}
+
+	preempt_disable();
+	if (unlikely(migrate_disabled_updated(p))) {
+		/*
+		 * Undo whatever update_migrate_disable() did, also see there
+		 * about locking.
+		 */
+		rq = this_rq();
+		raw_spin_lock_irqsave(&rq->lock, flags);
+
+		/*
+		 * Clearing migrate_disable causes tsk_cpus_allowed to
+		 * show the tasks original cpu affinity.
+		 */
+		p->migrate_disable = 0;
+		mask = tsk_cpus_allowed(p);
+		if (p->sched_class->set_cpus_allowed)
+			p->sched_class->set_cpus_allowed(p, mask);
+		p->nr_cpus_allowed = cpumask_weight(mask);
+		raw_spin_unlock_irqrestore(&rq->lock, flags);
+	} else
+		p->migrate_disable = 0;
+
+	unpin_current_cpu();
+	preempt_enable();
+	preempt_lazy_enable();
+}
+EXPORT_SYMBOL(migrate_enable);
+#else
+static inline void update_migrate_disable(struct task_struct *p) { }
+#define migrate_disabled_updated(p)		0
+#endif
+
 /*
  * Pick up the highest-prio task:
  */
@@ -6295,6 +6515,8 @@ need_resched:
 	smp_mb__before_spinlock();
 	raw_spin_lock_irq(&rq->lock);
 
+	update_migrate_disable(prev);
+
 	switch_count = &prev->nivcsw;
 	if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {
 		if (unlikely(signal_pending_state(prev->state, prev))) {
@@ -6302,19 +6524,6 @@ need_resched:
 		} else {
 			deactivate_task(rq, prev, DEQUEUE_SLEEP);
 			prev->on_rq = 0;
-
-			/*
-			 * If a worker went to sleep, notify and ask workqueue
-			 * whether it wants to wake up a task to maintain
-			 * concurrency.
-			 */
-			if (prev->flags & PF_WQ_WORKER) {
-				struct task_struct *to_wakeup;
-
-				to_wakeup = wq_worker_sleeping(prev, cpu);
-				if (to_wakeup)
-					try_to_wake_up_local(to_wakeup);
-			}
 		}
 		switch_count = &prev->nvcsw;
 	}
@@ -6327,6 +6536,7 @@ need_resched:
 	update_task_ravg(prev, rq, PUT_PREV_TASK, wallclock, 0);
 	update_task_ravg(next, rq, PICK_NEXT_TASK, wallclock, 0);
 	clear_tsk_need_resched(prev);
+	clear_tsk_need_resched_lazy(prev);
 	clear_preempt_need_resched();
 	rq->skip_clock_update = 0;
 
@@ -6360,8 +6570,19 @@ need_resched:
 
 static inline void sched_submit_work(struct task_struct *tsk)
 {
-	if (!tsk->state || tsk_is_pi_blocked(tsk))
+	if (!tsk->state)
 		return;
+	/*
+	 * If a worker went to sleep, notify and ask workqueue whether
+	 * it wants to wake up a task to maintain concurrency.
+	 */
+	if (tsk->flags & PF_WQ_WORKER)
+		wq_worker_sleeping(tsk);
+
+
+	if (tsk_is_pi_blocked(tsk))
+		return;
+
 	/*
 	 * If we are going to sleep and we have plugged IO queued,
 	 * make sure to submit it to avoid deadlocks.
@@ -6370,12 +6591,19 @@ static inline void sched_submit_work(struct task_struct *tsk)
 		blk_schedule_flush_plug(tsk);
 }
 
+static inline void sched_update_worker(struct task_struct *tsk)
+{
+	if (tsk->flags & PF_WQ_WORKER)
+		wq_worker_running(tsk);
+}
+
 asmlinkage __visible void __sched schedule(void)
 {
 	struct task_struct *tsk = current;
 
 	sched_submit_work(tsk);
 	__schedule();
+	sched_update_worker(tsk);
 }
 EXPORT_SYMBOL(schedule);
 
@@ -6410,6 +6638,30 @@ void __sched schedule_preempt_disabled(void)
 	preempt_disable();
 }
 
+#ifdef CONFIG_PREEMPT_LAZY
+/*
+ * If TIF_NEED_RESCHED is then we allow to be scheduled away since this is
+ * set by a RT task. Oterwise we try to avoid beeing scheduled out as long as
+ * preempt_lazy_count counter >0.
+ */
+static __always_inline int preemptible_lazy(void)
+{
+	if (test_thread_flag(TIF_NEED_RESCHED))
+		return 1;
+	if (current_thread_info()->preempt_lazy_count)
+		return 0;
+	return 1;
+}
+
+#else
+
+static inline int preemptible_lazy(void)
+{
+	return 1;
+}
+
+#endif
+
 #ifdef CONFIG_PREEMPT
 /*
  * this is the entry point to schedule() from in-kernel preemption
@@ -6424,10 +6676,21 @@ asmlinkage __visible void __sched notrace preempt_schedule(void)
 	 */
 	if (likely(!preemptible()))
 		return;
+	if (!preemptible_lazy())
+		return;
 
 	do {
 		__preempt_count_add(PREEMPT_ACTIVE);
+		/*
+		 * The add/subtract must not be traced by the function
+		 * tracer. But we still want to account for the
+		 * preempt off latency tracer. Since the _notrace versions
+		 * of add/subtract skip the accounting for latency tracer
+		 * we must force it manually.
+		 */
+		start_critical_timings();
 		__schedule();
+		stop_critical_timings();
 		__preempt_count_sub(PREEMPT_ACTIVE);
 
 		/*
@@ -7746,9 +8009,16 @@ SYSCALL_DEFINE0(sched_yield)
 
 static void __cond_resched(void)
 {
-	__preempt_count_add(PREEMPT_ACTIVE);
-	__schedule();
-	__preempt_count_sub(PREEMPT_ACTIVE);
+	do {
+		__preempt_count_add(PREEMPT_ACTIVE);
+		__schedule();
+		__preempt_count_sub(PREEMPT_ACTIVE);
+		/*
+		 * Check again in case we missed a preemption
+		 * opportunity between schedule and now.
+		 */
+		barrier();
+	} while (need_resched());
 }
 
 int __sched _cond_resched(void)
@@ -7789,6 +8059,7 @@ int __cond_resched_lock(spinlock_t *lock)
 }
 EXPORT_SYMBOL(__cond_resched_lock);
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 int __sched __cond_resched_softirq(void)
 {
 	BUG_ON(!in_softirq());
@@ -7802,6 +8073,7 @@ int __sched __cond_resched_softirq(void)
 	return 0;
 }
 EXPORT_SYMBOL(__cond_resched_softirq);
+#endif
 
 /**
  * yield - yield the current processor to other threads.
@@ -8165,7 +8437,9 @@ void init_idle(struct task_struct *idle, int cpu)
 
 	/* Set the preempt count _outside_ the spinlocks! */
 	init_idle_preempt_count(idle, cpu);
-
+#ifdef CONFIG_HAVE_PREEMPT_LAZY
+	task_thread_info(idle)->preempt_lazy_count = 0;
+#endif
 	/*
 	 * The idle tasks have their own, simple scheduling class:
 	 */
@@ -8209,11 +8483,91 @@ static struct rq *move_queued_task(struct task_struct *p, int new_cpu)
 
 void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
 {
-	if (p->sched_class && p->sched_class->set_cpus_allowed)
-		p->sched_class->set_cpus_allowed(p, new_mask);
+	if (!migrate_disabled_updated(p)) {
+		if (p->sched_class && p->sched_class->set_cpus_allowed)
+			p->sched_class->set_cpus_allowed(p, new_mask);
+		p->nr_cpus_allowed = cpumask_weight(new_mask);
+	}
 
 	cpumask_copy(&p->cpus_allowed, new_mask);
-	p->nr_cpus_allowed = cpumask_weight(new_mask);
+}
+
+static DEFINE_PER_CPU(struct cpumask, sched_cpumasks);
+static DEFINE_MUTEX(sched_down_mutex);
+static cpumask_t sched_down_cpumask;
+
+void tell_sched_cpu_down_begin(int cpu)
+{
+	mutex_lock(&sched_down_mutex);
+	cpumask_set_cpu(cpu, &sched_down_cpumask);
+	mutex_unlock(&sched_down_mutex);
+}
+
+void tell_sched_cpu_down_done(int cpu)
+{
+	mutex_lock(&sched_down_mutex);
+	cpumask_clear_cpu(cpu, &sched_down_cpumask);
+	mutex_unlock(&sched_down_mutex);
+}
+
+/**
+ * migrate_me - try to move the current task off this cpu
+ *
+ * Used by the pin_current_cpu() code to try to get tasks
+ * to move off the current CPU as it is going down.
+ * It will only move the task if the task isn't pinned to
+ * the CPU (with migrate_disable, affinity or NO_SETAFFINITY)
+ * and the task has to be in a RUNNING state. Otherwise the
+ * movement of the task will wake it up (change its state
+ * to running) when the task did not expect it.
+ *
+ * Returns 1 if it succeeded in moving the current task
+ *         0 otherwise.
+ */
+int migrate_me(void)
+{
+	struct task_struct *p = current;
+	struct migration_arg arg;
+	struct cpumask *cpumask;
+	struct cpumask *mask;
+	unsigned long flags;
+	unsigned int dest_cpu;
+	struct rq *rq;
+
+	/*
+	 * We can not migrate tasks bounded to a CPU or tasks not
+	 * running. The movement of the task will wake it up.
+	 */
+	if (p->flags & PF_NO_SETAFFINITY || p->state)
+		return 0;
+
+	mutex_lock(&sched_down_mutex);
+	rq = task_rq_lock(p, &flags);
+
+	cpumask = &__get_cpu_var(sched_cpumasks);
+	mask = &p->cpus_allowed;
+
+	cpumask_andnot(cpumask, mask, &sched_down_cpumask);
+
+	if (!cpumask_weight(cpumask)) {
+		/* It's only on this CPU? */
+		task_rq_unlock(rq, p, &flags);
+		mutex_unlock(&sched_down_mutex);
+		return 0;
+	}
+
+	dest_cpu = cpumask_any_and(cpu_active_mask, cpumask);
+
+	arg.task = p;
+	arg.dest_cpu = dest_cpu;
+
+	task_rq_unlock(rq, p, &flags);
+
+	stop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);
+	tlb_migrate_finish(p->mm);
+	mutex_unlock(&sched_down_mutex);
+
+	return 1;
 }
 
 /*
@@ -8260,7 +8614,7 @@ int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
 	do_set_cpus_allowed(p, new_mask);
 
 	/* Can the task run on the task's current CPU? If so, we're done */
-	if (cpumask_test_cpu(task_cpu(p), new_mask))
+	if (cpumask_test_cpu(task_cpu(p), new_mask) || __migrate_disabled(p))
 		goto out;
 
 	if (task_running(rq, p) || p->state == TASK_WAKING) {
@@ -8421,6 +8775,8 @@ static int migration_cpu_stop(void *data)
 
 #ifdef CONFIG_HOTPLUG_CPU
 
+static DEFINE_PER_CPU(struct mm_struct *, idle_last_mm);
+
 /*
  * Ensures that the idle task is using init_mm right before its cpu goes
  * offline.
@@ -8435,7 +8791,11 @@ void idle_task_exit(void)
 		switch_mm(mm, &init_mm, current);
 		finish_arch_post_lock_switch();
 	}
-	mmdrop(mm);
+	/*
+	 * Defer the cleanup to an alive cpu. On RT we can neither
+	 * call mmdrop() nor mmdrop_delayed() from here.
+	 */
+	per_cpu(idle_last_mm, smp_processor_id()) = mm;
 }
 
 /*
@@ -8784,6 +9144,10 @@ migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)
 	case CPU_DEAD:
 		clear_hmp_request(cpu);
 		calc_load_migrate(rq);
+		if (per_cpu(idle_last_mm, cpu)) {
+			mmdrop(per_cpu(idle_last_mm, cpu));
+			per_cpu(idle_last_mm, cpu) = NULL;
+		}
 		break;
 #endif
 	}
@@ -10791,7 +11155,8 @@ void __init sched_init(void)
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 static inline int preempt_count_equals(int preempt_offset)
 {
-	int nested = (preempt_count() & ~PREEMPT_ACTIVE) + rcu_preempt_depth();
+	int nested = (preempt_count() & ~PREEMPT_ACTIVE) +
+		sched_rcu_preempt_depth();
 
 	return (nested == preempt_offset);
 }
diff --git a/kernel/msm-3.18/kernel/sched/cputime.c b/kernel/msm-3.18/kernel/sched/cputime.c
index 3008d549a..1058451ce 100644
--- a/kernel/msm-3.18/kernel/sched/cputime.c
+++ b/kernel/msm-3.18/kernel/sched/cputime.c
@@ -697,37 +697,45 @@ static void __vtime_account_system(struct task_struct *tsk)
 
 void vtime_account_system(struct task_struct *tsk)
 {
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
+	write_seqcount_begin(&tsk->vtime_seq);
 	__vtime_account_system(tsk);
-	write_sequnlock(&tsk->vtime_seqlock);
+	write_seqcount_end(&tsk->vtime_seq);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 
 void vtime_gen_account_irq_exit(struct task_struct *tsk)
 {
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
+	write_seqcount_begin(&tsk->vtime_seq);
 	__vtime_account_system(tsk);
 	if (context_tracking_in_user())
 		tsk->vtime_snap_whence = VTIME_USER;
-	write_sequnlock(&tsk->vtime_seqlock);
+	write_seqcount_end(&tsk->vtime_seq);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 
 void vtime_account_user(struct task_struct *tsk)
 {
 	cputime_t delta_cpu;
 
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
+	write_seqcount_begin(&tsk->vtime_seq);
 	delta_cpu = get_vtime_delta(tsk);
 	tsk->vtime_snap_whence = VTIME_SYS;
 	account_user_time(tsk, delta_cpu, cputime_to_scaled(delta_cpu));
-	write_sequnlock(&tsk->vtime_seqlock);
+	write_seqcount_end(&tsk->vtime_seq);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 
 void vtime_user_enter(struct task_struct *tsk)
 {
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
+	write_seqcount_begin(&tsk->vtime_seq);
 	__vtime_account_system(tsk);
 	tsk->vtime_snap_whence = VTIME_USER;
-	write_sequnlock(&tsk->vtime_seqlock);
+	write_seqcount_end(&tsk->vtime_seq);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 
 void vtime_guest_enter(struct task_struct *tsk)
@@ -739,19 +747,23 @@ void vtime_guest_enter(struct task_struct *tsk)
 	 * synchronization against the reader (task_gtime())
 	 * that can thus safely catch up with a tickless delta.
 	 */
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
+	write_seqcount_begin(&tsk->vtime_seq);
 	__vtime_account_system(tsk);
 	current->flags |= PF_VCPU;
-	write_sequnlock(&tsk->vtime_seqlock);
+	write_seqcount_end(&tsk->vtime_seq);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 EXPORT_SYMBOL_GPL(vtime_guest_enter);
 
 void vtime_guest_exit(struct task_struct *tsk)
 {
-	write_seqlock(&tsk->vtime_seqlock);
+	raw_spin_lock(&tsk->vtime_lock);
+	write_seqcount_begin(&tsk->vtime_seq);
 	__vtime_account_system(tsk);
 	current->flags &= ~PF_VCPU;
-	write_sequnlock(&tsk->vtime_seqlock);
+	write_seqcount_end(&tsk->vtime_seq);
+	raw_spin_unlock(&tsk->vtime_lock);
 }
 EXPORT_SYMBOL_GPL(vtime_guest_exit);
 
@@ -764,24 +776,30 @@ void vtime_account_idle(struct task_struct *tsk)
 
 void arch_vtime_task_switch(struct task_struct *prev)
 {
-	write_seqlock(&prev->vtime_seqlock);
+	raw_spin_lock(&prev->vtime_lock);
+	write_seqcount_begin(&prev->vtime_seq);
 	prev->vtime_snap_whence = VTIME_SLEEPING;
-	write_sequnlock(&prev->vtime_seqlock);
+	write_seqcount_end(&prev->vtime_seq);
+	raw_spin_unlock(&prev->vtime_lock);
 
-	write_seqlock(&current->vtime_seqlock);
+	raw_spin_lock(&current->vtime_lock);
+	write_seqcount_begin(&current->vtime_seq);
 	current->vtime_snap_whence = VTIME_SYS;
 	current->vtime_snap = sched_clock_cpu(smp_processor_id());
-	write_sequnlock(&current->vtime_seqlock);
+	write_seqcount_end(&current->vtime_seq);
+	raw_spin_unlock(&current->vtime_lock);
 }
 
 void vtime_init_idle(struct task_struct *t, int cpu)
 {
 	unsigned long flags;
 
-	write_seqlock_irqsave(&t->vtime_seqlock, flags);
+	raw_spin_lock_irqsave(&t->vtime_lock, flags);
+	write_seqcount_begin(&t->vtime_seq);
 	t->vtime_snap_whence = VTIME_SYS;
 	t->vtime_snap = sched_clock_cpu(cpu);
-	write_sequnlock_irqrestore(&t->vtime_seqlock, flags);
+	write_seqcount_end(&t->vtime_seq);
+	raw_spin_unlock_irqrestore(&t->vtime_lock, flags);
 }
 
 cputime_t task_gtime(struct task_struct *t)
@@ -790,13 +808,13 @@ cputime_t task_gtime(struct task_struct *t)
 	cputime_t gtime;
 
 	do {
-		seq = read_seqbegin(&t->vtime_seqlock);
+		seq = read_seqcount_begin(&t->vtime_seq);
 
 		gtime = t->gtime;
 		if (t->flags & PF_VCPU)
 			gtime += vtime_delta(t);
 
-	} while (read_seqretry(&t->vtime_seqlock, seq));
+	} while (read_seqcount_retry(&t->vtime_seq, seq));
 
 	return gtime;
 }
@@ -819,7 +837,7 @@ fetch_task_cputime(struct task_struct *t,
 		*udelta = 0;
 		*sdelta = 0;
 
-		seq = read_seqbegin(&t->vtime_seqlock);
+		seq = read_seqcount_begin(&t->vtime_seq);
 
 		if (u_dst)
 			*u_dst = *u_src;
@@ -843,7 +861,7 @@ fetch_task_cputime(struct task_struct *t,
 			if (t->vtime_snap_whence == VTIME_SYS)
 				*sdelta = delta;
 		}
-	} while (read_seqretry(&t->vtime_seqlock, seq));
+	} while (read_seqcount_retry(&t->vtime_seq, seq));
 }
 
 
diff --git a/kernel/msm-3.18/kernel/sched/deadline.c b/kernel/msm-3.18/kernel/sched/deadline.c
index 570c553e1..5cbf41302 100644
--- a/kernel/msm-3.18/kernel/sched/deadline.c
+++ b/kernel/msm-3.18/kernel/sched/deadline.c
@@ -570,6 +570,7 @@ void init_dl_task_timer(struct sched_dl_entity *dl_se)
 
 	hrtimer_init(timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	timer->function = dl_task_timer;
+	timer->irqsafe = 1;
 }
 
 static
diff --git a/kernel/msm-3.18/kernel/sched/debug.c b/kernel/msm-3.18/kernel/sched/debug.c
index ff0692ba8..bd7138268 100644
--- a/kernel/msm-3.18/kernel/sched/debug.c
+++ b/kernel/msm-3.18/kernel/sched/debug.c
@@ -268,6 +268,9 @@ void print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq)
 	P(rt_throttled);
 	PN(rt_time);
 	PN(rt_runtime);
+#ifdef CONFIG_SMP
+	P(rt_nr_migratory);
+#endif
 
 #undef PN
 #undef P
@@ -701,6 +704,10 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 #endif
 	P(policy);
 	P(prio);
+#ifdef CONFIG_PREEMPT_RT_FULL
+	P(migrate_disable);
+#endif
+	P(nr_cpus_allowed);
 #undef PN
 #undef __PN
 #undef P
diff --git a/kernel/msm-3.18/kernel/sched/fair.c b/kernel/msm-3.18/kernel/sched/fair.c
index 0c608833d..354094758 100644
--- a/kernel/msm-3.18/kernel/sched/fair.c
+++ b/kernel/msm-3.18/kernel/sched/fair.c
@@ -4929,7 +4929,7 @@ check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 	ideal_runtime = sched_slice(cfs_rq, curr);
 	delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
 	if (delta_exec > ideal_runtime) {
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 		/*
 		 * The current task ran long enough, ensure it doesn't get
 		 * re-elected due to buddy favours.
@@ -4953,7 +4953,7 @@ check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 		return;
 
 	if (delta > ideal_runtime)
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 }
 
 static void
@@ -5093,7 +5093,7 @@ entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
 	 * validating it and just reschedule.
 	 */
 	if (queued) {
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 		return;
 	}
 	/*
@@ -5284,7 +5284,7 @@ static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)
 	 * hierarchy can be throttled
 	 */
 	if (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))
-		resched_curr(rq_of(cfs_rq));
+		resched_curr_lazy(rq_of(cfs_rq));
 }
 
 static __always_inline
@@ -5952,7 +5952,7 @@ static void hrtick_start_fair(struct rq *rq, struct task_struct *p)
 
 		if (delta < 0) {
 			if (rq->curr == p)
-				resched_curr(rq);
+				resched_curr_lazy(rq);
 			return;
 		}
 		hrtick_start(rq, delta);
@@ -6831,7 +6831,7 @@ static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_
 	return;
 
 preempt:
-	resched_curr(rq);
+	resched_curr_lazy(rq);
 	/*
 	 * Only set the backward buddy when the current task is still
 	 * on the rq. This can happen when a wakeup gets interleaved
@@ -10064,7 +10064,7 @@ static void task_fork_fair(struct task_struct *p)
 		 * 'current' within the tree based on its new key value.
 		 */
 		swap(curr->vruntime, se->vruntime);
-		resched_curr(rq);
+		resched_curr_lazy(rq);
 	}
 
 	se->vruntime -= cfs_rq->min_vruntime;
@@ -10089,7 +10089,7 @@ prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)
 	 */
 	if (rq->curr == p) {
 		if (p->prio > oldprio)
-			resched_curr(rq);
+			resched_curr_lazy(rq);
 	} else
 		check_preempt_curr(rq, p, 0);
 }
diff --git a/kernel/msm-3.18/kernel/sched/features.h b/kernel/msm-3.18/kernel/sched/features.h
index f38fd2133..b20b9a9b4 100644
--- a/kernel/msm-3.18/kernel/sched/features.h
+++ b/kernel/msm-3.18/kernel/sched/features.h
@@ -50,12 +50,18 @@ SCHED_FEAT(LB_BIAS, true)
  */
 SCHED_FEAT(NONTASK_CAPACITY, true)
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+SCHED_FEAT(TTWU_QUEUE, false)
+# ifdef CONFIG_PREEMPT_LAZY
+SCHED_FEAT(PREEMPT_LAZY, true)
+# endif
+#else
 /*
  * Queue remote wakeups on the target CPU and process them
  * using the scheduler IPI. Reduces rq->lock contention/bounces.
  */
-SCHED_FEAT(TTWU_QUEUE, false)
-
+SCHED_FEAT(TTWU_QUEUE, true)
+#endif
 SCHED_FEAT(FORCE_SD_OVERLAP, false)
 SCHED_FEAT(RT_RUNTIME_SHARE, true)
 SCHED_FEAT(LB_MIN, false)
diff --git a/kernel/msm-3.18/kernel/sched/rt.c b/kernel/msm-3.18/kernel/sched/rt.c
index ce5fe4aa0..6647aeca1 100644
--- a/kernel/msm-3.18/kernel/sched/rt.c
+++ b/kernel/msm-3.18/kernel/sched/rt.c
@@ -44,6 +44,7 @@ void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)
 
 	hrtimer_init(&rt_b->rt_period_timer,
 			CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	rt_b->rt_period_timer.irqsafe = 1;
 	rt_b->rt_period_timer.function = sched_rt_period_timer;
 }
 
diff --git a/kernel/msm-3.18/kernel/sched/sched.h b/kernel/msm-3.18/kernel/sched/sched.h
index 63f279710..e7fb50069 100644
--- a/kernel/msm-3.18/kernel/sched/sched.h
+++ b/kernel/msm-3.18/kernel/sched/sched.h
@@ -1617,6 +1617,7 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 #define WF_FORK		0x02		/* child wakeup after fork */
 #define WF_MIGRATED	0x4		/* internal use, task got migrated */
 #define WF_NO_NOTIFIER	0x08		/* do not notify governor */
+#define WF_LOCK_SLEEPER	0x10		/* wakeup spinlock "sleeper" */
 
 /*
  * To aid in avoiding the subversion of "niceness" due to uneven distribution
@@ -1843,6 +1844,15 @@ extern void init_sched_dl_class(void);
 extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
 
+#ifdef CONFIG_PREEMPT_LAZY
+extern void resched_curr_lazy(struct rq *rq);
+#else
+static inline void resched_curr_lazy(struct rq *rq)
+{
+	resched_curr(rq);
+}
+#endif
+
 extern struct rt_bandwidth def_rt_bandwidth;
 extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
 
diff --git a/kernel/msm-3.18/kernel/sched/wait-simple.c b/kernel/msm-3.18/kernel/sched/wait-simple.c
new file mode 100644
index 000000000..7dfa86d1f
--- /dev/null
+++ b/kernel/msm-3.18/kernel/sched/wait-simple.c
@@ -0,0 +1,115 @@
+/*
+ * Simple waitqueues without fancy flags and callbacks
+ *
+ * (C) 2011 Thomas Gleixner <tglx@linutronix.de>
+ *
+ * Based on kernel/wait.c
+ *
+ * For licencing details see kernel-base/COPYING
+ */
+#include <linux/init.h>
+#include <linux/export.h>
+#include <linux/sched.h>
+#include <linux/wait-simple.h>
+
+/* Adds w to head->list. Must be called with head->lock locked. */
+static inline void __swait_enqueue(struct swait_head *head, struct swaiter *w)
+{
+	list_add(&w->node, &head->list);
+	/* We can't let the condition leak before the setting of head */
+	smp_mb();
+}
+
+/* Removes w from head->list. Must be called with head->lock locked. */
+static inline void __swait_dequeue(struct swaiter *w)
+{
+	list_del_init(&w->node);
+}
+
+void __init_swait_head(struct swait_head *head, struct lock_class_key *key)
+{
+	raw_spin_lock_init(&head->lock);
+	lockdep_set_class(&head->lock, key);
+	INIT_LIST_HEAD(&head->list);
+}
+EXPORT_SYMBOL(__init_swait_head);
+
+void swait_prepare_locked(struct swait_head *head, struct swaiter *w)
+{
+	w->task = current;
+	if (list_empty(&w->node))
+		__swait_enqueue(head, w);
+}
+
+void swait_prepare(struct swait_head *head, struct swaiter *w, int state)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&head->lock, flags);
+	swait_prepare_locked(head, w);
+	__set_current_state(state);
+	raw_spin_unlock_irqrestore(&head->lock, flags);
+}
+EXPORT_SYMBOL(swait_prepare);
+
+void swait_finish_locked(struct swait_head *head, struct swaiter *w)
+{
+	__set_current_state(TASK_RUNNING);
+	if (w->task)
+		__swait_dequeue(w);
+}
+
+void swait_finish(struct swait_head *head, struct swaiter *w)
+{
+	unsigned long flags;
+
+	__set_current_state(TASK_RUNNING);
+	if (w->task) {
+		raw_spin_lock_irqsave(&head->lock, flags);
+		__swait_dequeue(w);
+		raw_spin_unlock_irqrestore(&head->lock, flags);
+	}
+}
+EXPORT_SYMBOL(swait_finish);
+
+unsigned int
+__swait_wake_locked(struct swait_head *head, unsigned int state, unsigned int num)
+{
+	struct swaiter *curr, *next;
+	int woken = 0;
+
+	list_for_each_entry_safe(curr, next, &head->list, node) {
+		if (wake_up_state(curr->task, state)) {
+			__swait_dequeue(curr);
+			/*
+			 * The waiting task can free the waiter as
+			 * soon as curr->task = NULL is written,
+			 * without taking any locks. A memory barrier
+			 * is required here to prevent the following
+			 * store to curr->task from getting ahead of
+			 * the dequeue operation.
+			 */
+			smp_wmb();
+			curr->task = NULL;
+			if (++woken == num)
+				break;
+		}
+	}
+	return woken;
+}
+
+unsigned int
+__swait_wake(struct swait_head *head, unsigned int state, unsigned int num)
+{
+	unsigned long flags;
+	int woken;
+
+	if (!swaitqueue_active(head))
+		return 0;
+
+	raw_spin_lock_irqsave(&head->lock, flags);
+	woken = __swait_wake_locked(head, state, num);
+	raw_spin_unlock_irqrestore(&head->lock, flags);
+	return woken;
+}
+EXPORT_SYMBOL(__swait_wake);
diff --git a/kernel/msm-3.18/kernel/sched/work-simple.c b/kernel/msm-3.18/kernel/sched/work-simple.c
new file mode 100644
index 000000000..c996f755d
--- /dev/null
+++ b/kernel/msm-3.18/kernel/sched/work-simple.c
@@ -0,0 +1,172 @@
+/*
+ * Copyright (C) 2014 BMW Car IT GmbH, Daniel Wagner daniel.wagner@bmw-carit.de
+ *
+ * Provides a framework for enqueuing callbacks from irq context
+ * PREEMPT_RT_FULL safe. The callbacks are executed in kthread context.
+ */
+
+#include <linux/wait-simple.h>
+#include <linux/work-simple.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+
+#define SWORK_EVENT_PENDING     (1 << 0)
+
+static DEFINE_MUTEX(worker_mutex);
+static struct sworker *glob_worker;
+
+struct sworker {
+	struct list_head events;
+	struct swait_head wq;
+
+	raw_spinlock_t lock;
+
+	struct task_struct *task;
+	int refs;
+};
+
+static bool swork_readable(struct sworker *worker)
+{
+	bool r;
+
+	if (kthread_should_stop())
+		return true;
+
+	raw_spin_lock_irq(&worker->lock);
+	r = !list_empty(&worker->events);
+	raw_spin_unlock_irq(&worker->lock);
+
+	return r;
+}
+
+static int swork_kthread(void *arg)
+{
+	struct sworker *worker = arg;
+
+	for (;;) {
+		swait_event_interruptible(worker->wq,
+					swork_readable(worker));
+		if (kthread_should_stop())
+			break;
+
+		raw_spin_lock_irq(&worker->lock);
+		while (!list_empty(&worker->events)) {
+			struct swork_event *sev;
+
+			sev = list_first_entry(&worker->events,
+					struct swork_event, item);
+			list_del(&sev->item);
+			raw_spin_unlock_irq(&worker->lock);
+
+			WARN_ON_ONCE(!test_and_clear_bit(SWORK_EVENT_PENDING,
+							 &sev->flags));
+			sev->func(sev);
+			raw_spin_lock_irq(&worker->lock);
+		}
+		raw_spin_unlock_irq(&worker->lock);
+	}
+	return 0;
+}
+
+static struct sworker *swork_create(void)
+{
+	struct sworker *worker;
+
+	worker = kzalloc(sizeof(*worker), GFP_KERNEL);
+	if (!worker)
+		return ERR_PTR(-ENOMEM);
+
+	INIT_LIST_HEAD(&worker->events);
+	raw_spin_lock_init(&worker->lock);
+	init_swait_head(&worker->wq);
+
+	worker->task = kthread_run(swork_kthread, worker, "kswork");
+	if (IS_ERR(worker->task)) {
+		kfree(worker);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	return worker;
+}
+
+static void swork_destroy(struct sworker *worker)
+{
+	kthread_stop(worker->task);
+
+	WARN_ON(!list_empty(&worker->events));
+	kfree(worker);
+}
+
+/**
+ * swork_queue - queue swork
+ *
+ * Returns %false if @work was already on a queue, %true otherwise.
+ *
+ * The work is queued and processed on a random CPU
+ */
+bool swork_queue(struct swork_event *sev)
+{
+	unsigned long flags;
+
+	if (test_and_set_bit(SWORK_EVENT_PENDING, &sev->flags))
+		return false;
+
+	raw_spin_lock_irqsave(&glob_worker->lock, flags);
+	list_add_tail(&sev->item, &glob_worker->events);
+	raw_spin_unlock_irqrestore(&glob_worker->lock, flags);
+
+	swait_wake(&glob_worker->wq);
+	return true;
+}
+EXPORT_SYMBOL_GPL(swork_queue);
+
+/**
+ * swork_get - get an instance of the sworker
+ *
+ * Returns an negative error code if the initialization if the worker did not
+ * work, %0 otherwise.
+ *
+ */
+int swork_get(void)
+{
+	struct sworker *worker;
+
+	mutex_lock(&worker_mutex);
+	if (!glob_worker) {
+		worker = swork_create();
+		if (IS_ERR(worker)) {
+			mutex_unlock(&worker_mutex);
+			return -ENOMEM;
+		}
+
+		glob_worker = worker;
+	}
+
+	glob_worker->refs++;
+	mutex_unlock(&worker_mutex);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(swork_get);
+
+/**
+ * swork_put - puts an instance of the sworker
+ *
+ * Will destroy the sworker thread. This function must not be called until all
+ * queued events have been completed.
+ */
+void swork_put(void)
+{
+	mutex_lock(&worker_mutex);
+
+	glob_worker->refs--;
+	if (glob_worker->refs > 0)
+		goto out;
+
+	swork_destroy(glob_worker);
+	glob_worker = NULL;
+out:
+	mutex_unlock(&worker_mutex);
+}
+EXPORT_SYMBOL_GPL(swork_put);
diff --git a/kernel/msm-3.18/kernel/signal.c b/kernel/msm-3.18/kernel/signal.c
index b7f46553d..2e2bdf76a 100644
--- a/kernel/msm-3.18/kernel/signal.c
+++ b/kernel/msm-3.18/kernel/signal.c
@@ -14,6 +14,7 @@
 #include <linux/export.h>
 #include <linux/init.h>
 #include <linux/sched.h>
+#include <linux/sched/rt.h>
 #include <linux/fs.h>
 #include <linux/tty.h>
 #include <linux/binfmts.h>
@@ -352,13 +353,45 @@ static bool task_participate_group_stop(struct task_struct *task)
 	return false;
 }
 
+#ifdef __HAVE_ARCH_CMPXCHG
+static inline struct sigqueue *get_task_cache(struct task_struct *t)
+{
+	struct sigqueue *q = t->sigqueue_cache;
+
+	if (cmpxchg(&t->sigqueue_cache, q, NULL) != q)
+		return NULL;
+	return q;
+}
+
+static inline int put_task_cache(struct task_struct *t, struct sigqueue *q)
+{
+	if (cmpxchg(&t->sigqueue_cache, NULL, q) == NULL)
+		return 0;
+	return 1;
+}
+
+#else
+
+static inline struct sigqueue *get_task_cache(struct task_struct *t)
+{
+	return NULL;
+}
+
+static inline int put_task_cache(struct task_struct *t, struct sigqueue *q)
+{
+	return 1;
+}
+
+#endif
+
 /*
  * allocate a new signal queue record
  * - this may be called without locks if and only if t == current, otherwise an
  *   appropriate lock must be held to stop the target task from exiting
  */
 static struct sigqueue *
-__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)
+__sigqueue_do_alloc(int sig, struct task_struct *t, gfp_t flags,
+		    int override_rlimit, int fromslab)
 {
 	struct sigqueue *q = NULL;
 	struct user_struct *user;
@@ -375,7 +408,10 @@ __sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimi
 	if (override_rlimit ||
 	    atomic_read(&user->sigpending) <=
 			task_rlimit(t, RLIMIT_SIGPENDING)) {
-		q = kmem_cache_alloc(sigqueue_cachep, flags);
+		if (!fromslab)
+			q = get_task_cache(t);
+		if (!q)
+			q = kmem_cache_alloc(sigqueue_cachep, flags);
 	} else {
 		print_dropped_signal(sig);
 	}
@@ -392,6 +428,13 @@ __sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimi
 	return q;
 }
 
+static struct sigqueue *
+__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags,
+		 int override_rlimit)
+{
+	return __sigqueue_do_alloc(sig, t, flags, override_rlimit, 0);
+}
+
 static void __sigqueue_free(struct sigqueue *q)
 {
 	if (q->flags & SIGQUEUE_PREALLOC)
@@ -401,6 +444,21 @@ static void __sigqueue_free(struct sigqueue *q)
 	kmem_cache_free(sigqueue_cachep, q);
 }
 
+static void sigqueue_free_current(struct sigqueue *q)
+{
+	struct user_struct *up;
+
+	if (q->flags & SIGQUEUE_PREALLOC)
+		return;
+
+	up = q->user;
+	if (rt_prio(current->normal_prio) && !put_task_cache(current, q)) {
+		atomic_dec(&up->sigpending);
+		free_uid(up);
+	} else
+		  __sigqueue_free(q);
+}
+
 void flush_sigqueue(struct sigpending *queue)
 {
 	struct sigqueue *q;
@@ -413,6 +471,21 @@ void flush_sigqueue(struct sigpending *queue)
 	}
 }
 
+/*
+ * Called from __exit_signal. Flush tsk->pending and
+ * tsk->sigqueue_cache
+ */
+void flush_task_sigqueue(struct task_struct *tsk)
+{
+	struct sigqueue *q;
+
+	flush_sigqueue(&tsk->pending);
+
+	q = get_task_cache(tsk);
+	if (q)
+		kmem_cache_free(sigqueue_cachep, q);
+}
+
 /*
  * Flush all pending signals for a task.
  */
@@ -572,7 +645,7 @@ still_pending:
 			(info->si_code == SI_TIMER) &&
 			(info->si_sys_private);
 
-		__sigqueue_free(first);
+		sigqueue_free_current(first);
 	} else {
 		/*
 		 * Ok, it wasn't in the queue.  This must be
@@ -619,6 +692,8 @@ int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)
 	bool resched_timer = false;
 	int signr;
 
+	WARN_ON_ONCE(tsk != current);
+
 	/* We only dequeue private signals from ourselves, we don't let
 	 * signalfd steal them
 	 */
@@ -1215,8 +1290,8 @@ int do_send_sig_info(int sig, struct siginfo *info, struct task_struct *p,
  * We don't want to have recursive SIGSEGV's etc, for example,
  * that is why we also clear SIGNAL_UNKILLABLE.
  */
-int
-force_sig_info(int sig, struct siginfo *info, struct task_struct *t)
+static int
+do_force_sig_info(int sig, struct siginfo *info, struct task_struct *t)
 {
 	unsigned long int flags;
 	int ret, blocked, ignored;
@@ -1241,6 +1316,39 @@ force_sig_info(int sig, struct siginfo *info, struct task_struct *t)
 	return ret;
 }
 
+int force_sig_info(int sig, struct siginfo *info, struct task_struct *t)
+{
+/*
+ * On some archs, PREEMPT_RT has to delay sending a signal from a trap
+ * since it can not enable preemption, and the signal code's spin_locks
+ * turn into mutexes. Instead, it must set TIF_NOTIFY_RESUME which will
+ * send the signal on exit of the trap.
+ */
+#ifdef ARCH_RT_DELAYS_SIGNAL_SEND
+	if (in_atomic()) {
+		if (WARN_ON_ONCE(t != current))
+			return 0;
+		if (WARN_ON_ONCE(t->forced_info.si_signo))
+			return 0;
+
+		if (is_si_special(info)) {
+			WARN_ON_ONCE(info != SEND_SIG_PRIV);
+			t->forced_info.si_signo = sig;
+			t->forced_info.si_errno = 0;
+			t->forced_info.si_code = SI_KERNEL;
+			t->forced_info.si_pid = 0;
+			t->forced_info.si_uid = 0;
+		} else {
+			t->forced_info = *info;
+		}
+
+		set_tsk_thread_flag(t, TIF_NOTIFY_RESUME);
+		return 0;
+	}
+#endif
+	return do_force_sig_info(sig, info, t);
+}
+
 /*
  * Nuke all other threads in the group.
  */
@@ -1275,12 +1383,12 @@ struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,
 		 * Disable interrupts early to avoid deadlocks.
 		 * See rcu_read_unlock() comment header for details.
 		 */
-		local_irq_save(*flags);
+		local_irq_save_nort(*flags);
 		rcu_read_lock();
 		sighand = rcu_dereference(tsk->sighand);
 		if (unlikely(sighand == NULL)) {
 			rcu_read_unlock();
-			local_irq_restore(*flags);
+			local_irq_restore_nort(*flags);
 			break;
 		}
 
@@ -1291,7 +1399,7 @@ struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,
 		}
 		spin_unlock(&sighand->siglock);
 		rcu_read_unlock();
-		local_irq_restore(*flags);
+		local_irq_restore_nort(*flags);
 	}
 
 	return sighand;
@@ -1536,7 +1644,8 @@ EXPORT_SYMBOL(kill_pid);
  */
 struct sigqueue *sigqueue_alloc(void)
 {
-	struct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);
+	/* Preallocated sigqueue objects always from the slabcache ! */
+	struct sigqueue *q = __sigqueue_do_alloc(-1, current, GFP_KERNEL, 0, 1);
 
 	if (q)
 		q->flags |= SIGQUEUE_PREALLOC;
@@ -1897,15 +2006,7 @@ static void ptrace_stop(int exit_code, int why, int clear_code, siginfo_t *info)
 		if (gstop_done && ptrace_reparented(current))
 			do_notify_parent_cldstop(current, false, why);
 
-		/*
-		 * Don't want to allow preemption here, because
-		 * sys_ptrace() needs this task to be inactive.
-		 *
-		 * XXX: implement read_unlock_no_resched().
-		 */
-		preempt_disable();
 		read_unlock(&tasklist_lock);
-		preempt_enable_no_resched();
 		freezable_schedule();
 	} else {
 		/*
diff --git a/kernel/msm-3.18/kernel/softirq.c b/kernel/msm-3.18/kernel/softirq.c
index 9e787d831..89c490b40 100644
--- a/kernel/msm-3.18/kernel/softirq.c
+++ b/kernel/msm-3.18/kernel/softirq.c
@@ -21,10 +21,12 @@
 #include <linux/freezer.h>
 #include <linux/kthread.h>
 #include <linux/rcupdate.h>
+#include <linux/delay.h>
 #include <linux/ftrace.h>
 #include <linux/smp.h>
 #include <linux/smpboot.h>
 #include <linux/tick.h>
+#include <linux/locallock.h>
 #include <linux/irq.h>
 
 #define CREATE_TRACE_POINTS
@@ -56,12 +58,108 @@ EXPORT_SYMBOL(irq_stat);
 static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;
 
 DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
+#ifdef CONFIG_PREEMPT_RT_FULL
+#define TIMER_SOFTIRQS	((1 << TIMER_SOFTIRQ) | (1 << HRTIMER_SOFTIRQ))
+DEFINE_PER_CPU(struct task_struct *, ktimer_softirqd);
+#endif
 
 const char * const softirq_to_name[NR_SOFTIRQS] = {
 	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL",
 	"TASKLET", "SCHED", "HRTIMER", "RCU"
 };
 
+#ifdef CONFIG_NO_HZ_COMMON
+# ifdef CONFIG_PREEMPT_RT_FULL
+
+struct softirq_runner {
+	struct task_struct *runner[NR_SOFTIRQS];
+};
+
+static DEFINE_PER_CPU(struct softirq_runner, softirq_runners);
+
+static inline void softirq_set_runner(unsigned int sirq)
+{
+	struct softirq_runner *sr = &__get_cpu_var(softirq_runners);
+
+	sr->runner[sirq] = current;
+}
+
+static inline void softirq_clr_runner(unsigned int sirq)
+{
+	struct softirq_runner *sr = &__get_cpu_var(softirq_runners);
+
+	sr->runner[sirq] = NULL;
+}
+
+/*
+ * On preempt-rt a softirq running context might be blocked on a
+ * lock. There might be no other runnable task on this CPU because the
+ * lock owner runs on some other CPU. So we have to go into idle with
+ * the pending bit set. Therefor we need to check this otherwise we
+ * warn about false positives which confuses users and defeats the
+ * whole purpose of this test.
+ *
+ * This code is called with interrupts disabled.
+ */
+void softirq_check_pending_idle(void)
+{
+	static int rate_limit;
+	struct softirq_runner *sr = &__get_cpu_var(softirq_runners);
+	u32 warnpending;
+	int i;
+
+	if (rate_limit >= 10)
+		return;
+
+	warnpending = local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK;
+	for (i = 0; i < NR_SOFTIRQS; i++) {
+		struct task_struct *tsk = sr->runner[i];
+
+		/*
+		 * The wakeup code in rtmutex.c wakes up the task
+		 * _before_ it sets pi_blocked_on to NULL under
+		 * tsk->pi_lock. So we need to check for both: state
+		 * and pi_blocked_on.
+		 */
+		if (tsk) {
+			raw_spin_lock(&tsk->pi_lock);
+			if (tsk->pi_blocked_on || tsk->state == TASK_RUNNING) {
+				/* Clear all bits pending in that task */
+				warnpending &= ~(tsk->softirqs_raised);
+				warnpending &= ~(1 << i);
+			}
+			raw_spin_unlock(&tsk->pi_lock);
+		}
+	}
+
+	if (warnpending) {
+		printk(KERN_ERR "NOHZ: local_softirq_pending %02x\n",
+		       warnpending);
+		rate_limit++;
+	}
+}
+# else
+/*
+ * On !PREEMPT_RT we just printk rate limited:
+ */
+void softirq_check_pending_idle(void)
+{
+	static int rate_limit;
+
+	if (rate_limit < 10 &&
+			(local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK)) {
+		printk(KERN_ERR "NOHZ: local_softirq_pending %02x\n",
+		       local_softirq_pending());
+		rate_limit++;
+	}
+}
+# endif
+
+#else /* !CONFIG_NO_HZ_COMMON */
+static inline void softirq_set_runner(unsigned int sirq) { }
+static inline void softirq_clr_runner(unsigned int sirq) { }
+#endif
+
 /*
  * we cannot loop indefinitely here to avoid userspace starvation,
  * but we also don't want to introduce a worst case 1/HZ latency
@@ -77,6 +175,84 @@ static void wakeup_softirqd(void)
 		wake_up_process(tsk);
 }
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+static void wakeup_timer_softirqd(void)
+{
+	/* Interrupts are disabled: no need to stop preemption */
+	struct task_struct *tsk = __this_cpu_read(ktimer_softirqd);
+
+	if (tsk && tsk->state != TASK_RUNNING)
+		wake_up_process(tsk);
+}
+#endif
+
+static void handle_softirq(unsigned int vec_nr)
+{
+	struct softirq_action *h = softirq_vec + vec_nr;
+	int prev_count;
+
+	prev_count = preempt_count();
+
+	kstat_incr_softirqs_this_cpu(vec_nr);
+
+	trace_softirq_entry(vec_nr);
+	h->action(h);
+	trace_softirq_exit(vec_nr);
+	if (unlikely(prev_count != preempt_count())) {
+		pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
+		       vec_nr, softirq_to_name[vec_nr], h->action,
+		       prev_count, preempt_count());
+		preempt_count_set(prev_count);
+	}
+}
+
+#ifndef CONFIG_PREEMPT_RT_FULL
+static inline int ksoftirqd_softirq_pending(void)
+{
+	return local_softirq_pending();
+}
+
+static void handle_pending_softirqs(u32 pending, int need_rcu_bh_qs)
+{
+	struct softirq_action *h = softirq_vec;
+	int softirq_bit;
+
+	local_irq_enable();
+
+	h = softirq_vec;
+
+	while ((softirq_bit = ffs(pending))) {
+		unsigned int vec_nr;
+
+		h += softirq_bit - 1;
+		vec_nr = h - softirq_vec;
+		handle_softirq(vec_nr);
+
+		h++;
+		pending >>= softirq_bit;
+	}
+
+	if (need_rcu_bh_qs)
+		rcu_bh_qs();
+	local_irq_disable();
+}
+
+static void run_ksoftirqd(unsigned int cpu)
+{
+	local_irq_disable();
+	if (ksoftirqd_softirq_pending()) {
+		__do_softirq();
+		local_irq_enable();
+		cond_resched();
+
+		preempt_disable();
+		rcu_note_context_switch(cpu);
+		preempt_enable();
+		return;
+	}
+	local_irq_enable();
+}
+
 /*
  * preempt_count and SOFTIRQ_OFFSET usage:
  * - preempt_count is changed by SOFTIRQ_OFFSET on entering or leaving
@@ -115,7 +291,7 @@ void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
 	raw_local_irq_restore(flags);
 
 	if (preempt_count() == cnt)
-		trace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));
+		trace_preempt_off(CALLER_ADDR0, get_lock_parent_ip());
 }
 EXPORT_SYMBOL(__local_bh_disable_ip);
 #endif /* CONFIG_TRACE_IRQFLAGS */
@@ -228,10 +404,8 @@ asmlinkage __visible void __do_softirq(void)
 	unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
 	unsigned long old_flags = current->flags;
 	int max_restart = MAX_SOFTIRQ_RESTART;
-	struct softirq_action *h;
 	bool in_hardirq;
 	__u32 pending;
-	int softirq_bit;
 
 	/*
 	 * Mask out PF_MEMALLOC s current task context is borrowed for the
@@ -250,36 +424,7 @@ restart:
 	/* Reset the pending bitmask before enabling irqs */
 	set_softirq_pending(0);
 
-	local_irq_enable();
-
-	h = softirq_vec;
-
-	while ((softirq_bit = ffs(pending))) {
-		unsigned int vec_nr;
-		int prev_count;
-
-		h += softirq_bit - 1;
-
-		vec_nr = h - softirq_vec;
-		prev_count = preempt_count();
-
-		kstat_incr_softirqs_this_cpu(vec_nr);
-
-		trace_softirq_entry(vec_nr);
-		h->action(h);
-		trace_softirq_exit(vec_nr);
-		if (unlikely(prev_count != preempt_count())) {
-			pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
-			       vec_nr, softirq_to_name[vec_nr], h->action,
-			       prev_count, preempt_count());
-			preempt_count_set(prev_count);
-		}
-		h++;
-		pending >>= softirq_bit;
-	}
-
-	rcu_bh_qs();
-	local_irq_disable();
+	handle_pending_softirqs(pending, 1);
 
 	pending = local_softirq_pending();
 	if (pending) {
@@ -315,6 +460,340 @@ asmlinkage __visible void do_softirq(void)
 	local_irq_restore(flags);
 }
 
+/*
+ * This function must run with irqs disabled!
+ */
+void raise_softirq_irqoff(unsigned int nr)
+{
+	__raise_softirq_irqoff(nr);
+
+	/*
+	 * If we're in an interrupt or softirq, we're done
+	 * (this also catches softirq-disabled code). We will
+	 * actually run the softirq once we return from
+	 * the irq or softirq.
+	 *
+	 * Otherwise we wake up ksoftirqd to make sure we
+	 * schedule the softirq soon.
+	 */
+	if (!in_interrupt())
+		wakeup_softirqd();
+}
+
+void __raise_softirq_irqoff(unsigned int nr)
+{
+	trace_softirq_raise(nr);
+	or_softirq_pending(1UL << nr);
+}
+
+static inline void local_bh_disable_nort(void) { local_bh_disable(); }
+static inline void _local_bh_enable_nort(void) { _local_bh_enable(); }
+static void ksoftirqd_set_sched_params(unsigned int cpu) { }
+
+#else /* !PREEMPT_RT_FULL */
+
+/*
+ * On RT we serialize softirq execution with a cpu local lock per softirq
+ */
+static DEFINE_PER_CPU(struct local_irq_lock [NR_SOFTIRQS], local_softirq_locks);
+
+void __init softirq_early_init(void)
+{
+	int i;
+
+	for (i = 0; i < NR_SOFTIRQS; i++)
+		local_irq_lock_init(local_softirq_locks[i]);
+}
+
+static void lock_softirq(int which)
+{
+	local_lock(local_softirq_locks[which]);
+}
+
+static void unlock_softirq(int which)
+{
+	local_unlock(local_softirq_locks[which]);
+}
+
+static void do_single_softirq(int which, int need_rcu_bh_qs)
+{
+	unsigned long old_flags = current->flags;
+
+	current->flags &= ~PF_MEMALLOC;
+	vtime_account_irq_enter(current);
+	current->flags |= PF_IN_SOFTIRQ;
+	lockdep_softirq_enter();
+	local_irq_enable();
+	handle_softirq(which);
+	local_irq_disable();
+	lockdep_softirq_exit();
+	current->flags &= ~PF_IN_SOFTIRQ;
+	vtime_account_irq_enter(current);
+	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
+}
+
+/*
+ * Called with interrupts disabled. Process softirqs which were raised
+ * in current context (or on behalf of ksoftirqd).
+ */
+static void do_current_softirqs(int need_rcu_bh_qs)
+{
+	while (current->softirqs_raised) {
+		int i = __ffs(current->softirqs_raised);
+		unsigned int pending, mask = (1U << i);
+
+		current->softirqs_raised &= ~mask;
+		local_irq_enable();
+
+		/*
+		 * If the lock is contended, we boost the owner to
+		 * process the softirq or leave the critical section
+		 * now.
+		 */
+		lock_softirq(i);
+		local_irq_disable();
+		softirq_set_runner(i);
+		/*
+		 * Check with the local_softirq_pending() bits,
+		 * whether we need to process this still or if someone
+		 * else took care of it.
+		 */
+		pending = local_softirq_pending();
+		if (pending & mask) {
+			set_softirq_pending(pending & ~mask);
+			do_single_softirq(i, need_rcu_bh_qs);
+		}
+		softirq_clr_runner(i);
+		WARN_ON(current->softirq_nestcnt != 1);
+		local_irq_enable();
+		unlock_softirq(i);
+		local_irq_disable();
+	}
+}
+
+static void __local_bh_disable(void)
+{
+	if (++current->softirq_nestcnt == 1)
+		migrate_disable();
+}
+
+void local_bh_disable(void)
+{
+	__local_bh_disable();
+}
+EXPORT_SYMBOL(local_bh_disable);
+
+void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
+{
+	__local_bh_disable();
+	if (cnt & PREEMPT_CHECK_OFFSET)
+		preempt_disable();
+}
+
+static void __local_bh_enable(void)
+{
+	if (WARN_ON(current->softirq_nestcnt == 0))
+		return;
+
+	local_irq_disable();
+	if (current->softirq_nestcnt == 1 && current->softirqs_raised)
+		do_current_softirqs(1);
+	local_irq_enable();
+
+	if (--current->softirq_nestcnt == 0)
+		migrate_enable();
+}
+
+void local_bh_enable(void)
+{
+	__local_bh_enable();
+}
+EXPORT_SYMBOL(local_bh_enable);
+
+extern void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
+{
+	__local_bh_enable();
+	if (cnt & PREEMPT_CHECK_OFFSET)
+		preempt_enable();
+}
+
+void local_bh_enable_ip(unsigned long ip)
+{
+	local_bh_enable();
+}
+EXPORT_SYMBOL(local_bh_enable_ip);
+
+void _local_bh_enable(void)
+{
+	if (WARN_ON(current->softirq_nestcnt == 0))
+		return;
+	if (--current->softirq_nestcnt == 0)
+		migrate_enable();
+}
+EXPORT_SYMBOL(_local_bh_enable);
+
+int in_serving_softirq(void)
+{
+	return current->flags & PF_IN_SOFTIRQ;
+}
+EXPORT_SYMBOL(in_serving_softirq);
+
+/* Called with preemption disabled */
+static void run_ksoftirqd(unsigned int cpu)
+{
+	local_irq_disable();
+	current->softirq_nestcnt++;
+
+	do_current_softirqs(1);
+	current->softirq_nestcnt--;
+	rcu_note_context_switch(cpu);
+	local_irq_enable();
+}
+
+/*
+ * Called from netif_rx_ni(). Preemption enabled, but migration
+ * disabled. So the cpu can't go away under us.
+ */
+void thread_do_softirq(void)
+{
+	if (!in_serving_softirq() && current->softirqs_raised) {
+		current->softirq_nestcnt++;
+		do_current_softirqs(0);
+		current->softirq_nestcnt--;
+	}
+}
+
+static void do_raise_softirq_irqoff(unsigned int nr)
+{
+	unsigned int mask;
+
+	mask = 1UL << nr;
+
+	trace_softirq_raise(nr);
+	or_softirq_pending(mask);
+
+	/*
+	 * If we are not in a hard interrupt and inside a bh disabled
+	 * region, we simply raise the flag on current. local_bh_enable()
+	 * will make sure that the softirq is executed. Otherwise we
+	 * delegate it to ksoftirqd.
+	 */
+	if (!in_irq() && current->softirq_nestcnt)
+		current->softirqs_raised |= mask;
+	else if (!__this_cpu_read(ksoftirqd) || !__this_cpu_read(ktimer_softirqd))
+		return;
+
+	if (mask & TIMER_SOFTIRQS)
+		__this_cpu_read(ktimer_softirqd)->softirqs_raised |= mask;
+	else
+		__this_cpu_read(ksoftirqd)->softirqs_raised |= mask;
+}
+
+static void wakeup_proper_softirq(unsigned int nr)
+{
+	if ((1UL << nr) & TIMER_SOFTIRQS)
+		wakeup_timer_softirqd();
+	else
+		wakeup_softirqd();
+}
+
+
+void __raise_softirq_irqoff(unsigned int nr)
+{
+	do_raise_softirq_irqoff(nr);
+	if (!in_irq() && !current->softirq_nestcnt)
+		wakeup_proper_softirq(nr);
+}
+
+/*
+ * Same as __raise_softirq_irqoff() but will process them in ksoftirqd
+ */
+void __raise_softirq_irqoff_ksoft(unsigned int nr)
+{
+	unsigned int mask;
+
+	if (WARN_ON_ONCE(!__this_cpu_read(ksoftirqd) ||
+			 !__this_cpu_read(ktimer_softirqd)))
+		return;
+	mask = 1UL << nr;
+
+	trace_softirq_raise(nr);
+	or_softirq_pending(mask);
+	if (mask & TIMER_SOFTIRQS)
+		__this_cpu_read(ktimer_softirqd)->softirqs_raised |= mask;
+	else
+		__this_cpu_read(ksoftirqd)->softirqs_raised |= mask;
+	wakeup_proper_softirq(nr);
+}
+
+/*
+ * This function must run with irqs disabled!
+ */
+void raise_softirq_irqoff(unsigned int nr)
+{
+	do_raise_softirq_irqoff(nr);
+
+	/*
+	 * If we're in an hard interrupt we let irq return code deal
+	 * with the wakeup of ksoftirqd.
+	 */
+	if (in_irq())
+		return;
+	/*
+	 * If we are in thread context but outside of a bh disabled
+	 * region, we need to wake ksoftirqd as well.
+	 *
+	 * CHECKME: Some of the places which do that could be wrapped
+	 * into local_bh_disable/enable pairs. Though it's unclear
+	 * whether this is worth the effort. To find those places just
+	 * raise a WARN() if the condition is met.
+	 */
+	if (!current->softirq_nestcnt)
+		wakeup_proper_softirq(nr);
+}
+
+static inline int ksoftirqd_softirq_pending(void)
+{
+	return current->softirqs_raised;
+}
+
+static inline void local_bh_disable_nort(void) { }
+static inline void _local_bh_enable_nort(void) { }
+
+static inline void ksoftirqd_set_sched_params(unsigned int cpu)
+{
+	/* Take over all but timer pending softirqs when starting */
+	local_irq_disable();
+	current->softirqs_raised = local_softirq_pending() & ~TIMER_SOFTIRQS;
+	local_irq_enable();
+}
+
+static inline void ktimer_softirqd_set_sched_params(unsigned int cpu)
+{
+	struct sched_param param = { .sched_priority = 1 };
+
+	sched_setscheduler(current, SCHED_FIFO, &param);
+
+	/* Take over timer pending softirqs when starting */
+	local_irq_disable();
+	current->softirqs_raised = local_softirq_pending() & TIMER_SOFTIRQS;
+	local_irq_enable();
+}
+
+static inline void ktimer_softirqd_clr_sched_params(unsigned int cpu,
+						    bool online)
+{
+	struct sched_param param = { .sched_priority = 0 };
+
+	sched_setscheduler(current, SCHED_NORMAL, &param);
+}
+
+static int ktimer_softirqd_should_run(unsigned int cpu)
+{
+	return current->softirqs_raised;
+}
+
+#endif /* PREEMPT_RT_FULL */
 /*
  * Enter an interrupt context.
  */
@@ -326,9 +805,9 @@ void irq_enter(void)
 		 * Prevent raise_softirq from needlessly waking up ksoftirqd
 		 * here, as softirq will be serviced on return from interrupt.
 		 */
-		local_bh_disable();
+		local_bh_disable_nort();
 		tick_irq_enter();
-		_local_bh_enable();
+		_local_bh_enable_nort();
 	}
 
 	__irq_enter();
@@ -336,6 +815,7 @@ void irq_enter(void)
 
 static inline void invoke_softirq(void)
 {
+#ifndef CONFIG_PREEMPT_RT_FULL
 	if (!force_irqthreads) {
 #ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
 		/*
@@ -355,6 +835,18 @@ static inline void invoke_softirq(void)
 	} else {
 		wakeup_softirqd();
 	}
+#else /* PREEMPT_RT_FULL */
+	unsigned long flags;
+
+	local_irq_save(flags);
+	if (__this_cpu_read(ksoftirqd) &&
+			__this_cpu_read(ksoftirqd)->softirqs_raised)
+		wakeup_softirqd();
+	if (__this_cpu_read(ktimer_softirqd) &&
+			__this_cpu_read(ktimer_softirqd)->softirqs_raised)
+		wakeup_timer_softirqd();
+	local_irq_restore(flags);
+#endif
 }
 
 static inline void tick_irq_exit(void)
@@ -391,26 +883,6 @@ void irq_exit(void)
 	trace_hardirq_exit(); /* must be last! */
 }
 
-/*
- * This function must run with irqs disabled!
- */
-inline void raise_softirq_irqoff(unsigned int nr)
-{
-	__raise_softirq_irqoff(nr);
-
-	/*
-	 * If we're in an interrupt or softirq, we're done
-	 * (this also catches softirq-disabled code). We will
-	 * actually run the softirq once we return from
-	 * the irq or softirq.
-	 *
-	 * Otherwise we wake up ksoftirqd to make sure we
-	 * schedule the softirq soon.
-	 */
-	if (!in_interrupt())
-		wakeup_softirqd();
-}
-
 void raise_softirq(unsigned int nr)
 {
 	unsigned long flags;
@@ -420,12 +892,6 @@ void raise_softirq(unsigned int nr)
 	local_irq_restore(flags);
 }
 
-void __raise_softirq_irqoff(unsigned int nr)
-{
-	trace_softirq_raise(nr);
-	or_softirq_pending(1UL << nr);
-}
-
 void open_softirq(int nr, void (*action)(struct softirq_action *))
 {
 	softirq_vec[nr].action = action;
@@ -442,15 +908,45 @@ struct tasklet_head {
 static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
 static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);
 
+static void inline
+__tasklet_common_schedule(struct tasklet_struct *t, struct tasklet_head *head, unsigned int nr)
+{
+	if (tasklet_trylock(t)) {
+again:
+		/* We may have been preempted before tasklet_trylock
+		 * and __tasklet_action may have already run.
+		 * So double check the sched bit while the takslet
+		 * is locked before adding it to the list.
+		 */
+		if (test_bit(TASKLET_STATE_SCHED, &t->state)) {
+			t->next = NULL;
+			*head->tail = t;
+			head->tail = &(t->next);
+			raise_softirq_irqoff(nr);
+			tasklet_unlock(t);
+		} else {
+			/* This is subtle. If we hit the corner case above
+			 * It is possible that we get preempted right here,
+			 * and another task has successfully called
+			 * tasklet_schedule(), then this function, and
+			 * failed on the trylock. Thus we must be sure
+			 * before releasing the tasklet lock, that the
+			 * SCHED_BIT is clear. Otherwise the tasklet
+			 * may get its SCHED_BIT set, but not added to the
+			 * list
+			 */
+			if (!tasklet_tryunlock(t))
+				goto again;
+		}
+	}
+}
+
 void __tasklet_schedule(struct tasklet_struct *t)
 {
 	unsigned long flags;
 
 	local_irq_save(flags);
-	t->next = NULL;
-	*__this_cpu_read(tasklet_vec.tail) = t;
-	__this_cpu_write(tasklet_vec.tail, &(t->next));
-	raise_softirq_irqoff(TASKLET_SOFTIRQ);
+	__tasklet_common_schedule(t, &__get_cpu_var(tasklet_vec), TASKLET_SOFTIRQ);
 	local_irq_restore(flags);
 }
 EXPORT_SYMBOL(__tasklet_schedule);
@@ -460,10 +956,7 @@ void __tasklet_hi_schedule(struct tasklet_struct *t)
 	unsigned long flags;
 
 	local_irq_save(flags);
-	t->next = NULL;
-	*__this_cpu_read(tasklet_hi_vec.tail) = t;
-	__this_cpu_write(tasklet_hi_vec.tail,  &(t->next));
-	raise_softirq_irqoff(HI_SOFTIRQ);
+	__tasklet_common_schedule(t, &__get_cpu_var(tasklet_hi_vec), HI_SOFTIRQ);
 	local_irq_restore(flags);
 }
 EXPORT_SYMBOL(__tasklet_hi_schedule);
@@ -472,48 +965,116 @@ void __tasklet_hi_schedule_first(struct tasklet_struct *t)
 {
 	BUG_ON(!irqs_disabled());
 
-	t->next = __this_cpu_read(tasklet_hi_vec.head);
-	__this_cpu_write(tasklet_hi_vec.head, t);
-	__raise_softirq_irqoff(HI_SOFTIRQ);
+	__tasklet_hi_schedule(t);
 }
 EXPORT_SYMBOL(__tasklet_hi_schedule_first);
 
-static void tasklet_action(struct softirq_action *a)
+void  tasklet_enable(struct tasklet_struct *t)
 {
-	struct tasklet_struct *list;
+	if (!atomic_dec_and_test(&t->count))
+		return;
+	if (test_and_clear_bit(TASKLET_STATE_PENDING, &t->state))
+		tasklet_schedule(t);
+}
+EXPORT_SYMBOL(tasklet_enable);
 
-	local_irq_disable();
-	list = __this_cpu_read(tasklet_vec.head);
-	__this_cpu_write(tasklet_vec.head, NULL);
-	__this_cpu_write(tasklet_vec.tail, this_cpu_ptr(&tasklet_vec.head));
-	local_irq_enable();
+void  tasklet_hi_enable(struct tasklet_struct *t)
+{
+	if (!atomic_dec_and_test(&t->count))
+		return;
+	if (test_and_clear_bit(TASKLET_STATE_PENDING, &t->state))
+		tasklet_hi_schedule(t);
+}
+EXPORT_SYMBOL(tasklet_hi_enable);
+
+static void __tasklet_action(struct softirq_action *a,
+			     struct tasklet_struct *list)
+{
+	int loops = 1000000;
 
 	while (list) {
 		struct tasklet_struct *t = list;
 
 		list = list->next;
 
-		if (tasklet_trylock(t)) {
-			if (!atomic_read(&t->count)) {
-				if (!test_and_clear_bit(TASKLET_STATE_SCHED,
-							&t->state))
-					BUG();
-				t->func(t->data);
-				tasklet_unlock(t);
-				continue;
-			}
-			tasklet_unlock(t);
+		/*
+		 * Should always succeed - after a tasklist got on the
+		 * list (after getting the SCHED bit set from 0 to 1),
+		 * nothing but the tasklet softirq it got queued to can
+		 * lock it:
+		 */
+		if (!tasklet_trylock(t)) {
+			WARN_ON(1);
+			continue;
 		}
 
-		local_irq_disable();
 		t->next = NULL;
-		*__this_cpu_read(tasklet_vec.tail) = t;
-		__this_cpu_write(tasklet_vec.tail, &(t->next));
-		__raise_softirq_irqoff(TASKLET_SOFTIRQ);
-		local_irq_enable();
+
+		/*
+		 * If we cannot handle the tasklet because it's disabled,
+		 * mark it as pending. tasklet_enable() will later
+		 * re-schedule the tasklet.
+		 */
+		if (unlikely(atomic_read(&t->count))) {
+out_disabled:
+			/* implicit unlock: */
+			wmb();
+			t->state = TASKLET_STATEF_PENDING;
+			continue;
+		}
+
+		/*
+		 * After this point on the tasklet might be rescheduled
+		 * on another CPU, but it can only be added to another
+		 * CPU's tasklet list if we unlock the tasklet (which we
+		 * dont do yet).
+		 */
+		if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
+			WARN_ON(1);
+
+again:
+		t->func(t->data);
+
+		/*
+		 * Try to unlock the tasklet. We must use cmpxchg, because
+		 * another CPU might have scheduled or disabled the tasklet.
+		 * We only allow the STATE_RUN -> 0 transition here.
+		 */
+		while (!tasklet_tryunlock(t)) {
+			/*
+			 * If it got disabled meanwhile, bail out:
+			 */
+			if (atomic_read(&t->count))
+				goto out_disabled;
+			/*
+			 * If it got scheduled meanwhile, re-execute
+			 * the tasklet function:
+			 */
+			if (test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
+				goto again;
+			if (!--loops) {
+				printk("hm, tasklet state: %08lx\n", t->state);
+				WARN_ON(1);
+				tasklet_unlock(t);
+				break;
+			}
+		}
 	}
 }
 
+static void tasklet_action(struct softirq_action *a)
+{
+	struct tasklet_struct *list;
+
+	local_irq_disable();
+	list = __get_cpu_var(tasklet_vec).head;
+	__get_cpu_var(tasklet_vec).head = NULL;
+	__get_cpu_var(tasklet_vec).tail = &__get_cpu_var(tasklet_vec).head;
+	local_irq_enable();
+
+	__tasklet_action(a, list);
+}
+
 static void tasklet_hi_action(struct softirq_action *a)
 {
 	struct tasklet_struct *list;
@@ -524,30 +1085,7 @@ static void tasklet_hi_action(struct softirq_action *a)
 	__this_cpu_write(tasklet_hi_vec.tail, this_cpu_ptr(&tasklet_hi_vec.head));
 	local_irq_enable();
 
-	while (list) {
-		struct tasklet_struct *t = list;
-
-		list = list->next;
-
-		if (tasklet_trylock(t)) {
-			if (!atomic_read(&t->count)) {
-				if (!test_and_clear_bit(TASKLET_STATE_SCHED,
-							&t->state))
-					BUG();
-				t->func(t->data);
-				tasklet_unlock(t);
-				continue;
-			}
-			tasklet_unlock(t);
-		}
-
-		local_irq_disable();
-		t->next = NULL;
-		*__this_cpu_read(tasklet_hi_vec.tail) = t;
-		__this_cpu_write(tasklet_hi_vec.tail, &(t->next));
-		__raise_softirq_irqoff(HI_SOFTIRQ);
-		local_irq_enable();
-	}
+	__tasklet_action(a, list);
 }
 
 void tasklet_init(struct tasklet_struct *t,
@@ -568,7 +1106,7 @@ void tasklet_kill(struct tasklet_struct *t)
 
 	while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
 		do {
-			yield();
+			msleep(1);
 		} while (test_bit(TASKLET_STATE_SCHED, &t->state));
 	}
 	tasklet_unlock_wait(t);
@@ -642,30 +1180,26 @@ void __init softirq_init(void)
 	open_softirq(HI_SOFTIRQ, tasklet_hi_action);
 }
 
-static int ksoftirqd_should_run(unsigned int cpu)
-{
-	return local_softirq_pending();
-}
-
-static void run_ksoftirqd(unsigned int cpu)
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT_FULL)
+void tasklet_unlock_wait(struct tasklet_struct *t)
 {
-	local_irq_disable();
-	if (local_softirq_pending()) {
+	while (test_bit(TASKLET_STATE_RUN, &(t)->state)) {
 		/*
-		 * We can safely run softirq on inline stack, as we are not deep
-		 * in the task stack here.
+		 * Hack for now to avoid this busy-loop:
 		 */
-		__do_softirq();
-		local_irq_enable();
-		cond_resched();
-
-		preempt_disable();
-		rcu_note_context_switch(cpu);
-		preempt_enable();
-
-		return;
+#ifdef CONFIG_PREEMPT_RT_FULL
+		msleep(1);
+#else
+		barrier();
+#endif
 	}
-	local_irq_enable();
+}
+EXPORT_SYMBOL(tasklet_unlock_wait);
+#endif
+
+static int ksoftirqd_should_run(unsigned int cpu)
+{
+	return ksoftirqd_softirq_pending();
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
@@ -747,16 +1281,31 @@ static struct notifier_block cpu_nfb = {
 
 static struct smp_hotplug_thread softirq_threads = {
 	.store			= &ksoftirqd,
+	.setup			= ksoftirqd_set_sched_params,
 	.thread_should_run	= ksoftirqd_should_run,
 	.thread_fn		= run_ksoftirqd,
 	.thread_comm		= "ksoftirqd/%u",
 };
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+static struct smp_hotplug_thread softirq_timer_threads = {
+	.store			= &ktimer_softirqd,
+	.setup			= ktimer_softirqd_set_sched_params,
+	.cleanup		= ktimer_softirqd_clr_sched_params,
+	.thread_should_run	= ktimer_softirqd_should_run,
+	.thread_fn		= run_ksoftirqd,
+	.thread_comm		= "ktimersoftd/%u",
+};
+#endif
+
 static __init int spawn_ksoftirqd(void)
 {
 	register_cpu_notifier(&cpu_nfb);
 
 	BUG_ON(smpboot_register_percpu_thread(&softirq_threads));
+#ifdef CONFIG_PREEMPT_RT_FULL
+	BUG_ON(smpboot_register_percpu_thread(&softirq_timer_threads));
+#endif
 
 	return 0;
 }
diff --git a/kernel/msm-3.18/kernel/stop_machine.c b/kernel/msm-3.18/kernel/stop_machine.c
index 695f0c6cd..d3ea2452e 100644
--- a/kernel/msm-3.18/kernel/stop_machine.c
+++ b/kernel/msm-3.18/kernel/stop_machine.c
@@ -35,7 +35,7 @@ struct cpu_stop_done {
 
 /* the actual stopper, one per every possible cpu, enabled on online cpus */
 struct cpu_stopper {
-	spinlock_t		lock;
+	raw_spinlock_t		lock;
 	bool			enabled;	/* is this stopper enabled? */
 	struct list_head	works;		/* list of pending works */
 };
@@ -78,7 +78,7 @@ static void cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)
 
 	unsigned long flags;
 
-	spin_lock_irqsave(&stopper->lock, flags);
+	raw_spin_lock_irqsave(&stopper->lock, flags);
 
 	if (stopper->enabled) {
 		list_add_tail(&work->list, &stopper->works);
@@ -86,7 +86,7 @@ static void cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)
 	} else
 		cpu_stop_signal_done(work->done, false);
 
-	spin_unlock_irqrestore(&stopper->lock, flags);
+	raw_spin_unlock_irqrestore(&stopper->lock, flags);
 }
 
 /**
@@ -248,7 +248,7 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
 	struct irq_cpu_stop_queue_work_info call_args;
 	struct multi_stop_data msdata;
 
-	preempt_disable();
+	preempt_disable_nort();
 	msdata = (struct multi_stop_data){
 		.fn = fn,
 		.data = arg,
@@ -281,7 +281,7 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
 	 * This relies on the stopper workqueues to be FIFO.
 	 */
 	if (!cpu_active(cpu1) || !cpu_active(cpu2)) {
-		preempt_enable();
+		preempt_enable_nort();
 		return -ENOENT;
 	}
 
@@ -295,7 +295,7 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
 				 &irq_cpu_stop_queue_work,
 				 &call_args, 1);
 	lg_local_unlock(&stop_cpus_lock);
-	preempt_enable();
+	preempt_enable_nort();
 
 	wait_for_completion(&done.completion);
 
@@ -329,7 +329,7 @@ static DEFINE_PER_CPU(struct cpu_stop_work, stop_cpus_work);
 
 static void queue_stop_cpus_work(const struct cpumask *cpumask,
 				 cpu_stop_fn_t fn, void *arg,
-				 struct cpu_stop_done *done)
+				 struct cpu_stop_done *done, bool inactive)
 {
 	struct cpu_stop_work *work;
 	unsigned int cpu;
@@ -343,11 +343,13 @@ static void queue_stop_cpus_work(const struct cpumask *cpumask,
 	}
 
 	/*
-	 * Disable preemption while queueing to avoid getting
-	 * preempted by a stopper which might wait for other stoppers
-	 * to enter @fn which can lead to deadlock.
+	 * Make sure that all work is queued on all cpus before
+	 * any of the cpus can execute it.
 	 */
-	lg_global_lock(&stop_cpus_lock);
+	if (!inactive)
+		lg_global_lock(&stop_cpus_lock);
+	else
+		lg_global_trylock_relax(&stop_cpus_lock);
 	for_each_cpu(cpu, cpumask)
 		cpu_stop_queue_work(cpu, &per_cpu(stop_cpus_work, cpu));
 	lg_global_unlock(&stop_cpus_lock);
@@ -359,7 +361,7 @@ static int __stop_cpus(const struct cpumask *cpumask,
 	struct cpu_stop_done done;
 
 	cpu_stop_init_done(&done, cpumask_weight(cpumask));
-	queue_stop_cpus_work(cpumask, fn, arg, &done);
+	queue_stop_cpus_work(cpumask, fn, arg, &done, false);
 	wait_for_completion(&done.completion);
 	return done.executed ? done.ret : -ENOENT;
 }
@@ -439,9 +441,9 @@ static int cpu_stop_should_run(unsigned int cpu)
 	unsigned long flags;
 	int run;
 
-	spin_lock_irqsave(&stopper->lock, flags);
+	raw_spin_lock_irqsave(&stopper->lock, flags);
 	run = !list_empty(&stopper->works);
-	spin_unlock_irqrestore(&stopper->lock, flags);
+	raw_spin_unlock_irqrestore(&stopper->lock, flags);
 	return run;
 }
 
@@ -453,13 +455,13 @@ static void cpu_stopper_thread(unsigned int cpu)
 
 repeat:
 	work = NULL;
-	spin_lock_irq(&stopper->lock);
+	raw_spin_lock_irq(&stopper->lock);
 	if (!list_empty(&stopper->works)) {
 		work = list_first_entry(&stopper->works,
 					struct cpu_stop_work, list);
 		list_del_init(&work->list);
 	}
-	spin_unlock_irq(&stopper->lock);
+	raw_spin_unlock_irq(&stopper->lock);
 
 	if (work) {
 		cpu_stop_fn_t fn = work->fn;
@@ -467,6 +469,16 @@ repeat:
 		struct cpu_stop_done *done = work->done;
 		char ksym_buf[KSYM_NAME_LEN] __maybe_unused;
 
+		/*
+		 * Wait until the stopper finished scheduling on all
+		 * cpus
+		 */
+		lg_global_lock(&stop_cpus_lock);
+		/*
+		 * Let other cpu threads continue as well
+		 */
+		lg_global_unlock(&stop_cpus_lock);
+
 		/* cpu stop callbacks are not allowed to sleep */
 		preempt_disable();
 
@@ -500,20 +512,20 @@ static void cpu_stop_park(unsigned int cpu)
 	unsigned long flags;
 
 	/* drain remaining works */
-	spin_lock_irqsave(&stopper->lock, flags);
+	raw_spin_lock_irqsave(&stopper->lock, flags);
 	list_for_each_entry(work, &stopper->works, list)
 		cpu_stop_signal_done(work->done, false);
 	stopper->enabled = false;
-	spin_unlock_irqrestore(&stopper->lock, flags);
+	raw_spin_unlock_irqrestore(&stopper->lock, flags);
 }
 
 static void cpu_stop_unpark(unsigned int cpu)
 {
 	struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
 
-	spin_lock_irq(&stopper->lock);
+	raw_spin_lock_irq(&stopper->lock);
 	stopper->enabled = true;
-	spin_unlock_irq(&stopper->lock);
+	raw_spin_unlock_irq(&stopper->lock);
 }
 
 static struct smp_hotplug_thread cpu_stop_threads = {
@@ -535,10 +547,12 @@ static int __init cpu_stop_init(void)
 	for_each_possible_cpu(cpu) {
 		struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
 
-		spin_lock_init(&stopper->lock);
+		raw_spin_lock_init(&stopper->lock);
 		INIT_LIST_HEAD(&stopper->works);
 	}
 
+	lg_lock_init(&stop_cpus_lock, "stop_cpus_lock");
+
 	BUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));
 	stop_machine_initialized = true;
 	return 0;
@@ -634,7 +648,7 @@ int stop_machine_from_inactive_cpu(int (*fn)(void *), void *data,
 	set_state(&msdata, MULTI_STOP_PREPARE);
 	cpu_stop_init_done(&done, num_active_cpus());
 	queue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,
-			     &done);
+			     &done, true);
 	ret = multi_cpu_stop(&msdata);
 
 	/* Busy wait for completion. */
diff --git a/kernel/msm-3.18/kernel/time/hrtimer.c b/kernel/msm-3.18/kernel/time/hrtimer.c
index 524df86ad..e4e0bbec3 100644
--- a/kernel/msm-3.18/kernel/time/hrtimer.c
+++ b/kernel/msm-3.18/kernel/time/hrtimer.c
@@ -48,11 +48,13 @@
 #include <linux/sched/rt.h>
 #include <linux/sched/deadline.h>
 #include <linux/timer.h>
+#include <linux/kthread.h>
 #include <linux/freezer.h>
 
 #include <asm/uaccess.h>
 
 #include <trace/events/timer.h>
+#include <trace/events/hist.h>
 
 #include "timekeeping.h"
 
@@ -576,8 +578,7 @@ static int hrtimer_reprogram(struct hrtimer *timer,
 	 * When the callback is running, we do not reprogram the clock event
 	 * device. The timer callback is either running on a different CPU or
 	 * the callback is executed in the hrtimer_interrupt context. The
-	 * reprogramming is handled either by the softirq, which called the
-	 * callback or at the end of the hrtimer_interrupt.
+	 * reprogramming is handled at the end of the hrtimer_interrupt.
 	 */
 	if (hrtimer_callback_running(timer))
 		return 0;
@@ -621,6 +622,9 @@ static int hrtimer_reprogram(struct hrtimer *timer,
 	return res;
 }
 
+static void __run_hrtimer(struct hrtimer *timer, ktime_t *now);
+static int hrtimer_rt_defer(struct hrtimer *timer);
+
 /*
  * Initialize the high resolution related parts of cpu_base
  */
@@ -630,6 +634,21 @@ static inline void hrtimer_init_hres(struct hrtimer_cpu_base *base)
 	base->hres_active = 0;
 }
 
+static inline int hrtimer_enqueue_reprogram(struct hrtimer *timer,
+					    struct hrtimer_clock_base *base,
+					    int wakeup)
+{
+	if (!hrtimer_reprogram(timer, base))
+		return 0;
+	if (!wakeup)
+		return -ETIME;
+#ifdef CONFIG_PREEMPT_RT_BASE
+	if (!hrtimer_rt_defer(timer))
+		return -ETIME;
+#endif
+	return 1;
+}
+
 static inline ktime_t hrtimer_update_base(struct hrtimer_cpu_base *base)
 {
 	ktime_t *offs_real = &base->clock_base[HRTIMER_BASE_REALTIME].offset;
@@ -695,6 +714,44 @@ static void clock_was_set_work(struct work_struct *work)
 
 static DECLARE_WORK(hrtimer_work, clock_was_set_work);
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+/*
+ * RT can not call schedule_work from real interrupt context.
+ * Need to make a thread to do the real work.
+ */
+static struct task_struct *clock_set_delay_thread;
+static bool do_clock_set_delay;
+
+static int run_clock_set_delay(void *ignore)
+{
+	while (!kthread_should_stop()) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		if (do_clock_set_delay) {
+			do_clock_set_delay = false;
+			schedule_work(&hrtimer_work);
+		}
+		schedule();
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+}
+
+void clock_was_set_delayed(void)
+{
+	do_clock_set_delay = true;
+	/* Make visible before waking up process */
+	smp_wmb();
+	wake_up_process(clock_set_delay_thread);
+}
+
+static __init int create_clock_set_delay_thread(void)
+{
+	clock_set_delay_thread = kthread_run(run_clock_set_delay, NULL, "kclksetdelayd");
+	BUG_ON(!clock_set_delay_thread);
+	return 0;
+}
+early_initcall(create_clock_set_delay_thread);
+#else /* PREEMPT_RT_FULL */
 /*
  * Called from timekeeping and resume code to reprogramm the hrtimer
  * interrupt device on all cpus.
@@ -703,6 +760,7 @@ void clock_was_set_delayed(void)
 {
 	schedule_work(&hrtimer_work);
 }
+#endif
 
 #else
 
@@ -711,6 +769,13 @@ static inline int hrtimer_is_hres_enabled(void) { return 0; }
 static inline int hrtimer_switch_to_hres(void) { return 0; }
 static inline void
 hrtimer_force_reprogram(struct hrtimer_cpu_base *base, int skip_equal) { }
+static inline int hrtimer_enqueue_reprogram(struct hrtimer *timer,
+					    struct hrtimer_clock_base *base,
+					    int wakeup)
+{
+	return 0;
+}
+
 static inline int hrtimer_reprogram(struct hrtimer *timer,
 				    struct hrtimer_clock_base *base)
 {
@@ -718,7 +783,6 @@ static inline int hrtimer_reprogram(struct hrtimer *timer,
 }
 static inline void hrtimer_init_hres(struct hrtimer_cpu_base *base) { }
 static inline void retrigger_next_event(void *arg) { }
-
 #endif /* CONFIG_HIGH_RES_TIMERS */
 
 /*
@@ -836,6 +900,32 @@ u64 hrtimer_forward(struct hrtimer *timer, ktime_t now, ktime_t interval)
 }
 EXPORT_SYMBOL_GPL(hrtimer_forward);
 
+#ifdef CONFIG_PREEMPT_RT_BASE
+# define wake_up_timer_waiters(b)	wake_up(&(b)->wait)
+
+/**
+ * hrtimer_wait_for_timer - Wait for a running timer
+ *
+ * @timer:	timer to wait for
+ *
+ * The function waits in case the timers callback function is
+ * currently executed on the waitqueue of the timer base. The
+ * waitqueue is woken up after the timer callback function has
+ * finished execution.
+ */
+void hrtimer_wait_for_timer(const struct hrtimer *timer)
+{
+	struct hrtimer_clock_base *base = timer->base;
+
+	if (base && base->cpu_base && !timer->irqsafe)
+		wait_event(base->cpu_base->wait,
+			   !(timer->state & HRTIMER_STATE_CALLBACK));
+}
+
+#else
+# define wake_up_timer_waiters(b)	do { } while (0)
+#endif
+
 /*
  * enqueue_hrtimer - internal function to (re)start a timer
  *
@@ -879,6 +969,11 @@ static void __remove_hrtimer(struct hrtimer *timer,
 	if (!(timer->state & HRTIMER_STATE_ENQUEUED))
 		goto out;
 
+	if (unlikely(!list_empty(&timer->cb_entry))) {
+		list_del_init(&timer->cb_entry);
+		goto out;
+	}
+
 	next_timer = timerqueue_getnext(&base->active);
 	timerqueue_del(&base->active, &timer->node);
 	if (&timer->node == next_timer) {
@@ -966,7 +1061,16 @@ int __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
 	new_base = switch_hrtimer_base(timer, base, mode & HRTIMER_MODE_PINNED);
 
 	timer_stats_hrtimer_set_start_info(timer);
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+	{
+		ktime_t now = new_base->get_time();
 
+		if (ktime_to_ns(tim) < ktime_to_ns(now))
+			timer->praecox = now;
+		else
+			timer->praecox = ktime_set(0, 0);
+	}
+#endif
 	leftmost = enqueue_hrtimer(timer, new_base);
 
 	if (!leftmost) {
@@ -980,15 +1084,26 @@ int __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
 		 * on dynticks target.
 		 */
 		wake_up_nohz_cpu(new_base->cpu_base->cpu);
-	} else if (new_base->cpu_base == this_cpu_ptr(&hrtimer_bases) &&
-			hrtimer_reprogram(timer, new_base)) {
+	} else if (new_base->cpu_base == this_cpu_ptr(&hrtimer_bases)) {
+
+		ret = hrtimer_enqueue_reprogram(timer, new_base, wakeup);
+		if (ret < 0) {
+			/*
+			 * In case we failed to reprogram the timer (mostly
+			 * because out current timer is already elapsed),
+			 * remove it again and report a failure. This avoids
+			 * stale base->first entries.
+			 */
+			debug_deactivate(timer);
+			__remove_hrtimer(timer, new_base,
+				timer->state & HRTIMER_STATE_CALLBACK, 0);
+		} else if (ret > 0) {
 		/*
 		 * Only allow reprogramming if the new base is on this CPU.
 		 * (it might still be on another CPU if the timer was pending)
 		 *
 		 * XXX send_remote_softirq() ?
 		 */
-		if (wakeup) {
 			/*
 			 * We need to drop cpu_base->lock to avoid a
 			 * lock ordering issue vs. rq->lock.
@@ -996,9 +1111,7 @@ int __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
 			raw_spin_unlock(&new_base->cpu_base->lock);
 			raise_softirq_irqoff(HRTIMER_SOFTIRQ);
 			local_irq_restore(flags);
-			return ret;
-		} else {
-			__raise_softirq_irqoff(HRTIMER_SOFTIRQ);
+			return 0;
 		}
 	}
 
@@ -1089,7 +1202,7 @@ int hrtimer_cancel(struct hrtimer *timer)
 
 		if (ret >= 0)
 			return ret;
-		cpu_relax();
+		hrtimer_wait_for_timer(timer);
 	}
 }
 EXPORT_SYMBOL_GPL(hrtimer_cancel);
@@ -1153,6 +1266,7 @@ static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
 
 	base = hrtimer_clockid_to_base(clock_id);
 	timer->base = &cpu_base->clock_base[base];
+	INIT_LIST_HEAD(&timer->cb_entry);
 	timerqueue_init(&timer->node);
 
 #ifdef CONFIG_TIMER_STATS
@@ -1236,6 +1350,126 @@ static void __run_hrtimer(struct hrtimer *timer, ktime_t *now)
 	timer->state &= ~HRTIMER_STATE_CALLBACK;
 }
 
+static enum hrtimer_restart hrtimer_wakeup(struct hrtimer *timer);
+
+#ifdef CONFIG_PREEMPT_RT_BASE
+static void hrtimer_rt_reprogram(int restart, struct hrtimer *timer,
+				 struct hrtimer_clock_base *base)
+{
+	/*
+	 * Note, we clear the callback flag before we requeue the
+	 * timer otherwise we trigger the callback_running() check
+	 * in hrtimer_reprogram().
+	 */
+	timer->state &= ~HRTIMER_STATE_CALLBACK;
+
+	if (restart != HRTIMER_NORESTART) {
+		BUG_ON(hrtimer_active(timer));
+		/*
+		 * Enqueue the timer, if it's the leftmost timer then
+		 * we need to reprogram it.
+		 */
+		if (!enqueue_hrtimer(timer, base))
+			return;
+
+#ifndef CONFIG_HIGH_RES_TIMERS
+	}
+#else
+		if (base->cpu_base->hres_active &&
+		    hrtimer_reprogram(timer, base))
+			goto requeue;
+
+	} else if (hrtimer_active(timer)) {
+		/*
+		 * If the timer was rearmed on another CPU, reprogram
+		 * the event device.
+		 */
+		if (&timer->node == base->active.next &&
+		    base->cpu_base->hres_active &&
+		    hrtimer_reprogram(timer, base))
+			goto requeue;
+	}
+	return;
+
+requeue:
+	/*
+	 * Timer is expired. Thus move it from tree to pending list
+	 * again.
+	 */
+	__remove_hrtimer(timer, base, timer->state, 0);
+	list_add_tail(&timer->cb_entry, &base->expired);
+#endif
+}
+
+/*
+ * The changes in mainline which removed the callback modes from
+ * hrtimer are not yet working with -rt. The non wakeup_process()
+ * based callbacks which involve sleeping locks need to be treated
+ * seperately.
+ */
+static void hrtimer_rt_run_pending(void)
+{
+	enum hrtimer_restart (*fn)(struct hrtimer *);
+	struct hrtimer_cpu_base *cpu_base;
+	struct hrtimer_clock_base *base;
+	struct hrtimer *timer;
+	int index, restart;
+
+	local_irq_disable();
+	cpu_base = &per_cpu(hrtimer_bases, smp_processor_id());
+
+	raw_spin_lock(&cpu_base->lock);
+
+	for (index = 0; index < HRTIMER_MAX_CLOCK_BASES; index++) {
+		base = &cpu_base->clock_base[index];
+
+		while (!list_empty(&base->expired)) {
+			timer = list_first_entry(&base->expired,
+						 struct hrtimer, cb_entry);
+
+			/*
+			 * Same as the above __run_hrtimer function
+			 * just we run with interrupts enabled.
+			 */
+			debug_hrtimer_deactivate(timer);
+			__remove_hrtimer(timer, base, HRTIMER_STATE_CALLBACK, 0);
+			timer_stats_account_hrtimer(timer);
+			fn = timer->function;
+
+			raw_spin_unlock_irq(&cpu_base->lock);
+			restart = fn(timer);
+			raw_spin_lock_irq(&cpu_base->lock);
+
+			hrtimer_rt_reprogram(restart, timer, base);
+		}
+	}
+
+	raw_spin_unlock_irq(&cpu_base->lock);
+
+	wake_up_timer_waiters(cpu_base);
+}
+
+static int hrtimer_rt_defer(struct hrtimer *timer)
+{
+	if (timer->irqsafe)
+		return 0;
+
+	__remove_hrtimer(timer, timer->base, timer->state, 0);
+	list_add_tail(&timer->cb_entry, &timer->base->expired);
+	return 1;
+}
+
+#else
+
+static inline void hrtimer_rt_run_pending(void)
+{
+	hrtimer_peek_ahead_timers();
+}
+
+static inline int hrtimer_rt_defer(struct hrtimer *timer) { return 0; }
+
+#endif
+
 #ifdef CONFIG_HIGH_RES_TIMERS
 
 /*
@@ -1246,7 +1480,7 @@ void hrtimer_interrupt(struct clock_event_device *dev)
 {
 	struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);
 	ktime_t expires_next, now, entry_time, delta;
-	int i, retries = 0;
+	int i, retries = 0, raise = 0;
 
 	BUG_ON(!cpu_base->hres_active);
 	cpu_base->nr_events++;
@@ -1281,6 +1515,15 @@ retry:
 
 			timer = container_of(node, struct hrtimer, node);
 
+			trace_hrtimer_interrupt(raw_smp_processor_id(),
+			    ktime_to_ns(ktime_sub(ktime_to_ns(timer->praecox) ?
+				timer->praecox : hrtimer_get_expires(timer),
+				basenow)),
+			    current,
+			    timer->function == hrtimer_wakeup ?
+			    container_of(timer, struct hrtimer_sleeper,
+				timer)->task : NULL);
+
 			/*
 			 * The immediate goal for using the softexpires is
 			 * minimizing wakeups, not running timers at the
@@ -1296,7 +1539,10 @@ retry:
 			if (basenow.tv64 < hrtimer_get_softexpires_tv64(timer))
 				break;
 
-			__run_hrtimer(timer, &basenow);
+			if (!hrtimer_rt_defer(timer))
+				__run_hrtimer(timer, &basenow);
+			else
+				raise = 1;
 		}
 	}
 	/* Reevaluate the clock bases for the next expiry */
@@ -1313,7 +1559,7 @@ retry:
 	if (expires_next.tv64 == KTIME_MAX ||
 	    !tick_program_event(expires_next, 0)) {
 		cpu_base->hang_detected = 0;
-		return;
+		goto out;
 	}
 
 	/*
@@ -1357,6 +1603,9 @@ retry:
 	tick_program_event(expires_next, 1);
 	printk_once(KERN_WARNING "hrtimer: interrupt took %llu ns\n",
 		    ktime_to_ns(delta));
+out:
+	if (raise)
+		raise_softirq_irqoff(HRTIMER_SOFTIRQ);
 }
 
 /*
@@ -1392,18 +1641,18 @@ void hrtimer_peek_ahead_timers(void)
 	__hrtimer_peek_ahead_timers();
 	local_irq_restore(flags);
 }
-
-static void run_hrtimer_softirq(struct softirq_action *h)
-{
-	hrtimer_peek_ahead_timers();
-}
-
 #else /* CONFIG_HIGH_RES_TIMERS */
 
 static inline void __hrtimer_peek_ahead_timers(void) { }
 
 #endif	/* !CONFIG_HIGH_RES_TIMERS */
 
+
+static void run_hrtimer_softirq(struct softirq_action *h)
+{
+	hrtimer_rt_run_pending();
+}
+
 /*
  * Called from timer softirq every jiffy, expire hrtimers:
  *
@@ -1436,7 +1685,7 @@ void hrtimer_run_queues(void)
 	struct timerqueue_node *node;
 	struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);
 	struct hrtimer_clock_base *base;
-	int index, gettime = 1;
+	int index, gettime = 1, raise = 0;
 
 	if (hrtimer_hres_active())
 		return;
@@ -1461,10 +1710,16 @@ void hrtimer_run_queues(void)
 					hrtimer_get_expires_tv64(timer))
 				break;
 
-			__run_hrtimer(timer, &base->softirq_time);
+			if (!hrtimer_rt_defer(timer))
+				__run_hrtimer(timer, &base->softirq_time);
+			else
+				raise = 1;
 		}
 		raw_spin_unlock(&cpu_base->lock);
 	}
+
+	if (raise)
+		raise_softirq_irqoff(HRTIMER_SOFTIRQ);
 }
 
 /*
@@ -1486,16 +1741,18 @@ static enum hrtimer_restart hrtimer_wakeup(struct hrtimer *timer)
 void hrtimer_init_sleeper(struct hrtimer_sleeper *sl, struct task_struct *task)
 {
 	sl->timer.function = hrtimer_wakeup;
+	sl->timer.irqsafe = 1;
 	sl->task = task;
 }
 EXPORT_SYMBOL_GPL(hrtimer_init_sleeper);
 
-static int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode)
+static int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode,
+				unsigned long state)
 {
 	hrtimer_init_sleeper(t, current);
 
 	do {
-		set_current_state(TASK_INTERRUPTIBLE);
+		set_current_state(state);
 		hrtimer_start_expires(&t->timer, mode);
 		if (!hrtimer_active(&t->timer))
 			t->task = NULL;
@@ -1539,7 +1796,8 @@ long __sched hrtimer_nanosleep_restart(struct restart_block *restart)
 				HRTIMER_MODE_ABS);
 	hrtimer_set_expires_tv64(&t.timer, restart->nanosleep.expires);
 
-	if (do_nanosleep(&t, HRTIMER_MODE_ABS))
+	/* cpu_chill() does not care about restart state. */
+	if (do_nanosleep(&t, HRTIMER_MODE_ABS, TASK_INTERRUPTIBLE))
 		goto out;
 
 	rmtp = restart->nanosleep.rmtp;
@@ -1556,8 +1814,10 @@ out:
 	return ret;
 }
 
-long hrtimer_nanosleep(struct timespec *rqtp, struct timespec __user *rmtp,
-		       const enum hrtimer_mode mode, const clockid_t clockid)
+static long
+__hrtimer_nanosleep(struct timespec *rqtp, struct timespec __user *rmtp,
+		    const enum hrtimer_mode mode, const clockid_t clockid,
+		    unsigned long state)
 {
 	struct restart_block *restart;
 	struct hrtimer_sleeper t;
@@ -1570,7 +1830,7 @@ long hrtimer_nanosleep(struct timespec *rqtp, struct timespec __user *rmtp,
 
 	hrtimer_init_on_stack(&t.timer, clockid, mode);
 	hrtimer_set_expires_range_ns(&t.timer, timespec_to_ktime(*rqtp), slack);
-	if (do_nanosleep(&t, mode))
+	if (do_nanosleep(&t, mode, state))
 		goto out;
 
 	/* Absolute timers do not update the rmtp value and restart: */
@@ -1597,6 +1857,12 @@ out:
 	return ret;
 }
 
+long hrtimer_nanosleep(struct timespec *rqtp, struct timespec __user *rmtp,
+		       const enum hrtimer_mode mode, const clockid_t clockid)
+{
+	return __hrtimer_nanosleep(rqtp, rmtp, mode, clockid, TASK_INTERRUPTIBLE);
+}
+
 SYSCALL_DEFINE2(nanosleep, struct timespec __user *, rqtp,
 		struct timespec __user *, rmtp)
 {
@@ -1611,6 +1877,26 @@ SYSCALL_DEFINE2(nanosleep, struct timespec __user *, rqtp,
 	return hrtimer_nanosleep(&tu, rmtp, HRTIMER_MODE_REL, CLOCK_MONOTONIC);
 }
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+/*
+ * Sleep for 1 ms in hope whoever holds what we want will let it go.
+ */
+void cpu_chill(void)
+{
+	struct timespec tu = {
+		.tv_nsec = NSEC_PER_MSEC,
+	};
+	unsigned int freeze_flag = current->flags & PF_NOFREEZE;
+
+	current->flags |= PF_NOFREEZE;
+	__hrtimer_nanosleep(&tu, NULL, HRTIMER_MODE_REL, CLOCK_MONOTONIC,
+			    TASK_UNINTERRUPTIBLE);
+	if (!freeze_flag)
+		current->flags &= ~PF_NOFREEZE;
+}
+EXPORT_SYMBOL(cpu_chill);
+#endif
+
 /*
  * Functions related to boot-time initialization:
  */
@@ -1622,10 +1908,14 @@ static void init_hrtimers_cpu(int cpu)
 	for (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {
 		cpu_base->clock_base[i].cpu_base = cpu_base;
 		timerqueue_init_head(&cpu_base->clock_base[i].active);
+		INIT_LIST_HEAD(&cpu_base->clock_base[i].expired);
 	}
 
 	cpu_base->cpu = cpu;
 	hrtimer_init_hres(cpu_base);
+#ifdef CONFIG_PREEMPT_RT_BASE
+	init_waitqueue_head(&cpu_base->wait);
+#endif
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
@@ -1738,9 +2028,7 @@ void __init hrtimers_init(void)
 	hrtimer_cpu_notify(&hrtimers_nb, (unsigned long)CPU_UP_PREPARE,
 			  (void *)(long)smp_processor_id());
 	register_cpu_notifier(&hrtimers_nb);
-#ifdef CONFIG_HIGH_RES_TIMERS
 	open_softirq(HRTIMER_SOFTIRQ, run_hrtimer_softirq);
-#endif
 }
 
 /**
diff --git a/kernel/msm-3.18/kernel/time/itimer.c b/kernel/msm-3.18/kernel/time/itimer.c
index 8d262b467..d0513909d 100644
--- a/kernel/msm-3.18/kernel/time/itimer.c
+++ b/kernel/msm-3.18/kernel/time/itimer.c
@@ -213,6 +213,7 @@ again:
 		/* We are sharing ->siglock with it_real_fn() */
 		if (hrtimer_try_to_cancel(timer) < 0) {
 			spin_unlock_irq(&tsk->sighand->siglock);
+			hrtimer_wait_for_timer(&tsk->signal->real_timer);
 			goto again;
 		}
 		expires = timeval_to_ktime(value->it_value);
diff --git a/kernel/msm-3.18/kernel/time/jiffies.c b/kernel/msm-3.18/kernel/time/jiffies.c
index a6a5bf53e..23d7203cc 100644
--- a/kernel/msm-3.18/kernel/time/jiffies.c
+++ b/kernel/msm-3.18/kernel/time/jiffies.c
@@ -73,7 +73,8 @@ static struct clocksource clocksource_jiffies = {
 	.shift		= JIFFIES_SHIFT,
 };
 
-__cacheline_aligned_in_smp DEFINE_SEQLOCK(jiffies_lock);
+__cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(jiffies_lock);
+__cacheline_aligned_in_smp seqcount_t jiffies_seq;
 
 #if (BITS_PER_LONG < 64)
 u64 get_jiffies_64(void)
@@ -82,9 +83,9 @@ u64 get_jiffies_64(void)
 	u64 ret;
 
 	do {
-		seq = read_seqbegin(&jiffies_lock);
+		seq = read_seqcount_begin(&jiffies_seq);
 		ret = jiffies_64;
-	} while (read_seqretry(&jiffies_lock, seq));
+	} while (read_seqcount_retry(&jiffies_seq, seq));
 	return ret;
 }
 EXPORT_SYMBOL(get_jiffies_64);
diff --git a/kernel/msm-3.18/kernel/time/ntp.c b/kernel/msm-3.18/kernel/time/ntp.c
index 85fb3d632..02897eb74 100644
--- a/kernel/msm-3.18/kernel/time/ntp.c
+++ b/kernel/msm-3.18/kernel/time/ntp.c
@@ -10,6 +10,7 @@
 #include <linux/workqueue.h>
 #include <linux/hrtimer.h>
 #include <linux/jiffies.h>
+#include <linux/kthread.h>
 #include <linux/math64.h>
 #include <linux/timex.h>
 #include <linux/time.h>
@@ -519,10 +520,52 @@ static void sync_cmos_clock(struct work_struct *work)
 			   &sync_cmos_work, timespec_to_jiffies(&next));
 }
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+/*
+ * RT can not call schedule_delayed_work from real interrupt context.
+ * Need to make a thread to do the real work.
+ */
+static struct task_struct *cmos_delay_thread;
+static bool do_cmos_delay;
+
+static int run_cmos_delay(void *ignore)
+{
+	while (!kthread_should_stop()) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		if (do_cmos_delay) {
+			do_cmos_delay = false;
+			queue_delayed_work(system_power_efficient_wq,
+					   &sync_cmos_work, 0);
+		}
+		schedule();
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+}
+
+void ntp_notify_cmos_timer(void)
+{
+	do_cmos_delay = true;
+	/* Make visible before waking up process */
+	smp_wmb();
+	wake_up_process(cmos_delay_thread);
+}
+
+static __init int create_cmos_delay_thread(void)
+{
+	cmos_delay_thread = kthread_run(run_cmos_delay, NULL, "kcmosdelayd");
+	BUG_ON(!cmos_delay_thread);
+	return 0;
+}
+early_initcall(create_cmos_delay_thread);
+
+#else
+
 void ntp_notify_cmos_timer(void)
 {
 	queue_delayed_work(system_power_efficient_wq, &sync_cmos_work, 0);
 }
+#endif /* CONFIG_PREEMPT_RT_FULL */
 
 #else
 void ntp_notify_cmos_timer(void) { }
diff --git a/kernel/msm-3.18/kernel/time/posix-cpu-timers.c b/kernel/msm-3.18/kernel/time/posix-cpu-timers.c
index b6e20a95b..17cd1d22a 100644
--- a/kernel/msm-3.18/kernel/time/posix-cpu-timers.c
+++ b/kernel/msm-3.18/kernel/time/posix-cpu-timers.c
@@ -3,6 +3,7 @@
  */
 
 #include <linux/sched.h>
+#include <linux/sched/rt.h>
 #include <linux/posix-timers.h>
 #include <linux/errno.h>
 #include <linux/math64.h>
@@ -626,7 +627,7 @@ static int posix_cpu_timer_set(struct k_itimer *timer, int timer_flags,
 	/*
 	 * Disarm any old timer after extracting its expiry time.
 	 */
-	WARN_ON_ONCE(!irqs_disabled());
+	WARN_ON_ONCE_NONRT(!irqs_disabled());
 
 	ret = 0;
 	old_incr = timer->it.cpu.incr;
@@ -1048,7 +1049,7 @@ void posix_cpu_timer_schedule(struct k_itimer *timer)
 	/*
 	 * Now re-arm for the new expiry time.
 	 */
-	WARN_ON_ONCE(!irqs_disabled());
+	WARN_ON_ONCE_NONRT(!irqs_disabled());
 	arm_timer(timer);
 	unlock_task_sighand(p, &flags);
 
@@ -1114,10 +1115,11 @@ static inline int fastpath_timer_check(struct task_struct *tsk)
 	sig = tsk->signal;
 	if (sig->cputimer.running) {
 		struct task_cputime group_sample;
+		unsigned long flags;
 
-		raw_spin_lock(&sig->cputimer.lock);
+		raw_spin_lock_irqsave(&sig->cputimer.lock, flags);
 		group_sample = sig->cputimer.cputime;
-		raw_spin_unlock(&sig->cputimer.lock);
+		raw_spin_unlock_irqrestore(&sig->cputimer.lock, flags);
 
 		if (task_cputime_expired(&group_sample, &sig->cputime_expires))
 			return 1;
@@ -1131,13 +1133,13 @@ static inline int fastpath_timer_check(struct task_struct *tsk)
  * already updated our counts.  We need to check if any timers fire now.
  * Interrupts are disabled.
  */
-void run_posix_cpu_timers(struct task_struct *tsk)
+static void __run_posix_cpu_timers(struct task_struct *tsk)
 {
 	LIST_HEAD(firing);
 	struct k_itimer *timer, *next;
 	unsigned long flags;
 
-	WARN_ON_ONCE(!irqs_disabled());
+	WARN_ON_ONCE_NONRT(!irqs_disabled());
 
 	/*
 	 * The fast path checks that there are no expired thread or thread
@@ -1195,6 +1197,190 @@ void run_posix_cpu_timers(struct task_struct *tsk)
 	}
 }
 
+#ifdef CONFIG_PREEMPT_RT_BASE
+#include <linux/kthread.h>
+#include <linux/cpu.h>
+DEFINE_PER_CPU(struct task_struct *, posix_timer_task);
+DEFINE_PER_CPU(struct task_struct *, posix_timer_tasklist);
+
+static int posix_cpu_timers_thread(void *data)
+{
+	int cpu = (long)data;
+
+	BUG_ON(per_cpu(posix_timer_task,cpu) != current);
+
+	while (!kthread_should_stop()) {
+		struct task_struct *tsk = NULL;
+		struct task_struct *next = NULL;
+
+		if (cpu_is_offline(cpu))
+			goto wait_to_die;
+
+		/* grab task list */
+		raw_local_irq_disable();
+		tsk = per_cpu(posix_timer_tasklist, cpu);
+		per_cpu(posix_timer_tasklist, cpu) = NULL;
+		raw_local_irq_enable();
+
+		/* its possible the list is empty, just return */
+		if (!tsk) {
+			set_current_state(TASK_INTERRUPTIBLE);
+			schedule();
+			__set_current_state(TASK_RUNNING);
+			continue;
+		}
+
+		/* Process task list */
+		while (1) {
+			/* save next */
+			next = tsk->posix_timer_list;
+
+			/* run the task timers, clear its ptr and
+			 * unreference it
+			 */
+			__run_posix_cpu_timers(tsk);
+			tsk->posix_timer_list = NULL;
+			put_task_struct(tsk);
+
+			/* check if this is the last on the list */
+			if (next == tsk)
+				break;
+			tsk = next;
+		}
+	}
+	return 0;
+
+wait_to_die:
+	/* Wait for kthread_stop */
+	set_current_state(TASK_INTERRUPTIBLE);
+	while (!kthread_should_stop()) {
+		schedule();
+		set_current_state(TASK_INTERRUPTIBLE);
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+}
+
+static inline int __fastpath_timer_check(struct task_struct *tsk)
+{
+	/* tsk == current, ensure it is safe to use ->signal/sighand */
+	if (unlikely(tsk->exit_state))
+		return 0;
+
+	if (!task_cputime_zero(&tsk->cputime_expires))
+			return 1;
+
+	if (!task_cputime_zero(&tsk->signal->cputime_expires))
+			return 1;
+
+	return 0;
+}
+
+void run_posix_cpu_timers(struct task_struct *tsk)
+{
+	unsigned long cpu = smp_processor_id();
+	struct task_struct *tasklist;
+
+	BUG_ON(!irqs_disabled());
+	if(!per_cpu(posix_timer_task, cpu))
+		return;
+	/* get per-cpu references */
+	tasklist = per_cpu(posix_timer_tasklist, cpu);
+
+	/* check to see if we're already queued */
+	if (!tsk->posix_timer_list && __fastpath_timer_check(tsk)) {
+		get_task_struct(tsk);
+		if (tasklist) {
+			tsk->posix_timer_list = tasklist;
+		} else {
+			/*
+			 * The list is terminated by a self-pointing
+			 * task_struct
+			 */
+			tsk->posix_timer_list = tsk;
+		}
+		per_cpu(posix_timer_tasklist, cpu) = tsk;
+
+		wake_up_process(per_cpu(posix_timer_task, cpu));
+	}
+}
+
+/*
+ * posix_cpu_thread_call - callback that gets triggered when a CPU is added.
+ * Here we can start up the necessary migration thread for the new CPU.
+ */
+static int posix_cpu_thread_call(struct notifier_block *nfb,
+				 unsigned long action, void *hcpu)
+{
+	int cpu = (long)hcpu;
+	struct task_struct *p;
+	struct sched_param param;
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+		p = kthread_create(posix_cpu_timers_thread, hcpu,
+					"posixcputmr/%d",cpu);
+		if (IS_ERR(p))
+			return NOTIFY_BAD;
+		p->flags |= PF_NOFREEZE;
+		kthread_bind(p, cpu);
+		/* Must be high prio to avoid getting starved */
+		param.sched_priority = MAX_RT_PRIO-1;
+		sched_setscheduler(p, SCHED_FIFO, &param);
+		per_cpu(posix_timer_task,cpu) = p;
+		break;
+	case CPU_ONLINE:
+		/* Strictly unneccessary, as first user will wake it. */
+		wake_up_process(per_cpu(posix_timer_task,cpu));
+		break;
+#ifdef CONFIG_HOTPLUG_CPU
+	case CPU_UP_CANCELED:
+		/* Unbind it from offline cpu so it can run.  Fall thru. */
+		kthread_bind(per_cpu(posix_timer_task, cpu),
+			     cpumask_any(cpu_online_mask));
+		kthread_stop(per_cpu(posix_timer_task,cpu));
+		per_cpu(posix_timer_task,cpu) = NULL;
+		break;
+	case CPU_DEAD:
+		kthread_stop(per_cpu(posix_timer_task,cpu));
+		per_cpu(posix_timer_task,cpu) = NULL;
+		break;
+#endif
+	}
+	return NOTIFY_OK;
+}
+
+/* Register at highest priority so that task migration (migrate_all_tasks)
+ * happens before everything else.
+ */
+static struct notifier_block posix_cpu_thread_notifier = {
+	.notifier_call = posix_cpu_thread_call,
+	.priority = 10
+};
+
+static int __init posix_cpu_thread_init(void)
+{
+	void *hcpu = (void *)(long)smp_processor_id();
+	/* Start one for boot CPU. */
+	unsigned long cpu;
+
+	/* init the per-cpu posix_timer_tasklets */
+	for_each_possible_cpu(cpu)
+		per_cpu(posix_timer_tasklist, cpu) = NULL;
+
+	posix_cpu_thread_call(&posix_cpu_thread_notifier, CPU_UP_PREPARE, hcpu);
+	posix_cpu_thread_call(&posix_cpu_thread_notifier, CPU_ONLINE, hcpu);
+	register_cpu_notifier(&posix_cpu_thread_notifier);
+	return 0;
+}
+early_initcall(posix_cpu_thread_init);
+#else /* CONFIG_PREEMPT_RT_BASE */
+void run_posix_cpu_timers(struct task_struct *tsk)
+{
+	__run_posix_cpu_timers(tsk);
+}
+#endif /* CONFIG_PREEMPT_RT_BASE */
+
 /*
  * Set one of the process-wide special case CPU timers or RLIMIT_CPU.
  * The tsk->sighand->siglock must be held by the caller.
diff --git a/kernel/msm-3.18/kernel/time/posix-timers.c b/kernel/msm-3.18/kernel/time/posix-timers.c
index 31ea01f42..0f5d7eae6 100644
--- a/kernel/msm-3.18/kernel/time/posix-timers.c
+++ b/kernel/msm-3.18/kernel/time/posix-timers.c
@@ -499,6 +499,7 @@ static enum hrtimer_restart posix_timer_fn(struct hrtimer *timer)
 static struct pid *good_sigevent(sigevent_t * event)
 {
 	struct task_struct *rtn = current->group_leader;
+	int sig = event->sigev_signo;
 
 	if ((event->sigev_notify & SIGEV_THREAD_ID ) &&
 		(!(rtn = find_task_by_vpid(event->sigev_notify_thread_id)) ||
@@ -507,7 +508,8 @@ static struct pid *good_sigevent(sigevent_t * event)
 		return NULL;
 
 	if (((event->sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_NONE) &&
-	    ((event->sigev_signo <= 0) || (event->sigev_signo > SIGRTMAX)))
+	    (sig <= 0 || sig > SIGRTMAX || sig_kernel_only(sig) ||
+	     sig_kernel_coredump(sig)))
 		return NULL;
 
 	return task_pid(rtn);
@@ -819,6 +821,20 @@ SYSCALL_DEFINE1(timer_getoverrun, timer_t, timer_id)
 	return overrun;
 }
 
+/*
+ * Protected by RCU!
+ */
+static void timer_wait_for_callback(struct k_clock *kc, struct k_itimer *timr)
+{
+#ifdef CONFIG_PREEMPT_RT_FULL
+	if (kc->timer_set == common_timer_set)
+		hrtimer_wait_for_timer(&timr->it.real.timer);
+	else
+		/* FIXME: Whacky hack for posix-cpu-timers */
+		schedule_timeout(1);
+#endif
+}
+
 /* Set a POSIX.1b interval timer. */
 /* timr->it_lock is taken. */
 static int
@@ -896,6 +912,7 @@ retry:
 	if (!timr)
 		return -EINVAL;
 
+	rcu_read_lock();
 	kc = clockid_to_kclock(timr->it_clock);
 	if (WARN_ON_ONCE(!kc || !kc->timer_set))
 		error = -EINVAL;
@@ -904,9 +921,12 @@ retry:
 
 	unlock_timer(timr, flag);
 	if (error == TIMER_RETRY) {
+		timer_wait_for_callback(kc, timr);
 		rtn = NULL;	// We already got the old time...
+		rcu_read_unlock();
 		goto retry;
 	}
+	rcu_read_unlock();
 
 	if (old_setting && !error &&
 	    copy_to_user(old_setting, &old_spec, sizeof (old_spec)))
@@ -944,10 +964,15 @@ retry_delete:
 	if (!timer)
 		return -EINVAL;
 
+	rcu_read_lock();
 	if (timer_delete_hook(timer) == TIMER_RETRY) {
 		unlock_timer(timer, flags);
+		timer_wait_for_callback(clockid_to_kclock(timer->it_clock),
+					timer);
+		rcu_read_unlock();
 		goto retry_delete;
 	}
+	rcu_read_unlock();
 
 	spin_lock(&current->sighand->siglock);
 	list_del(&timer->list);
@@ -973,8 +998,18 @@ static void itimer_delete(struct k_itimer *timer)
 retry_delete:
 	spin_lock_irqsave(&timer->it_lock, flags);
 
+	/* On RT we can race with a deletion */
+	if (!timer->it_signal) {
+		unlock_timer(timer, flags);
+		return;
+	}
+
 	if (timer_delete_hook(timer) == TIMER_RETRY) {
+		rcu_read_lock();
 		unlock_timer(timer, flags);
+		timer_wait_for_callback(clockid_to_kclock(timer->it_clock),
+					timer);
+		rcu_read_unlock();
 		goto retry_delete;
 	}
 	list_del(&timer->list);
diff --git a/kernel/msm-3.18/kernel/time/tick-broadcast-hrtimer.c b/kernel/msm-3.18/kernel/time/tick-broadcast-hrtimer.c
index 6aac4beed..943c03395 100644
--- a/kernel/msm-3.18/kernel/time/tick-broadcast-hrtimer.c
+++ b/kernel/msm-3.18/kernel/time/tick-broadcast-hrtimer.c
@@ -109,5 +109,6 @@ void tick_setup_hrtimer_broadcast(void)
 {
 	hrtimer_init(&bctimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
 	bctimer.function = bc_handler;
+	bctimer.irqsafe = true;
 	clockevents_register_device(&ce_broadcast_hrtimer);
 }
diff --git a/kernel/msm-3.18/kernel/time/tick-common.c b/kernel/msm-3.18/kernel/time/tick-common.c
index 7efeedf53..e3dd07d0e 100644
--- a/kernel/msm-3.18/kernel/time/tick-common.c
+++ b/kernel/msm-3.18/kernel/time/tick-common.c
@@ -78,13 +78,15 @@ int tick_is_oneshot_available(void)
 static void tick_periodic(int cpu)
 {
 	if (tick_do_timer_cpu == cpu) {
-		write_seqlock(&jiffies_lock);
+		raw_spin_lock(&jiffies_lock);
+		write_seqcount_begin(&jiffies_seq);
 
 		/* Keep track of the next tick event */
 		tick_next_period = ktime_add(tick_next_period, tick_period);
 
 		do_timer(1);
-		write_sequnlock(&jiffies_lock);
+		write_seqcount_end(&jiffies_seq);
+		raw_spin_unlock(&jiffies_lock);
 		update_wall_time();
 	}
 
@@ -146,9 +148,9 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 		ktime_t next;
 
 		do {
-			seq = read_seqbegin(&jiffies_lock);
+			seq = read_seqcount_begin(&jiffies_seq);
 			next = tick_next_period;
-		} while (read_seqretry(&jiffies_lock, seq));
+		} while (read_seqcount_retry(&jiffies_seq, seq));
 
 		clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
 
diff --git a/kernel/msm-3.18/kernel/time/tick-internal.h b/kernel/msm-3.18/kernel/time/tick-internal.h
index 366aeb4f2..8e118bb8e 100644
--- a/kernel/msm-3.18/kernel/time/tick-internal.h
+++ b/kernel/msm-3.18/kernel/time/tick-internal.h
@@ -6,7 +6,8 @@
 
 #include "timekeeping.h"
 
-extern seqlock_t jiffies_lock;
+extern raw_spinlock_t jiffies_lock;
+extern seqcount_t jiffies_seq;
 
 #define CS_NAME_LEN	32
 
diff --git a/kernel/msm-3.18/kernel/time/tick-sched.c b/kernel/msm-3.18/kernel/time/tick-sched.c
index 71558e32a..d46acce03 100644
--- a/kernel/msm-3.18/kernel/time/tick-sched.c
+++ b/kernel/msm-3.18/kernel/time/tick-sched.c
@@ -83,7 +83,8 @@ static void tick_do_update_jiffies64(ktime_t now)
 		return;
 
 	/* Reevalute with jiffies_lock held */
-	write_seqlock(&jiffies_lock);
+	raw_spin_lock(&jiffies_lock);
+	write_seqcount_begin(&jiffies_seq);
 
 	delta = ktime_sub(now, last_jiffies_update);
 	if (delta.tv64 >= tick_period.tv64) {
@@ -106,10 +107,12 @@ static void tick_do_update_jiffies64(ktime_t now)
 		/* Keep the tick_next_period variable up to date */
 		tick_next_period = ktime_add(last_jiffies_update, tick_period);
 	} else {
-		write_sequnlock(&jiffies_lock);
+		write_seqcount_end(&jiffies_seq);
+		raw_spin_unlock(&jiffies_lock);
 		return;
 	}
-	write_sequnlock(&jiffies_lock);
+	write_seqcount_end(&jiffies_seq);
+	raw_spin_unlock(&jiffies_lock);
 	update_wall_time();
 }
 
@@ -120,12 +123,14 @@ static ktime_t tick_init_jiffy_update(void)
 {
 	ktime_t period;
 
-	write_seqlock(&jiffies_lock);
+	raw_spin_lock(&jiffies_lock);
+	write_seqcount_begin(&jiffies_seq);
 	/* Did we start the jiffies update yet ? */
 	if (last_jiffies_update.tv64 == 0)
 		last_jiffies_update = tick_next_period;
 	period = last_jiffies_update;
-	write_sequnlock(&jiffies_lock);
+	write_seqcount_end(&jiffies_seq);
+	raw_spin_unlock(&jiffies_lock);
 	return period;
 }
 
@@ -197,6 +202,11 @@ static bool can_stop_full_tick(void)
 		return false;
 	}
 
+	if (!arch_irq_work_has_interrupt()) {
+		trace_tick_stop(0, "missing irq work interrupt\n");
+		return false;
+	}
+
 	/* sched_clock_tick() needs us? */
 #ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
 	/*
@@ -238,11 +248,17 @@ void __tick_nohz_full_check(void)
 
 static void nohz_full_kick_work_func(struct irq_work *work)
 {
+	unsigned long flags;
+
+	/* ksoftirqd processes sirqs with interrupts enabled */
+	local_irq_save(flags);
 	__tick_nohz_full_check();
+	local_irq_restore(flags);
 }
 
 static DEFINE_PER_CPU(struct irq_work, nohz_full_kick_work) = {
 	.func = nohz_full_kick_work_func,
+	.flags = IRQ_WORK_HARD_IRQ,
 };
 
 /*
@@ -601,10 +617,10 @@ static ktime_t tick_nohz_stop_sched_tick(struct tick_sched *ts,
 
 	/* Read jiffies and the time when jiffies were updated last */
 	do {
-		seq = read_seqbegin(&jiffies_lock);
+		seq = read_seqcount_begin(&jiffies_seq);
 		last_update = last_jiffies_update;
 		last_jiffies = jiffies;
-	} while (read_seqretry(&jiffies_lock, seq));
+	} while (read_seqcount_retry(&jiffies_seq, seq));
 
 	if (rcu_needs_cpu(cpu, &rcu_delta_jiffies) ||
 	    arch_needs_cpu() || irq_work_needs_cpu()) {
@@ -782,14 +798,7 @@ static bool can_stop_idle_tick(int cpu, struct tick_sched *ts)
 		return false;
 
 	if (unlikely(local_softirq_pending() && cpu_online(cpu))) {
-		static int ratelimit;
-
-		if (ratelimit < 10 &&
-		    (local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK)) {
-			pr_warn("NOHZ: local_softirq_pending %02x\n",
-				(unsigned int) local_softirq_pending());
-			ratelimit++;
-		}
+		softirq_check_pending_idle();
 		return false;
 	}
 
@@ -1238,6 +1247,7 @@ void tick_setup_sched_timer(void)
 	 * Emulate tick processing via per-CPU hrtimers:
 	 */
 	hrtimer_init(&ts->sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+	ts->sched_timer.irqsafe = 1;
 	ts->sched_timer.function = tick_sched_timer;
 
 	/* Get the next period (per cpu) */
diff --git a/kernel/msm-3.18/kernel/time/timekeeping.c b/kernel/msm-3.18/kernel/time/timekeeping.c
index 5ba37f44a..d68f58e38 100644
--- a/kernel/msm-3.18/kernel/time/timekeeping.c
+++ b/kernel/msm-3.18/kernel/time/timekeeping.c
@@ -1841,8 +1841,10 @@ EXPORT_SYMBOL(hardpps);
  */
 void xtime_update(unsigned long ticks)
 {
-	write_seqlock(&jiffies_lock);
+	raw_spin_lock(&jiffies_lock);
+	write_seqcount_begin(&jiffies_seq);
 	do_timer(ticks);
-	write_sequnlock(&jiffies_lock);
+	write_seqcount_end(&jiffies_seq);
+	raw_spin_unlock(&jiffies_lock);
 	update_wall_time();
 }
diff --git a/kernel/msm-3.18/kernel/time/timer.c b/kernel/msm-3.18/kernel/time/timer.c
index b865857f8..0d9e6817c 100644
--- a/kernel/msm-3.18/kernel/time/timer.c
+++ b/kernel/msm-3.18/kernel/time/timer.c
@@ -80,6 +80,9 @@ struct tvec_root {
 struct tvec_base {
 	spinlock_t lock;
 	struct timer_list *running_timer;
+#ifdef CONFIG_PREEMPT_RT_FULL
+	wait_queue_head_t wait_for_running_timer;
+#endif
 	unsigned long timer_jiffies;
 	unsigned long next_timer;
 	unsigned long active_timers;
@@ -828,6 +831,36 @@ static struct tvec_base *lock_timer_base(struct timer_list *timer,
 	}
 }
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+static inline struct tvec_base *switch_timer_base(struct timer_list *timer,
+						  struct tvec_base *old,
+						  struct tvec_base *new)
+{
+	/* See the comment in lock_timer_base() */
+	timer_set_base(timer, NULL);
+	spin_unlock(&old->lock);
+	spin_lock(&new->lock);
+	timer_set_base(timer, new);
+	return new;
+}
+#else
+static inline struct tvec_base *switch_timer_base(struct timer_list *timer,
+						  struct tvec_base *old,
+						  struct tvec_base *new)
+{
+	/*
+	 * We cannot do the above because we might be preempted and
+	 * then the preempter would see NULL and loop forever.
+	 */
+	if (spin_trylock(&new->lock)) {
+		timer_set_base(timer, new);
+		spin_unlock(&old->lock);
+		return new;
+	}
+	return old;
+}
+#endif
+
 static inline int
 __mod_timer(struct timer_list *timer, unsigned long expires,
 						bool pending_only, int pinned)
@@ -854,21 +887,14 @@ __mod_timer(struct timer_list *timer, unsigned long expires,
 		if (base != new_base) {
 			/*
 			 * We are trying to schedule the timer on the local CPU.
-			 * However we can't change timer's base while it is
-			 * running, otherwise del_timer_sync() can't detect that
-			 * the timer's handler yet has not finished. This also
-			 * guarantees that the timer is serialized wrt itself.
+			 * However we can't change timer's base while it is running,
+			 * otherwise del_timer_sync() can't detect that the timer's
+			 * handler yet has not finished. This also guarantees that
+			 * the timer is serialized wrt itself.
 			 */
-			if (likely(base->running_timer != timer)) {
-				/* See the comment in lock_timer_base() */
-				timer_set_base(timer, NULL);
-				spin_unlock(&base->lock);
-				base = new_base;
-				spin_lock(&base->lock);
-				timer_set_base(timer, base);
-			}
+			if (likely(base->running_timer != timer))
+				base = switch_timer_base(timer, base, new_base);
 		}
-	}
 
 	timer->expires = expires;
 	internal_add_timer(base, timer);
@@ -1055,6 +1081,29 @@ void add_timer_on(struct timer_list *timer, int cpu)
 }
 EXPORT_SYMBOL_GPL(add_timer_on);
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+/*
+ * Wait for a running timer
+ */
+static void wait_for_running_timer(struct timer_list *timer)
+{
+	struct tvec_base *base = timer->base;
+
+	if (base->running_timer == timer)
+		wait_event(base->wait_for_running_timer,
+			   base->running_timer != timer);
+}
+
+# define wakeup_timer_waiters(b)	wake_up_all(&(b)->wait_for_running_timer)
+#else
+static inline void wait_for_running_timer(struct timer_list *timer)
+{
+	cpu_relax();
+}
+
+# define wakeup_timer_waiters(b)	do { } while (0)
+#endif
+
 /**
  * del_timer - deactive a timer.
  * @timer: the timer to be deactivated
@@ -1112,7 +1161,7 @@ int try_to_del_timer_sync(struct timer_list *timer)
 }
 EXPORT_SYMBOL(try_to_del_timer_sync);
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT_FULL)
 /**
  * del_timer_sync - deactivate a timer and wait for the handler to finish.
  * @timer: the timer to be deactivated
@@ -1172,7 +1221,7 @@ int del_timer_sync(struct timer_list *timer)
 		int ret = try_to_del_timer_sync(timer);
 		if (ret >= 0)
 			return ret;
-		cpu_relax();
+		wait_for_running_timer(timer);
 	}
 }
 EXPORT_SYMBOL(del_timer_sync);
@@ -1293,16 +1342,18 @@ static inline void __run_timers(struct tvec_base *base)
 			if (irqsafe) {
 				spin_unlock(&base->lock);
 				call_timer_fn(timer, fn, data);
+				base->running_timer = NULL;
 				spin_lock(&base->lock);
 			} else {
 				spin_unlock_irq(&base->lock);
 				call_timer_fn(timer, fn, data);
+				base->running_timer = NULL;
 				spin_lock_irq(&base->lock);
 			}
 		}
 	}
-	base->running_timer = NULL;
 	spin_unlock_irq(&base->lock);
+	wakeup_timer_waiters(base);
 }
 
 #ifdef CONFIG_NO_HZ_COMMON
@@ -1465,17 +1516,31 @@ unsigned long get_next_timer_interrupt(unsigned long now)
 	if (cpu_is_offline(smp_processor_id()))
 		return expires;
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+	/*
+	 * On PREEMPT_RT we cannot sleep here. If the trylock does not
+	 * succeed then we return the worst-case 'expires in 1 tick'
+	 * value.  We use the rt functions here directly to avoid a
+	 * migrate_disable() call.
+	 */
+	if (!spin_do_trylock(&base->lock))
+		return  now + 1;
+#else
 	spin_lock(&base->lock);
+#endif
 	if (base->active_timers) {
 		if (time_before_eq(base->next_timer, base->timer_jiffies))
 			base->next_timer = __next_timer_interrupt(base);
 		expires = base->next_timer;
 	}
+#ifdef CONFIG_PREEMPT_RT_FULL
+	rt_spin_unlock_after_trylock_in_irq(&base->lock);
+#else
 	spin_unlock(&base->lock);
+#endif
 
 	if (time_before_eq(expires, now))
 		return now;
-
 	return cmp_next_hrtimer_event(now, expires);
 }
 #endif
@@ -1491,13 +1556,14 @@ void update_process_times(int user_tick)
 
 	/* Note: this timer irq context must be accounted for as well. */
 	account_process_tick(p, user_tick);
+	scheduler_tick();
 	run_local_timers();
 	rcu_check_callbacks(cpu, user_tick);
-#ifdef CONFIG_IRQ_WORK
+
+#if defined(CONFIG_IRQ_WORK)
 	if (in_irq())
 		irq_work_tick();
 #endif
-	scheduler_tick();
 	run_posix_cpu_timers(p);
 }
 
@@ -1511,6 +1577,7 @@ static void run_timer_softirq(struct softirq_action *h)
 	hrtimer_run_pending();
 
 	__run_deferrable_timers();
+	irq_work_tick_soft();
 
 	if (time_after_eq(jiffies, base->timer_jiffies))
 		__run_timers(base);
@@ -1705,6 +1772,9 @@ static int init_timers_cpu(int cpu)
 	}
 
 	__init_timers(base);
+#ifdef CONFIG_PREEMPT_RT_FULL
+	init_waitqueue_head(&base->wait_for_running_timer);
+#endif
 
 	return 0;
 }
@@ -1731,7 +1801,7 @@ static void migrate_timers(int cpu)
 
 	BUG_ON(cpu_online(cpu));
 	old_base = per_cpu(tvec_bases, cpu);
-	new_base = get_cpu_var(tvec_bases);
+	new_base = get_local_var(tvec_bases);
 	/*
 	 * The caller is globally serialized and nobody else
 	 * takes two locks at once, deadlock is not possible.
@@ -1752,7 +1822,7 @@ static void migrate_timers(int cpu)
 
 	spin_unlock(&old_base->lock);
 	spin_unlock_irq(&new_base->lock);
-	put_cpu_var(tvec_bases);
+	put_local_var(tvec_bases);
 }
 #endif /* CONFIG_HOTPLUG_CPU */
 
diff --git a/kernel/msm-3.18/kernel/trace/Kconfig b/kernel/msm-3.18/kernel/trace/Kconfig
index cb9b3245a..32b5f03f8 100644
--- a/kernel/msm-3.18/kernel/trace/Kconfig
+++ b/kernel/msm-3.18/kernel/trace/Kconfig
@@ -216,6 +216,24 @@ config IRQSOFF_TRACER
 	  enabled. This option and the preempt-off timing option can be
 	  used together or separately.)
 
+config INTERRUPT_OFF_HIST
+	bool "Interrupts-off Latency Histogram"
+	depends on IRQSOFF_TRACER
+	help
+	  This option generates continuously updated histograms (one per cpu)
+	  of the duration of time periods with interrupts disabled. The
+	  histograms are disabled by default. To enable them, write a non-zero
+	  number to
+
+	      /sys/kernel/debug/tracing/latency_hist/enable/preemptirqsoff
+
+	  If PREEMPT_OFF_HIST is also selected, additional histograms (one
+	  per cpu) are generated that accumulate the duration of time periods
+	  when both interrupts and preemption are disabled. The histogram data
+	  will be located in the debug file system at
+
+	      /sys/kernel/debug/tracing/latency_hist/irqsoff
+
 config PREEMPT_TRACER
 	bool "Preemption-off Latency Tracer"
 	default n
@@ -240,6 +258,24 @@ config PREEMPT_TRACER
 	  enabled. This option and the irqs-off timing option can be
 	  used together or separately.)
 
+config PREEMPT_OFF_HIST
+	bool "Preemption-off Latency Histogram"
+	depends on PREEMPT_TRACER
+	help
+	  This option generates continuously updated histograms (one per cpu)
+	  of the duration of time periods with preemption disabled. The
+	  histograms are disabled by default. To enable them, write a non-zero
+	  number to
+
+	      /sys/kernel/debug/tracing/latency_hist/enable/preemptirqsoff
+
+	  If INTERRUPT_OFF_HIST is also selected, additional histograms (one
+	  per cpu) are generated that accumulate the duration of time periods
+	  when both interrupts and preemption are disabled. The histogram data
+	  will be located in the debug file system at
+
+	      /sys/kernel/debug/tracing/latency_hist/preemptoff
+
 config SCHED_TRACER
 	bool "Scheduling Latency Tracer"
 	select GENERIC_TRACER
@@ -250,6 +286,74 @@ config SCHED_TRACER
 	  This tracer tracks the latency of the highest priority task
 	  to be scheduled in, starting from the point it has woken up.
 
+config WAKEUP_LATENCY_HIST
+	bool "Scheduling Latency Histogram"
+	depends on SCHED_TRACER
+	help
+	  This option generates continuously updated histograms (one per cpu)
+	  of the scheduling latency of the highest priority task.
+	  The histograms are disabled by default. To enable them, write a
+	  non-zero number to
+
+	      /sys/kernel/debug/tracing/latency_hist/enable/wakeup
+
+	  Two different algorithms are used, one to determine the latency of
+	  processes that exclusively use the highest priority of the system and
+	  another one to determine the latency of processes that share the
+	  highest system priority with other processes. The former is used to
+	  improve hardware and system software, the latter to optimize the
+	  priority design of a given system. The histogram data will be
+	  located in the debug file system at
+
+	      /sys/kernel/debug/tracing/latency_hist/wakeup
+
+	  and
+
+	      /sys/kernel/debug/tracing/latency_hist/wakeup/sharedprio
+
+	  If both Scheduling Latency Histogram and Missed Timer Offsets
+	  Histogram are selected, additional histogram data will be collected
+	  that contain, in addition to the wakeup latency, the timer latency, in
+	  case the wakeup was triggered by an expired timer. These histograms
+	  are available in the
+
+	      /sys/kernel/debug/tracing/latency_hist/timerandwakeup
+
+	  directory. They reflect the apparent interrupt and scheduling latency
+	  and are best suitable to determine the worst-case latency of a given
+	  system. To enable these histograms, write a non-zero number to
+
+	      /sys/kernel/debug/tracing/latency_hist/enable/timerandwakeup
+
+config MISSED_TIMER_OFFSETS_HIST
+	depends on HIGH_RES_TIMERS
+	select GENERIC_TRACER
+	bool "Missed Timer Offsets Histogram"
+	help
+	  Generate a histogram of missed timer offsets in microseconds. The
+	  histograms are disabled by default. To enable them, write a non-zero
+	  number to
+
+	      /sys/kernel/debug/tracing/latency_hist/enable/missed_timer_offsets
+
+	  The histogram data will be located in the debug file system at
+
+	      /sys/kernel/debug/tracing/latency_hist/missed_timer_offsets
+
+	  If both Scheduling Latency Histogram and Missed Timer Offsets
+	  Histogram are selected, additional histogram data will be collected
+	  that contain, in addition to the wakeup latency, the timer latency, in
+	  case the wakeup was triggered by an expired timer. These histograms
+	  are available in the
+
+	      /sys/kernel/debug/tracing/latency_hist/timerandwakeup
+
+	  directory. They reflect the apparent interrupt and scheduling latency
+	  and are best suitable to determine the worst-case latency of a given
+	  system. To enable these histograms, write a non-zero number to
+
+	      /sys/kernel/debug/tracing/latency_hist/enable/timerandwakeup
+
 config ENABLE_DEFAULT_TRACERS
 	bool "Trace process context switches and events"
 	depends on !GENERIC_TRACER
diff --git a/kernel/msm-3.18/kernel/trace/Makefile b/kernel/msm-3.18/kernel/trace/Makefile
index 68836d38f..f56676d5b 100644
--- a/kernel/msm-3.18/kernel/trace/Makefile
+++ b/kernel/msm-3.18/kernel/trace/Makefile
@@ -37,6 +37,10 @@ obj-$(CONFIG_IRQSOFF_TRACER) += trace_irqsoff.o
 obj-$(CONFIG_PREEMPT_TRACER) += trace_irqsoff.o
 obj-$(CONFIG_SCHED_TRACER) += trace_sched_wakeup.o
 obj-$(CONFIG_CPU_FREQ_SWITCH_PROFILER) += trace_cpu_freq_switch.o
+obj-$(CONFIG_INTERRUPT_OFF_HIST) += latency_hist.o
+obj-$(CONFIG_PREEMPT_OFF_HIST) += latency_hist.o
+obj-$(CONFIG_WAKEUP_LATENCY_HIST) += latency_hist.o
+obj-$(CONFIG_MISSED_TIMER_OFFSETS_HIST) += latency_hist.o
 obj-$(CONFIG_NOP_TRACER) += trace_nop.o
 obj-$(CONFIG_STACK_TRACER) += trace_stack.o
 obj-$(CONFIG_MMIOTRACE) += trace_mmiotrace.o
diff --git a/kernel/msm-3.18/kernel/trace/latency_hist.c b/kernel/msm-3.18/kernel/trace/latency_hist.c
new file mode 100644
index 000000000..b6c1d14b7
--- /dev/null
+++ b/kernel/msm-3.18/kernel/trace/latency_hist.c
@@ -0,0 +1,1178 @@
+/*
+ * kernel/trace/latency_hist.c
+ *
+ * Add support for histograms of preemption-off latency and
+ * interrupt-off latency and wakeup latency, it depends on
+ * Real-Time Preemption Support.
+ *
+ *  Copyright (C) 2005 MontaVista Software, Inc.
+ *  Yi Yang <yyang@ch.mvista.com>
+ *
+ *  Converted to work with the new latency tracer.
+ *  Copyright (C) 2008 Red Hat, Inc.
+ *    Steven Rostedt <srostedt@redhat.com>
+ *
+ */
+#include <linux/module.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/percpu.h>
+#include <linux/kallsyms.h>
+#include <linux/uaccess.h>
+#include <linux/sched.h>
+#include <linux/sched/rt.h>
+#include <linux/slab.h>
+#include <linux/atomic.h>
+#include <asm/div64.h>
+
+#include "trace.h"
+#include <trace/events/sched.h>
+
+#define NSECS_PER_USECS 1000L
+
+#define CREATE_TRACE_POINTS
+#include <trace/events/hist.h>
+
+enum {
+	IRQSOFF_LATENCY = 0,
+	PREEMPTOFF_LATENCY,
+	PREEMPTIRQSOFF_LATENCY,
+	WAKEUP_LATENCY,
+	WAKEUP_LATENCY_SHAREDPRIO,
+	MISSED_TIMER_OFFSETS,
+	TIMERANDWAKEUP_LATENCY,
+	MAX_LATENCY_TYPE,
+};
+
+#define MAX_ENTRY_NUM 10240
+
+struct hist_data {
+	atomic_t hist_mode; /* 0 log, 1 don't log */
+	long offset; /* set it to MAX_ENTRY_NUM/2 for a bipolar scale */
+	long min_lat;
+	long max_lat;
+	unsigned long long below_hist_bound_samples;
+	unsigned long long above_hist_bound_samples;
+	long long accumulate_lat;
+	unsigned long long total_samples;
+	unsigned long long hist_array[MAX_ENTRY_NUM];
+};
+
+struct enable_data {
+	int latency_type;
+	int enabled;
+};
+
+static char *latency_hist_dir_root = "latency_hist";
+
+#ifdef CONFIG_INTERRUPT_OFF_HIST
+static DEFINE_PER_CPU(struct hist_data, irqsoff_hist);
+static char *irqsoff_hist_dir = "irqsoff";
+static DEFINE_PER_CPU(cycles_t, hist_irqsoff_start);
+static DEFINE_PER_CPU(int, hist_irqsoff_counting);
+#endif
+
+#ifdef CONFIG_PREEMPT_OFF_HIST
+static DEFINE_PER_CPU(struct hist_data, preemptoff_hist);
+static char *preemptoff_hist_dir = "preemptoff";
+static DEFINE_PER_CPU(cycles_t, hist_preemptoff_start);
+static DEFINE_PER_CPU(int, hist_preemptoff_counting);
+#endif
+
+#if defined(CONFIG_PREEMPT_OFF_HIST) && defined(CONFIG_INTERRUPT_OFF_HIST)
+static DEFINE_PER_CPU(struct hist_data, preemptirqsoff_hist);
+static char *preemptirqsoff_hist_dir = "preemptirqsoff";
+static DEFINE_PER_CPU(cycles_t, hist_preemptirqsoff_start);
+static DEFINE_PER_CPU(int, hist_preemptirqsoff_counting);
+#endif
+
+#if defined(CONFIG_PREEMPT_OFF_HIST) || defined(CONFIG_INTERRUPT_OFF_HIST)
+static notrace void probe_preemptirqsoff_hist(void *v, int reason, int start);
+static struct enable_data preemptirqsoff_enabled_data = {
+	.latency_type = PREEMPTIRQSOFF_LATENCY,
+	.enabled = 0,
+};
+#endif
+
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+struct maxlatproc_data {
+	char comm[FIELD_SIZEOF(struct task_struct, comm)];
+	char current_comm[FIELD_SIZEOF(struct task_struct, comm)];
+	int pid;
+	int current_pid;
+	int prio;
+	int current_prio;
+	long latency;
+	long timeroffset;
+	cycle_t timestamp;
+};
+#endif
+
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+static DEFINE_PER_CPU(struct hist_data, wakeup_latency_hist);
+static DEFINE_PER_CPU(struct hist_data, wakeup_latency_hist_sharedprio);
+static char *wakeup_latency_hist_dir = "wakeup";
+static char *wakeup_latency_hist_dir_sharedprio = "sharedprio";
+static notrace void probe_wakeup_latency_hist_start(void *v,
+	struct task_struct *p);
+static notrace void probe_wakeup_latency_hist_stop(void *v,
+	struct task_struct *prev, struct task_struct *next);
+static notrace void probe_sched_migrate_task(void *,
+	struct task_struct *task, int cpu);
+static struct enable_data wakeup_latency_enabled_data = {
+	.latency_type = WAKEUP_LATENCY,
+	.enabled = 0,
+};
+static DEFINE_PER_CPU(struct maxlatproc_data, wakeup_maxlatproc);
+static DEFINE_PER_CPU(struct maxlatproc_data, wakeup_maxlatproc_sharedprio);
+static DEFINE_PER_CPU(struct task_struct *, wakeup_task);
+static DEFINE_PER_CPU(int, wakeup_sharedprio);
+static unsigned long wakeup_pid;
+#endif
+
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+static DEFINE_PER_CPU(struct hist_data, missed_timer_offsets);
+static char *missed_timer_offsets_dir = "missed_timer_offsets";
+static notrace void probe_hrtimer_interrupt(void *v, int cpu,
+	long long offset, struct task_struct *curr, struct task_struct *task);
+static struct enable_data missed_timer_offsets_enabled_data = {
+	.latency_type = MISSED_TIMER_OFFSETS,
+	.enabled = 0,
+};
+static DEFINE_PER_CPU(struct maxlatproc_data, missed_timer_offsets_maxlatproc);
+static unsigned long missed_timer_offsets_pid;
+#endif
+
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) && \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+static DEFINE_PER_CPU(struct hist_data, timerandwakeup_latency_hist);
+static char *timerandwakeup_latency_hist_dir = "timerandwakeup";
+static struct enable_data timerandwakeup_enabled_data = {
+	.latency_type = TIMERANDWAKEUP_LATENCY,
+	.enabled = 0,
+};
+static DEFINE_PER_CPU(struct maxlatproc_data, timerandwakeup_maxlatproc);
+#endif
+
+void notrace latency_hist(int latency_type, int cpu, long latency,
+			  long timeroffset, cycle_t stop,
+			  struct task_struct *p)
+{
+	struct hist_data *my_hist;
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+	struct maxlatproc_data *mp = NULL;
+#endif
+
+	if (!cpu_possible(cpu) || latency_type < 0 ||
+	    latency_type >= MAX_LATENCY_TYPE)
+		return;
+
+	switch (latency_type) {
+#ifdef CONFIG_INTERRUPT_OFF_HIST
+	case IRQSOFF_LATENCY:
+		my_hist = &per_cpu(irqsoff_hist, cpu);
+		break;
+#endif
+#ifdef CONFIG_PREEMPT_OFF_HIST
+	case PREEMPTOFF_LATENCY:
+		my_hist = &per_cpu(preemptoff_hist, cpu);
+		break;
+#endif
+#if defined(CONFIG_PREEMPT_OFF_HIST) && defined(CONFIG_INTERRUPT_OFF_HIST)
+	case PREEMPTIRQSOFF_LATENCY:
+		my_hist = &per_cpu(preemptirqsoff_hist, cpu);
+		break;
+#endif
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+	case WAKEUP_LATENCY:
+		my_hist = &per_cpu(wakeup_latency_hist, cpu);
+		mp = &per_cpu(wakeup_maxlatproc, cpu);
+		break;
+	case WAKEUP_LATENCY_SHAREDPRIO:
+		my_hist = &per_cpu(wakeup_latency_hist_sharedprio, cpu);
+		mp = &per_cpu(wakeup_maxlatproc_sharedprio, cpu);
+		break;
+#endif
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+	case MISSED_TIMER_OFFSETS:
+		my_hist = &per_cpu(missed_timer_offsets, cpu);
+		mp = &per_cpu(missed_timer_offsets_maxlatproc, cpu);
+		break;
+#endif
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) && \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+	case TIMERANDWAKEUP_LATENCY:
+		my_hist = &per_cpu(timerandwakeup_latency_hist, cpu);
+		mp = &per_cpu(timerandwakeup_maxlatproc, cpu);
+		break;
+#endif
+
+	default:
+		return;
+	}
+
+	latency += my_hist->offset;
+
+	if (atomic_read(&my_hist->hist_mode) == 0)
+		return;
+
+	if (latency < 0 || latency >= MAX_ENTRY_NUM) {
+		if (latency < 0)
+			my_hist->below_hist_bound_samples++;
+		else
+			my_hist->above_hist_bound_samples++;
+	} else
+		my_hist->hist_array[latency]++;
+
+	if (unlikely(latency > my_hist->max_lat ||
+	    my_hist->min_lat == LONG_MAX)) {
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+		if (latency_type == WAKEUP_LATENCY ||
+		    latency_type == WAKEUP_LATENCY_SHAREDPRIO ||
+		    latency_type == MISSED_TIMER_OFFSETS ||
+		    latency_type == TIMERANDWAKEUP_LATENCY) {
+			strncpy(mp->comm, p->comm, sizeof(mp->comm));
+			strncpy(mp->current_comm, current->comm,
+			    sizeof(mp->current_comm));
+			mp->pid = task_pid_nr(p);
+			mp->current_pid = task_pid_nr(current);
+			mp->prio = p->prio;
+			mp->current_prio = current->prio;
+			mp->latency = latency;
+			mp->timeroffset = timeroffset;
+			mp->timestamp = stop;
+		}
+#endif
+		my_hist->max_lat = latency;
+	}
+	if (unlikely(latency < my_hist->min_lat))
+		my_hist->min_lat = latency;
+	my_hist->total_samples++;
+	my_hist->accumulate_lat += latency;
+}
+
+static void *l_start(struct seq_file *m, loff_t *pos)
+{
+	loff_t *index_ptr = NULL;
+	loff_t index = *pos;
+	struct hist_data *my_hist = m->private;
+
+	if (index == 0) {
+		char minstr[32], avgstr[32], maxstr[32];
+
+		atomic_dec(&my_hist->hist_mode);
+
+		if (likely(my_hist->total_samples)) {
+			long avg = (long) div64_s64(my_hist->accumulate_lat,
+			    my_hist->total_samples);
+			snprintf(minstr, sizeof(minstr), "%ld",
+			    my_hist->min_lat - my_hist->offset);
+			snprintf(avgstr, sizeof(avgstr), "%ld",
+			    avg - my_hist->offset);
+			snprintf(maxstr, sizeof(maxstr), "%ld",
+			    my_hist->max_lat - my_hist->offset);
+		} else {
+			strcpy(minstr, "<undef>");
+			strcpy(avgstr, minstr);
+			strcpy(maxstr, minstr);
+		}
+
+		seq_printf(m, "#Minimum latency: %s microseconds\n"
+			   "#Average latency: %s microseconds\n"
+			   "#Maximum latency: %s microseconds\n"
+			   "#Total samples: %llu\n"
+			   "#There are %llu samples lower than %ld"
+			   " microseconds.\n"
+			   "#There are %llu samples greater or equal"
+			   " than %ld microseconds.\n"
+			   "#usecs\t%16s\n",
+			   minstr, avgstr, maxstr,
+			   my_hist->total_samples,
+			   my_hist->below_hist_bound_samples,
+			   -my_hist->offset,
+			   my_hist->above_hist_bound_samples,
+			   MAX_ENTRY_NUM - my_hist->offset,
+			   "samples");
+	}
+	if (index < MAX_ENTRY_NUM) {
+		index_ptr = kmalloc(sizeof(loff_t), GFP_KERNEL);
+		if (index_ptr)
+			*index_ptr = index;
+	}
+
+	return index_ptr;
+}
+
+static void *l_next(struct seq_file *m, void *p, loff_t *pos)
+{
+	loff_t *index_ptr = p;
+	struct hist_data *my_hist = m->private;
+
+	if (++*pos >= MAX_ENTRY_NUM) {
+		atomic_inc(&my_hist->hist_mode);
+		return NULL;
+	}
+	*index_ptr = *pos;
+	return index_ptr;
+}
+
+static void l_stop(struct seq_file *m, void *p)
+{
+	kfree(p);
+}
+
+static int l_show(struct seq_file *m, void *p)
+{
+	int index = *(loff_t *) p;
+	struct hist_data *my_hist = m->private;
+
+	seq_printf(m, "%6ld\t%16llu\n", index - my_hist->offset,
+	    my_hist->hist_array[index]);
+	return 0;
+}
+
+static const struct seq_operations latency_hist_seq_op = {
+	.start = l_start,
+	.next  = l_next,
+	.stop  = l_stop,
+	.show  = l_show
+};
+
+static int latency_hist_open(struct inode *inode, struct file *file)
+{
+	int ret;
+
+	ret = seq_open(file, &latency_hist_seq_op);
+	if (!ret) {
+		struct seq_file *seq = file->private_data;
+		seq->private = inode->i_private;
+	}
+	return ret;
+}
+
+static const struct file_operations latency_hist_fops = {
+	.open = latency_hist_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+static void clear_maxlatprocdata(struct maxlatproc_data *mp)
+{
+	mp->comm[0] = mp->current_comm[0] = '\0';
+	mp->prio = mp->current_prio = mp->pid = mp->current_pid =
+	    mp->latency = mp->timeroffset = -1;
+	mp->timestamp = 0;
+}
+#endif
+
+static void hist_reset(struct hist_data *hist)
+{
+	atomic_dec(&hist->hist_mode);
+
+	memset(hist->hist_array, 0, sizeof(hist->hist_array));
+	hist->below_hist_bound_samples = 0ULL;
+	hist->above_hist_bound_samples = 0ULL;
+	hist->min_lat = LONG_MAX;
+	hist->max_lat = LONG_MIN;
+	hist->total_samples = 0ULL;
+	hist->accumulate_lat = 0LL;
+
+	atomic_inc(&hist->hist_mode);
+}
+
+static ssize_t
+latency_hist_reset(struct file *file, const char __user *a,
+		   size_t size, loff_t *off)
+{
+	int cpu;
+	struct hist_data *hist = NULL;
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+	struct maxlatproc_data *mp = NULL;
+#endif
+	off_t latency_type = (off_t) file->private_data;
+
+	for_each_online_cpu(cpu) {
+
+		switch (latency_type) {
+#ifdef CONFIG_PREEMPT_OFF_HIST
+		case PREEMPTOFF_LATENCY:
+			hist = &per_cpu(preemptoff_hist, cpu);
+			break;
+#endif
+#ifdef CONFIG_INTERRUPT_OFF_HIST
+		case IRQSOFF_LATENCY:
+			hist = &per_cpu(irqsoff_hist, cpu);
+			break;
+#endif
+#if defined(CONFIG_INTERRUPT_OFF_HIST) && defined(CONFIG_PREEMPT_OFF_HIST)
+		case PREEMPTIRQSOFF_LATENCY:
+			hist = &per_cpu(preemptirqsoff_hist, cpu);
+			break;
+#endif
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+		case WAKEUP_LATENCY:
+			hist = &per_cpu(wakeup_latency_hist, cpu);
+			mp = &per_cpu(wakeup_maxlatproc, cpu);
+			break;
+		case WAKEUP_LATENCY_SHAREDPRIO:
+			hist = &per_cpu(wakeup_latency_hist_sharedprio, cpu);
+			mp = &per_cpu(wakeup_maxlatproc_sharedprio, cpu);
+			break;
+#endif
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+		case MISSED_TIMER_OFFSETS:
+			hist = &per_cpu(missed_timer_offsets, cpu);
+			mp = &per_cpu(missed_timer_offsets_maxlatproc, cpu);
+			break;
+#endif
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) && \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+		case TIMERANDWAKEUP_LATENCY:
+			hist = &per_cpu(timerandwakeup_latency_hist, cpu);
+			mp = &per_cpu(timerandwakeup_maxlatproc, cpu);
+			break;
+#endif
+		}
+
+		hist_reset(hist);
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+		if (latency_type == WAKEUP_LATENCY ||
+		    latency_type == WAKEUP_LATENCY_SHAREDPRIO ||
+		    latency_type == MISSED_TIMER_OFFSETS ||
+		    latency_type == TIMERANDWAKEUP_LATENCY)
+			clear_maxlatprocdata(mp);
+#endif
+	}
+
+	return size;
+}
+
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+static ssize_t
+show_pid(struct file *file, char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	char buf[64];
+	int r;
+	unsigned long *this_pid = file->private_data;
+
+	r = snprintf(buf, sizeof(buf), "%lu\n", *this_pid);
+	return simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
+}
+
+static ssize_t do_pid(struct file *file, const char __user *ubuf,
+		      size_t cnt, loff_t *ppos)
+{
+	char buf[64];
+	unsigned long pid;
+	unsigned long *this_pid = file->private_data;
+
+	if (cnt >= sizeof(buf))
+		return -EINVAL;
+
+	if (copy_from_user(&buf, ubuf, cnt))
+		return -EFAULT;
+
+	buf[cnt] = '\0';
+
+	if (kstrtoul(buf, 10, &pid))
+		return -EINVAL;
+
+	*this_pid = pid;
+
+	return cnt;
+}
+#endif
+
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+static ssize_t
+show_maxlatproc(struct file *file, char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	int r;
+	struct maxlatproc_data *mp = file->private_data;
+	int strmaxlen = (TASK_COMM_LEN * 2) + (8 * 8);
+	unsigned long long t;
+	unsigned long usecs, secs;
+	char *buf;
+
+	if (mp->pid == -1 || mp->current_pid == -1) {
+		buf = "(none)\n";
+		return simple_read_from_buffer(ubuf, cnt, ppos, buf,
+		    strlen(buf));
+	}
+
+	buf = kmalloc(strmaxlen, GFP_KERNEL);
+	if (buf == NULL)
+		return -ENOMEM;
+
+	t = ns2usecs(mp->timestamp);
+	usecs = do_div(t, USEC_PER_SEC);
+	secs = (unsigned long) t;
+	r = snprintf(buf, strmaxlen,
+	    "%d %d %ld (%ld) %s <- %d %d %s %lu.%06lu\n", mp->pid,
+	    MAX_RT_PRIO-1 - mp->prio, mp->latency, mp->timeroffset, mp->comm,
+	    mp->current_pid, MAX_RT_PRIO-1 - mp->current_prio, mp->current_comm,
+	    secs, usecs);
+	r = simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
+	kfree(buf);
+	return r;
+}
+#endif
+
+static ssize_t
+show_enable(struct file *file, char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	char buf[64];
+	struct enable_data *ed = file->private_data;
+	int r;
+
+	r = snprintf(buf, sizeof(buf), "%d\n", ed->enabled);
+	return simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
+}
+
+static ssize_t
+do_enable(struct file *file, const char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	char buf[64];
+	long enable;
+	struct enable_data *ed = file->private_data;
+
+	if (cnt >= sizeof(buf))
+		return -EINVAL;
+
+	if (copy_from_user(&buf, ubuf, cnt))
+		return -EFAULT;
+
+	buf[cnt] = 0;
+
+	if (kstrtoul(buf, 10, &enable))
+		return -EINVAL;
+
+	if ((enable && ed->enabled) || (!enable && !ed->enabled))
+		return cnt;
+
+	if (enable) {
+		int ret;
+
+		switch (ed->latency_type) {
+#if defined(CONFIG_INTERRUPT_OFF_HIST) || defined(CONFIG_PREEMPT_OFF_HIST)
+		case PREEMPTIRQSOFF_LATENCY:
+			ret = register_trace_preemptirqsoff_hist(
+			    probe_preemptirqsoff_hist, NULL);
+			if (ret) {
+				pr_info("wakeup trace: Couldn't assign "
+				    "probe_preemptirqsoff_hist "
+				    "to trace_preemptirqsoff_hist\n");
+				return ret;
+			}
+			break;
+#endif
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+		case WAKEUP_LATENCY:
+			ret = register_trace_sched_wakeup(
+			    probe_wakeup_latency_hist_start, NULL);
+			if (ret) {
+				pr_info("wakeup trace: Couldn't assign "
+				    "probe_wakeup_latency_hist_start "
+				    "to trace_sched_wakeup\n");
+				return ret;
+			}
+			ret = register_trace_sched_wakeup_new(
+			    probe_wakeup_latency_hist_start, NULL);
+			if (ret) {
+				pr_info("wakeup trace: Couldn't assign "
+				    "probe_wakeup_latency_hist_start "
+				    "to trace_sched_wakeup_new\n");
+				unregister_trace_sched_wakeup(
+				    probe_wakeup_latency_hist_start, NULL);
+				return ret;
+			}
+			ret = register_trace_sched_switch(
+			    probe_wakeup_latency_hist_stop, NULL);
+			if (ret) {
+				pr_info("wakeup trace: Couldn't assign "
+				    "probe_wakeup_latency_hist_stop "
+				    "to trace_sched_switch\n");
+				unregister_trace_sched_wakeup(
+				    probe_wakeup_latency_hist_start, NULL);
+				unregister_trace_sched_wakeup_new(
+				    probe_wakeup_latency_hist_start, NULL);
+				return ret;
+			}
+			ret = register_trace_sched_migrate_task(
+			    probe_sched_migrate_task, NULL);
+			if (ret) {
+				pr_info("wakeup trace: Couldn't assign "
+				    "probe_sched_migrate_task "
+				    "to trace_sched_migrate_task\n");
+				unregister_trace_sched_wakeup(
+				    probe_wakeup_latency_hist_start, NULL);
+				unregister_trace_sched_wakeup_new(
+				    probe_wakeup_latency_hist_start, NULL);
+				unregister_trace_sched_switch(
+				    probe_wakeup_latency_hist_stop, NULL);
+				return ret;
+			}
+			break;
+#endif
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+		case MISSED_TIMER_OFFSETS:
+			ret = register_trace_hrtimer_interrupt(
+			    probe_hrtimer_interrupt, NULL);
+			if (ret) {
+				pr_info("wakeup trace: Couldn't assign "
+				    "probe_hrtimer_interrupt "
+				    "to trace_hrtimer_interrupt\n");
+				return ret;
+			}
+			break;
+#endif
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) && \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+		case TIMERANDWAKEUP_LATENCY:
+			if (!wakeup_latency_enabled_data.enabled ||
+			    !missed_timer_offsets_enabled_data.enabled)
+				return -EINVAL;
+			break;
+#endif
+		default:
+			break;
+		}
+	} else {
+		switch (ed->latency_type) {
+#if defined(CONFIG_INTERRUPT_OFF_HIST) || defined(CONFIG_PREEMPT_OFF_HIST)
+		case PREEMPTIRQSOFF_LATENCY:
+			{
+				int cpu;
+
+				unregister_trace_preemptirqsoff_hist(
+				    probe_preemptirqsoff_hist, NULL);
+				for_each_online_cpu(cpu) {
+#ifdef CONFIG_INTERRUPT_OFF_HIST
+					per_cpu(hist_irqsoff_counting,
+					    cpu) = 0;
+#endif
+#ifdef CONFIG_PREEMPT_OFF_HIST
+					per_cpu(hist_preemptoff_counting,
+					    cpu) = 0;
+#endif
+#if defined(CONFIG_INTERRUPT_OFF_HIST) && defined(CONFIG_PREEMPT_OFF_HIST)
+					per_cpu(hist_preemptirqsoff_counting,
+					    cpu) = 0;
+#endif
+				}
+			}
+			break;
+#endif
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+		case WAKEUP_LATENCY:
+			{
+				int cpu;
+
+				unregister_trace_sched_wakeup(
+				    probe_wakeup_latency_hist_start, NULL);
+				unregister_trace_sched_wakeup_new(
+				    probe_wakeup_latency_hist_start, NULL);
+				unregister_trace_sched_switch(
+				    probe_wakeup_latency_hist_stop, NULL);
+				unregister_trace_sched_migrate_task(
+				    probe_sched_migrate_task, NULL);
+
+				for_each_online_cpu(cpu) {
+					per_cpu(wakeup_task, cpu) = NULL;
+					per_cpu(wakeup_sharedprio, cpu) = 0;
+				}
+			}
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+			timerandwakeup_enabled_data.enabled = 0;
+#endif
+			break;
+#endif
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+		case MISSED_TIMER_OFFSETS:
+			unregister_trace_hrtimer_interrupt(
+			    probe_hrtimer_interrupt, NULL);
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+			timerandwakeup_enabled_data.enabled = 0;
+#endif
+			break;
+#endif
+		default:
+			break;
+		}
+	}
+	ed->enabled = enable;
+	return cnt;
+}
+
+static const struct file_operations latency_hist_reset_fops = {
+	.open = tracing_open_generic,
+	.write = latency_hist_reset,
+};
+
+static const struct file_operations enable_fops = {
+	.open = tracing_open_generic,
+	.read = show_enable,
+	.write = do_enable,
+};
+
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+static const struct file_operations pid_fops = {
+	.open = tracing_open_generic,
+	.read = show_pid,
+	.write = do_pid,
+};
+
+static const struct file_operations maxlatproc_fops = {
+	.open = tracing_open_generic,
+	.read = show_maxlatproc,
+};
+#endif
+
+#if defined(CONFIG_INTERRUPT_OFF_HIST) || defined(CONFIG_PREEMPT_OFF_HIST)
+static notrace void probe_preemptirqsoff_hist(void *v, int reason,
+	int starthist)
+{
+	int cpu = raw_smp_processor_id();
+	int time_set = 0;
+
+	if (starthist) {
+		cycle_t uninitialized_var(start);
+
+		if (!preempt_count() && !irqs_disabled())
+			return;
+
+#ifdef CONFIG_INTERRUPT_OFF_HIST
+		if ((reason == IRQS_OFF || reason == TRACE_START) &&
+		    !per_cpu(hist_irqsoff_counting, cpu)) {
+			per_cpu(hist_irqsoff_counting, cpu) = 1;
+			start = ftrace_now(cpu);
+			time_set++;
+			per_cpu(hist_irqsoff_start, cpu) = start;
+		}
+#endif
+
+#ifdef CONFIG_PREEMPT_OFF_HIST
+		if ((reason == PREEMPT_OFF || reason == TRACE_START) &&
+		    !per_cpu(hist_preemptoff_counting, cpu)) {
+			per_cpu(hist_preemptoff_counting, cpu) = 1;
+			if (!(time_set++))
+				start = ftrace_now(cpu);
+			per_cpu(hist_preemptoff_start, cpu) = start;
+		}
+#endif
+
+#if defined(CONFIG_INTERRUPT_OFF_HIST) && defined(CONFIG_PREEMPT_OFF_HIST)
+		if (per_cpu(hist_irqsoff_counting, cpu) &&
+		    per_cpu(hist_preemptoff_counting, cpu) &&
+		    !per_cpu(hist_preemptirqsoff_counting, cpu)) {
+			per_cpu(hist_preemptirqsoff_counting, cpu) = 1;
+			if (!time_set)
+				start = ftrace_now(cpu);
+			per_cpu(hist_preemptirqsoff_start, cpu) = start;
+		}
+#endif
+	} else {
+		cycle_t uninitialized_var(stop);
+
+#ifdef CONFIG_INTERRUPT_OFF_HIST
+		if ((reason == IRQS_ON || reason == TRACE_STOP) &&
+		    per_cpu(hist_irqsoff_counting, cpu)) {
+			cycle_t start = per_cpu(hist_irqsoff_start, cpu);
+
+			stop = ftrace_now(cpu);
+			time_set++;
+			if (start) {
+				long latency = ((long) (stop - start)) /
+				    NSECS_PER_USECS;
+
+				latency_hist(IRQSOFF_LATENCY, cpu, latency, 0,
+				    stop, NULL);
+			}
+			per_cpu(hist_irqsoff_counting, cpu) = 0;
+		}
+#endif
+
+#ifdef CONFIG_PREEMPT_OFF_HIST
+		if ((reason == PREEMPT_ON || reason == TRACE_STOP) &&
+		    per_cpu(hist_preemptoff_counting, cpu)) {
+			cycle_t start = per_cpu(hist_preemptoff_start, cpu);
+
+			if (!(time_set++))
+				stop = ftrace_now(cpu);
+			if (start) {
+				long latency = ((long) (stop - start)) /
+				    NSECS_PER_USECS;
+
+				latency_hist(PREEMPTOFF_LATENCY, cpu, latency,
+				    0, stop, NULL);
+			}
+			per_cpu(hist_preemptoff_counting, cpu) = 0;
+		}
+#endif
+
+#if defined(CONFIG_INTERRUPT_OFF_HIST) && defined(CONFIG_PREEMPT_OFF_HIST)
+		if ((!per_cpu(hist_irqsoff_counting, cpu) ||
+		     !per_cpu(hist_preemptoff_counting, cpu)) &&
+		   per_cpu(hist_preemptirqsoff_counting, cpu)) {
+			cycle_t start = per_cpu(hist_preemptirqsoff_start, cpu);
+
+			if (!time_set)
+				stop = ftrace_now(cpu);
+			if (start) {
+				long latency = ((long) (stop - start)) /
+				    NSECS_PER_USECS;
+
+				latency_hist(PREEMPTIRQSOFF_LATENCY, cpu,
+				    latency, 0, stop, NULL);
+			}
+			per_cpu(hist_preemptirqsoff_counting, cpu) = 0;
+		}
+#endif
+	}
+}
+#endif
+
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+static DEFINE_RAW_SPINLOCK(wakeup_lock);
+static notrace void probe_sched_migrate_task(void *v, struct task_struct *task,
+	int cpu)
+{
+	int old_cpu = task_cpu(task);
+
+	if (cpu != old_cpu) {
+		unsigned long flags;
+		struct task_struct *cpu_wakeup_task;
+
+		raw_spin_lock_irqsave(&wakeup_lock, flags);
+
+		cpu_wakeup_task = per_cpu(wakeup_task, old_cpu);
+		if (task == cpu_wakeup_task) {
+			put_task_struct(cpu_wakeup_task);
+			per_cpu(wakeup_task, old_cpu) = NULL;
+			cpu_wakeup_task = per_cpu(wakeup_task, cpu) = task;
+			get_task_struct(cpu_wakeup_task);
+		}
+
+		raw_spin_unlock_irqrestore(&wakeup_lock, flags);
+	}
+}
+
+static notrace void probe_wakeup_latency_hist_start(void *v,
+	struct task_struct *p)
+{
+	unsigned long flags;
+	struct task_struct *curr = current;
+	int cpu = task_cpu(p);
+	struct task_struct *cpu_wakeup_task;
+
+	raw_spin_lock_irqsave(&wakeup_lock, flags);
+
+	cpu_wakeup_task = per_cpu(wakeup_task, cpu);
+
+	if (wakeup_pid) {
+		if ((cpu_wakeup_task && p->prio == cpu_wakeup_task->prio) ||
+		    p->prio == curr->prio)
+			per_cpu(wakeup_sharedprio, cpu) = 1;
+		if (likely(wakeup_pid != task_pid_nr(p)))
+			goto out;
+	} else {
+		if (likely(!rt_task(p)) ||
+		    (cpu_wakeup_task && p->prio > cpu_wakeup_task->prio) ||
+		    p->prio > curr->prio)
+			goto out;
+		if ((cpu_wakeup_task && p->prio == cpu_wakeup_task->prio) ||
+		    p->prio == curr->prio)
+			per_cpu(wakeup_sharedprio, cpu) = 1;
+	}
+
+	if (cpu_wakeup_task)
+		put_task_struct(cpu_wakeup_task);
+	cpu_wakeup_task = per_cpu(wakeup_task, cpu) = p;
+	get_task_struct(cpu_wakeup_task);
+	cpu_wakeup_task->preempt_timestamp_hist =
+		ftrace_now(raw_smp_processor_id());
+out:
+	raw_spin_unlock_irqrestore(&wakeup_lock, flags);
+}
+
+static notrace void probe_wakeup_latency_hist_stop(void *v,
+	struct task_struct *prev, struct task_struct *next)
+{
+	unsigned long flags;
+	int cpu = task_cpu(next);
+	long latency;
+	cycle_t stop;
+	struct task_struct *cpu_wakeup_task;
+
+	raw_spin_lock_irqsave(&wakeup_lock, flags);
+
+	cpu_wakeup_task = per_cpu(wakeup_task, cpu);
+
+	if (cpu_wakeup_task == NULL)
+		goto out;
+
+	/* Already running? */
+	if (unlikely(current == cpu_wakeup_task))
+		goto out_reset;
+
+	if (next != cpu_wakeup_task) {
+		if (next->prio < cpu_wakeup_task->prio)
+			goto out_reset;
+
+		if (next->prio == cpu_wakeup_task->prio)
+			per_cpu(wakeup_sharedprio, cpu) = 1;
+
+		goto out;
+	}
+
+	if (current->prio == cpu_wakeup_task->prio)
+		per_cpu(wakeup_sharedprio, cpu) = 1;
+
+	/*
+	 * The task we are waiting for is about to be switched to.
+	 * Calculate latency and store it in histogram.
+	 */
+	stop = ftrace_now(raw_smp_processor_id());
+
+	latency = ((long) (stop - next->preempt_timestamp_hist)) /
+	    NSECS_PER_USECS;
+
+	if (per_cpu(wakeup_sharedprio, cpu)) {
+		latency_hist(WAKEUP_LATENCY_SHAREDPRIO, cpu, latency, 0, stop,
+		    next);
+		per_cpu(wakeup_sharedprio, cpu) = 0;
+	} else {
+		latency_hist(WAKEUP_LATENCY, cpu, latency, 0, stop, next);
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+		if (timerandwakeup_enabled_data.enabled) {
+			latency_hist(TIMERANDWAKEUP_LATENCY, cpu,
+			    next->timer_offset + latency, next->timer_offset,
+			    stop, next);
+		}
+#endif
+	}
+
+out_reset:
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+	next->timer_offset = 0;
+#endif
+	put_task_struct(cpu_wakeup_task);
+	per_cpu(wakeup_task, cpu) = NULL;
+out:
+	raw_spin_unlock_irqrestore(&wakeup_lock, flags);
+}
+#endif
+
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+static notrace void probe_hrtimer_interrupt(void *v, int cpu,
+	long long latency_ns, struct task_struct *curr,
+	struct task_struct *task)
+{
+	if (latency_ns <= 0 && task != NULL && rt_task(task) &&
+	    (task->prio < curr->prio ||
+	    (task->prio == curr->prio &&
+	    !cpumask_test_cpu(cpu, &task->cpus_allowed)))) {
+		long latency;
+		cycle_t now;
+
+		if (missed_timer_offsets_pid) {
+			if (likely(missed_timer_offsets_pid !=
+			    task_pid_nr(task)))
+				return;
+		}
+
+		now = ftrace_now(cpu);
+		latency = (long) div_s64(-latency_ns, NSECS_PER_USECS);
+		latency_hist(MISSED_TIMER_OFFSETS, cpu, latency, latency, now,
+		    task);
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+		task->timer_offset = latency;
+#endif
+	}
+}
+#endif
+
+static __init int latency_hist_init(void)
+{
+	struct dentry *latency_hist_root = NULL;
+	struct dentry *dentry;
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+	struct dentry *dentry_sharedprio;
+#endif
+	struct dentry *entry;
+	struct dentry *enable_root;
+	int i = 0;
+	struct hist_data *my_hist;
+	char name[64];
+	char *cpufmt = "CPU%d";
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+	char *cpufmt_maxlatproc = "max_latency-CPU%d";
+	struct maxlatproc_data *mp = NULL;
+#endif
+
+	dentry = tracing_init_dentry();
+	latency_hist_root = debugfs_create_dir(latency_hist_dir_root, dentry);
+	enable_root = debugfs_create_dir("enable", latency_hist_root);
+
+#ifdef CONFIG_INTERRUPT_OFF_HIST
+	dentry = debugfs_create_dir(irqsoff_hist_dir, latency_hist_root);
+	for_each_possible_cpu(i) {
+		sprintf(name, cpufmt, i);
+		entry = debugfs_create_file(name, 0444, dentry,
+		    &per_cpu(irqsoff_hist, i), &latency_hist_fops);
+		my_hist = &per_cpu(irqsoff_hist, i);
+		atomic_set(&my_hist->hist_mode, 1);
+		my_hist->min_lat = LONG_MAX;
+	}
+	entry = debugfs_create_file("reset", 0644, dentry,
+	    (void *)IRQSOFF_LATENCY, &latency_hist_reset_fops);
+#endif
+
+#ifdef CONFIG_PREEMPT_OFF_HIST
+	dentry = debugfs_create_dir(preemptoff_hist_dir,
+	    latency_hist_root);
+	for_each_possible_cpu(i) {
+		sprintf(name, cpufmt, i);
+		entry = debugfs_create_file(name, 0444, dentry,
+		    &per_cpu(preemptoff_hist, i), &latency_hist_fops);
+		my_hist = &per_cpu(preemptoff_hist, i);
+		atomic_set(&my_hist->hist_mode, 1);
+		my_hist->min_lat = LONG_MAX;
+	}
+	entry = debugfs_create_file("reset", 0644, dentry,
+	    (void *)PREEMPTOFF_LATENCY, &latency_hist_reset_fops);
+#endif
+
+#if defined(CONFIG_INTERRUPT_OFF_HIST) && defined(CONFIG_PREEMPT_OFF_HIST)
+	dentry = debugfs_create_dir(preemptirqsoff_hist_dir,
+	    latency_hist_root);
+	for_each_possible_cpu(i) {
+		sprintf(name, cpufmt, i);
+		entry = debugfs_create_file(name, 0444, dentry,
+		    &per_cpu(preemptirqsoff_hist, i), &latency_hist_fops);
+		my_hist = &per_cpu(preemptirqsoff_hist, i);
+		atomic_set(&my_hist->hist_mode, 1);
+		my_hist->min_lat = LONG_MAX;
+	}
+	entry = debugfs_create_file("reset", 0644, dentry,
+	    (void *)PREEMPTIRQSOFF_LATENCY, &latency_hist_reset_fops);
+#endif
+
+#if defined(CONFIG_INTERRUPT_OFF_HIST) || defined(CONFIG_PREEMPT_OFF_HIST)
+	entry = debugfs_create_file("preemptirqsoff", 0644,
+	    enable_root, (void *)&preemptirqsoff_enabled_data,
+	    &enable_fops);
+#endif
+
+#ifdef CONFIG_WAKEUP_LATENCY_HIST
+	dentry = debugfs_create_dir(wakeup_latency_hist_dir,
+	    latency_hist_root);
+	dentry_sharedprio = debugfs_create_dir(
+	    wakeup_latency_hist_dir_sharedprio, dentry);
+	for_each_possible_cpu(i) {
+		sprintf(name, cpufmt, i);
+
+		entry = debugfs_create_file(name, 0444, dentry,
+		    &per_cpu(wakeup_latency_hist, i),
+		    &latency_hist_fops);
+		my_hist = &per_cpu(wakeup_latency_hist, i);
+		atomic_set(&my_hist->hist_mode, 1);
+		my_hist->min_lat = LONG_MAX;
+
+		entry = debugfs_create_file(name, 0444, dentry_sharedprio,
+		    &per_cpu(wakeup_latency_hist_sharedprio, i),
+		    &latency_hist_fops);
+		my_hist = &per_cpu(wakeup_latency_hist_sharedprio, i);
+		atomic_set(&my_hist->hist_mode, 1);
+		my_hist->min_lat = LONG_MAX;
+
+		sprintf(name, cpufmt_maxlatproc, i);
+
+		mp = &per_cpu(wakeup_maxlatproc, i);
+		entry = debugfs_create_file(name, 0444, dentry, mp,
+		    &maxlatproc_fops);
+		clear_maxlatprocdata(mp);
+
+		mp = &per_cpu(wakeup_maxlatproc_sharedprio, i);
+		entry = debugfs_create_file(name, 0444, dentry_sharedprio, mp,
+		    &maxlatproc_fops);
+		clear_maxlatprocdata(mp);
+	}
+	entry = debugfs_create_file("pid", 0644, dentry,
+	    (void *)&wakeup_pid, &pid_fops);
+	entry = debugfs_create_file("reset", 0644, dentry,
+	    (void *)WAKEUP_LATENCY, &latency_hist_reset_fops);
+	entry = debugfs_create_file("reset", 0644, dentry_sharedprio,
+	    (void *)WAKEUP_LATENCY_SHAREDPRIO, &latency_hist_reset_fops);
+	entry = debugfs_create_file("wakeup", 0644,
+	    enable_root, (void *)&wakeup_latency_enabled_data,
+	    &enable_fops);
+#endif
+
+#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
+	dentry = debugfs_create_dir(missed_timer_offsets_dir,
+	    latency_hist_root);
+	for_each_possible_cpu(i) {
+		sprintf(name, cpufmt, i);
+		entry = debugfs_create_file(name, 0444, dentry,
+		    &per_cpu(missed_timer_offsets, i), &latency_hist_fops);
+		my_hist = &per_cpu(missed_timer_offsets, i);
+		atomic_set(&my_hist->hist_mode, 1);
+		my_hist->min_lat = LONG_MAX;
+
+		sprintf(name, cpufmt_maxlatproc, i);
+		mp = &per_cpu(missed_timer_offsets_maxlatproc, i);
+		entry = debugfs_create_file(name, 0444, dentry, mp,
+		    &maxlatproc_fops);
+		clear_maxlatprocdata(mp);
+	}
+	entry = debugfs_create_file("pid", 0644, dentry,
+	    (void *)&missed_timer_offsets_pid, &pid_fops);
+	entry = debugfs_create_file("reset", 0644, dentry,
+	    (void *)MISSED_TIMER_OFFSETS, &latency_hist_reset_fops);
+	entry = debugfs_create_file("missed_timer_offsets", 0644,
+	    enable_root, (void *)&missed_timer_offsets_enabled_data,
+	    &enable_fops);
+#endif
+
+#if defined(CONFIG_WAKEUP_LATENCY_HIST) && \
+	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
+	dentry = debugfs_create_dir(timerandwakeup_latency_hist_dir,
+	    latency_hist_root);
+	for_each_possible_cpu(i) {
+		sprintf(name, cpufmt, i);
+		entry = debugfs_create_file(name, 0444, dentry,
+		    &per_cpu(timerandwakeup_latency_hist, i),
+		    &latency_hist_fops);
+		my_hist = &per_cpu(timerandwakeup_latency_hist, i);
+		atomic_set(&my_hist->hist_mode, 1);
+		my_hist->min_lat = LONG_MAX;
+
+		sprintf(name, cpufmt_maxlatproc, i);
+		mp = &per_cpu(timerandwakeup_maxlatproc, i);
+		entry = debugfs_create_file(name, 0444, dentry, mp,
+		    &maxlatproc_fops);
+		clear_maxlatprocdata(mp);
+	}
+	entry = debugfs_create_file("reset", 0644, dentry,
+	    (void *)TIMERANDWAKEUP_LATENCY, &latency_hist_reset_fops);
+	entry = debugfs_create_file("timerandwakeup", 0644,
+	    enable_root, (void *)&timerandwakeup_enabled_data,
+	    &enable_fops);
+#endif
+	return 0;
+}
+
+device_initcall(latency_hist_init);
diff --git a/kernel/msm-3.18/kernel/trace/trace.c b/kernel/msm-3.18/kernel/trace/trace.c
index 8a8749186..90d87117a 100644
--- a/kernel/msm-3.18/kernel/trace/trace.c
+++ b/kernel/msm-3.18/kernel/trace/trace.c
@@ -1631,6 +1631,7 @@ tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,
 	struct task_struct *tsk = current;
 
 	entry->preempt_count		= pc & 0xff;
+	entry->preempt_lazy_count	= preempt_lazy_count();
 	entry->pid			= (tsk) ? tsk->pid : 0;
 	entry->flags =
 #ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT
@@ -1640,8 +1641,11 @@ tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,
 #endif
 		((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |
 		((pc & SOFTIRQ_OFFSET) ? TRACE_FLAG_SOFTIRQ : 0) |
-		(tif_need_resched() ? TRACE_FLAG_NEED_RESCHED : 0) |
+		(tif_need_resched_now() ? TRACE_FLAG_NEED_RESCHED : 0) |
+		(need_resched_lazy() ? TRACE_FLAG_NEED_RESCHED_LAZY : 0) |
 		(test_preempt_need_resched() ? TRACE_FLAG_PREEMPT_RESCHED : 0);
+
+	entry->migrate_disable = (tsk) ? __migrate_disabled(tsk) & 0xFF : 0;
 }
 EXPORT_SYMBOL_GPL(tracing_generic_entry_update);
 
@@ -2562,14 +2566,17 @@ get_total_entries(struct trace_buffer *buf,
 
 static void print_lat_help_header(struct seq_file *m)
 {
-	seq_puts(m, "#                  _------=> CPU#            \n");
-	seq_puts(m, "#                 / _-----=> irqs-off        \n");
-	seq_puts(m, "#                | / _----=> need-resched    \n");
-	seq_puts(m, "#                || / _---=> hardirq/softirq \n");
-	seq_puts(m, "#                ||| / _--=> preempt-depth   \n");
-	seq_puts(m, "#                |||| /     delay             \n");
-	seq_puts(m, "#  cmd     pid   ||||| time  |   caller      \n");
-	seq_puts(m, "#     \\   /      |||||  \\    |   /           \n");
+	seq_puts(m, "#                  _--------=> CPU#              \n"
+		    "#                 / _-------=> irqs-off          \n"
+		    "#                | / _------=> need-resched      \n"
+		    "#                || / _-----=> need-resched_lazy \n"
+		    "#                ||| / _----=> hardirq/softirq   \n"
+		    "#                |||| / _---=> preempt-depth     \n"
+		    "#                ||||| / _--=> preempt-lazy-depth\n"
+		    "#                |||||| / _-=> migrate-disable   \n"
+		    "#                ||||||| /     delay             \n"
+		    "# cmd     pid    |||||||| time   |  caller       \n"
+		    "#     \\   /      ||||||||   \\    |  /            \n");
 }
 
 static void print_event_info(struct trace_buffer *buf, struct seq_file *m)
@@ -2593,13 +2600,16 @@ static void print_func_help_header(struct trace_buffer *buf, struct seq_file *m)
 static void print_func_help_header_irq(struct trace_buffer *buf, struct seq_file *m)
 {
 	print_event_info(buf, m);
-	seq_puts(m, "#                              _-----=> irqs-off\n");
-	seq_puts(m, "#                             / _----=> need-resched\n");
-	seq_puts(m, "#                            | / _---=> hardirq/softirq\n");
-	seq_puts(m, "#                            || / _--=> preempt-depth\n");
-	seq_puts(m, "#                            ||| /     delay\n");
-	seq_puts(m, "#           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION\n");
-	seq_puts(m, "#              | |       |   ||||       |         |\n");
+	seq_puts(m, "#                              _-------=> irqs-off          \n");
+	seq_puts(m, "#                            /  _------=> need-resched      \n");
+	seq_puts(m, "#                            |/  _-----=> need-resched_lazy \n");
+	seq_puts(m, "#                            ||/  _----=> hardirq/softirq   \n");
+	seq_puts(m, "#                            |||/  _---=> preempt-depth     \n");
+	seq_puts(m, "#                            ||||/  _--=> preempt-lazy-depth\n");
+	seq_puts(m, "#                            ||||| / _-=> migrate-disable   \n");
+	seq_puts(m, "#                            |||||| /     delay\n");
+	seq_puts(m, "#           TASK-PID   CPU#  ||||||  TIMESTAMP  FUNCTION\n");
+	seq_puts(m, "#              | |       |   ||||||     |         |\n");
 }
 
 void
diff --git a/kernel/msm-3.18/kernel/trace/trace.h b/kernel/msm-3.18/kernel/trace/trace.h
index 1371cd0b6..ffc5e423c 100644
--- a/kernel/msm-3.18/kernel/trace/trace.h
+++ b/kernel/msm-3.18/kernel/trace/trace.h
@@ -119,6 +119,7 @@ struct kretprobe_trace_entry_head {
  *  NEED_RESCHED	- reschedule is requested
  *  HARDIRQ		- inside an interrupt handler
  *  SOFTIRQ		- inside a softirq handler
+ *  NEED_RESCHED_LAZY	- lazy reschedule is requested
  */
 enum trace_flag_type {
 	TRACE_FLAG_IRQS_OFF		= 0x01,
@@ -127,6 +128,7 @@ enum trace_flag_type {
 	TRACE_FLAG_HARDIRQ		= 0x08,
 	TRACE_FLAG_SOFTIRQ		= 0x10,
 	TRACE_FLAG_PREEMPT_RESCHED	= 0x20,
+	TRACE_FLAG_NEED_RESCHED_LAZY    = 0x40,
 };
 
 #define TRACE_BUF_SIZE		1024
diff --git a/kernel/msm-3.18/kernel/trace/trace_events.c b/kernel/msm-3.18/kernel/trace/trace_events.c
index 7f879c8a6..c941cc7d1 100644
--- a/kernel/msm-3.18/kernel/trace/trace_events.c
+++ b/kernel/msm-3.18/kernel/trace/trace_events.c
@@ -162,6 +162,8 @@ static int trace_define_common_fields(void)
 	__common_field(unsigned char, flags);
 	__common_field(unsigned char, preempt_count);
 	__common_field(int, pid);
+	__common_field(unsigned short, migrate_disable);
+	__common_field(unsigned short, padding);
 
 	return ret;
 }
@@ -198,6 +200,14 @@ void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
 
 	local_save_flags(fbuffer->flags);
 	fbuffer->pc = preempt_count();
+	/*
+	 * If CONFIG_PREEMPT is enabled, then the tracepoint itself disables
+	 * preemption (adding one to the preempt_count). Since we are
+	 * interested in the preempt_count at the time the tracepoint was
+	 * hit, we need to subtract one to offset the increment.
+	 */
+	if (IS_ENABLED(CONFIG_PREEMPT))
+		fbuffer->pc--;
 	fbuffer->ftrace_file = ftrace_file;
 
 	fbuffer->event =
diff --git a/kernel/msm-3.18/kernel/trace/trace_irqsoff.c b/kernel/msm-3.18/kernel/trace/trace_irqsoff.c
index 9bb104f74..d1940b095 100644
--- a/kernel/msm-3.18/kernel/trace/trace_irqsoff.c
+++ b/kernel/msm-3.18/kernel/trace/trace_irqsoff.c
@@ -17,6 +17,7 @@
 #include <linux/fs.h>
 
 #include "trace.h"
+#include <trace/events/hist.h>
 
 static struct trace_array		*irqsoff_trace __read_mostly;
 static int				tracer_enabled __read_mostly;
@@ -435,11 +436,13 @@ void start_critical_timings(void)
 {
 	if (preempt_trace() || irq_trace())
 		start_critical_timing(CALLER_ADDR0, CALLER_ADDR1);
+	trace_preemptirqsoff_hist_rcuidle(TRACE_START, 1);
 }
 EXPORT_SYMBOL_GPL(start_critical_timings);
 
 void stop_critical_timings(void)
 {
+	trace_preemptirqsoff_hist_rcuidle(TRACE_STOP, 0);
 	if (preempt_trace() || irq_trace())
 		stop_critical_timing(CALLER_ADDR0, CALLER_ADDR1);
 }
@@ -449,6 +452,7 @@ EXPORT_SYMBOL_GPL(stop_critical_timings);
 #ifdef CONFIG_PROVE_LOCKING
 void time_hardirqs_on(unsigned long a0, unsigned long a1)
 {
+	trace_preemptirqsoff_hist(IRQS_ON, 0);
 	if (!preempt_trace() && irq_trace())
 		stop_critical_timing(a0, a1);
 }
@@ -457,6 +461,7 @@ void time_hardirqs_off(unsigned long a0, unsigned long a1)
 {
 	if (!preempt_trace() && irq_trace())
 		start_critical_timing(a0, a1);
+	trace_preemptirqsoff_hist(IRQS_OFF, 1);
 }
 
 #else /* !CONFIG_PROVE_LOCKING */
@@ -482,6 +487,7 @@ inline void print_irqtrace_events(struct task_struct *curr)
  */
 void trace_hardirqs_on(void)
 {
+	trace_preemptirqsoff_hist_rcuidle(IRQS_ON, 0);
 	if (!preempt_trace() && irq_trace())
 		stop_critical_timing(CALLER_ADDR0, CALLER_ADDR1);
 }
@@ -491,11 +497,13 @@ void trace_hardirqs_off(void)
 {
 	if (!preempt_trace() && irq_trace())
 		start_critical_timing(CALLER_ADDR0, CALLER_ADDR1);
+	trace_preemptirqsoff_hist_rcuidle(IRQS_OFF, 1);
 }
 EXPORT_SYMBOL(trace_hardirqs_off);
 
 __visible void trace_hardirqs_on_caller(unsigned long caller_addr)
 {
+	trace_preemptirqsoff_hist(IRQS_ON, 0);
 	if (!preempt_trace() && irq_trace())
 		stop_critical_timing(CALLER_ADDR0, caller_addr);
 }
@@ -505,6 +513,7 @@ __visible void trace_hardirqs_off_caller(unsigned long caller_addr)
 {
 	if (!preempt_trace() && irq_trace())
 		start_critical_timing(CALLER_ADDR0, caller_addr);
+	trace_preemptirqsoff_hist(IRQS_OFF, 1);
 }
 EXPORT_SYMBOL(trace_hardirqs_off_caller);
 
@@ -514,12 +523,14 @@ EXPORT_SYMBOL(trace_hardirqs_off_caller);
 #ifdef CONFIG_PREEMPT_TRACER
 void trace_preempt_on(unsigned long a0, unsigned long a1)
 {
+	trace_preemptirqsoff_hist(PREEMPT_ON, 0);
 	if (preempt_trace() && !irq_trace())
 		stop_critical_timing(a0, a1);
 }
 
 void trace_preempt_off(unsigned long a0, unsigned long a1)
 {
+	trace_preemptirqsoff_hist(PREEMPT_ON, 1);
 	if (preempt_trace() && !irq_trace())
 		start_critical_timing(a0, a1);
 }
diff --git a/kernel/msm-3.18/kernel/trace/trace_output.c b/kernel/msm-3.18/kernel/trace/trace_output.c
index a5ebcd584..1a91b68ec 100644
--- a/kernel/msm-3.18/kernel/trace/trace_output.c
+++ b/kernel/msm-3.18/kernel/trace/trace_output.c
@@ -454,6 +454,7 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
 {
 	char hardsoft_irq;
 	char need_resched;
+	char need_resched_lazy;
 	char irqs_off;
 	int hardirq;
 	int softirq;
@@ -482,6 +483,8 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
 		need_resched = '.';
 		break;
 	}
+	need_resched_lazy =
+		(entry->flags & TRACE_FLAG_NEED_RESCHED_LAZY) ? 'L' : '.';
 
 	hardsoft_irq =
 		(hardirq && softirq) ? 'H' :
@@ -489,8 +492,9 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
 		softirq ? 's' :
 		'.';
 
-	if (!trace_seq_printf(s, "%c%c%c",
-			      irqs_off, need_resched, hardsoft_irq))
+	if (!trace_seq_printf(s, "%c%c%c%c",
+			      irqs_off, need_resched, need_resched_lazy,
+			      hardsoft_irq))
 		return 0;
 
 	if (entry->preempt_count)
@@ -498,6 +502,16 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
 	else
 		ret = trace_seq_putc(s, '.');
 
+	if (entry->preempt_lazy_count)
+		ret = trace_seq_printf(s, "%x", entry->preempt_lazy_count);
+	else
+		ret = trace_seq_putc(s, '.');
+
+	if (entry->migrate_disable)
+		ret = trace_seq_printf(s, "%x", entry->migrate_disable);
+	else
+		ret = trace_seq_putc(s, '.');
+
 	return ret;
 }
 
diff --git a/kernel/msm-3.18/kernel/trace/trace_sched_switch.c b/kernel/msm-3.18/kernel/trace/trace_sched_switch.c
index 3f34dc9b4..9586cde52 100644
--- a/kernel/msm-3.18/kernel/trace/trace_sched_switch.c
+++ b/kernel/msm-3.18/kernel/trace/trace_sched_switch.c
@@ -106,7 +106,7 @@ tracing_sched_wakeup_trace(struct trace_array *tr,
 }
 
 static void
-probe_sched_wakeup(void *ignore, struct task_struct *wakee, int success)
+probe_sched_wakeup(void *ignore, struct task_struct *wakee)
 {
 	struct trace_array_cpu *data;
 	unsigned long flags;
diff --git a/kernel/msm-3.18/kernel/trace/trace_sched_wakeup.c b/kernel/msm-3.18/kernel/trace/trace_sched_wakeup.c
index 19bd8928c..808258ccf 100644
--- a/kernel/msm-3.18/kernel/trace/trace_sched_wakeup.c
+++ b/kernel/msm-3.18/kernel/trace/trace_sched_wakeup.c
@@ -460,7 +460,7 @@ static void wakeup_reset(struct trace_array *tr)
 }
 
 static void
-probe_wakeup(void *ignore, struct task_struct *p, int success)
+probe_wakeup(void *ignore, struct task_struct *p)
 {
 	struct trace_array_cpu *data;
 	int cpu = smp_processor_id();
diff --git a/kernel/msm-3.18/kernel/user.c b/kernel/msm-3.18/kernel/user.c
index 2d09940c9..ea7d0ec75 100644
--- a/kernel/msm-3.18/kernel/user.c
+++ b/kernel/msm-3.18/kernel/user.c
@@ -158,11 +158,11 @@ void free_uid(struct user_struct *up)
 	if (!up)
 		return;
 
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 	if (atomic_dec_and_lock(&up->__count, &uidhash_lock))
 		free_user(up, flags);
 	else
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 }
 
 struct user_struct *alloc_uid(kuid_t uid)
diff --git a/kernel/msm-3.18/kernel/watchdog.c b/kernel/msm-3.18/kernel/watchdog.c
index 7eccf9b6d..6ae60aab3 100644
--- a/kernel/msm-3.18/kernel/watchdog.c
+++ b/kernel/msm-3.18/kernel/watchdog.c
@@ -323,6 +323,8 @@ static int is_softlockup(unsigned long touch_ts)
 
 #ifdef CONFIG_HARDLOCKUP_DETECTOR_NMI
 
+static DEFINE_RAW_SPINLOCK(watchdog_output_lock);
+
 static struct perf_event_attr wd_hw_attr = {
 	.type		= PERF_TYPE_HARDWARE,
 	.config		= PERF_COUNT_HW_CPU_CYCLES,
@@ -356,13 +358,21 @@ static void watchdog_overflow_callback(struct perf_event *event,
 		/* only print hardlockups once */
 		if (__this_cpu_read(hard_watchdog_warn) == true)
 			return;
+		/*
+		 * If early-printk is enabled then make sure we do not
+		 * lock up in printk() and kill console logging:
+		 */
+		printk_kill();
 
-		if (hardlockup_panic)
+		if (hardlockup_panic) {
 			panic("Watchdog detected hard LOCKUP on cpu %d",
 			      this_cpu);
-		else
+		} else {
+			raw_spin_lock(&watchdog_output_lock);
 			WARN(1, "Watchdog detected hard LOCKUP on cpu %d",
 			     this_cpu);
+			raw_spin_unlock(&watchdog_output_lock);
+		}
 
 		__this_cpu_write(hard_watchdog_warn, true);
 		return;
@@ -508,6 +518,7 @@ static void watchdog_enable(unsigned int cpu)
 	/* kick off the timer for the hardlockup detector */
 	hrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	hrtimer->function = watchdog_timer_fn;
+	hrtimer->irqsafe = 1;
 
 	/* Enable the perf event */
 	watchdog_nmi_enable(cpu);
diff --git a/kernel/msm-3.18/kernel/workqueue.c b/kernel/msm-3.18/kernel/workqueue.c
index e1cae9d21..a6e44d002 100644
--- a/kernel/msm-3.18/kernel/workqueue.c
+++ b/kernel/msm-3.18/kernel/workqueue.c
@@ -49,6 +49,8 @@
 #include <linux/moduleparam.h>
 #include <linux/uaccess.h>
 #include <linux/bug.h>
+#include <linux/locallock.h>
+#include <linux/delay.h>
 
 #include "workqueue_internal.h"
 
@@ -122,15 +124,20 @@ enum {
  *    cpu or grabbing pool->lock is enough for read access.  If
  *    POOL_DISASSOCIATED is set, it's identical to L.
  *
+ *    On RT we need the extra protection via rt_lock_idle_list() for
+ *    the list manipulations against read access from
+ *    wq_worker_sleeping(). All other places are nicely serialized via
+ *    pool->lock.
+ *
  * A: pool->attach_mutex protected.
  *
  * PL: wq_pool_mutex protected.
  *
- * PR: wq_pool_mutex protected for writes.  Sched-RCU protected for reads.
+ * PR: wq_pool_mutex protected for writes.  RCU protected for reads.
  *
  * WQ: wq->mutex protected.
  *
- * WR: wq->mutex protected for writes.  Sched-RCU protected for reads.
+ * WR: wq->mutex protected for writes.  RCU protected for reads.
  *
  * MD: wq_mayday_lock protected.
  */
@@ -178,7 +185,7 @@ struct worker_pool {
 	atomic_t		nr_running ____cacheline_aligned_in_smp;
 
 	/*
-	 * Destruction of pool is sched-RCU protected to allow dereferences
+	 * Destruction of pool is RCU protected to allow dereferences
 	 * from get_work_pool().
 	 */
 	struct rcu_head		rcu;
@@ -207,7 +214,7 @@ struct pool_workqueue {
 	/*
 	 * Release of unbound pwq is punted to system_wq.  See put_pwq()
 	 * and pwq_unbound_release_workfn() for details.  pool_workqueue
-	 * itself is also sched-RCU protected so that the first pwq can be
+	 * itself is also RCU protected so that the first pwq can be
 	 * determined without grabbing wq->mutex.
 	 */
 	struct work_struct	unbound_release_work;
@@ -322,6 +329,8 @@ EXPORT_SYMBOL_GPL(system_power_efficient_wq);
 struct workqueue_struct *system_freezable_power_efficient_wq __read_mostly;
 EXPORT_SYMBOL_GPL(system_freezable_power_efficient_wq);
 
+static DEFINE_LOCAL_IRQ_LOCK(pendingb_lock);
+
 static int worker_thread(void *__worker);
 static void copy_workqueue_attrs(struct workqueue_attrs *to,
 				 const struct workqueue_attrs *from);
@@ -330,14 +339,14 @@ static void copy_workqueue_attrs(struct workqueue_attrs *to,
 #include <trace/events/workqueue.h>
 
 #define assert_rcu_or_pool_mutex()					\
-	rcu_lockdep_assert(rcu_read_lock_sched_held() ||		\
+	rcu_lockdep_assert(rcu_read_lock_held() ||			\
 			   lockdep_is_held(&wq_pool_mutex),		\
-			   "sched RCU or wq_pool_mutex should be held")
+			   "RCU or wq_pool_mutex should be held")
 
 #define assert_rcu_or_wq_mutex(wq)					\
-	rcu_lockdep_assert(rcu_read_lock_sched_held() ||		\
+	rcu_lockdep_assert(rcu_read_lock_held() ||			\
 			   lockdep_is_held(&wq->mutex),			\
-			   "sched RCU or wq->mutex should be held")
+			   "RCU or wq->mutex should be held")
 
 #define for_each_cpu_worker_pool(pool, cpu)				\
 	for ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];		\
@@ -349,7 +358,7 @@ static void copy_workqueue_attrs(struct workqueue_attrs *to,
  * @pool: iteration cursor
  * @pi: integer used for iteration
  *
- * This must be called either with wq_pool_mutex held or sched RCU read
+ * This must be called either with wq_pool_mutex held or RCU read
  * locked.  If the pool needs to be used beyond the locking in effect, the
  * caller is responsible for guaranteeing that the pool stays online.
  *
@@ -381,7 +390,7 @@ static void copy_workqueue_attrs(struct workqueue_attrs *to,
  * @pwq: iteration cursor
  * @wq: the target workqueue
  *
- * This must be called either with wq->mutex held or sched RCU read locked.
+ * This must be called either with wq->mutex held or RCU read locked.
  * If the pwq needs to be used beyond the locking in effect, the caller is
  * responsible for guaranteeing that the pwq stays online.
  *
@@ -393,6 +402,31 @@ static void copy_workqueue_attrs(struct workqueue_attrs *to,
 		if (({ assert_rcu_or_wq_mutex(wq); false; })) { }	\
 		else
 
+#ifdef CONFIG_PREEMPT_RT_BASE
+static inline void rt_lock_idle_list(struct worker_pool *pool)
+{
+	preempt_disable();
+}
+static inline void rt_unlock_idle_list(struct worker_pool *pool)
+{
+	preempt_enable();
+}
+static inline void sched_lock_idle_list(struct worker_pool *pool) { }
+static inline void sched_unlock_idle_list(struct worker_pool *pool) { }
+#else
+static inline void rt_lock_idle_list(struct worker_pool *pool) { }
+static inline void rt_unlock_idle_list(struct worker_pool *pool) { }
+static inline void sched_lock_idle_list(struct worker_pool *pool)
+{
+	spin_lock_irq(&pool->lock);
+}
+static inline void sched_unlock_idle_list(struct worker_pool *pool)
+{
+	spin_unlock_irq(&pool->lock);
+}
+#endif
+
+
 #ifdef CONFIG_DEBUG_OBJECTS_WORK
 
 static struct debug_obj_descr work_debug_descr;
@@ -543,7 +577,7 @@ static int worker_pool_assign_id(struct worker_pool *pool)
  * @wq: the target workqueue
  * @node: the node ID
  *
- * This must be called either with pwq_lock held or sched RCU read locked.
+ * This must be called either with pwq_lock held or RCU read locked.
  * If the pwq needs to be used beyond the locking in effect, the caller is
  * responsible for guaranteeing that the pwq stays online.
  *
@@ -676,8 +710,8 @@ static struct pool_workqueue *get_work_pwq(struct work_struct *work)
  * @work: the work item of interest
  *
  * Pools are created and destroyed under wq_pool_mutex, and allows read
- * access under sched-RCU read lock.  As such, this function should be
- * called under wq_pool_mutex or with preemption disabled.
+ * access under RCU read lock.  As such, this function should be
+ * called under wq_pool_mutex or inside of a rcu_read_lock() region.
  *
  * All fields of the returned pool are accessible as long as the above
  * mentioned locking is in effect.  If the returned pool needs to be used
@@ -814,51 +848,44 @@ static struct worker *first_idle_worker(struct worker_pool *pool)
  */
 static void wake_up_worker(struct worker_pool *pool)
 {
-	struct worker *worker = first_idle_worker(pool);
+	struct worker *worker;
+
+	rt_lock_idle_list(pool);
+
+	worker = first_idle_worker(pool);
 
 	if (likely(worker))
 		wake_up_process(worker->task);
+
+	rt_unlock_idle_list(pool);
 }
 
 /**
- * wq_worker_waking_up - a worker is waking up
- * @task: task waking up
- * @cpu: CPU @task is waking up to
- *
- * This function is called during try_to_wake_up() when a worker is
- * being awoken.
+ * wq_worker_running - a worker is running again
+ * @task: task returning from sleep
  *
- * CONTEXT:
- * spin_lock_irq(rq->lock)
+ * This function is called when a worker returns from schedule()
  */
-void wq_worker_waking_up(struct task_struct *task, int cpu)
+void wq_worker_running(struct task_struct *task)
 {
 	struct worker *worker = kthread_data(task);
 
-	if (!(worker->flags & WORKER_NOT_RUNNING)) {
-		WARN_ON_ONCE(worker->pool->cpu != cpu);
+	if (!worker->sleeping)
+		return;
+	if (!(worker->flags & WORKER_NOT_RUNNING))
 		atomic_inc(&worker->pool->nr_running);
-	}
+	worker->sleeping = 0;
 }
 
 /**
  * wq_worker_sleeping - a worker is going to sleep
  * @task: task going to sleep
- * @cpu: CPU in question, must be the current CPU number
- *
- * This function is called during schedule() when a busy worker is
- * going to sleep.  Worker on the same cpu can be woken up by
- * returning pointer to its task.
- *
- * CONTEXT:
- * spin_lock_irq(rq->lock)
- *
- * Return:
- * Worker task on @cpu to wake up, %NULL if none.
+ * This function is called from schedule() when a busy worker is
+ * going to sleep.
  */
-struct task_struct *wq_worker_sleeping(struct task_struct *task, int cpu)
+void wq_worker_sleeping(struct task_struct *task)
 {
-	struct worker *worker = kthread_data(task), *to_wakeup = NULL;
+	struct worker *worker = kthread_data(task);
 	struct worker_pool *pool;
 
 	/*
@@ -867,29 +894,26 @@ struct task_struct *wq_worker_sleeping(struct task_struct *task, int cpu)
 	 * checking NOT_RUNNING.
 	 */
 	if (worker->flags & WORKER_NOT_RUNNING)
-		return NULL;
+		return;
 
 	pool = worker->pool;
 
-	/* this can only happen on the local cpu */
-	if (WARN_ON_ONCE(cpu != raw_smp_processor_id() || pool->cpu != cpu))
-		return NULL;
+	if (WARN_ON_ONCE(worker->sleeping))
+		return;
+
+	worker->sleeping = 1;
 
 	/*
 	 * The counterpart of the following dec_and_test, implied mb,
 	 * worklist not empty test sequence is in insert_work().
 	 * Please read comment there.
-	 *
-	 * NOT_RUNNING is clear.  This means that we're bound to and
-	 * running on the local cpu w/ rq lock held and preemption
-	 * disabled, which in turn means that none else could be
-	 * manipulating idle_list, so dereferencing idle_list without pool
-	 * lock is safe.
 	 */
 	if (atomic_dec_and_test(&pool->nr_running) &&
-	    !list_empty(&pool->worklist))
-		to_wakeup = first_idle_worker(pool);
-	return to_wakeup ? to_wakeup->task : NULL;
+	    !list_empty(&pool->worklist)) {
+		sched_lock_idle_list(pool);
+		wake_up_worker(pool);
+		sched_unlock_idle_list(pool);
+	}
 }
 
 /**
@@ -1083,12 +1107,14 @@ static void put_pwq_unlocked(struct pool_workqueue *pwq)
 {
 	if (pwq) {
 		/*
-		 * As both pwqs and pools are sched-RCU protected, the
+		 * As both pwqs and pools are RCU protected, the
 		 * following lock operations are safe.
 		 */
-		spin_lock_irq(&pwq->pool->lock);
+		rcu_read_lock();
+		local_spin_lock_irq(pendingb_lock, &pwq->pool->lock);
 		put_pwq(pwq);
-		spin_unlock_irq(&pwq->pool->lock);
+		local_spin_unlock_irq(pendingb_lock, &pwq->pool->lock);
+		rcu_read_unlock();
 	}
 }
 
@@ -1190,7 +1216,7 @@ static int try_to_grab_pending(struct work_struct *work, bool is_dwork,
 	struct worker_pool *pool;
 	struct pool_workqueue *pwq;
 
-	local_irq_save(*flags);
+	local_lock_irqsave(pendingb_lock, *flags);
 
 	/* try to steal the timer if it exists */
 	if (is_dwork) {
@@ -1209,6 +1235,7 @@ static int try_to_grab_pending(struct work_struct *work, bool is_dwork,
 	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)))
 		return 0;
 
+	rcu_read_lock();
 	/*
 	 * The queueing is in progress, or it is already queued. Try to
 	 * steal it from ->worklist without clearing WORK_STRUCT_PENDING.
@@ -1247,14 +1274,16 @@ static int try_to_grab_pending(struct work_struct *work, bool is_dwork,
 		set_work_pool_and_keep_pending(work, pool->id);
 
 		spin_unlock(&pool->lock);
+		rcu_read_unlock();
 		return 1;
 	}
 	spin_unlock(&pool->lock);
 fail:
-	local_irq_restore(*flags);
+	rcu_read_unlock();
+	local_unlock_irqrestore(pendingb_lock, *flags);
 	if (work_is_canceling(work))
 		return -ENOENT;
-	cpu_relax();
+	cpu_chill();
 	return -EAGAIN;
 }
 
@@ -1323,7 +1352,7 @@ static void __queue_work(int cpu, struct workqueue_struct *wq,
 	 * queued or lose PENDING.  Grabbing PENDING and queueing should
 	 * happen with IRQ disabled.
 	 */
-	WARN_ON_ONCE(!irqs_disabled());
+	WARN_ON_ONCE_NONRT(!irqs_disabled());
 
 	debug_work_activate(work);
 
@@ -1331,6 +1360,8 @@ static void __queue_work(int cpu, struct workqueue_struct *wq,
 	if (unlikely(wq->flags & __WQ_DRAINING) &&
 	    WARN_ON_ONCE(!is_chained_work(wq)))
 		return;
+
+	rcu_read_lock();
 retry:
 	if (req_cpu == WORK_CPU_UNBOUND)
 		cpu = raw_smp_processor_id();
@@ -1387,10 +1418,8 @@ retry:
 	/* pwq determined, queue */
 	trace_workqueue_queue_work(req_cpu, pwq, work);
 
-	if (WARN_ON(!list_empty(&work->entry))) {
-		spin_unlock(&pwq->pool->lock);
-		return;
-	}
+	if (WARN_ON(!list_empty(&work->entry)))
+		goto out;
 
 	pwq->nr_in_flight[pwq->work_color]++;
 	work_flags = work_color_to_flags(pwq->work_color);
@@ -1406,7 +1435,9 @@ retry:
 
 	insert_work(pwq, work, worklist, work_flags);
 
+out:
 	spin_unlock(&pwq->pool->lock);
+	rcu_read_unlock();
 }
 
 /**
@@ -1426,14 +1457,14 @@ bool queue_work_on(int cpu, struct workqueue_struct *wq,
 	bool ret = false;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	local_lock_irqsave(pendingb_lock,flags);
 
 	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {
 		__queue_work(cpu, wq, work);
 		ret = true;
 	}
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pendingb_lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL(queue_work_on);
@@ -1500,14 +1531,14 @@ bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
 	unsigned long flags;
 
 	/* read the comment in __queue_work() */
-	local_irq_save(flags);
+	local_lock_irqsave(pendingb_lock, flags);
 
 	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {
 		__queue_delayed_work(cpu, wq, dwork, delay);
 		ret = true;
 	}
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pendingb_lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL(queue_delayed_work_on);
@@ -1542,7 +1573,7 @@ bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
 
 	if (likely(ret >= 0)) {
 		__queue_delayed_work(cpu, wq, dwork, delay);
-		local_irq_restore(flags);
+		local_unlock_irqrestore(pendingb_lock, flags);
 	}
 
 	/* -ENOENT from try_to_grab_pending() becomes %true */
@@ -1575,7 +1606,9 @@ static void worker_enter_idle(struct worker *worker)
 	worker->last_active = jiffies;
 
 	/* idle_list is LIFO */
+	rt_lock_idle_list(pool);
 	list_add(&worker->entry, &pool->idle_list);
+	rt_unlock_idle_list(pool);
 
 	if (too_many_workers(pool) && !timer_pending(&pool->idle_timer))
 		mod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);
@@ -1608,7 +1641,9 @@ static void worker_leave_idle(struct worker *worker)
 		return;
 	worker_clr_flags(worker, WORKER_IDLE);
 	pool->nr_idle--;
+	rt_lock_idle_list(pool);
 	list_del_init(&worker->entry);
+	rt_unlock_idle_list(pool);
 }
 
 static struct worker *alloc_worker(int node)
@@ -1774,7 +1809,9 @@ static void destroy_worker(struct worker *worker)
 	pool->nr_workers--;
 	pool->nr_idle--;
 
+	rt_lock_idle_list(pool);
 	list_del_init(&worker->entry);
+	rt_unlock_idle_list(pool);
 	worker->flags |= WORKER_DIE;
 	wake_up_process(worker->task);
 }
@@ -2670,14 +2707,14 @@ static bool start_flush_work(struct work_struct *work, struct wq_barrier *barr)
 
 	might_sleep();
 
-	local_irq_disable();
+	rcu_read_lock();
 	pool = get_work_pool(work);
 	if (!pool) {
-		local_irq_enable();
+		rcu_read_unlock();
 		return false;
 	}
 
-	spin_lock(&pool->lock);
+	spin_lock_irq(&pool->lock);
 	/* see the comment in try_to_grab_pending() with the same code */
 	pwq = get_work_pwq(work);
 	if (pwq) {
@@ -2704,10 +2741,11 @@ static bool start_flush_work(struct work_struct *work, struct wq_barrier *barr)
 	else
 		lock_map_acquire_read(&pwq->wq->lockdep_map);
 	lock_map_release(&pwq->wq->lockdep_map);
-
+	rcu_read_unlock();
 	return true;
 already_gone:
 	spin_unlock_irq(&pool->lock);
+	rcu_read_unlock();
 	return false;
 }
 
@@ -2794,7 +2832,7 @@ static bool __cancel_work_timer(struct work_struct *work, bool is_dwork)
 
 	/* tell other tasks trying to grab @work to back off */
 	mark_work_canceling(work);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pendingb_lock, flags);
 
 	flush_work(work);
 	clear_work_data(work);
@@ -2849,10 +2887,10 @@ EXPORT_SYMBOL_GPL(cancel_work_sync);
  */
 bool flush_delayed_work(struct delayed_work *dwork)
 {
-	local_irq_disable();
+	local_lock_irq(pendingb_lock);
 	if (del_timer_sync(&dwork->timer))
 		__queue_work(dwork->cpu, dwork->wq, &dwork->work);
-	local_irq_enable();
+	local_unlock_irq(pendingb_lock);
 	return flush_work(&dwork->work);
 }
 EXPORT_SYMBOL(flush_delayed_work);
@@ -2887,7 +2925,7 @@ bool cancel_delayed_work(struct delayed_work *dwork)
 
 	set_work_pool_and_clear_pending(&dwork->work,
 					get_work_pool_id(&dwork->work));
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pendingb_lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL(cancel_delayed_work);
@@ -3073,7 +3111,8 @@ static ssize_t wq_pool_ids_show(struct device *dev,
 	const char *delim = "";
 	int node, written = 0;
 
-	rcu_read_lock_sched();
+	get_online_cpus();
+	rcu_read_lock();
 	for_each_node(node) {
 		written += scnprintf(buf + written, PAGE_SIZE - written,
 				     "%s%d:%d", delim, node,
@@ -3081,7 +3120,8 @@ static ssize_t wq_pool_ids_show(struct device *dev,
 		delim = " ";
 	}
 	written += scnprintf(buf + written, PAGE_SIZE - written, "\n");
-	rcu_read_unlock_sched();
+	rcu_read_unlock();
+	put_online_cpus();
 
 	return written;
 }
@@ -3449,7 +3489,7 @@ static void rcu_free_pool(struct rcu_head *rcu)
  * put_unbound_pool - put a worker_pool
  * @pool: worker_pool to put
  *
- * Put @pool.  If its refcnt reaches zero, it gets destroyed in sched-RCU
+ * Put @pool.  If its refcnt reaches zero, it gets destroyed in RCU
  * safe manner.  get_unbound_pool() calls this function on its failure path
  * and this function should be able to release pools which went through,
  * successfully or not, init_worker_pool().
@@ -3503,8 +3543,8 @@ static void put_unbound_pool(struct worker_pool *pool)
 	del_timer_sync(&pool->idle_timer);
 	del_timer_sync(&pool->mayday_timer);
 
-	/* sched-RCU protected to allow dereferences from get_work_pool() */
-	call_rcu_sched(&pool->rcu, rcu_free_pool);
+	/* RCU protected to allow dereferences from get_work_pool() */
+	call_rcu(&pool->rcu, rcu_free_pool);
 }
 
 /**
@@ -3609,7 +3649,7 @@ static void pwq_unbound_release_workfn(struct work_struct *work)
 	put_unbound_pool(pool);
 	mutex_unlock(&wq_pool_mutex);
 
-	call_rcu_sched(&pwq->rcu, rcu_free_pwq);
+	call_rcu(&pwq->rcu, rcu_free_pwq);
 
 	/*
 	 * If we're the last pwq going away, @wq is already dead and no one
@@ -4336,7 +4376,8 @@ bool workqueue_congested(int cpu, struct workqueue_struct *wq)
 	struct pool_workqueue *pwq;
 	bool ret;
 
-	rcu_read_lock_sched();
+	rcu_read_lock();
+	preempt_disable();
 
 	if (cpu == WORK_CPU_UNBOUND)
 		cpu = smp_processor_id();
@@ -4347,7 +4388,8 @@ bool workqueue_congested(int cpu, struct workqueue_struct *wq)
 		pwq = unbound_pwq_by_node(wq, cpu_to_node(cpu));
 
 	ret = !list_empty(&pwq->delayed_works);
-	rcu_read_unlock_sched();
+	preempt_enable();
+	rcu_read_unlock();
 
 	return ret;
 }
@@ -4373,16 +4415,15 @@ unsigned int work_busy(struct work_struct *work)
 	if (work_pending(work))
 		ret |= WORK_BUSY_PENDING;
 
-	local_irq_save(flags);
+	rcu_read_lock();
 	pool = get_work_pool(work);
 	if (pool) {
-		spin_lock(&pool->lock);
+		spin_lock_irqsave(&pool->lock, flags);
 		if (find_worker_executing_work(pool, work))
 			ret |= WORK_BUSY_RUNNING;
-		spin_unlock(&pool->lock);
+		spin_unlock_irqrestore(&pool->lock, flags);
 	}
-	local_irq_restore(flags);
-
+	rcu_read_unlock();
 	return ret;
 }
 EXPORT_SYMBOL_GPL(work_busy);
@@ -4829,16 +4870,16 @@ bool freeze_workqueues_busy(void)
 		 * nr_active is monotonically decreasing.  It's safe
 		 * to peek without lock.
 		 */
-		rcu_read_lock_sched();
+		rcu_read_lock();
 		for_each_pwq(pwq, wq) {
 			WARN_ON_ONCE(pwq->nr_active < 0);
 			if (pwq->nr_active) {
 				busy = true;
-				rcu_read_unlock_sched();
+				rcu_read_unlock();
 				goto out_unlock;
 			}
 		}
-		rcu_read_unlock_sched();
+		rcu_read_unlock();
 	}
 out_unlock:
 	mutex_unlock(&wq_pool_mutex);
diff --git a/kernel/msm-3.18/kernel/workqueue_internal.h b/kernel/msm-3.18/kernel/workqueue_internal.h
index 45215870a..f000c4d69 100644
--- a/kernel/msm-3.18/kernel/workqueue_internal.h
+++ b/kernel/msm-3.18/kernel/workqueue_internal.h
@@ -43,6 +43,7 @@ struct worker {
 	unsigned long		last_active;	/* L: last active timestamp */
 	unsigned int		flags;		/* X: flags */
 	int			id;		/* I: worker id */
+	int			sleeping;	/* None */
 
 	/*
 	 * Opaque string set with work_set_desc().  Printed out with task
@@ -68,7 +69,7 @@ static inline struct worker *current_wq_worker(void)
  * Scheduler hooks for concurrency managed workqueue.  Only to be used from
  * sched/core.c and workqueue.c.
  */
-void wq_worker_waking_up(struct task_struct *task, int cpu);
-struct task_struct *wq_worker_sleeping(struct task_struct *task, int cpu);
+void wq_worker_running(struct task_struct *task);
+void wq_worker_sleeping(struct task_struct *task);
 
 #endif /* _KERNEL_WORKQUEUE_INTERNAL_H */
diff --git a/kernel/msm-3.18/lib/Kconfig b/kernel/msm-3.18/lib/Kconfig
index c06584f45..6466e485c 100644
--- a/kernel/msm-3.18/lib/Kconfig
+++ b/kernel/msm-3.18/lib/Kconfig
@@ -383,6 +383,7 @@ config CHECK_SIGNATURE
 
 config CPUMASK_OFFSTACK
 	bool "Force CPU masks off stack" if DEBUG_PER_CPU_MAPS
+	depends on !PREEMPT_RT_FULL
 	help
 	  Use dynamic allocation for cpumask_var_t, instead of putting
 	  them on the stack.  This is a bit more expensive, but avoids
diff --git a/kernel/msm-3.18/lib/Kconfig.debug b/kernel/msm-3.18/lib/Kconfig.debug
index d1a61fd89..96b0f2a6b 100644
--- a/kernel/msm-3.18/lib/Kconfig.debug
+++ b/kernel/msm-3.18/lib/Kconfig.debug
@@ -684,7 +684,7 @@ config KCOV
 
 config DEBUG_SHIRQ
 	bool "Debug shared IRQ handlers"
-	depends on DEBUG_KERNEL
+	depends on DEBUG_KERNEL && !PREEMPT_RT_BASE
 	help
 	  Enable this to generate a spurious interrupt as soon as a shared
 	  interrupt handler is registered, and just before one is deregistered.
diff --git a/kernel/msm-3.18/lib/debugobjects.c b/kernel/msm-3.18/lib/debugobjects.c
index 9ddee8c27..36550d0d9 100644
--- a/kernel/msm-3.18/lib/debugobjects.c
+++ b/kernel/msm-3.18/lib/debugobjects.c
@@ -311,7 +311,10 @@ __debug_object_init(void *addr, struct debug_obj_descr *descr, int onstack)
 	struct debug_obj *obj;
 	unsigned long flags;
 
-	fill_pool();
+#ifdef CONFIG_PREEMPT_RT_FULL
+	if (preempt_count() == 0 && !irqs_disabled())
+#endif
+		fill_pool();
 
 	db = get_bucket((unsigned long) addr);
 
diff --git a/kernel/msm-3.18/lib/dump_stack.c b/kernel/msm-3.18/lib/dump_stack.c
index c30d07e99..01ca6dae9 100644
--- a/kernel/msm-3.18/lib/dump_stack.c
+++ b/kernel/msm-3.18/lib/dump_stack.c
@@ -25,7 +25,6 @@ static atomic_t dump_lock = ATOMIC_INIT(-1);
 
 asmlinkage __visible void dump_stack(void)
 {
-	unsigned long flags;
 	int was_locked;
 	int old;
 	int cpu;
@@ -34,8 +33,8 @@ asmlinkage __visible void dump_stack(void)
 	 * Permit this cpu to perform nested stack dumps while serialising
 	 * against other CPUs
 	 */
+	migrate_disable();
 retry:
-	local_irq_save(flags);
 	cpu = smp_processor_id();
 	old = atomic_cmpxchg(&dump_lock, -1, cpu);
 	if (old == -1) {
@@ -43,7 +42,6 @@ retry:
 	} else if (old == cpu) {
 		was_locked = 1;
 	} else {
-		local_irq_restore(flags);
 		cpu_relax();
 		goto retry;
 	}
@@ -53,7 +51,7 @@ retry:
 	if (!was_locked)
 		atomic_set(&dump_lock, -1);
 
-	local_irq_restore(flags);
+	migrate_enable();
 }
 #else
 asmlinkage __visible void dump_stack(void)
diff --git a/kernel/msm-3.18/lib/idr.c b/kernel/msm-3.18/lib/idr.c
index e654aebd5..83758924c 100644
--- a/kernel/msm-3.18/lib/idr.c
+++ b/kernel/msm-3.18/lib/idr.c
@@ -31,6 +31,7 @@
 #include <linux/spinlock.h>
 #include <linux/percpu.h>
 #include <linux/hardirq.h>
+#include <linux/locallock.h>
 
 #define MAX_IDR_SHIFT		(sizeof(int) * 8 - 1)
 #define MAX_IDR_BIT		(1U << MAX_IDR_SHIFT)
@@ -367,6 +368,35 @@ static void idr_fill_slot(struct idr *idr, void *ptr, int id,
 	idr_mark_full(pa, id);
 }
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+static DEFINE_LOCAL_IRQ_LOCK(idr_lock);
+
+static inline void idr_preload_lock(void)
+{
+	local_lock(idr_lock);
+}
+
+static inline void idr_preload_unlock(void)
+{
+	local_unlock(idr_lock);
+}
+
+void idr_preload_end(void)
+{
+	idr_preload_unlock();
+}
+EXPORT_SYMBOL(idr_preload_end);
+#else
+static inline void idr_preload_lock(void)
+{
+	preempt_disable();
+}
+
+static inline void idr_preload_unlock(void)
+{
+	preempt_enable();
+}
+#endif
 
 /**
  * idr_preload - preload for idr_alloc()
@@ -402,7 +432,7 @@ void idr_preload(gfp_t gfp_mask)
 	WARN_ON_ONCE(in_interrupt());
 	might_sleep_if(gfp_mask & __GFP_WAIT);
 
-	preempt_disable();
+	idr_preload_lock();
 
 	/*
 	 * idr_alloc() is likely to succeed w/o full idr_layer buffer and
@@ -414,9 +444,9 @@ void idr_preload(gfp_t gfp_mask)
 	while (__this_cpu_read(idr_preload_cnt) < MAX_IDR_FREE) {
 		struct idr_layer *new;
 
-		preempt_enable();
+		idr_preload_unlock();
 		new = kmem_cache_zalloc(idr_layer_cache, gfp_mask);
-		preempt_disable();
+		idr_preload_lock();
 		if (!new)
 			break;
 
diff --git a/kernel/msm-3.18/lib/locking-selftest.c b/kernel/msm-3.18/lib/locking-selftest.c
index 872a15a2a..b93a6103f 100644
--- a/kernel/msm-3.18/lib/locking-selftest.c
+++ b/kernel/msm-3.18/lib/locking-selftest.c
@@ -590,6 +590,8 @@ GENERATE_TESTCASE(init_held_rsem)
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_hard_rlock)
 
@@ -605,9 +607,12 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_soft_rlock)
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 /*
  * Enabling hardirqs with a softirq-safe lock held:
  */
@@ -640,6 +645,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2A_rlock)
 #undef E1
 #undef E2
 
+#endif
+
 /*
  * Enabling irqs with an irq-safe lock held:
  */
@@ -663,6 +670,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2A_rlock)
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_hard_rlock)
 
@@ -678,6 +687,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_rlock)
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 
@@ -709,6 +720,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_wlock)
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_hard_rlock)
 
@@ -724,6 +737,8 @@ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_rlock)
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 #undef E3
@@ -757,6 +772,8 @@ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_wlock)
 #include "locking-selftest-spin-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_hard_spin)
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+
 #include "locking-selftest-rlock-hardirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_hard_rlock)
 
@@ -772,10 +789,14 @@ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_soft_rlock)
 #include "locking-selftest-wlock-softirq.h"
 GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_soft_wlock)
 
+#endif
+
 #undef E1
 #undef E2
 #undef E3
 
+#ifndef CONFIG_PREEMPT_RT_FULL
+
 /*
  * read-lock / write-lock irq inversion.
  *
@@ -838,6 +859,10 @@ GENERATE_PERMUTATIONS_3_EVENTS(irq_inversion_soft_wlock)
 #undef E2
 #undef E3
 
+#endif
+
+#ifndef CONFIG_PREEMPT_RT_FULL
+
 /*
  * read-lock / write-lock recursion that is actually safe.
  */
@@ -876,6 +901,8 @@ GENERATE_PERMUTATIONS_3_EVENTS(irq_read_recursion_soft)
 #undef E2
 #undef E3
 
+#endif
+
 /*
  * read-lock / write-lock recursion that is unsafe.
  */
@@ -1858,6 +1885,7 @@ void locking_selftest(void)
 
 	printk("  --------------------------------------------------------------------------\n");
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 	/*
 	 * irq-context testcases:
 	 */
@@ -1870,6 +1898,28 @@ void locking_selftest(void)
 
 	DO_TESTCASE_6x2("irq read-recursion", irq_read_recursion);
 //	DO_TESTCASE_6x2B("irq read-recursion #2", irq_read_recursion2);
+#else
+	/* On -rt, we only do hardirq context test for raw spinlock */
+	DO_TESTCASE_1B("hard-irqs-on + irq-safe-A", irqsafe1_hard_spin, 12);
+	DO_TESTCASE_1B("hard-irqs-on + irq-safe-A", irqsafe1_hard_spin, 21);
+
+	DO_TESTCASE_1B("hard-safe-A + irqs-on", irqsafe2B_hard_spin, 12);
+	DO_TESTCASE_1B("hard-safe-A + irqs-on", irqsafe2B_hard_spin, 21);
+
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 123);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 132);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 213);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 231);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 312);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 321);
+
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 123);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 132);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 213);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 231);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 312);
+	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 321);
+#endif
 
 	ww_tests();
 
diff --git a/kernel/msm-3.18/lib/percpu_ida.c b/kernel/msm-3.18/lib/percpu_ida.c
index 93d145e55..32a608ddb 100644
--- a/kernel/msm-3.18/lib/percpu_ida.c
+++ b/kernel/msm-3.18/lib/percpu_ida.c
@@ -29,6 +29,9 @@
 #include <linux/string.h>
 #include <linux/spinlock.h>
 #include <linux/percpu_ida.h>
+#include <linux/locallock.h>
+
+static DEFINE_LOCAL_IRQ_LOCK(irq_off_lock);
 
 struct percpu_ida_cpu {
 	/*
@@ -151,13 +154,13 @@ int percpu_ida_alloc(struct percpu_ida *pool, int state)
 	unsigned long flags;
 	int tag;
 
-	local_irq_save(flags);
+	local_lock_irqsave(irq_off_lock, flags);
 	tags = this_cpu_ptr(pool->tag_cpu);
 
 	/* Fastpath */
 	tag = alloc_local_tag(tags);
 	if (likely(tag >= 0)) {
-		local_irq_restore(flags);
+		local_unlock_irqrestore(irq_off_lock, flags);
 		return tag;
 	}
 
@@ -176,6 +179,7 @@ int percpu_ida_alloc(struct percpu_ida *pool, int state)
 
 		if (!tags->nr_free)
 			alloc_global_tags(pool, tags);
+
 		if (!tags->nr_free)
 			steal_tags(pool, tags);
 
@@ -187,7 +191,7 @@ int percpu_ida_alloc(struct percpu_ida *pool, int state)
 		}
 
 		spin_unlock(&pool->lock);
-		local_irq_restore(flags);
+		local_unlock_irqrestore(irq_off_lock, flags);
 
 		if (tag >= 0 || state == TASK_RUNNING)
 			break;
@@ -199,7 +203,7 @@ int percpu_ida_alloc(struct percpu_ida *pool, int state)
 
 		schedule();
 
-		local_irq_save(flags);
+		local_lock_irqsave(irq_off_lock, flags);
 		tags = this_cpu_ptr(pool->tag_cpu);
 	}
 	if (state != TASK_RUNNING)
@@ -224,7 +228,7 @@ void percpu_ida_free(struct percpu_ida *pool, unsigned tag)
 
 	BUG_ON(tag >= pool->nr_tags);
 
-	local_irq_save(flags);
+	local_lock_irqsave(irq_off_lock, flags);
 	tags = this_cpu_ptr(pool->tag_cpu);
 
 	spin_lock(&tags->lock);
@@ -256,7 +260,7 @@ void percpu_ida_free(struct percpu_ida *pool, unsigned tag)
 		spin_unlock(&pool->lock);
 	}
 
-	local_irq_restore(flags);
+	local_unlock_irqrestore(irq_off_lock, flags);
 }
 EXPORT_SYMBOL_GPL(percpu_ida_free);
 
@@ -348,7 +352,7 @@ int percpu_ida_for_each_free(struct percpu_ida *pool, percpu_ida_cb fn,
 	struct percpu_ida_cpu *remote;
 	unsigned cpu, i, err = 0;
 
-	local_irq_save(flags);
+	local_lock_irqsave(irq_off_lock, flags);
 	for_each_possible_cpu(cpu) {
 		remote = per_cpu_ptr(pool->tag_cpu, cpu);
 		spin_lock(&remote->lock);
@@ -370,7 +374,7 @@ int percpu_ida_for_each_free(struct percpu_ida *pool, percpu_ida_cb fn,
 	}
 	spin_unlock(&pool->lock);
 out:
-	local_irq_restore(flags);
+	local_unlock_irqrestore(irq_off_lock, flags);
 	return err;
 }
 EXPORT_SYMBOL_GPL(percpu_ida_for_each_free);
diff --git a/kernel/msm-3.18/lib/radix-tree.c b/kernel/msm-3.18/lib/radix-tree.c
index 7fb005799..98c57e046 100644
--- a/kernel/msm-3.18/lib/radix-tree.c
+++ b/kernel/msm-3.18/lib/radix-tree.c
@@ -34,6 +34,7 @@
 #include <linux/bitops.h>
 #include <linux/rcupdate.h>
 #include <linux/preempt.h>		/* in_interrupt() */
+#include <linux/locallock.h>
 
 /*
  * The height_to_maxindex array needs to be one deeper than the maximum
@@ -67,6 +68,7 @@ struct radix_tree_preload {
 	struct radix_tree_node *nodes[RADIX_TREE_PRELOAD_SIZE];
 };
 static DEFINE_PER_CPU(struct radix_tree_preload, radix_tree_preloads) = { 0, };
+static DEFINE_LOCAL_IRQ_LOCK(radix_tree_preloads_lock);
 
 static inline void *ptr_to_indirect(void *ptr)
 {
@@ -194,12 +196,13 @@ radix_tree_node_alloc(struct radix_tree_root *root)
 		 * succeed in getting a node here (and never reach
 		 * kmem_cache_alloc)
 		 */
-		rtp = this_cpu_ptr(&radix_tree_preloads);
+		rtp = &get_locked_var(radix_tree_preloads_lock, radix_tree_preloads);
 		if (rtp->nr) {
 			ret = rtp->nodes[rtp->nr - 1];
 			rtp->nodes[rtp->nr - 1] = NULL;
 			rtp->nr--;
 		}
+		put_locked_var(radix_tree_preloads_lock, radix_tree_preloads);
 		/*
 		 * Update the allocation stack trace as this is more useful
 		 * for debugging.
@@ -254,14 +257,14 @@ static int __radix_tree_preload(gfp_t gfp_mask)
 	struct radix_tree_node *node;
 	int ret = -ENOMEM;
 
-	preempt_disable();
+	local_lock(radix_tree_preloads_lock);
 	rtp = this_cpu_ptr(&radix_tree_preloads);
 	while (rtp->nr < ARRAY_SIZE(rtp->nodes)) {
-		preempt_enable();
+		local_unlock(radix_tree_preloads_lock);
 		node = kmem_cache_alloc(radix_tree_node_cachep, gfp_mask);
 		if (node == NULL)
 			goto out;
-		preempt_disable();
+		local_lock(radix_tree_preloads_lock);
 		rtp = this_cpu_ptr(&radix_tree_preloads);
 		if (rtp->nr < ARRAY_SIZE(rtp->nodes))
 			rtp->nodes[rtp->nr++] = node;
@@ -300,11 +303,17 @@ int radix_tree_maybe_preload(gfp_t gfp_mask)
 	if (gfp_mask & __GFP_WAIT)
 		return __radix_tree_preload(gfp_mask);
 	/* Preloading doesn't help anything with this gfp mask, skip it */
-	preempt_disable();
+	local_lock(radix_tree_preloads_lock);
 	return 0;
 }
 EXPORT_SYMBOL(radix_tree_maybe_preload);
 
+void radix_tree_preload_end(void)
+{
+	local_unlock(radix_tree_preloads_lock);
+}
+EXPORT_SYMBOL(radix_tree_preload_end);
+
 /*
  *	Return the maximum key which can be store into a
  *	radix tree with height HEIGHT.
diff --git a/kernel/msm-3.18/lib/scatterlist.c b/kernel/msm-3.18/lib/scatterlist.c
index c9f2e8c6c..f6d1f8899 100644
--- a/kernel/msm-3.18/lib/scatterlist.c
+++ b/kernel/msm-3.18/lib/scatterlist.c
@@ -592,7 +592,7 @@ void sg_miter_stop(struct sg_mapping_iter *miter)
 			flush_kernel_dcache_page(miter->page);
 
 		if (miter->__flags & SG_MITER_ATOMIC) {
-			WARN_ON_ONCE(preemptible());
+			WARN_ON_ONCE(!pagefault_disabled());
 			kunmap_atomic(miter->addr);
 		} else
 			kunmap(miter->page);
@@ -637,7 +637,7 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
 	if (!sg_miter_skip(&miter, skip))
 		return false;
 
-	local_irq_save(flags);
+	local_irq_save_nort(flags);
 
 	while (sg_miter_next(&miter) && offset < buflen) {
 		unsigned int len;
@@ -654,7 +654,7 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
 
 	sg_miter_stop(&miter);
 
-	local_irq_restore(flags);
+	local_irq_restore_nort(flags);
 	return offset;
 }
 
diff --git a/kernel/msm-3.18/lib/smp_processor_id.c b/kernel/msm-3.18/lib/smp_processor_id.c
index 1afec32de..11fa43104 100644
--- a/kernel/msm-3.18/lib/smp_processor_id.c
+++ b/kernel/msm-3.18/lib/smp_processor_id.c
@@ -39,8 +39,9 @@ notrace static unsigned int check_preemption_disabled(const char *what1,
 	if (!printk_ratelimit())
 		goto out_enable;
 
-	printk(KERN_ERR "BUG: using %s%s() in preemptible [%08x] code: %s/%d\n",
-		what1, what2, preempt_count() - 1, current->comm, current->pid);
+	printk(KERN_ERR "BUG: using %s%s() in preemptible [%08x %08x] code: %s/%d\n",
+		what1, what2, preempt_count() - 1, __migrate_disabled(current),
+		current->comm, current->pid);
 
 	print_symbol("caller is %s\n", (long)__builtin_return_address(0));
 	dump_stack();
diff --git a/kernel/msm-3.18/localversion-rt b/kernel/msm-3.18/localversion-rt
new file mode 100644
index 000000000..2c95a3cdb
--- /dev/null
+++ b/kernel/msm-3.18/localversion-rt
@@ -0,0 +1 @@
+-rt72
diff --git a/kernel/msm-3.18/mm/Kconfig b/kernel/msm-3.18/mm/Kconfig
index 7aaf26896..9d3f1b1f1 100644
--- a/kernel/msm-3.18/mm/Kconfig
+++ b/kernel/msm-3.18/mm/Kconfig
@@ -408,7 +408,7 @@ config NOMMU_INITIAL_TRIM_EXCESS
 
 config TRANSPARENT_HUGEPAGE
 	bool "Transparent Hugepage Support"
-	depends on HAVE_ARCH_TRANSPARENT_HUGEPAGE
+	depends on HAVE_ARCH_TRANSPARENT_HUGEPAGE && !PREEMPT_RT_FULL
 	select COMPACTION
 	help
 	  Transparent Hugepages allows the kernel to use huge pages and
diff --git a/kernel/msm-3.18/mm/filemap.c b/kernel/msm-3.18/mm/filemap.c
index 78b5c392b..be4b5adaa 100644
--- a/kernel/msm-3.18/mm/filemap.c
+++ b/kernel/msm-3.18/mm/filemap.c
@@ -168,7 +168,9 @@ static void page_cache_tree_delete(struct address_space *mapping,
 	if (!workingset_node_pages(node) &&
 	    list_empty(&node->private_list)) {
 		node->private_data = mapping;
-		list_lru_add(&workingset_shadow_nodes, &node->private_list);
+		local_lock(workingset_shadow_lock);
+		list_lru_add(&__workingset_shadow_nodes, &node->private_list);
+		local_unlock(workingset_shadow_lock);
 	}
 }
 
@@ -537,9 +539,12 @@ static int page_cache_tree_insert(struct address_space *mapping,
 		 * node->private_list is protected by
 		 * mapping->tree_lock.
 		 */
-		if (!list_empty(&node->private_list))
-			list_lru_del(&workingset_shadow_nodes,
+		if (!list_empty(&node->private_list)) {
+			local_lock(workingset_shadow_lock);
+			list_lru_del(&__workingset_shadow_nodes,
 				     &node->private_list);
+			local_unlock(workingset_shadow_lock);
+		}
 	}
 	return 0;
 }
diff --git a/kernel/msm-3.18/mm/highmem.c b/kernel/msm-3.18/mm/highmem.c
index 123bcd3ed..16e8cf26d 100644
--- a/kernel/msm-3.18/mm/highmem.c
+++ b/kernel/msm-3.18/mm/highmem.c
@@ -29,10 +29,11 @@
 #include <linux/kgdb.h>
 #include <asm/tlbflush.h>
 
-
+#ifndef CONFIG_PREEMPT_RT_FULL
 #if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
 DEFINE_PER_CPU(int, __kmap_atomic_idx);
 #endif
+#endif
 
 /*
  * Virtual_count is not a pure "count".
@@ -107,8 +108,9 @@ static inline wait_queue_head_t *get_pkmap_wait_queue_head(unsigned int color)
 unsigned long totalhigh_pages __read_mostly;
 EXPORT_SYMBOL(totalhigh_pages);
 
-
+#ifndef CONFIG_PREEMPT_RT_FULL
 EXPORT_PER_CPU_SYMBOL(__kmap_atomic_idx);
+#endif
 
 unsigned int nr_free_highpages (void)
 {
diff --git a/kernel/msm-3.18/mm/memcontrol.c b/kernel/msm-3.18/mm/memcontrol.c
index 762746a43..416f47a71 100644
--- a/kernel/msm-3.18/mm/memcontrol.c
+++ b/kernel/msm-3.18/mm/memcontrol.c
@@ -60,6 +60,8 @@
 #include <net/sock.h>
 #include <net/ip.h>
 #include <net/tcp_memcontrol.h>
+#include <linux/locallock.h>
+
 #include "slab.h"
 
 #include <asm/uaccess.h>
@@ -87,6 +89,7 @@ static int really_do_swap_account __initdata;
 #define do_swap_account		0
 #endif
 
+static DEFINE_LOCAL_IRQ_LOCK(event_lock);
 
 static const char * const mem_cgroup_stat_names[] = {
 	"cache",
@@ -2357,14 +2360,17 @@ static void __init memcg_stock_init(void)
  */
 static void refill_stock(struct mem_cgroup *memcg, unsigned int nr_pages)
 {
-	struct memcg_stock_pcp *stock = &get_cpu_var(memcg_stock);
+	struct memcg_stock_pcp *stock;
+	int cpu = get_cpu_light();
+
+	stock = &per_cpu(memcg_stock, cpu);
 
 	if (stock->cached != memcg) { /* reset if necessary */
 		drain_stock(stock);
 		stock->cached = memcg;
 	}
 	stock->nr_pages += nr_pages;
-	put_cpu_var(memcg_stock);
+	put_cpu_light();
 }
 
 /*
@@ -2378,7 +2384,7 @@ static void drain_all_stock(struct mem_cgroup *root_memcg, bool sync)
 
 	/* Notify other cpus that system-wide "drain" is running */
 	get_online_cpus();
-	curcpu = get_cpu();
+	curcpu = get_cpu_light();
 	for_each_online_cpu(cpu) {
 		struct memcg_stock_pcp *stock = &per_cpu(memcg_stock, cpu);
 		struct mem_cgroup *memcg;
@@ -2395,7 +2401,7 @@ static void drain_all_stock(struct mem_cgroup *root_memcg, bool sync)
 				schedule_work_on(cpu, &stock->work);
 		}
 	}
-	put_cpu();
+	put_cpu_light();
 
 	if (!sync)
 		goto out;
@@ -3372,12 +3378,12 @@ static int mem_cgroup_move_account(struct page *page,
 	move_unlock_mem_cgroup(from, &flags);
 	ret = 0;
 
-	local_irq_disable();
+	local_lock_irq(event_lock);
 	mem_cgroup_charge_statistics(to, page, nr_pages);
 	memcg_check_events(to, page);
 	mem_cgroup_charge_statistics(from, page, -nr_pages);
 	memcg_check_events(from, page);
-	local_irq_enable();
+	local_unlock_irq(event_lock);
 out_unlock:
 	unlock_page(page);
 out:
@@ -6360,10 +6366,10 @@ void mem_cgroup_commit_charge(struct page *page, struct mem_cgroup *memcg,
 		VM_BUG_ON_PAGE(!PageTransHuge(page), page);
 	}
 
-	local_irq_disable();
+	local_lock_irq(event_lock);
 	mem_cgroup_charge_statistics(memcg, page, nr_pages);
 	memcg_check_events(memcg, page);
-	local_irq_enable();
+	local_unlock_irq(event_lock);
 
 	if (do_swap_account && PageSwapCache(page)) {
 		swp_entry_t entry = { .val = page_private(page) };
@@ -6422,14 +6428,14 @@ static void uncharge_batch(struct mem_cgroup *memcg, unsigned long pgpgout,
 		memcg_oom_recover(memcg);
 	}
 
-	local_irq_save(flags);
+	local_lock_irqsave(event_lock, flags);
 	__this_cpu_sub(memcg->stat->count[MEM_CGROUP_STAT_RSS], nr_anon);
 	__this_cpu_sub(memcg->stat->count[MEM_CGROUP_STAT_CACHE], nr_file);
 	__this_cpu_sub(memcg->stat->count[MEM_CGROUP_STAT_RSS_HUGE], nr_huge);
 	__this_cpu_add(memcg->stat->events[MEM_CGROUP_EVENTS_PGPGOUT], pgpgout);
 	__this_cpu_add(memcg->stat->nr_page_events, nr_anon + nr_file);
 	memcg_check_events(memcg, dummy_page);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(event_lock, flags);
 }
 
 static void uncharge_list(struct list_head *page_list)
diff --git a/kernel/msm-3.18/mm/memory.c b/kernel/msm-3.18/mm/memory.c
index 8e6557603..2b05ba066 100644
--- a/kernel/msm-3.18/mm/memory.c
+++ b/kernel/msm-3.18/mm/memory.c
@@ -3212,6 +3212,32 @@ unlock:
 	return 0;
 }
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+void pagefault_disable(void)
+{
+	migrate_disable();
+	current->pagefault_disabled++;
+	/*
+	 * make sure to have issued the store before a pagefault
+	 * can hit.
+	 */
+	barrier();
+}
+EXPORT_SYMBOL(pagefault_disable);
+
+void pagefault_enable(void)
+{
+	/*
+	 * make sure to issue those last loads/stores before enabling
+	 * the pagefault handler again.
+	 */
+	barrier();
+	current->pagefault_disabled--;
+	migrate_enable();
+}
+EXPORT_SYMBOL(pagefault_enable);
+#endif
+
 /*
  * By the time we get here, we already hold the mm semaphore
  *
diff --git a/kernel/msm-3.18/mm/mmu_context.c b/kernel/msm-3.18/mm/mmu_context.c
index f802c2d21..b1b6f238e 100644
--- a/kernel/msm-3.18/mm/mmu_context.c
+++ b/kernel/msm-3.18/mm/mmu_context.c
@@ -23,6 +23,7 @@ void use_mm(struct mm_struct *mm)
 	struct task_struct *tsk = current;
 
 	task_lock(tsk);
+	preempt_disable_rt();
 	active_mm = tsk->active_mm;
 	if (active_mm != mm) {
 		atomic_inc(&mm->mm_count);
@@ -30,6 +31,7 @@ void use_mm(struct mm_struct *mm)
 	}
 	tsk->mm = mm;
 	switch_mm(active_mm, mm, tsk);
+	preempt_enable_rt();
 	task_unlock(tsk);
 #ifdef finish_arch_post_lock_switch
 	finish_arch_post_lock_switch();
diff --git a/kernel/msm-3.18/mm/page_alloc.c b/kernel/msm-3.18/mm/page_alloc.c
index ad44d8b27..223d23865 100644
--- a/kernel/msm-3.18/mm/page_alloc.c
+++ b/kernel/msm-3.18/mm/page_alloc.c
@@ -60,6 +60,7 @@
 #include <linux/page-debug-flags.h>
 #include <linux/hugetlb.h>
 #include <linux/sched/rt.h>
+#include <linux/locallock.h>
 
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
@@ -244,6 +245,18 @@ EXPORT_SYMBOL(nr_node_ids);
 EXPORT_SYMBOL(nr_online_nodes);
 #endif
 
+static DEFINE_LOCAL_IRQ_LOCK(pa_lock);
+
+#ifdef CONFIG_PREEMPT_RT_BASE
+# define cpu_lock_irqsave(cpu, flags)		\
+	local_lock_irqsave_on(pa_lock, flags, cpu)
+# define cpu_unlock_irqrestore(cpu, flags)	\
+	local_unlock_irqrestore_on(pa_lock, flags, cpu)
+#else
+# define cpu_lock_irqsave(cpu, flags)		local_irq_save(flags)
+# define cpu_unlock_irqrestore(cpu, flags)	local_irq_restore(flags)
+#endif
+
 int page_group_by_mobility_disabled __read_mostly;
 
 void set_pageblock_migratetype(struct page *page, int migratetype)
@@ -629,7 +642,7 @@ continue_merging:
 
 			if (migratetype != buddy_mt
 					&& (is_migrate_isolate(migratetype) ||
-					is_migrate_isolate(buddy_mt)))
+						is_migrate_isolate(buddy_mt)))
 				goto done_merging;
 		}
 		max_order++;
@@ -695,7 +708,7 @@ static inline int free_pages_check(struct page *page)
 }
 
 /*
- * Frees a number of pages from the PCP lists
+ * Frees a number of pages which have been collected from the pcp lists.
  * Assumes all pages on list are in same zone, and of same order.
  * count is the number of pages to free.
  *
@@ -706,18 +719,51 @@ static inline int free_pages_check(struct page *page)
  * pinned" detection logic.
  */
 static void free_pcppages_bulk(struct zone *zone, int count,
-					struct per_cpu_pages *pcp)
+			       struct list_head *list)
 {
-	int migratetype = 0;
-	int batch_free = 0;
 	int to_free = count;
 	unsigned long nr_scanned;
+	unsigned long flags;
+
+	spin_lock_irqsave(&zone->lock, flags);
 
-	spin_lock(&zone->lock);
 	nr_scanned = zone_page_state(zone, NR_PAGES_SCANNED);
 	if (nr_scanned)
 		__mod_zone_page_state(zone, NR_PAGES_SCANNED, -nr_scanned);
 
+	while (!list_empty(list)) {
+		struct page *page = list_first_entry(list, struct page, lru);
+		int mt;	/* migratetype of the to-be-freed page */
+
+		/* must delete as __free_one_page list manipulates */
+		list_del(&page->lru);
+
+		mt = get_freepage_migratetype(page);
+		if (unlikely(has_isolate_pageblock(zone)))
+			mt = get_pageblock_migratetype(page);
+
+		/* MIGRATE_MOVABLE list may include MIGRATE_RESERVEs */
+		__free_one_page(page, page_to_pfn(page), zone, 0, mt);
+		trace_mm_page_pcpu_drain(page, 0, mt);
+		to_free--;
+	}
+	WARN_ON(to_free != 0);
+	spin_unlock_irqrestore(&zone->lock, flags);
+}
+
+/*
+ * Moves a number of pages from the PCP lists to free list which
+ * is freed outside of the locked region.
+ *
+ * Assumes all pages on list are in same zone, and of same order.
+ * count is the number of pages to free.
+ */
+static void isolate_pcp_pages(int to_free, struct per_cpu_pages *src,
+			      struct list_head *dst)
+{
+	int migratetype = 0;
+	int batch_free = 0;
+
 	while (to_free) {
 		struct page *page;
 		struct list_head *list;
@@ -733,7 +779,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 			batch_free++;
 			if (++migratetype == MIGRATE_PCPTYPES)
 				migratetype = 0;
-			list = &pcp->lists[migratetype];
+			list = &src->lists[migratetype];
 		} while (list_empty(list));
 
 		/* This is the only non-empty list. Free them all. */
@@ -741,25 +787,11 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 			batch_free = to_free;
 
 		do {
-			int mt;	/* migratetype of the to-be-freed page */
-
-			page = list_entry(list->prev, struct page, lru);
-			/* must delete as __free_one_page list manipulates */
+			page = list_last_entry(list, struct page, lru);
 			list_del(&page->lru);
-
-			mt = get_freepage_migratetype(page);
-			/* MIGRATE_ISOLATE page should not go to pcplists */
-			VM_BUG_ON_PAGE(is_migrate_isolate(mt), page);
-			/* Pageblock could have been isolated meanwhile */
-			if (unlikely(has_isolate_pageblock(zone)))
-				mt = get_pageblock_migratetype(page);
-
-			/* MIGRATE_MOVABLE list may include MIGRATE_RESERVEs */
-			__free_one_page(page, page_to_pfn(page), zone, 0, mt);
-			trace_mm_page_pcpu_drain(page, 0, mt);
+			list_add(&page->lru, dst);
 		} while (--to_free && --batch_free && !list_empty(list));
 	}
-	spin_unlock(&zone->lock);
 }
 
 static void free_one_page(struct zone *zone,
@@ -768,7 +800,9 @@ static void free_one_page(struct zone *zone,
 				int migratetype)
 {
 	unsigned long nr_scanned;
-	spin_lock(&zone->lock);
+	unsigned long flags;
+
+	spin_lock_irqsave(&zone->lock, flags);
 	nr_scanned = zone_page_state(zone, NR_PAGES_SCANNED);
 	if (nr_scanned)
 		__mod_zone_page_state(zone, NR_PAGES_SCANNED, -nr_scanned);
@@ -778,7 +812,7 @@ static void free_one_page(struct zone *zone,
 		migratetype = get_pfnblock_migratetype(page, pfn);
 	}
 	__free_one_page(page, pfn, zone, order, migratetype);
-	spin_unlock(&zone->lock);
+	spin_unlock_irqrestore(&zone->lock, flags);
 }
 
 static bool free_pages_prepare(struct page *page, unsigned int order)
@@ -819,11 +853,11 @@ static void __free_pages_ok(struct page *page, unsigned int order)
 		return;
 
 	migratetype = get_pfnblock_migratetype(page, pfn);
-	local_irq_save(flags);
+	local_lock_irqsave(pa_lock, flags);
 	__count_vm_events(PGFREE, 1 << order);
 	set_freepage_migratetype(page, migratetype);
 	free_one_page(page_zone(page), page, pfn, order, migratetype);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
 }
 
 void  __free_pages_bootmem(struct page *page, unsigned long pfn,
@@ -1429,16 +1463,18 @@ static struct list_head *get_populated_pcp_list(struct zone *zone,
 void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp)
 {
 	unsigned long flags;
+	LIST_HEAD(dst);
 	int to_drain, batch;
 
-	local_irq_save(flags);
+	local_lock_irqsave(pa_lock, flags);
 	batch = ACCESS_ONCE(pcp->batch);
 	to_drain = min(pcp->count, batch);
 	if (to_drain > 0) {
-		free_pcppages_bulk(zone, to_drain, pcp);
+		isolate_pcp_pages(to_drain, pcp, &dst);
 		pcp->count -= to_drain;
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
+	free_pcppages_bulk(zone, to_drain, &dst);
 }
 #endif
 
@@ -1457,16 +1493,21 @@ static void drain_pages(unsigned int cpu)
 	for_each_populated_zone(zone) {
 		struct per_cpu_pageset *pset;
 		struct per_cpu_pages *pcp;
+		LIST_HEAD(dst);
+		int count;
 
-		local_irq_save(flags);
+		cpu_lock_irqsave(cpu, flags);
 		pset = per_cpu_ptr(zone->pageset, cpu);
 
 		pcp = &pset->pcp;
-		if (pcp->count) {
-			free_pcppages_bulk(zone, pcp->count, pcp);
+		count = pcp->count;
+		if (count) {
+			isolate_pcp_pages(count, pcp, &dst);
 			pcp->count = 0;
 		}
-		local_irq_restore(flags);
+		cpu_unlock_irqrestore(cpu, flags);
+		if (count)
+			free_pcppages_bulk(zone, count, &dst);
 	}
 }
 
@@ -1519,7 +1560,12 @@ void drain_all_pages(void)
 		else
 			cpumask_clear_cpu(cpu, &cpus_with_pcps);
 	}
+#ifndef CONFIG_PREEMPT_RT_BASE
 	on_each_cpu_mask(&cpus_with_pcps, drain_local_pages, NULL, 1);
+#else
+	for_each_cpu(cpu, &cpus_with_pcps)
+		drain_pages(cpu);
+#endif
 }
 
 #ifdef CONFIG_HIBERNATION
@@ -1575,7 +1621,7 @@ void free_hot_cold_page(struct page *page, bool cold)
 
 	migratetype = get_pfnblock_migratetype(page, pfn);
 	set_freepage_migratetype(page, migratetype);
-	local_irq_save(flags);
+	local_lock_irqsave(pa_lock, flags);
 	__count_vm_event(PGFREE);
 
 	/*
@@ -1601,12 +1647,17 @@ void free_hot_cold_page(struct page *page, bool cold)
 	pcp->count++;
 	if (pcp->count >= pcp->high) {
 		unsigned long batch = ACCESS_ONCE(pcp->batch);
-		free_pcppages_bulk(zone, batch, pcp);
+		LIST_HEAD(dst);
+
+		isolate_pcp_pages(batch, pcp, &dst);
 		pcp->count -= batch;
+		local_unlock_irqrestore(pa_lock, flags);
+		free_pcppages_bulk(zone, batch, &dst);
+		return;
 	}
 
 out:
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
 }
 
 /*
@@ -1739,7 +1790,7 @@ again:
 		struct per_cpu_pages *pcp;
 		struct list_head *list = NULL;
 
-		local_irq_save(flags);
+		local_lock_irqsave(pa_lock, flags);
 		pcp = &this_cpu_ptr(zone->pageset)->pcp;
 
 		/* First try to get CMA pages */
@@ -1782,18 +1833,15 @@ again:
 			 */
 			WARN_ON_ONCE(order > 1);
 		}
-		spin_lock_irqsave(&zone->lock, flags);
-		if (migratetype == MIGRATE_MOVABLE && gfp_flags & __GFP_CMA)
-			page = __rmqueue_cma(zone, order);
-
-		if (!page)
-			page = __rmqueue(zone, order, migratetype);
-
-		spin_unlock(&zone->lock);
-		if (!page)
+		local_spin_lock_irqsave(pa_lock, &zone->lock, flags);
+		page = __rmqueue(zone, order, migratetype);
+		if (!page) {
+			spin_unlock(&zone->lock);
 			goto failed;
+		}
 		__mod_zone_freepage_state(zone, -(1 << order),
 					  get_freepage_migratetype(page));
+		spin_unlock(&zone->lock);
 	}
 
 	__mod_zone_page_state(zone, NR_ALLOC_BATCH, -(1 << order));
@@ -1803,7 +1851,7 @@ again:
 
 	__count_zone_vm_events(PGALLOC, zone, 1 << order);
 	zone_statistics(preferred_zone, zone, gfp_flags);
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
 
 	VM_BUG_ON_PAGE(bad_range(zone, page), page);
 	if (prep_new_page(page, order, gfp_flags))
@@ -1811,7 +1859,7 @@ again:
 	return page;
 
 failed:
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
 	return NULL;
 }
 
@@ -2531,8 +2579,8 @@ __alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,
 	count_vm_event(COMPACTSTALL);
 
 	/* Page migration frees to the PCP lists but we want merging */
-	drain_pages(get_cpu());
-	put_cpu();
+	drain_pages(get_cpu_light());
+	put_cpu_light();
 
 	page = get_page_from_freelist(gfp_mask, nodemask,
 			order, zonelist, high_zoneidx,
@@ -5804,6 +5852,7 @@ static int page_alloc_cpu_notify(struct notifier_block *self,
 void __init page_alloc_init(void)
 {
 	hotcpu_notifier(page_alloc_cpu_notify, 0);
+	local_irq_lock_init(pa_lock);
 }
 
 /*
@@ -6716,7 +6765,7 @@ void zone_pcp_reset(struct zone *zone)
 	struct per_cpu_pageset *pset;
 
 	/* avoid races with drain_pages()  */
-	local_irq_save(flags);
+	local_lock_irqsave(pa_lock, flags);
 	if (zone->pageset != &boot_pageset) {
 		for_each_online_cpu(cpu) {
 			pset = per_cpu_ptr(zone->pageset, cpu);
@@ -6725,7 +6774,7 @@ void zone_pcp_reset(struct zone *zone)
 		free_percpu(zone->pageset);
 		zone->pageset = &boot_pageset;
 	}
-	local_irq_restore(flags);
+	local_unlock_irqrestore(pa_lock, flags);
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
diff --git a/kernel/msm-3.18/mm/percpu.c b/kernel/msm-3.18/mm/percpu.c
index f7da3a307..809b1547d 100644
--- a/kernel/msm-3.18/mm/percpu.c
+++ b/kernel/msm-3.18/mm/percpu.c
@@ -1285,18 +1285,7 @@ void free_percpu(void __percpu *ptr)
 }
 EXPORT_SYMBOL_GPL(free_percpu);
 
-/**
- * is_kernel_percpu_address - test whether address is from static percpu area
- * @addr: address to test
- *
- * Test whether @addr belongs to in-kernel static percpu area.  Module
- * static percpu areas are not considered.  For those, use
- * is_module_percpu_address().
- *
- * RETURNS:
- * %true if @addr is from in-kernel static percpu area, %false otherwise.
- */
-bool is_kernel_percpu_address(unsigned long addr)
+bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr)
 {
 #ifdef CONFIG_SMP
 	const size_t static_size = __per_cpu_end - __per_cpu_start;
@@ -1305,15 +1294,35 @@ bool is_kernel_percpu_address(unsigned long addr)
 
 	for_each_possible_cpu(cpu) {
 		void *start = per_cpu_ptr(base, cpu);
+		void *va = (void *)addr;
 
-		if ((void *)addr >= start && (void *)addr < start + static_size)
+		if (va >= start && va < start + static_size) {
+			if (can_addr)
+				*can_addr = (unsigned long) (va - start);
 			return true;
-        }
+		}
+	}
 #endif
 	/* on UP, can't distinguish from other static vars, always false */
 	return false;
 }
 
+/**
+ * is_kernel_percpu_address - test whether address is from static percpu area
+ * @addr: address to test
+ *
+ * Test whether @addr belongs to in-kernel static percpu area.  Module
+ * static percpu areas are not considered.  For those, use
+ * is_module_percpu_address().
+ *
+ * RETURNS:
+ * %true if @addr is from in-kernel static percpu area, %false otherwise.
+ */
+bool is_kernel_percpu_address(unsigned long addr)
+{
+	return __is_kernel_percpu_address(addr, NULL);
+}
+
 /**
  * per_cpu_ptr_to_phys - convert translated percpu address to physical address
  * @addr: the address to be converted to physical address
diff --git a/kernel/msm-3.18/mm/slab.h b/kernel/msm-3.18/mm/slab.h
index ab019e63e..c5d1c194f 100644
--- a/kernel/msm-3.18/mm/slab.h
+++ b/kernel/msm-3.18/mm/slab.h
@@ -315,7 +315,11 @@ static inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)
  * The slab lists for all objects.
  */
 struct kmem_cache_node {
+#ifdef CONFIG_SLUB
+	raw_spinlock_t list_lock;
+#else
 	spinlock_t list_lock;
+#endif
 
 #ifdef CONFIG_SLAB
 	struct list_head slabs_partial;	/* partial list first, better asm code */
diff --git a/kernel/msm-3.18/mm/slub.c b/kernel/msm-3.18/mm/slub.c
index 6a2f3f115..729e6c75f 100644
--- a/kernel/msm-3.18/mm/slub.c
+++ b/kernel/msm-3.18/mm/slub.c
@@ -1120,7 +1120,7 @@ static noinline struct kmem_cache_node *free_debug_processing(
 {
 	struct kmem_cache_node *n = get_node(s, page_to_nid(page));
 
-	spin_lock_irqsave(&n->list_lock, *flags);
+	raw_spin_lock_irqsave(&n->list_lock, *flags);
 	slab_lock(page);
 
 	if (!check_slab(s, page))
@@ -1167,7 +1167,7 @@ out:
 
 fail:
 	slab_unlock(page);
-	spin_unlock_irqrestore(&n->list_lock, *flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, *flags);
 	slab_fix(s, "Object at 0x%p not freed", object);
 	return NULL;
 }
@@ -1295,6 +1295,12 @@ static inline void dec_slabs_node(struct kmem_cache *s, int node,
 
 #endif /* CONFIG_SLUB_DEBUG */
 
+struct slub_free_list {
+	raw_spinlock_t		lock;
+	struct list_head	list;
+};
+static DEFINE_PER_CPU(struct slub_free_list, slub_free_list);
+
 /*
  * Hooks for other subsystems that check memory allocations. In a typical
  * production configuration these hooks all should produce no code at all.
@@ -1354,6 +1360,16 @@ static inline void slab_free_hook(struct kmem_cache *s, void *x)
 	kasan_slab_free(s, x);
 }
 
+static void setup_object(struct kmem_cache *s, struct page *page,
+				void *object)
+{
+	setup_object_debug(s, page, object);
+	if (unlikely(s->ctor)) {
+		kasan_unpoison_object_data(s, object);
+		s->ctor(object);
+		kasan_poison_object_data(s, object);
+	}
+}
 /*
  * Slab allocation and freeing
  */
@@ -1384,10 +1400,17 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 	struct page *page;
 	struct kmem_cache_order_objects oo = s->oo;
 	gfp_t alloc_gfp;
+	bool enableirqs;
+	void *start, *p;
+	int idx, order;
 
 	flags &= gfp_allowed_mask;
 
-	if (flags & __GFP_WAIT)
+	enableirqs = (flags & __GFP_WAIT) != 0;
+#ifdef CONFIG_PREEMPT_RT_FULL
+	enableirqs |= system_state == SYSTEM_RUNNING;
+#endif
+	if (enableirqs)
 		local_irq_enable();
 
 	flags |= s->allocflags;
@@ -1409,13 +1432,13 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 		 * Try a lower order alloc if possible
 		 */
 		page = alloc_slab_page(s, alloc_gfp, node, oo);
-
-		if (page)
-			stat(s, ORDER_FALLBACK);
+		if (unlikely(!page))
+			goto out;
+		stat(s, ORDER_FALLBACK);
 	}
 
-	if (kmemcheck_enabled && page
-		&& !(s->flags & (SLAB_NOTRACK | DEBUG_DEFAULT_FLAGS))) {
+	if (kmemcheck_enabled &&
+	    !(s->flags & (SLAB_NOTRACK | DEBUG_DEFAULT_FLAGS))) {
 		int pages = 1 << oo_order(oo);
 
 		kmemcheck_alloc_shadow(page, oo_order(oo), alloc_gfp, node);
@@ -1430,48 +1453,9 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
 			kmemcheck_mark_unallocated_pages(page, pages);
 	}
 
-	if (flags & __GFP_WAIT)
-		local_irq_disable();
-	if (!page)
-		return NULL;
-
 	page->objects = oo_objects(oo);
-	mod_zone_page_state(page_zone(page),
-		(s->flags & SLAB_RECLAIM_ACCOUNT) ?
-		NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
-		1 << oo_order(oo));
-
-	return page;
-}
-
-static void setup_object(struct kmem_cache *s, struct page *page,
-				void *object)
-{
-	setup_object_debug(s, page, object);
-	if (unlikely(s->ctor)) {
-		kasan_unpoison_object_data(s, object);
-		s->ctor(object);
-		kasan_poison_object_data(s, object);
-	}
-}
-
-static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
-{
-	struct page *page;
-	void *start;
-	void *p;
-	int order;
-	int idx;
-
-	BUG_ON(flags & GFP_SLAB_BUG_MASK);
-
-	page = allocate_slab(s,
-		flags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node);
-	if (!page)
-		goto out;
 
 	order = compound_order(page);
-	inc_slabs_node(s, page_to_nid(page), page->objects);
 	page->slab_cache = s;
 	__SetPageSlab(page);
 	if (page->pfmemalloc)
@@ -1482,8 +1466,6 @@ static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
 	if (unlikely(s->flags & SLAB_POISON))
 		memset(start, POISON_INUSE, PAGE_SIZE << order);
 
-	kasan_poison_slab(page);
-
 	for_each_object_idx(p, idx, s, start, page->objects) {
 		setup_object(s, page, p);
 		if (likely(idx < page->objects))
@@ -1492,13 +1474,37 @@ static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
 			set_freepointer(s, p, NULL);
 	}
 
-	page->freelist = fixup_red_left(s, start);
+	page->freelist = start;
 	page->inuse = page->objects;
 	page->frozen = 1;
+
 out:
+	if (enableirqs)
+		local_irq_disable();
+	if (!page)
+		return NULL;
+
+	mod_zone_page_state(page_zone(page),
+		(s->flags & SLAB_RECLAIM_ACCOUNT) ?
+		NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
+		1 << oo_order(oo));
+
+	inc_slabs_node(s, page_to_nid(page), page->objects);
+
 	return page;
 }
 
+static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
+{
+	if (unlikely(flags & GFP_SLAB_BUG_MASK)) {
+		pr_emerg("gfp: %u\n", flags & GFP_SLAB_BUG_MASK);
+		BUG();
+	}
+
+	return allocate_slab(s,
+		flags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node);
+}
+
 static void __free_slab(struct kmem_cache *s, struct page *page)
 {
 	int order = compound_order(page);
@@ -1531,6 +1537,16 @@ static void __free_slab(struct kmem_cache *s, struct page *page)
 	memcg_uncharge_slab(s, order);
 }
 
+static void free_delayed(struct list_head *h)
+{
+	while(!list_empty(h)) {
+		struct page *page = list_first_entry(h, struct page, lru);
+
+		list_del(&page->lru);
+		__free_slab(page->slab_cache, page);
+	}
+}
+
 #define need_reserve_slab_rcu						\
 	(sizeof(((struct page *)NULL)->lru) < sizeof(struct rcu_head))
 
@@ -1565,6 +1581,12 @@ static void free_slab(struct kmem_cache *s, struct page *page)
 		}
 
 		call_rcu(head, rcu_free_slab);
+	} else if (irqs_disabled()) {
+		struct slub_free_list *f = &__get_cpu_var(slub_free_list);
+
+		raw_spin_lock(&f->lock);
+		list_add(&page->lru, &f->list);
+		raw_spin_unlock(&f->lock);
 	} else
 		__free_slab(s, page);
 }
@@ -1678,7 +1700,7 @@ static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
 	if (!n || !n->nr_partial)
 		return NULL;
 
-	spin_lock(&n->list_lock);
+	raw_spin_lock(&n->list_lock);
 	list_for_each_entry_safe(page, page2, &n->partial, lru) {
 		void *t;
 
@@ -1703,7 +1725,7 @@ static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
 			break;
 
 	}
-	spin_unlock(&n->list_lock);
+	raw_spin_unlock(&n->list_lock);
 	return object;
 }
 
@@ -1949,7 +1971,7 @@ redo:
 			 * that acquire_slab() will see a slab page that
 			 * is frozen
 			 */
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 	} else {
 		m = M_FULL;
@@ -1960,7 +1982,7 @@ redo:
 			 * slabs from diagnostic functions will not see
 			 * any frozen slabs.
 			 */
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 	}
 
@@ -1995,7 +2017,7 @@ redo:
 		goto redo;
 
 	if (lock)
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 
 	if (m == M_FREE) {
 		stat(s, DEACTIVATE_EMPTY);
@@ -2027,10 +2049,10 @@ static void unfreeze_partials(struct kmem_cache *s,
 		n2 = get_node(s, page_to_nid(page));
 		if (n != n2) {
 			if (n)
-				spin_unlock(&n->list_lock);
+				raw_spin_unlock(&n->list_lock);
 
 			n = n2;
-			spin_lock(&n->list_lock);
+			raw_spin_lock(&n->list_lock);
 		}
 
 		do {
@@ -2059,7 +2081,7 @@ static void unfreeze_partials(struct kmem_cache *s,
 	}
 
 	if (n)
-		spin_unlock(&n->list_lock);
+		raw_spin_unlock(&n->list_lock);
 
 	while (discard_page) {
 		page = discard_page;
@@ -2097,14 +2119,21 @@ static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
 			pobjects = oldpage->pobjects;
 			pages = oldpage->pages;
 			if (drain && pobjects > s->cpu_partial) {
+				struct slub_free_list *f;
 				unsigned long flags;
+				LIST_HEAD(tofree);
 				/*
 				 * partial array is full. Move the existing
 				 * set to the per node partial list.
 				 */
 				local_irq_save(flags);
 				unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+				f = &__get_cpu_var(slub_free_list);
+				raw_spin_lock(&f->lock);
+				list_splice_init(&f->list, &tofree);
+				raw_spin_unlock(&f->lock);
 				local_irq_restore(flags);
+				free_delayed(&tofree);
 				oldpage = NULL;
 				pobjects = 0;
 				pages = 0;
@@ -2168,7 +2197,22 @@ static bool has_cpu_slab(int cpu, void *info)
 
 static void flush_all(struct kmem_cache *s)
 {
+	LIST_HEAD(tofree);
+	int cpu;
+
 	on_each_cpu_cond(has_cpu_slab, flush_cpu_slab, s, 1, GFP_ATOMIC);
+	for_each_online_cpu(cpu) {
+		struct slub_free_list *f;
+
+		if (!has_cpu_slab(cpu, s))
+			continue;
+
+		f = &per_cpu(slub_free_list, cpu);
+		raw_spin_lock_irq(&f->lock);
+		list_splice_init(&f->list, &tofree);
+		raw_spin_unlock_irq(&f->lock);
+		free_delayed(&tofree);
+	}
 }
 
 /*
@@ -2204,10 +2248,10 @@ static unsigned long count_partial(struct kmem_cache_node *n,
 	unsigned long x = 0;
 	struct page *page;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 	list_for_each_entry(page, &n->partial, lru)
 		x += get_count(page);
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return x;
 }
 #endif /* CONFIG_SLUB_DEBUG || CONFIG_SYSFS */
@@ -2344,9 +2388,11 @@ static inline void *get_freelist(struct kmem_cache *s, struct page *page)
 static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 			  unsigned long addr, struct kmem_cache_cpu *c)
 {
+	struct slub_free_list *f;
 	void *freelist;
 	struct page *page;
 	unsigned long flags;
+	LIST_HEAD(tofree);
 
 	local_irq_save(flags);
 #ifdef CONFIG_PREEMPT
@@ -2414,7 +2460,13 @@ load_freelist:
 	VM_BUG_ON(!c->page->frozen);
 	c->freelist = get_freepointer(s, freelist);
 	c->tid = next_tid(c->tid);
+out:
+	f = &__get_cpu_var(slub_free_list);
+	raw_spin_lock(&f->lock);
+	list_splice_init(&f->list, &tofree);
+	raw_spin_unlock(&f->lock);
 	local_irq_restore(flags);
+	free_delayed(&tofree);
 	return freelist;
 
 new_slab:
@@ -2431,8 +2483,7 @@ new_slab:
 
 	if (unlikely(!freelist)) {
 		slab_out_of_memory(s, gfpflags, node);
-		local_irq_restore(flags);
-		return NULL;
+		goto out;
 	}
 
 	page = c->page;
@@ -2447,8 +2498,7 @@ new_slab:
 	deactivate_slab(s, page, get_freepointer(s, freelist));
 	c->page = NULL;
 	c->freelist = NULL;
-	local_irq_restore(flags);
-	return freelist;
+	goto out;
 }
 
 /*
@@ -2623,7 +2673,7 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 
 	do {
 		if (unlikely(n)) {
-			spin_unlock_irqrestore(&n->list_lock, flags);
+			raw_spin_unlock_irqrestore(&n->list_lock, flags);
 			n = NULL;
 		}
 		prior = page->freelist;
@@ -2655,7 +2705,7 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 				 * Otherwise the list_lock will synchronize with
 				 * other processors updating the list of slabs.
 				 */
-				spin_lock_irqsave(&n->list_lock, flags);
+				raw_spin_lock_irqsave(&n->list_lock, flags);
 
 			}
 		}
@@ -2697,7 +2747,7 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 		add_partial(n, page, DEACTIVATE_TO_TAIL);
 		stat(s, FREE_ADD_PARTIAL);
 	}
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return;
 
 slab_empty:
@@ -2712,7 +2762,7 @@ slab_empty:
 		remove_full(s, n, page);
 	}
 
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	stat(s, FREE_SLAB);
 	discard_slab(s, page);
 }
@@ -2908,7 +2958,7 @@ static void
 init_kmem_cache_node(struct kmem_cache_node *n)
 {
 	n->nr_partial = 0;
-	spin_lock_init(&n->list_lock);
+	raw_spin_lock_init(&n->list_lock);
 	INIT_LIST_HEAD(&n->partial);
 #ifdef CONFIG_SLUB_DEBUG
 	atomic_long_set(&n->nr_slabs, 0);
@@ -3525,7 +3575,7 @@ int __kmem_cache_shrink(struct kmem_cache *s)
 		for (i = 0; i < objects; i++)
 			INIT_LIST_HEAD(slabs_by_inuse + i);
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 
 		/*
 		 * Build lists indexed by the items in use in each slab.
@@ -3546,7 +3596,7 @@ int __kmem_cache_shrink(struct kmem_cache *s)
 		for (i = objects - 1; i > 0; i--)
 			list_splice(slabs_by_inuse + i, n->partial.prev);
 
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 
 		/* Release empty slabs */
 		list_for_each_entry_safe(page, t, slabs_by_inuse, lru)
@@ -3719,6 +3769,12 @@ void __init kmem_cache_init(void)
 {
 	static __initdata struct kmem_cache boot_kmem_cache,
 		boot_kmem_cache_node;
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		raw_spin_lock_init(&per_cpu(slub_free_list, cpu).lock);
+		INIT_LIST_HEAD(&per_cpu(slub_free_list, cpu).list);
+	}
 
 	if (debug_guardpage_minorder())
 		slub_max_order = 0;
@@ -3967,7 +4023,7 @@ static int validate_slab_node(struct kmem_cache *s,
 	struct page *page;
 	unsigned long flags;
 
-	spin_lock_irqsave(&n->list_lock, flags);
+	raw_spin_lock_irqsave(&n->list_lock, flags);
 
 	list_for_each_entry(page, &n->partial, lru) {
 		validate_slab_slab(s, page, map);
@@ -3989,7 +4045,7 @@ static int validate_slab_node(struct kmem_cache *s,
 		       s->name, count, atomic_long_read(&n->nr_slabs));
 
 out:
-	spin_unlock_irqrestore(&n->list_lock, flags);
+	raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	return count;
 }
 
@@ -4177,12 +4233,12 @@ static int list_locations(struct kmem_cache *s, char *buf,
 		if (!atomic_long_read(&n->nr_slabs))
 			continue;
 
-		spin_lock_irqsave(&n->list_lock, flags);
+		raw_spin_lock_irqsave(&n->list_lock, flags);
 		list_for_each_entry(page, &n->partial, lru)
 			process_slab(&t, s, page, alloc, map);
 		list_for_each_entry(page, &n->full, lru)
 			process_slab(&t, s, page, alloc, map);
-		spin_unlock_irqrestore(&n->list_lock, flags);
+		raw_spin_unlock_irqrestore(&n->list_lock, flags);
 	}
 
 	for (i = 0; i < t.count; i++) {
diff --git a/kernel/msm-3.18/mm/swap.c b/kernel/msm-3.18/mm/swap.c
index e657ba642..8f8fa64e2 100644
--- a/kernel/msm-3.18/mm/swap.c
+++ b/kernel/msm-3.18/mm/swap.c
@@ -31,6 +31,7 @@
 #include <linux/memcontrol.h>
 #include <linux/gfp.h>
 #include <linux/uio.h>
+#include <linux/locallock.h>
 
 #include "internal.h"
 
@@ -44,6 +45,9 @@ static DEFINE_PER_CPU(struct pagevec, lru_add_pvec);
 static DEFINE_PER_CPU(struct pagevec, lru_rotate_pvecs);
 static DEFINE_PER_CPU(struct pagevec, lru_deactivate_file_pvecs);
 
+static DEFINE_LOCAL_IRQ_LOCK(rotate_lock);
+static DEFINE_LOCAL_IRQ_LOCK(swapvec_lock);
+
 /*
  * This path almost never happens for VM activity - pages are normally
  * freed via pagevecs.  But it gets used by networking.
@@ -473,11 +477,11 @@ void rotate_reclaimable_page(struct page *page)
 		unsigned long flags;
 
 		page_cache_get(page);
-		local_irq_save(flags);
+		local_lock_irqsave(rotate_lock, flags);
 		pvec = this_cpu_ptr(&lru_rotate_pvecs);
 		if (!pagevec_add(pvec, page) || PageCompound(page))
 			pagevec_move_tail(pvec);
-		local_irq_restore(flags);
+		local_unlock_irqrestore(rotate_lock, flags);
 	}
 }
 
@@ -528,12 +532,13 @@ static bool need_activate_page_drain(int cpu)
 void activate_page(struct page *page)
 {
 	if (PageLRU(page) && !PageActive(page) && !PageUnevictable(page)) {
-		struct pagevec *pvec = &get_cpu_var(activate_page_pvecs);
+		struct pagevec *pvec = &get_locked_var(swapvec_lock,
+						       activate_page_pvecs);
 
 		page_cache_get(page);
 		if (!pagevec_add(pvec, page) || PageCompound(page))
 			pagevec_lru_move_fn(pvec, __activate_page, NULL);
-		put_cpu_var(activate_page_pvecs);
+		put_locked_var(swapvec_lock, activate_page_pvecs);
 	}
 }
 
@@ -559,7 +564,7 @@ void activate_page(struct page *page)
 
 static void __lru_cache_activate_page(struct page *page)
 {
-	struct pagevec *pvec = &get_cpu_var(lru_add_pvec);
+	struct pagevec *pvec = &get_locked_var(swapvec_lock, lru_add_pvec);
 	int i;
 
 	/*
@@ -581,7 +586,7 @@ static void __lru_cache_activate_page(struct page *page)
 		}
 	}
 
-	put_cpu_var(lru_add_pvec);
+	put_locked_var(swapvec_lock, lru_add_pvec);
 }
 
 /*
@@ -620,12 +625,12 @@ EXPORT_SYMBOL(mark_page_accessed);
 
 static void __lru_cache_add(struct page *page)
 {
-	struct pagevec *pvec = &get_cpu_var(lru_add_pvec);
+	struct pagevec *pvec = &get_locked_var(swapvec_lock, lru_add_pvec);
 
 	page_cache_get(page);
 	if (!pagevec_add(pvec, page) || PageCompound(page))
 		__pagevec_lru_add(pvec);
-	put_cpu_var(lru_add_pvec);
+	put_locked_var(swapvec_lock, lru_add_pvec);
 }
 
 /**
@@ -805,9 +810,15 @@ void lru_add_drain_cpu(int cpu)
 		unsigned long flags;
 
 		/* No harm done if a racing interrupt already did this */
-		local_irq_save(flags);
+#ifdef CONFIG_PREEMPT_RT_BASE
+		local_lock_irqsave_on(rotate_lock, flags, cpu);
+		pagevec_move_tail(pvec);
+		local_unlock_irqrestore_on(rotate_lock, flags, cpu);
+#else
+		local_lock_irqsave(rotate_lock, flags);
 		pagevec_move_tail(pvec);
-		local_irq_restore(flags);
+		local_unlock_irqrestore(rotate_lock, flags);
+#endif
 	}
 
 	pvec = &per_cpu(lru_deactivate_file_pvecs, cpu);
@@ -835,26 +846,47 @@ void deactivate_file_page(struct page *page)
 		return;
 
 	if (likely(get_page_unless_zero(page))) {
-		struct pagevec *pvec = &get_cpu_var(lru_deactivate_file_pvecs);
+		struct pagevec *pvec = &get_locked_var(swapvec_lock,
+						       lru_deactivate_file_pvecs);
 
-		if (!pagevec_add(pvec, page) || PageCompound(page))
+		if (!pagevec_add(pvec, page))
 			pagevec_lru_move_fn(pvec, lru_deactivate_file_fn, NULL);
-		put_cpu_var(lru_deactivate_file_pvecs);
+		put_locked_var(swapvec_lock, lru_deactivate_file_pvecs);
 	}
 }
 
 void lru_add_drain(void)
 {
-	lru_add_drain_cpu(get_cpu());
-	put_cpu();
+	lru_add_drain_cpu(local_lock_cpu(swapvec_lock));
+	local_unlock_cpu(swapvec_lock);
+}
+
+
+#ifdef CONFIG_PREEMPT_RT_BASE
+static inline void remote_lru_add_drain(int cpu, struct cpumask *has_work)
+{
+	local_lock_on(swapvec_lock, cpu);
+	lru_add_drain_cpu(cpu);
+	local_unlock_on(swapvec_lock, cpu);
 }
 
+#else
+
 static void lru_add_drain_per_cpu(struct work_struct *dummy)
 {
 	lru_add_drain();
 }
 
 static DEFINE_PER_CPU(struct work_struct, lru_add_drain_work);
+static inline void remote_lru_add_drain(int cpu, struct cpumask *has_work)
+{
+	struct work_struct *work = &per_cpu(lru_add_drain_work, cpu);
+
+	INIT_WORK(work, lru_add_drain_per_cpu);
+	schedule_work_on(cpu, work);
+	cpumask_set_cpu(cpu, has_work);
+}
+#endif
 
 void lru_add_drain_all(void)
 {
@@ -867,20 +899,17 @@ void lru_add_drain_all(void)
 	cpumask_clear(&has_work);
 
 	for_each_online_cpu(cpu) {
-		struct work_struct *work = &per_cpu(lru_add_drain_work, cpu);
-
 		if (pagevec_count(&per_cpu(lru_add_pvec, cpu)) ||
 		    pagevec_count(&per_cpu(lru_rotate_pvecs, cpu)) ||
 		    pagevec_count(&per_cpu(lru_deactivate_file_pvecs, cpu)) ||
-		    need_activate_page_drain(cpu)) {
-			INIT_WORK(work, lru_add_drain_per_cpu);
-			schedule_work_on(cpu, work);
-			cpumask_set_cpu(cpu, &has_work);
-		}
+		    need_activate_page_drain(cpu))
+			remote_lru_add_drain(cpu, &has_work);
 	}
 
+#ifndef CONFIG_PREEMPT_RT_BASE
 	for_each_cpu(cpu, &has_work)
 		flush_work(&per_cpu(lru_add_drain_work, cpu));
+#endif
 
 	put_online_cpus();
 	mutex_unlock(&lock);
diff --git a/kernel/msm-3.18/mm/truncate.c b/kernel/msm-3.18/mm/truncate.c
index ef735c909..77d8602f6 100644
--- a/kernel/msm-3.18/mm/truncate.c
+++ b/kernel/msm-3.18/mm/truncate.c
@@ -56,8 +56,11 @@ static void clear_exceptional_entry(struct address_space *mapping,
 	 * protected by mapping->tree_lock.
 	 */
 	if (!workingset_node_shadows(node) &&
-	    !list_empty(&node->private_list))
-		list_lru_del(&workingset_shadow_nodes, &node->private_list);
+	    !list_empty(&node->private_list)) {
+		local_lock(workingset_shadow_lock);
+		list_lru_del(&__workingset_shadow_nodes, &node->private_list);
+		local_unlock(workingset_shadow_lock);
+	}
 	__radix_tree_delete_node(&mapping->page_tree, node);
 unlock:
 	spin_unlock_irq(&mapping->tree_lock);
diff --git a/kernel/msm-3.18/mm/vmalloc.c b/kernel/msm-3.18/mm/vmalloc.c
index 89d5abbe5..c36bd219b 100644
--- a/kernel/msm-3.18/mm/vmalloc.c
+++ b/kernel/msm-3.18/mm/vmalloc.c
@@ -849,7 +849,7 @@ static struct vmap_block *new_vmap_block(gfp_t gfp_mask)
 	struct vmap_block *vb;
 	struct vmap_area *va;
 	unsigned long vb_idx;
-	int node, err;
+	int node, err, cpu;
 
 	node = numa_node_id();
 
@@ -887,11 +887,12 @@ static struct vmap_block *new_vmap_block(gfp_t gfp_mask)
 	BUG_ON(err);
 	radix_tree_preload_end();
 
-	vbq = &get_cpu_var(vmap_block_queue);
+	cpu = get_cpu_light();
+	vbq = &__get_cpu_var(vmap_block_queue);
 	spin_lock(&vbq->lock);
 	list_add_rcu(&vb->free_list, &vbq->free);
 	spin_unlock(&vbq->lock);
-	put_cpu_var(vmap_block_queue);
+	put_cpu_light();
 
 	return vb;
 }
@@ -959,6 +960,7 @@ static void *vb_alloc(unsigned long size, gfp_t gfp_mask)
 	struct vmap_block *vb;
 	unsigned long addr = 0;
 	unsigned int order;
+	int cpu = 0;
 
 	BUG_ON(size & ~PAGE_MASK);
 	BUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);
@@ -974,7 +976,8 @@ static void *vb_alloc(unsigned long size, gfp_t gfp_mask)
 
 again:
 	rcu_read_lock();
-	vbq = &get_cpu_var(vmap_block_queue);
+	cpu = get_cpu_light();
+	vbq = &__get_cpu_var(vmap_block_queue);
 	list_for_each_entry_rcu(vb, &vbq->free, free_list) {
 		int i;
 
@@ -998,7 +1001,7 @@ next:
 		spin_unlock(&vb->lock);
 	}
 
-	put_cpu_var(vmap_block_queue);
+	put_cpu_light();
 	rcu_read_unlock();
 
 	if (!addr) {
diff --git a/kernel/msm-3.18/mm/vmstat.c b/kernel/msm-3.18/mm/vmstat.c
index 5435105db..f7bdd77bf 100644
--- a/kernel/msm-3.18/mm/vmstat.c
+++ b/kernel/msm-3.18/mm/vmstat.c
@@ -221,6 +221,7 @@ void __mod_zone_page_state(struct zone *zone, enum zone_stat_item item,
 	long x;
 	long t;
 
+	preempt_disable_rt();
 	x = delta + __this_cpu_read(*p);
 
 	t = __this_cpu_read(pcp->stat_threshold);
@@ -230,6 +231,7 @@ void __mod_zone_page_state(struct zone *zone, enum zone_stat_item item,
 		x = 0;
 	}
 	__this_cpu_write(*p, x);
+	preempt_enable_rt();
 }
 EXPORT_SYMBOL(__mod_zone_page_state);
 
@@ -262,6 +264,7 @@ void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
 	s8 __percpu *p = pcp->vm_stat_diff + item;
 	s8 v, t;
 
+	preempt_disable_rt();
 	v = __this_cpu_inc_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v > t)) {
@@ -270,6 +273,7 @@ void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
 		zone_page_state_add(v + overstep, zone, item);
 		__this_cpu_write(*p, -overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __inc_zone_page_state(struct page *page, enum zone_stat_item item)
@@ -284,6 +288,7 @@ void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 	s8 __percpu *p = pcp->vm_stat_diff + item;
 	s8 v, t;
 
+	preempt_disable_rt();
 	v = __this_cpu_dec_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v < - t)) {
@@ -292,6 +297,7 @@ void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 		zone_page_state_add(v - overstep, zone, item);
 		__this_cpu_write(*p, overstep);
 	}
+	preempt_enable_rt();
 }
 
 void __dec_zone_page_state(struct page *page, enum zone_stat_item item)
diff --git a/kernel/msm-3.18/mm/workingset.c b/kernel/msm-3.18/mm/workingset.c
index f7216fa7d..0decb9ca8 100644
--- a/kernel/msm-3.18/mm/workingset.c
+++ b/kernel/msm-3.18/mm/workingset.c
@@ -264,7 +264,8 @@ void workingset_activation(struct page *page)
  * point where they would still be useful.
  */
 
-struct list_lru workingset_shadow_nodes;
+struct list_lru __workingset_shadow_nodes;
+DEFINE_LOCAL_IRQ_LOCK(workingset_shadow_lock);
 
 static unsigned long count_shadow_nodes(struct shrinker *shrinker,
 					struct shrink_control *sc)
@@ -274,9 +275,9 @@ static unsigned long count_shadow_nodes(struct shrinker *shrinker,
 	unsigned long pages;
 
 	/* list_lru lock nests inside IRQ-safe mapping->tree_lock */
-	local_irq_disable();
-	shadow_nodes = list_lru_count_node(&workingset_shadow_nodes, sc->nid);
-	local_irq_enable();
+	local_lock_irq(workingset_shadow_lock);
+	shadow_nodes = list_lru_count_node(&__workingset_shadow_nodes, sc->nid);
+	local_unlock_irq(workingset_shadow_lock);
 
 	pages = node_present_pages(sc->nid);
 	/*
@@ -362,9 +363,9 @@ static enum lru_status shadow_lru_isolate(struct list_head *item,
 	spin_unlock(&mapping->tree_lock);
 	ret = LRU_REMOVED_RETRY;
 out:
-	local_irq_enable();
+	local_unlock_irq(workingset_shadow_lock);
 	cond_resched();
-	local_irq_disable();
+	local_lock_irq(workingset_shadow_lock);
 	spin_lock(lru_lock);
 	return ret;
 }
@@ -375,10 +376,10 @@ static unsigned long scan_shadow_nodes(struct shrinker *shrinker,
 	unsigned long ret;
 
 	/* list_lru lock nests inside IRQ-safe mapping->tree_lock */
-	local_irq_disable();
-	ret =  list_lru_walk_node(&workingset_shadow_nodes, sc->nid,
+	local_lock_irq(workingset_shadow_lock);
+	ret =  list_lru_walk_node(&__workingset_shadow_nodes, sc->nid,
 				  shadow_lru_isolate, NULL, &sc->nr_to_scan);
-	local_irq_enable();
+	local_unlock_irq(workingset_shadow_lock);
 	return ret;
 }
 
@@ -399,7 +400,7 @@ static int __init workingset_init(void)
 {
 	int ret;
 
-	ret = list_lru_init_key(&workingset_shadow_nodes, &shadow_nodes_key);
+	ret = list_lru_init_key(&__workingset_shadow_nodes, &shadow_nodes_key);
 	if (ret)
 		goto err;
 	ret = register_shrinker(&workingset_shadow_shrinker);
@@ -407,7 +408,7 @@ static int __init workingset_init(void)
 		goto err_list_lru;
 	return 0;
 err_list_lru:
-	list_lru_destroy(&workingset_shadow_nodes);
+	list_lru_destroy(&__workingset_shadow_nodes);
 err:
 	return ret;
 }
diff --git a/kernel/msm-3.18/mm/zsmalloc.c b/kernel/msm-3.18/mm/zsmalloc.c
index a8b5e749e..8b01618b5 100644
--- a/kernel/msm-3.18/mm/zsmalloc.c
+++ b/kernel/msm-3.18/mm/zsmalloc.c
@@ -68,6 +68,7 @@
 #include <linux/debugfs.h>
 #include <linux/zsmalloc.h>
 #include <linux/zpool.h>
+#include <linux/locallock.h>
 
 /*
  * This must be power of 2 and greater than of equal to sizeof(link_free).
@@ -393,6 +394,7 @@ static unsigned int get_maxobj_per_zspage(int size, int pages_per_zspage)
 
 /* per-cpu VM mapping areas for zspage accesses that cross page boundaries */
 static DEFINE_PER_CPU(struct mapping_area, zs_map_area);
+static DEFINE_LOCAL_IRQ_LOCK(zs_map_area_lock);
 
 static int is_first_page(struct page *page)
 {
@@ -1284,7 +1286,7 @@ void *zs_map_object(struct zs_pool *pool, unsigned long handle,
 	class = pool->size_class[class_idx];
 	off = obj_idx_to_offset(page, obj_idx, class->size);
 
-	area = &get_cpu_var(zs_map_area);
+	area = &get_locked_var(zs_map_area_lock, zs_map_area);
 	area->vm_mm = mm;
 	if (off + class->size <= PAGE_SIZE) {
 		/* this object is contained entirely within a page */
@@ -1337,7 +1339,7 @@ void zs_unmap_object(struct zs_pool *pool, unsigned long handle)
 
 		__zs_unmap_object(area, pages, off, class->size);
 	}
-	put_cpu_var(zs_map_area);
+	put_locked_var(zs_map_area_lock, zs_map_area);
 	unpin_tag(handle);
 }
 EXPORT_SYMBOL_GPL(zs_unmap_object);
diff --git a/kernel/msm-3.18/net/core/dev.c b/kernel/msm-3.18/net/core/dev.c
index e8194572d..5dd5aa88a 100644
--- a/kernel/msm-3.18/net/core/dev.c
+++ b/kernel/msm-3.18/net/core/dev.c
@@ -184,6 +184,7 @@ static unsigned int napi_gen_id;
 static DEFINE_HASHTABLE(napi_hash, 8);
 
 static seqcount_t devnet_rename_seq;
+static DEFINE_MUTEX(devnet_rename_mutex);
 
 static inline void dev_base_seq_inc(struct net *net)
 {
@@ -205,14 +206,14 @@ static inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)
 static inline void rps_lock(struct softnet_data *sd)
 {
 #ifdef CONFIG_RPS
-	spin_lock(&sd->input_pkt_queue.lock);
+	raw_spin_lock(&sd->input_pkt_queue.raw_lock);
 #endif
 }
 
 static inline void rps_unlock(struct softnet_data *sd)
 {
 #ifdef CONFIG_RPS
-	spin_unlock(&sd->input_pkt_queue.lock);
+	raw_spin_unlock(&sd->input_pkt_queue.raw_lock);
 #endif
 }
 
@@ -834,7 +835,8 @@ retry:
 	strcpy(name, dev->name);
 	rcu_read_unlock();
 	if (read_seqcount_retry(&devnet_rename_seq, seq)) {
-		cond_resched();
+		mutex_lock(&devnet_rename_mutex);
+		mutex_unlock(&devnet_rename_mutex);
 		goto retry;
 	}
 
@@ -1103,20 +1105,17 @@ int dev_change_name(struct net_device *dev, const char *newname)
 	if (dev->flags & IFF_UP)
 		return -EBUSY;
 
-	write_seqcount_begin(&devnet_rename_seq);
+	mutex_lock(&devnet_rename_mutex);
+	__raw_write_seqcount_begin(&devnet_rename_seq);
 
-	if (strncmp(newname, dev->name, IFNAMSIZ) == 0) {
-		write_seqcount_end(&devnet_rename_seq);
-		return 0;
-	}
+	if (strncmp(newname, dev->name, IFNAMSIZ) == 0)
+		goto outunlock;
 
 	memcpy(oldname, dev->name, IFNAMSIZ);
 
 	err = dev_get_valid_name(net, dev, newname);
-	if (err < 0) {
-		write_seqcount_end(&devnet_rename_seq);
-		return err;
-	}
+	if (err < 0)
+		goto outunlock;
 
 	if (oldname[0] && !strchr(oldname, '%'))
 		netdev_info(dev, "renamed from %s\n", oldname);
@@ -1129,11 +1128,12 @@ rollback:
 	if (ret) {
 		memcpy(dev->name, oldname, IFNAMSIZ);
 		dev->name_assign_type = old_assign_type;
-		write_seqcount_end(&devnet_rename_seq);
-		return ret;
+		err = ret;
+		goto outunlock;
 	}
 
-	write_seqcount_end(&devnet_rename_seq);
+	__raw_write_seqcount_end(&devnet_rename_seq);
+	mutex_unlock(&devnet_rename_mutex);
 
 	netdev_adjacent_rename_links(dev, oldname);
 
@@ -1154,7 +1154,8 @@ rollback:
 		/* err >= 0 after dev_alloc_name() or stores the first errno */
 		if (err >= 0) {
 			err = ret;
-			write_seqcount_begin(&devnet_rename_seq);
+			mutex_lock(&devnet_rename_mutex);
+			__raw_write_seqcount_begin(&devnet_rename_seq);
 			memcpy(dev->name, oldname, IFNAMSIZ);
 			memcpy(oldname, newname, IFNAMSIZ);
 			dev->name_assign_type = old_assign_type;
@@ -1167,6 +1168,11 @@ rollback:
 	}
 
 	return err;
+
+outunlock:
+	__raw_write_seqcount_end(&devnet_rename_seq);
+	mutex_unlock(&devnet_rename_mutex);
+	return err;
 }
 
 /**
@@ -2158,6 +2164,7 @@ static inline void __netif_reschedule(struct Qdisc *q)
 	sd->output_queue_tailp = &q->next_sched;
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 
 void __netif_schedule(struct Qdisc *q)
@@ -2239,6 +2246,7 @@ void __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason)
 	__this_cpu_write(softnet_data.completion_queue, skb);
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(__dev_kfree_skb_irq);
 
@@ -2853,7 +2861,11 @@ static inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,
 	 * This permits __QDISC___STATE_RUNNING owner to get the lock more
 	 * often and dequeue packets faster.
 	 */
+#ifdef CONFIG_PREEMPT_RT_FULL
+	contended = true;
+#else
 	contended = qdisc_is_running(q);
+#endif
 	if (unlikely(contended))
 		spin_lock(&q->busylock);
 
@@ -2913,9 +2925,44 @@ static void skb_update_prio(struct sk_buff *skb)
 #define skb_update_prio(skb)
 #endif
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+
+static inline int xmit_rec_read(void)
+{
+       return current->xmit_recursion;
+}
+
+static inline void xmit_rec_inc(void)
+{
+       current->xmit_recursion++;
+}
+
+static inline void xmit_rec_dec(void)
+{
+       current->xmit_recursion--;
+}
+
+#else
+
 DEFINE_PER_CPU(int, xmit_recursion);
 EXPORT_SYMBOL(xmit_recursion);
 
+static inline int xmit_rec_read(void)
+{
+	return __this_cpu_read(xmit_recursion);
+}
+
+static inline void xmit_rec_inc(void)
+{
+	__this_cpu_inc(xmit_recursion);
+}
+
+static inline int xmit_rec_dec(void)
+{
+	__this_cpu_dec(xmit_recursion);
+}
+#endif
+
 #define RECURSION_LIMIT 10
 
 /**
@@ -3019,7 +3066,7 @@ static int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv,
 
 		if (txq->xmit_lock_owner != cpu) {
 
-			if (__this_cpu_read(xmit_recursion) > RECURSION_LIMIT)
+			if (xmit_rec_read() > RECURSION_LIMIT)
 				goto recursion_alert;
 
 			skb = validate_xmit_skb(skb, dev);
@@ -3029,7 +3076,7 @@ static int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv,
 			HARD_TX_LOCK(dev, txq, cpu);
 
 			if (!netif_xmit_stopped(txq)) {
-				__this_cpu_inc(xmit_recursion);
+				xmit_rec_inc();
 				if (likely(!skb_list))
 					skb = dev_hard_start_xmit(skb, dev,
 								  txq, &rc);
@@ -3038,7 +3085,7 @@ static int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv,
 								       dev,
 								       txq,
 								       &rc);
-				__this_cpu_dec(xmit_recursion);
+				xmit_rec_dec();
 				if (dev_xmit_complete(rc)) {
 					HARD_TX_UNLOCK(dev, txq);
 					goto out;
@@ -3412,6 +3459,7 @@ drop:
 	rps_unlock(sd);
 
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 
 	atomic_long_inc(&skb->dev->rx_dropped);
 	kfree_skb(skb);
@@ -3433,7 +3481,7 @@ static int netif_rx_internal(struct sk_buff *skb)
 		struct rps_dev_flow voidflow, *rflow = &voidflow;
 		int cpu;
 
-		preempt_disable();
+		migrate_disable();
 		rcu_read_lock();
 
 		cpu = get_rps_cpu(skb->dev, skb, &rflow);
@@ -3443,13 +3491,13 @@ static int netif_rx_internal(struct sk_buff *skb)
 		ret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);
 
 		rcu_read_unlock();
-		preempt_enable();
+		migrate_enable();
 	} else
 #endif
 	{
 		unsigned int qtail;
-		ret = enqueue_to_backlog(skb, get_cpu(), &qtail);
-		put_cpu();
+		ret = enqueue_to_backlog(skb, get_cpu_light(), &qtail);
+		put_cpu_light();
 	}
 	return ret;
 }
@@ -3483,16 +3531,44 @@ int netif_rx_ni(struct sk_buff *skb)
 
 	trace_netif_rx_ni_entry(skb);
 
-	preempt_disable();
+	local_bh_disable();
 	err = netif_rx_internal(skb);
-	if (local_softirq_pending())
-		do_softirq();
-	preempt_enable();
+	local_bh_enable();
 
 	return err;
 }
 EXPORT_SYMBOL(netif_rx_ni);
 
+#ifdef CONFIG_PREEMPT_RT_FULL
+/*
+ * RT runs ksoftirqd as a real time thread and the root_lock is a
+ * "sleeping spinlock". If the trylock fails then we can go into an
+ * infinite loop when ksoftirqd preempted the task which actually
+ * holds the lock, because we requeue q and raise NET_TX softirq
+ * causing ksoftirqd to loop forever.
+ *
+ * It's safe to use spin_lock on RT here as softirqs run in thread
+ * context and cannot deadlock against the thread which is holding
+ * root_lock.
+ *
+ * On !RT the trylock might fail, but there we bail out from the
+ * softirq loop after 10 attempts which we can't do on RT. And the
+ * task holding root_lock cannot be preempted, so the only downside of
+ * that trylock is that we need 10 loops to decide that we should have
+ * given up in the first one :)
+ */
+static inline int take_root_lock(spinlock_t *lock)
+{
+	spin_lock(lock);
+	return 1;
+}
+#else
+static inline int take_root_lock(spinlock_t *lock)
+{
+	return spin_trylock(lock);
+}
+#endif
+
 static void net_tx_action(struct softirq_action *h)
 {
 	struct softnet_data *sd = this_cpu_ptr(&softnet_data);
@@ -3534,7 +3610,7 @@ static void net_tx_action(struct softirq_action *h)
 			head = head->next_sched;
 
 			root_lock = qdisc_lock(q);
-			if (spin_trylock(root_lock)) {
+			if (take_root_lock(root_lock)) {
 				smp_mb__before_atomic();
 				clear_bit(__QDISC_STATE_SCHED,
 					  &q->state);
@@ -3958,7 +4034,7 @@ static void flush_backlog(void *arg)
 	skb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {
 		if (skb->dev == dev) {
 			__skb_unlink(skb, &sd->input_pkt_queue);
-			kfree_skb(skb);
+			__skb_queue_tail(&sd->tofree_queue, skb);
 			input_queue_head_incr(sd);
 		}
 	}
@@ -3967,10 +4043,13 @@ static void flush_backlog(void *arg)
 	skb_queue_walk_safe(&sd->process_queue, skb, tmp) {
 		if (skb->dev == dev) {
 			__skb_unlink(skb, &sd->process_queue);
-			kfree_skb(skb);
+			__skb_queue_tail(&sd->tofree_queue, skb);
 			input_queue_head_incr(sd);
 		}
 	}
+
+	if (!skb_queue_empty(&sd->tofree_queue))
+		raise_softirq_irqoff(NET_RX_SOFTIRQ);
 }
 
 static int napi_gro_complete(struct sk_buff *skb)
@@ -4442,6 +4521,7 @@ static void net_rps_action_and_irq_enable(struct softnet_data *sd)
 	} else
 #endif
 		local_irq_enable();
+	preempt_check_resched_rt();
 }
 
 static int process_backlog(struct napi_struct *napi, int quota)
@@ -4517,6 +4597,7 @@ void __napi_schedule(struct napi_struct *n)
 	local_irq_save(flags);
 	____napi_schedule(this_cpu_ptr(&softnet_data), n);
 	local_irq_restore(flags);
+	preempt_check_resched_rt();
 }
 EXPORT_SYMBOL(__napi_schedule);
 
@@ -4650,10 +4731,17 @@ static void net_rx_action(struct softirq_action *h)
 	struct softnet_data *sd = this_cpu_ptr(&softnet_data);
 	unsigned long time_limit = jiffies + 2;
 	int budget = netdev_budget;
+	struct sk_buff *skb;
 	void *have;
 
 	local_irq_disable();
 
+	while ((skb = __skb_dequeue(&sd->tofree_queue))) {
+		local_irq_enable();
+		kfree_skb(skb);
+		local_irq_disable();
+	}
+
 	while (!list_empty(&sd->poll_list)) {
 		struct napi_struct *n;
 		int work, weight;
@@ -4729,7 +4817,7 @@ out:
 
 softnet_break:
 	sd->time_squeeze++;
-	__raise_softirq_irqoff(NET_RX_SOFTIRQ);
+	__raise_softirq_irqoff_ksoft(NET_RX_SOFTIRQ);
 	goto out;
 }
 
@@ -6904,7 +6992,7 @@ EXPORT_SYMBOL(free_netdev);
 void synchronize_net(void)
 {
 	might_sleep();
-	if (rtnl_is_locked())
+	if (rtnl_is_locked() && !IS_ENABLED(CONFIG_PREEMPT_RT_FULL))
 		synchronize_rcu_expedited();
 	else
 		synchronize_rcu();
@@ -7149,16 +7237,20 @@ static int dev_cpu_callback(struct notifier_block *nfb,
 
 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
 	local_irq_enable();
+	preempt_check_resched_rt();
 
 	/* Process offline CPU's input_pkt_queue */
 	while ((skb = __skb_dequeue(&oldsd->process_queue))) {
 		netif_rx_internal(skb);
 		input_queue_head_incr(oldsd);
 	}
-	while ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {
+	while ((skb = __skb_dequeue(&oldsd->input_pkt_queue))) {
 		netif_rx_internal(skb);
 		input_queue_head_incr(oldsd);
 	}
+	while ((skb = __skb_dequeue(&oldsd->tofree_queue))) {
+		kfree_skb(skb);
+	}
 
 	return NOTIFY_OK;
 }
@@ -7460,8 +7552,9 @@ static int __init net_dev_init(void)
 	for_each_possible_cpu(i) {
 		struct softnet_data *sd = &per_cpu(softnet_data, i);
 
-		skb_queue_head_init(&sd->input_pkt_queue);
-		skb_queue_head_init(&sd->process_queue);
+		skb_queue_head_init_raw(&sd->input_pkt_queue);
+		skb_queue_head_init_raw(&sd->process_queue);
+		skb_queue_head_init_raw(&sd->tofree_queue);
 		INIT_LIST_HEAD(&sd->poll_list);
 		sd->output_queue_tailp = &sd->output_queue;
 #ifdef CONFIG_RPS
diff --git a/kernel/msm-3.18/net/core/skbuff.c b/kernel/msm-3.18/net/core/skbuff.c
index 31d2aad99..cc45f3e9a 100644
--- a/kernel/msm-3.18/net/core/skbuff.c
+++ b/kernel/msm-3.18/net/core/skbuff.c
@@ -63,6 +63,7 @@
 #include <linux/errqueue.h>
 #include <linux/prefetch.h>
 #include <linux/if_vlan.h>
+#include <linux/locallock.h>
 
 #include <net/protocol.h>
 #include <net/dst.h>
@@ -358,6 +359,7 @@ struct netdev_alloc_cache {
 	unsigned int		pagecnt_bias;
 };
 static DEFINE_PER_CPU(struct netdev_alloc_cache, netdev_alloc_cache);
+static DEFINE_LOCAL_IRQ_LOCK(netdev_alloc_lock);
 
 static void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)
 {
@@ -369,7 +371,7 @@ static void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)
 	if (IS_ENABLED(CONFIG_FORCE_ALLOC_FROM_DMA_ZONE))
 		gfp_mask |= GFP_DMA;
 
-	local_irq_save(flags);
+	local_lock_irqsave(netdev_alloc_lock, flags);
 	nc = this_cpu_ptr(&netdev_alloc_cache);
 	if (unlikely(!nc->frag.page)) {
 refill:
@@ -415,7 +417,7 @@ refill:
 	nc->frag.offset += fragsz;
 	nc->pagecnt_bias--;
 end:
-	local_irq_restore(flags);
+	local_unlock_irqrestore(netdev_alloc_lock, flags);
 	return data;
 }
 
diff --git a/kernel/msm-3.18/net/core/sock.c b/kernel/msm-3.18/net/core/sock.c
index 774e91410..2042783a5 100644
--- a/kernel/msm-3.18/net/core/sock.c
+++ b/kernel/msm-3.18/net/core/sock.c
@@ -2358,12 +2358,11 @@ void lock_sock_nested(struct sock *sk, int subclass)
 	if (sk->sk_lock.owned)
 		__lock_sock(sk);
 	sk->sk_lock.owned = 1;
-	spin_unlock(&sk->sk_lock.slock);
+	spin_unlock_bh(&sk->sk_lock.slock);
 	/*
 	 * The sk_lock has mutex_lock() semantics here:
 	 */
 	mutex_acquire(&sk->sk_lock.dep_map, subclass, 0, _RET_IP_);
-	local_bh_enable();
 }
 EXPORT_SYMBOL(lock_sock_nested);
 
diff --git a/kernel/msm-3.18/net/ipv4/icmp.c b/kernel/msm-3.18/net/ipv4/icmp.c
index 7775a4760..4e8ad1580 100644
--- a/kernel/msm-3.18/net/ipv4/icmp.c
+++ b/kernel/msm-3.18/net/ipv4/icmp.c
@@ -69,6 +69,7 @@
 #include <linux/jiffies.h>
 #include <linux/kernel.h>
 #include <linux/fcntl.h>
+#include <linux/sysrq.h>
 #include <linux/socket.h>
 #include <linux/in.h>
 #include <linux/inet.h>
@@ -77,6 +78,7 @@
 #include <linux/string.h>
 #include <linux/netfilter_ipv4.h>
 #include <linux/slab.h>
+#include <linux/locallock.h>
 #include <net/snmp.h>
 #include <net/ip.h>
 #include <net/route.h>
@@ -203,6 +205,8 @@ static const struct icmp_control icmp_pointers[NR_ICMP_TYPES+1];
  *
  *	On SMP we have one ICMP socket per-cpu.
  */
+static DEFINE_LOCAL_IRQ_LOCK(icmp_sk_lock);
+
 static struct sock *icmp_sk(struct net *net)
 {
 	return net->ipv4.icmp_sk[smp_processor_id()];
@@ -214,12 +218,14 @@ static inline struct sock *icmp_xmit_lock(struct net *net)
 
 	local_bh_disable();
 
+	local_lock(icmp_sk_lock);
 	sk = icmp_sk(net);
 
 	if (unlikely(!spin_trylock(&sk->sk_lock.slock))) {
 		/* This can happen if the output path signals a
 		 * dst_link_failure() for an outgoing ICMP packet.
 		 */
+		local_unlock(icmp_sk_lock);
 		local_bh_enable();
 		return NULL;
 	}
@@ -229,6 +235,7 @@ static inline struct sock *icmp_xmit_lock(struct net *net)
 static inline void icmp_xmit_unlock(struct sock *sk)
 {
 	spin_unlock_bh(&sk->sk_lock.slock);
+	local_unlock(icmp_sk_lock);
 }
 
 int sysctl_icmp_msgs_per_sec __read_mostly = 1000;
@@ -356,6 +363,7 @@ static void icmp_push_reply(struct icmp_bxm *icmp_param,
 	struct sock *sk;
 	struct sk_buff *skb;
 
+	local_lock(icmp_sk_lock);
 	sk = icmp_sk(dev_net((*rt)->dst.dev));
 	if (ip_append_data(sk, fl4, icmp_glue_bits, icmp_param,
 			   icmp_param->data_len+icmp_param->head_len,
@@ -378,6 +386,7 @@ static void icmp_push_reply(struct icmp_bxm *icmp_param,
 		skb->ip_summed = CHECKSUM_NONE;
 		ip_push_pending_frames(sk, fl4);
 	}
+	local_unlock(icmp_sk_lock);
 }
 
 /*
@@ -865,6 +874,30 @@ static void icmp_redirect(struct sk_buff *skb)
 	icmp_socket_deliver(skb, icmp_hdr(skb)->un.gateway);
 }
 
+/*
+ * 32bit and 64bit have different timestamp length, so we check for
+ * the cookie at offset 20 and verify it is repeated at offset 50
+ */
+#define CO_POS0		20
+#define CO_POS1		50
+#define CO_SIZE		sizeof(int)
+#define ICMP_SYSRQ_SIZE	57
+
+/*
+ * We got a ICMP_SYSRQ_SIZE sized ping request. Check for the cookie
+ * pattern and if it matches send the next byte as a trigger to sysrq.
+ */
+static void icmp_check_sysrq(struct net *net, struct sk_buff *skb)
+{
+	int cookie = htonl(net->ipv4.sysctl_icmp_echo_sysrq);
+	char *p = skb->data;
+
+	if (!memcmp(&cookie, p + CO_POS0, CO_SIZE) &&
+	    !memcmp(&cookie, p + CO_POS1, CO_SIZE) &&
+	    p[CO_POS0 + CO_SIZE] == p[CO_POS1 + CO_SIZE])
+		handle_sysrq(p[CO_POS0 + CO_SIZE]);
+}
+
 /*
  *	Handle ICMP_ECHO ("ping") requests.
  *
@@ -892,6 +925,11 @@ static void icmp_echo(struct sk_buff *skb)
 		icmp_param.data_len	   = skb->len;
 		icmp_param.head_len	   = sizeof(struct icmphdr);
 		icmp_reply(&icmp_param, skb);
+
+		if (skb->len == ICMP_SYSRQ_SIZE &&
+		    net->ipv4.sysctl_icmp_echo_sysrq) {
+			icmp_check_sysrq(net, skb);
+		}
 	}
 }
 
diff --git a/kernel/msm-3.18/net/ipv4/sysctl_net_ipv4.c b/kernel/msm-3.18/net/ipv4/sysctl_net_ipv4.c
index 03b66b12d..30a4bbbfe 100644
--- a/kernel/msm-3.18/net/ipv4/sysctl_net_ipv4.c
+++ b/kernel/msm-3.18/net/ipv4/sysctl_net_ipv4.c
@@ -823,6 +823,13 @@ static struct ctl_table ipv4_net_table[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec
 	},
+	{
+		.procname	= "icmp_echo_sysrq",
+		.data		= &init_net.ipv4.sysctl_icmp_echo_sysrq,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec
+	},
 	{
 		.procname	= "icmp_ignore_bogus_error_responses",
 		.data		= &init_net.ipv4.sysctl_icmp_ignore_bogus_error_responses,
diff --git a/kernel/msm-3.18/net/ipv4/tcp_ipv4.c b/kernel/msm-3.18/net/ipv4/tcp_ipv4.c
index d9a4ef4cd..22366a941 100644
--- a/kernel/msm-3.18/net/ipv4/tcp_ipv4.c
+++ b/kernel/msm-3.18/net/ipv4/tcp_ipv4.c
@@ -62,6 +62,7 @@
 #include <linux/init.h>
 #include <linux/times.h>
 #include <linux/slab.h>
+#include <linux/locallock.h>
 
 #include <net/net_namespace.h>
 #include <net/icmp.h>
@@ -566,6 +567,7 @@ void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb)
 }
 EXPORT_SYMBOL(tcp_v4_send_check);
 
+static DEFINE_LOCAL_IRQ_LOCK(tcp_sk_lock);
 /*
  *	This routine will send an RST to the other tcp.
  *
@@ -688,10 +690,12 @@ static void tcp_v4_send_reset(struct sock *sk, struct sk_buff *skb)
 
 	arg.tos = ip_hdr(skb)->tos;
 	arg.uid = sock_net_uid(net, sk && sk_fullsock(sk) ? sk : NULL);
+	local_lock(tcp_sk_lock);
 	ip_send_unicast_reply(*this_cpu_ptr(net->ipv4.tcp_sk),
 			      skb, &TCP_SKB_CB(skb)->header.h4.opt,
 			      ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,
 			      &arg, arg.iov[0].iov_len);
+	local_unlock(tcp_sk_lock);
 
 	TCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);
 	TCP_INC_STATS_BH(net, TCP_MIB_OUTRSTS);
@@ -775,10 +779,12 @@ static void tcp_v4_send_ack(const struct sock *sk, struct sk_buff *skb,
 		arg.bound_dev_if = oif;
 	arg.tos = tos;
 	arg.uid = sock_net_uid(net, sk_fullsock(sk) ? sk : NULL);
+	local_lock(tcp_sk_lock);
 	ip_send_unicast_reply(*this_cpu_ptr(net->ipv4.tcp_sk),
 			      skb, &TCP_SKB_CB(skb)->header.h4.opt,
 			      ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,
 			      &arg, arg.iov[0].iov_len);
+	local_unlock(tcp_sk_lock);
 
 	TCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);
 }
diff --git a/kernel/msm-3.18/net/mac80211/rx.c b/kernel/msm-3.18/net/mac80211/rx.c
index ea3b13987..d206434a7 100644
--- a/kernel/msm-3.18/net/mac80211/rx.c
+++ b/kernel/msm-3.18/net/mac80211/rx.c
@@ -3373,7 +3373,7 @@ void ieee80211_rx(struct ieee80211_hw *hw, struct sk_buff *skb)
 	struct ieee80211_supported_band *sband;
 	struct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);
 
-	WARN_ON_ONCE(softirq_count() == 0);
+	WARN_ON_ONCE_NONRT(softirq_count() == 0);
 
 	if (WARN_ON(status->band >= IEEE80211_NUM_BANDS))
 		goto drop;
diff --git a/kernel/msm-3.18/net/netfilter/core.c b/kernel/msm-3.18/net/netfilter/core.c
index d047a6679..942479a61 100644
--- a/kernel/msm-3.18/net/netfilter/core.c
+++ b/kernel/msm-3.18/net/netfilter/core.c
@@ -21,11 +21,17 @@
 #include <linux/proc_fs.h>
 #include <linux/mutex.h>
 #include <linux/slab.h>
+#include <linux/locallock.h>
 #include <net/net_namespace.h>
 #include <net/sock.h>
 
 #include "nf_internals.h"
 
+#ifdef CONFIG_PREEMPT_RT_BASE
+DEFINE_LOCAL_IRQ_LOCK(xt_write_lock);
+EXPORT_PER_CPU_SYMBOL(xt_write_lock);
+#endif
+
 static DEFINE_MUTEX(afinfo_mutex);
 
 const struct nf_afinfo __rcu *nf_afinfo[NFPROTO_NUMPROTO] __read_mostly;
diff --git a/kernel/msm-3.18/net/packet/af_packet.c b/kernel/msm-3.18/net/packet/af_packet.c
index 3878c34b5..a74d14cde 100644
--- a/kernel/msm-3.18/net/packet/af_packet.c
+++ b/kernel/msm-3.18/net/packet/af_packet.c
@@ -63,6 +63,7 @@
 #include <linux/if_packet.h>
 #include <linux/wireless.h>
 #include <linux/kernel.h>
+#include <linux/delay.h>
 #include <linux/kmod.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
@@ -691,7 +692,7 @@ static void prb_retire_rx_blk_timer_expired(unsigned long data)
 	if (BLOCK_NUM_PKTS(pbd)) {
 		while (atomic_read(&pkc->blk_fill_in_prog)) {
 			/* Waiting for skb_copy_bits to finish... */
-			cpu_relax();
+			cpu_chill();
 		}
 	}
 
@@ -942,7 +943,7 @@ static void prb_retire_current_block(struct tpacket_kbdq_core *pkc,
 		if (!(status & TP_STATUS_BLK_TMO)) {
 			while (atomic_read(&pkc->blk_fill_in_prog)) {
 				/* Waiting for skb_copy_bits to finish... */
-				cpu_relax();
+				cpu_chill();
 			}
 		}
 		prb_close_block(pkc, pbd, po, status);
diff --git a/kernel/msm-3.18/net/rds/ib_rdma.c b/kernel/msm-3.18/net/rds/ib_rdma.c
index 657ba9f5d..c8faaf364 100644
--- a/kernel/msm-3.18/net/rds/ib_rdma.c
+++ b/kernel/msm-3.18/net/rds/ib_rdma.c
@@ -34,6 +34,7 @@
 #include <linux/slab.h>
 #include <linux/rculist.h>
 #include <linux/llist.h>
+#include <linux/delay.h>
 
 #include "rds.h"
 #include "ib.h"
@@ -286,7 +287,7 @@ static inline void wait_clean_list_grace(void)
 	for_each_online_cpu(cpu) {
 		flag = &per_cpu(clean_list_grace, cpu);
 		while (test_bit(CLEAN_LIST_BUSY_BIT, flag))
-			cpu_relax();
+			cpu_chill();
 	}
 }
 
diff --git a/kernel/msm-3.18/net/sched/sch_generic.c b/kernel/msm-3.18/net/sched/sch_generic.c
index 62a9bc7bf..2f205c9f5 100644
--- a/kernel/msm-3.18/net/sched/sch_generic.c
+++ b/kernel/msm-3.18/net/sched/sch_generic.c
@@ -904,7 +904,7 @@ void dev_deactivate_many(struct list_head *head)
 	/* Wait for outstanding qdisc_run calls. */
 	list_for_each_entry(dev, head, close_list)
 		while (some_qdisc_is_busy(dev))
-			yield();
+			msleep(1);
 }
 
 void dev_deactivate(struct net_device *dev)
diff --git a/kernel/msm-3.18/net/sunrpc/svc_xprt.c b/kernel/msm-3.18/net/sunrpc/svc_xprt.c
index c179ca2a5..ac93e74d5 100644
--- a/kernel/msm-3.18/net/sunrpc/svc_xprt.c
+++ b/kernel/msm-3.18/net/sunrpc/svc_xprt.c
@@ -357,7 +357,7 @@ static void svc_xprt_do_enqueue(struct svc_xprt *xprt)
 		return;
 	}
 
-	cpu = get_cpu();
+	cpu = get_cpu_light();
 	pool = svc_pool_for_cpu(xprt->xpt_server, cpu);
 	spin_lock_bh(&pool->sp_lock);
 
@@ -390,7 +390,7 @@ static void svc_xprt_do_enqueue(struct svc_xprt *xprt)
 	}
 
 	spin_unlock_bh(&pool->sp_lock);
-	put_cpu();
+	put_cpu_light();
 }
 
 /*
diff --git a/kernel/msm-3.18/scripts/mkcompile_h b/kernel/msm-3.18/scripts/mkcompile_h
index 6fdc97ef6..523e0420d 100755
--- a/kernel/msm-3.18/scripts/mkcompile_h
+++ b/kernel/msm-3.18/scripts/mkcompile_h
@@ -4,7 +4,8 @@ TARGET=$1
 ARCH=$2
 SMP=$3
 PREEMPT=$4
-CC=$5
+RT=$5
+CC=$6
 
 vecho() { [ "${quiet}" = "silent_" ] || echo "$@" ; }
 
@@ -57,6 +58,7 @@ UTS_VERSION="#$VERSION"
 CONFIG_FLAGS=""
 if [ -n "$SMP" ] ; then CONFIG_FLAGS="SMP"; fi
 if [ -n "$PREEMPT" ] ; then CONFIG_FLAGS="$CONFIG_FLAGS PREEMPT"; fi
+if [ -n "$RT" ] ; then CONFIG_FLAGS="$CONFIG_FLAGS RT"; fi
 UTS_VERSION="$UTS_VERSION $CONFIG_FLAGS $TIMESTAMP"
 
 # Truncate to maximum length
diff --git a/kernel/msm-3.18/sound/core/pcm_native.c b/kernel/msm-3.18/sound/core/pcm_native.c
index 61b7126db..2d9d25876 100644
--- a/kernel/msm-3.18/sound/core/pcm_native.c
+++ b/kernel/msm-3.18/sound/core/pcm_native.c
@@ -139,7 +139,7 @@ EXPORT_SYMBOL_GPL(snd_pcm_stream_unlock);
 void snd_pcm_stream_lock_irq(struct snd_pcm_substream *substream)
 {
 	if (!substream->pcm->nonatomic)
-		local_irq_disable();
+		local_irq_disable_nort();
 	snd_pcm_stream_lock(substream);
 }
 EXPORT_SYMBOL_GPL(snd_pcm_stream_lock_irq);
@@ -154,7 +154,7 @@ void snd_pcm_stream_unlock_irq(struct snd_pcm_substream *substream)
 {
 	snd_pcm_stream_unlock(substream);
 	if (!substream->pcm->nonatomic)
-		local_irq_enable();
+		local_irq_enable_nort();
 }
 EXPORT_SYMBOL_GPL(snd_pcm_stream_unlock_irq);
 
@@ -162,7 +162,7 @@ unsigned long _snd_pcm_stream_lock_irqsave(struct snd_pcm_substream *substream)
 {
 	unsigned long flags = 0;
 	if (!substream->pcm->nonatomic)
-		local_irq_save(flags);
+		local_irq_save_nort(flags);
 	snd_pcm_stream_lock(substream);
 	return flags;
 }
@@ -180,7 +180,7 @@ void snd_pcm_stream_unlock_irqrestore(struct snd_pcm_substream *substream,
 {
 	snd_pcm_stream_unlock(substream);
 	if (!substream->pcm->nonatomic)
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 }
 EXPORT_SYMBOL_GPL(snd_pcm_stream_unlock_irqrestore);
 
diff --git a/kernel/msm-3.18/virt/kvm/async_pf.c b/kernel/msm-3.18/virt/kvm/async_pf.c
index e06785470..33fb81095 100644
--- a/kernel/msm-3.18/virt/kvm/async_pf.c
+++ b/kernel/msm-3.18/virt/kvm/async_pf.c
@@ -94,8 +94,8 @@ static void async_pf_execute(struct work_struct *work)
 
 	trace_kvm_async_pf_completed(addr, gva);
 
-	if (waitqueue_active(&vcpu->wq))
-		wake_up_interruptible(&vcpu->wq);
+	if (swaitqueue_active(&vcpu->wq))
+		swait_wake_interruptible(&vcpu->wq);
 
 	mmput(mm);
 	kvm_put_kvm(vcpu->kvm);
diff --git a/kernel/msm-3.18/virt/kvm/kvm_main.c b/kernel/msm-3.18/virt/kvm/kvm_main.c
index 3d5ae6f65..56e12853c 100644
--- a/kernel/msm-3.18/virt/kvm/kvm_main.c
+++ b/kernel/msm-3.18/virt/kvm/kvm_main.c
@@ -223,7 +223,7 @@ int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
 	vcpu->kvm = kvm;
 	vcpu->vcpu_id = id;
 	vcpu->pid = NULL;
-	init_waitqueue_head(&vcpu->wq);
+	init_swait_head(&vcpu->wq);
 	kvm_async_pf_vcpu_init(vcpu);
 
 	page = alloc_page(GFP_KERNEL | __GFP_ZERO);
@@ -1747,10 +1747,10 @@ EXPORT_SYMBOL_GPL(mark_page_dirty);
  */
 void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
-	DEFINE_WAIT(wait);
+	DEFINE_SWAITER(wait);
 
 	for (;;) {
-		prepare_to_wait(&vcpu->wq, &wait, TASK_INTERRUPTIBLE);
+		swait_prepare(&vcpu->wq, &wait, TASK_INTERRUPTIBLE);
 
 		if (kvm_arch_vcpu_runnable(vcpu)) {
 			kvm_make_request(KVM_REQ_UNHALT, vcpu);
@@ -1764,7 +1764,7 @@ void kvm_vcpu_block(struct kvm_vcpu *vcpu)
 		schedule();
 	}
 
-	finish_wait(&vcpu->wq, &wait);
+	swait_finish(&vcpu->wq, &wait);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_block);
 
@@ -1776,11 +1776,11 @@ void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
 {
 	int me;
 	int cpu = vcpu->cpu;
-	wait_queue_head_t *wqp;
+	struct swait_head *wqp;
 
 	wqp = kvm_arch_vcpu_wq(vcpu);
-	if (waitqueue_active(wqp)) {
-		wake_up_interruptible(wqp);
+	if (swaitqueue_active(wqp)) {
+		swait_wake_interruptible(wqp);
 		++vcpu->stat.halt_wakeup;
 	}
 
@@ -1885,7 +1885,7 @@ void kvm_vcpu_on_spin(struct kvm_vcpu *me)
 				continue;
 			if (vcpu == me)
 				continue;
-			if (waitqueue_active(&vcpu->wq) && !kvm_arch_vcpu_runnable(vcpu))
+			if (swaitqueue_active(&vcpu->wq) && !kvm_arch_vcpu_runnable(vcpu))
 				continue;
 			if (!kvm_vcpu_eligible_for_directed_yield(vcpu))
 				continue;
-- 
2.49.0


From e1f04fd114c84bebad9b01b144f3a6ba921a6162 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Tue, 8 Jan 2019 15:31:01 -0800
Subject: [PATCH 02/20] patch touchup

---
 kernel/msm-3.18/drivers/clk/msm/clock-rpm.c   |     2 +-
 kernel/msm-3.18/drivers/clk/msm/clock-voter.c |     2 +-
 .../drivers/gpu/msm/adreno_dispatch.c         |     2 +
 .../drivers/pinctrl/qcom/pinctrl-msm.c        |     4 +-
 .../msm-3.18/drivers/soc/qcom/perf_event_l2.c |     2 +-
 .../android/ion/ion_system_secure_heap.c      |    24 +-
 kernel/msm-3.18/drivers/usb/gadget/debug.c    |     2 +-
 kernel/msm-3.18/drivers/usb/phy/phy-msm-usb.c |     2 +-
 kernel/msm-3.18/include/linux/preempt.h       |    16 +-
 kernel/msm-3.18/include/linux/rbtree.h        |    16 +-
 .../msm-3.18/include/linux/rbtree_augmented.h |    21 +-
 .../msm-3.18/include/linux/rwlock_types_rt.h  |     3 +-
 kernel/msm-3.18/include/linux/sched.h         |     7 -
 kernel/msm-3.18/include/linux/timer.h         |    14 +-
 kernel/msm-3.18/include/sound/q6asm-v2.h      |     6 +-
 kernel/msm-3.18/kernel/sched/core.c           |     1 +
 kernel/msm-3.18/kernel/time/tick-sched.c      |    10 +-
 kernel/msm-3.18/kernel/time/timer.c           |   164 +-
 kernel/msm-3.18/kernel/time/timer.c.rej       |    68 +
 kernel/msm-3.18/lib/rbtree.c                  |    76 +-
 kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6asm.c |   154 +-
 kernel/patch-3.18.69-rt75.patch               | 27818 ++++++++++++++++
 22 files changed, 28065 insertions(+), 349 deletions(-)
 create mode 100644 kernel/msm-3.18/kernel/time/timer.c.rej
 create mode 100644 kernel/patch-3.18.69-rt75.patch

diff --git a/kernel/msm-3.18/drivers/clk/msm/clock-rpm.c b/kernel/msm-3.18/drivers/clk/msm/clock-rpm.c
index ac093463f..b5ee0edd6 100644
--- a/kernel/msm-3.18/drivers/clk/msm/clock-rpm.c
+++ b/kernel/msm-3.18/drivers/clk/msm/clock-rpm.c
@@ -14,7 +14,7 @@
 #define pr_fmt(fmt) "%s: " fmt, __func__
 
 #include <linux/err.h>
-#include <linux/rtmutex.h>
+#include <linux/spinlock_types.h>
 #include <linux/clk/msm-clk-provider.h>
 #include <soc/qcom/clock-rpm.h>
 #include <soc/qcom/msm-clock-controller.h>
diff --git a/kernel/msm-3.18/drivers/clk/msm/clock-voter.c b/kernel/msm-3.18/drivers/clk/msm/clock-voter.c
index 0d609743f..8272ab3f4 100644
--- a/kernel/msm-3.18/drivers/clk/msm/clock-voter.c
+++ b/kernel/msm-3.18/drivers/clk/msm/clock-voter.c
@@ -13,7 +13,7 @@
 #define pr_fmt(fmt) "%s: " fmt, __func__
 
 #include <linux/err.h>
-#include <linux/rtmutex.h>
+#include <linux/spinlock_types.h>
 #include <linux/clk.h>
 #include <linux/clk/msm-clk-provider.h>
 #include <soc/qcom/clock-voter.h>
diff --git a/kernel/msm-3.18/drivers/gpu/msm/adreno_dispatch.c b/kernel/msm-3.18/drivers/gpu/msm/adreno_dispatch.c
index ab1aaee51..0f1c3d1a7 100644
--- a/kernel/msm-3.18/drivers/gpu/msm/adreno_dispatch.c
+++ b/kernel/msm-3.18/drivers/gpu/msm/adreno_dispatch.c
@@ -2512,9 +2512,11 @@ int adreno_dispatcher_idle(struct adreno_device *adreno_dev)
 	 * Ensure that this function is not called when dispatcher
 	 * mutex is held and device is started
 	 */
+#if !defined(CONFIG_PREEMPT_RT_BASE)
 	if (mutex_is_locked(&dispatcher->mutex) &&
 		dispatcher->mutex.owner == current)
 		BUG_ON(1);
+#endif
 
 	adreno_get_gpu_halt(adreno_dev);
 
diff --git a/kernel/msm-3.18/drivers/pinctrl/qcom/pinctrl-msm.c b/kernel/msm-3.18/drivers/pinctrl/qcom/pinctrl-msm.c
index a414fe4ca..7e7e96239 100644
--- a/kernel/msm-3.18/drivers/pinctrl/qcom/pinctrl-msm.c
+++ b/kernel/msm-3.18/drivers/pinctrl/qcom/pinctrl-msm.c
@@ -946,7 +946,7 @@ static void msm_pinctrl_resume(void)
 	if (!msm_show_resume_irq_mask)
 		return;
 
-	spin_lock_irqsave(&pctrl->lock, flags);
+	raw_spin_lock_irqsave(&pctrl->lock, flags);
 	for_each_set_bit(i, pctrl->enabled_irqs, pctrl->chip.ngpio) {
 		g = &pctrl->soc->groups[i];
 		val = readl_relaxed(pctrl->regs + g->intr_status_reg);
@@ -961,7 +961,7 @@ static void msm_pinctrl_resume(void)
 			pr_warn("%s: %d triggered %s\n", __func__, irq, name);
 		}
 	}
-	spin_unlock_irqrestore(&pctrl->lock, flags);
+	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
 }
 #else
 #define msm_pinctrl_suspend NULL
diff --git a/kernel/msm-3.18/drivers/soc/qcom/perf_event_l2.c b/kernel/msm-3.18/drivers/soc/qcom/perf_event_l2.c
index 8d813dc99..914fd9275 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/perf_event_l2.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/perf_event_l2.c
@@ -914,7 +914,7 @@ static int l2_cache_pmu_probe(struct platform_device *pdev)
 		}
 
 		slice->cluster = affinity_cpu >> 1;
-		slice->pmu_lock = __SPIN_LOCK_UNLOCKED(slice->pmu_lock);
+		spin_lock_init(&slice->pmu_lock);
 
 		hml2_pmu__init(slice);
 		list_add(&slice->entry, &l2cache_pmu.pmus);
diff --git a/kernel/msm-3.18/drivers/staging/android/ion/ion_system_secure_heap.c b/kernel/msm-3.18/drivers/staging/android/ion/ion_system_secure_heap.c
index 3e55753b9..9b1262d63 100644
--- a/kernel/msm-3.18/drivers/staging/android/ion/ion_system_secure_heap.c
+++ b/kernel/msm-3.18/drivers/staging/android/ion/ion_system_secure_heap.c
@@ -25,7 +25,7 @@ struct ion_system_secure_heap {
 	struct ion_heap *sys_heap;
 	struct ion_heap heap;
 
-	spinlock_t work_lock;
+	raw_spinlock_t work_lock;
 	bool destroy_heap;
 	struct list_head prefetch_list;
 	struct work_struct prefetch_work;
@@ -139,11 +139,11 @@ static void ion_system_secure_heap_prefetch_work(struct work_struct *work)
 	if (!buffer)
 		return;
 
-	spin_lock_irqsave(&secure_heap->work_lock, flags);
+	raw_spin_lock_irqsave(&secure_heap->work_lock, flags);
 	list_for_each_entry_safe(info, tmp,
 				&secure_heap->prefetch_list, list) {
 		list_del(&info->list);
-		spin_unlock_irqrestore(&secure_heap->work_lock, flags);
+		raw_spin_unlock_irqrestore(&secure_heap->work_lock, flags);
 		size = info->size;
 		vmid_flags = info->vmid;
 		kfree(info);
@@ -158,14 +158,14 @@ static void ion_system_secure_heap_prefetch_work(struct work_struct *work)
 			pr_debug("%s: Failed to get %zx allocation for %s, ret = %d\n",
 				__func__, info->size, secure_heap->heap.name,
 				ret);
-			spin_lock_irqsave(&secure_heap->work_lock, flags);
+			raw_spin_lock_irqsave(&secure_heap->work_lock, flags);
 			continue;
 		}
 
 		ion_system_secure_heap_free(buffer);
-		spin_lock_irqsave(&secure_heap->work_lock, flags);
+		raw_spin_lock_irqsave(&secure_heap->work_lock, flags);
 	}
-	spin_unlock_irqrestore(&secure_heap->work_lock, flags);
+	raw_spin_unlock_irqrestore(&secure_heap->work_lock, flags);
 	kfree(buffer);
 }
 
@@ -226,14 +226,14 @@ int ion_system_secure_heap_prefetch(struct ion_heap *heap, void *ptr)
 			goto out_free;
 	}
 
-	spin_lock_irqsave(&secure_heap->work_lock, flags);
+	raw_spin_lock_irqsave(&secure_heap->work_lock, flags);
 	if (secure_heap->destroy_heap) {
-		spin_unlock_irqrestore(&secure_heap->work_lock, flags);
+		raw_spin_unlock_irqrestore(&secure_heap->work_lock, flags);
 		goto out_free;
 	}
 	list_splice_init(&items, &secure_heap->prefetch_list);
 	schedule_work(&secure_heap->prefetch_work);
-	spin_unlock_irqrestore(&secure_heap->work_lock, flags);
+	raw_spin_unlock_irqrestore(&secure_heap->work_lock, flags);
 
 	return 0;
 
@@ -323,7 +323,7 @@ struct ion_heap *ion_system_secure_heap_create(struct ion_platform_heap *unused)
 	heap->sys_heap = get_ion_heap(ION_SYSTEM_HEAP_ID);
 
 	heap->destroy_heap = false;
-	heap->work_lock = __SPIN_LOCK_UNLOCKED(heap->work_lock);
+	raw_spin_lock_init(&heap->work_lock);
 	INIT_LIST_HEAD(&heap->prefetch_list);
 	INIT_WORK(&heap->prefetch_work, ion_system_secure_heap_prefetch_work);
 	return &heap->heap;
@@ -339,10 +339,10 @@ void ion_system_secure_heap_destroy(struct ion_heap *heap)
 	struct prefetch_info *info, *tmp;
 
 	/* Stop any pending/future work */
-	spin_lock_irqsave(&secure_heap->work_lock, flags);
+	raw_spin_lock_irqsave(&secure_heap->work_lock, flags);
 	secure_heap->destroy_heap = true;
 	list_splice_init(&secure_heap->prefetch_list, &items);
-	spin_unlock_irqrestore(&secure_heap->work_lock, flags);
+	raw_spin_unlock_irqrestore(&secure_heap->work_lock, flags);
 
 	cancel_work_sync(&secure_heap->prefetch_work);
 
diff --git a/kernel/msm-3.18/drivers/usb/gadget/debug.c b/kernel/msm-3.18/drivers/usb/gadget/debug.c
index 32a532994..9e6bb79d2 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/debug.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/debug.c
@@ -15,7 +15,7 @@
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/sched.h>
-#include <linux/rwlock.h>
+#include <linux/spinlock.h>
 #include <linux/debugfs.h>
 
 #include "debug.h"
diff --git a/kernel/msm-3.18/drivers/usb/phy/phy-msm-usb.c b/kernel/msm-3.18/drivers/usb/phy/phy-msm-usb.c
index 55e83e1c8..0c6bc9522 100644
--- a/kernel/msm-3.18/drivers/usb/phy/phy-msm-usb.c
+++ b/kernel/msm-3.18/drivers/usb/phy/phy-msm-usb.c
@@ -4506,7 +4506,7 @@ static int msm_otg_probe(struct platform_device *pdev)
 	phy->dev = &pdev->dev;
 	motg->pdev = pdev;
 	motg->dbg_idx = 0;
-	motg->dbg_lock = __RW_LOCK_UNLOCKED(lck);
+  rwlock_init(&motg->dbg_lock);
 
 	if (motg->pdata->bus_scale_table) {
 		motg->bus_perf_client =
diff --git a/kernel/msm-3.18/include/linux/preempt.h b/kernel/msm-3.18/include/linux/preempt.h
index 55d327eae..e215a869f 100644
--- a/kernel/msm-3.18/include/linux/preempt.h
+++ b/kernel/msm-3.18/include/linux/preempt.h
@@ -69,7 +69,6 @@
 #define PREEMPT_ACTIVE	(__IRQ_MASK(PREEMPT_ACTIVE_BITS) << PREEMPT_ACTIVE_SHIFT)
 
 #define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
-#define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
 #define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK \
 				 | NMI_MASK))
 #ifndef CONFIG_PREEMPT_RT_FULL
@@ -96,6 +95,12 @@ extern int in_serving_softirq(void);
  */
 #define in_nmi()	(preempt_count() & NMI_MASK)
 
+#if defined(CONFIG_PREEMPT_COUNT)
+# define PREEMPT_CHECK_OFFSET 1
+#else
+# define PREEMPT_CHECK_OFFSET 0
+#endif
+
 /*
  * The preempt_count offset after preempt_disable();
  */
@@ -105,10 +110,11 @@ extern int in_serving_softirq(void);
 # define PREEMPT_DISABLE_OFFSET	0
 #endif
 
-/*
- * The preempt_count offset after spin_lock()
- */
-#define PREEMPT_LOCK_OFFSET	PREEMPT_DISABLE_OFFSET
+#if defined(CONFIG_PREEMPT_COUNT) && !defined(CONFIG_PREEMPT_RT_FULL)
+#define PREEMPT_LOCK_OFFSET	PREEMPT_OFFSET
+#else
+#define PREEMPT_LOCK_OFFSET	0
+#endif
 
 /*
  * The preempt_count offset needed for things like:
diff --git a/kernel/msm-3.18/include/linux/rbtree.h b/kernel/msm-3.18/include/linux/rbtree.h
index 829c5a8b4..57e75ae99 100644
--- a/kernel/msm-3.18/include/linux/rbtree.h
+++ b/kernel/msm-3.18/include/linux/rbtree.h
@@ -31,7 +31,6 @@
 
 #include <linux/kernel.h>
 #include <linux/stddef.h>
-#include <linux/rcupdate.h>
 
 struct rb_node {
 	unsigned long  __rb_parent_color;
@@ -74,11 +73,11 @@ extern struct rb_node *rb_first_postorder(const struct rb_root *);
 extern struct rb_node *rb_next_postorder(const struct rb_node *);
 
 /* Fast replacement of a single node without remove/rebalance/add/rebalance */
-extern void rb_replace_node(struct rb_node *victim, struct rb_node *new,
+extern void rb_replace_node(struct rb_node *victim, struct rb_node *new, 
 			    struct rb_root *root);
 
-static inline void rb_link_node(struct rb_node *node, struct rb_node *parent,
-				struct rb_node **rb_link)
+static inline void rb_link_node(struct rb_node * node, struct rb_node * parent,
+				struct rb_node ** rb_link)
 {
 	node->__rb_parent_color = (unsigned long)parent;
 	node->rb_left = node->rb_right = NULL;
@@ -86,15 +85,6 @@ static inline void rb_link_node(struct rb_node *node, struct rb_node *parent,
 	*rb_link = node;
 }
 
-static inline void rb_link_node_rcu(struct rb_node *node, struct rb_node *parent,
-				    struct rb_node **rb_link)
-{
-	node->__rb_parent_color = (unsigned long)parent;
-	node->rb_left = node->rb_right = NULL;
-
-	rcu_assign_pointer(*rb_link, node);
-}
-
 #define rb_entry_safe(ptr, type, member) \
 	({ typeof(ptr) ____ptr = (ptr); \
 	   ____ptr ? rb_entry(____ptr, type, member) : NULL; \
diff --git a/kernel/msm-3.18/include/linux/rbtree_augmented.h b/kernel/msm-3.18/include/linux/rbtree_augmented.h
index 14d7b831b..378c5ee75 100644
--- a/kernel/msm-3.18/include/linux/rbtree_augmented.h
+++ b/kernel/msm-3.18/include/linux/rbtree_augmented.h
@@ -123,11 +123,11 @@ __rb_change_child(struct rb_node *old, struct rb_node *new,
 {
 	if (parent) {
 		if (parent->rb_left == old)
-			WRITE_ONCE(parent->rb_left, new);
+			parent->rb_left = new;
 		else
-			WRITE_ONCE(parent->rb_right, new);
+			parent->rb_right = new;
 	} else
-		WRITE_ONCE(root->rb_node, new);
+		root->rb_node = new;
 }
 
 extern void __rb_erase_color(struct rb_node *parent, struct rb_root *root,
@@ -137,8 +137,7 @@ static __always_inline struct rb_node *
 __rb_erase_augmented(struct rb_node *node, struct rb_root *root,
 		     const struct rb_augment_callbacks *augment)
 {
-	struct rb_node *child = node->rb_right;
-	struct rb_node *tmp = node->rb_left;
+	struct rb_node *child = node->rb_right, *tmp = node->rb_left;
 	struct rb_node *parent, *rebalance;
 	unsigned long pc;
 
@@ -168,7 +167,6 @@ __rb_erase_augmented(struct rb_node *node, struct rb_root *root,
 		tmp = parent;
 	} else {
 		struct rb_node *successor = child, *child2;
-
 		tmp = child->rb_left;
 		if (!tmp) {
 			/*
@@ -182,7 +180,6 @@ __rb_erase_augmented(struct rb_node *node, struct rb_root *root,
 			 */
 			parent = successor;
 			child2 = successor->rb_right;
-
 			augment->copy(node, successor);
 		} else {
 			/*
@@ -204,23 +201,19 @@ __rb_erase_augmented(struct rb_node *node, struct rb_root *root,
 				successor = tmp;
 				tmp = tmp->rb_left;
 			} while (tmp);
-			child2 = successor->rb_right;
-			WRITE_ONCE(parent->rb_left, child2);
-			WRITE_ONCE(successor->rb_right, child);
+			parent->rb_left = child2 = successor->rb_right;
+			successor->rb_right = child;
 			rb_set_parent(child, successor);
-
 			augment->copy(node, successor);
 			augment->propagate(parent, successor);
 		}
 
-		tmp = node->rb_left;
-		WRITE_ONCE(successor->rb_left, tmp);
+		successor->rb_left = tmp = node->rb_left;
 		rb_set_parent(tmp, successor);
 
 		pc = node->__rb_parent_color;
 		tmp = __rb_parent(pc);
 		__rb_change_child(node, successor, tmp, root);
-
 		if (child2) {
 			successor->__rb_parent_color = pc;
 			rb_set_parent_color(child2, parent, RB_BLACK);
diff --git a/kernel/msm-3.18/include/linux/rwlock_types_rt.h b/kernel/msm-3.18/include/linux/rwlock_types_rt.h
index b13832119..09a02d3f1 100644
--- a/kernel/msm-3.18/include/linux/rwlock_types_rt.h
+++ b/kernel/msm-3.18/include/linux/rwlock_types_rt.h
@@ -24,8 +24,7 @@ typedef struct {
 #endif
 
 #define __RW_LOCK_UNLOCKED(name) \
-	{ .lock = __RT_MUTEX_INITIALIZER_SAVE_STATE(name.lock),	\
-	  RW_DEP_MAP_INIT(name) }
+	{ .lock = __RT_MUTEX_INITIALIZER_SAVE_STATE(name.lock),	 RW_DEP_MAP_INIT(name) }
 
 #define DEFINE_RWLOCK(name) \
 	rwlock_t name __cacheline_aligned_in_smp = __RW_LOCK_UNLOCKED(name)
diff --git a/kernel/msm-3.18/include/linux/sched.h b/kernel/msm-3.18/include/linux/sched.h
index 29ff40ddf..701d88e20 100644
--- a/kernel/msm-3.18/include/linux/sched.h
+++ b/kernel/msm-3.18/include/linux/sched.h
@@ -3192,13 +3192,6 @@ extern int _cond_resched(void);
 })
 
 extern int __cond_resched_lock(spinlock_t *lock);
-
-#if defined(CONFIG_PREEMPT_COUNT) && !defined(CONFIG_PREEMPT_RT_FULL)
-#define PREEMPT_LOCK_OFFSET	PREEMPT_OFFSET
-#else
-#define PREEMPT_LOCK_OFFSET	0
-#endif
-
 #define cond_resched_lock(lock) ({				\
 	__might_sleep(__FILE__, __LINE__, PREEMPT_LOCK_OFFSET);	\
 	__cond_resched_lock(lock);				\
diff --git a/kernel/msm-3.18/include/linux/timer.h b/kernel/msm-3.18/include/linux/timer.h
index d7ebff766..d7163289a 100644
--- a/kernel/msm-3.18/include/linux/timer.h
+++ b/kernel/msm-3.18/include/linux/timer.h
@@ -34,9 +34,6 @@ struct timer_list {
 };
 
 extern struct tvec_base boot_tvec_bases;
-#ifdef CONFIG_SMP
-extern struct tvec_base tvec_base_deferrable;
-#endif
 
 #ifdef CONFIG_LOCKDEP
 /*
@@ -73,21 +70,12 @@ extern struct tvec_base tvec_base_deferrable;
 
 #define TIMER_FLAG_MASK			0x3LU
 
-#ifdef CONFIG_SMP
-#define __TIMER_BASE(_flags) \
-	((_flags) & TIMER_DEFERRABLE ? \
-	 (unsigned long)&tvec_base_deferrable + (_flags) : \
-	 (unsigned long)&boot_tvec_bases + (_flags))
-#else
-#define __TIMER_BASE(_flags) ((unsigned long)&boot_tvec_bases + (_flags))
-#endif
-
 #define __TIMER_INITIALIZER(_function, _expires, _data, _flags) { \
 		.entry = { .prev = TIMER_ENTRY_STATIC },	\
 		.function = (_function),			\
 		.expires = (_expires),				\
 		.data = (_data),				\
-		.base = (void *)(__TIMER_BASE(_flags)),		\
+		.base = (void *)((unsigned long)&boot_tvec_bases + (_flags)), \
 		.slack = -1,					\
 		__TIMER_LOCKDEP_MAP_INITIALIZER(		\
 			__FILE__ ":" __stringify(__LINE__))	\
diff --git a/kernel/msm-3.18/include/sound/q6asm-v2.h b/kernel/msm-3.18/include/sound/q6asm-v2.h
index 7771f2468..21ecfb47d 100644
--- a/kernel/msm-3.18/include/sound/q6asm-v2.h
+++ b/kernel/msm-3.18/include/sound/q6asm-v2.h
@@ -208,9 +208,9 @@ struct audio_client {
 	struct mutex	       cmd_lock;
 	/* idx:1 out port, 0: in port*/
 	struct audio_port_data port[2];
-	wait_queue_head_t      cmd_wait;
-	wait_queue_head_t      time_wait;
-	wait_queue_head_t      mem_wait;
+	struct swait_head      cmd_wait;
+	struct swait_head      time_wait;
+	struct swait_head      mem_wait;
 	int                    perf_mode;
 	int					   stream_id;
 	struct device *dev;
diff --git a/kernel/msm-3.18/kernel/sched/core.c b/kernel/msm-3.18/kernel/sched/core.c
index 057d72c78..6368dfe83 100644
--- a/kernel/msm-3.18/kernel/sched/core.c
+++ b/kernel/msm-3.18/kernel/sched/core.c
@@ -5293,6 +5293,7 @@ int wake_up_process_no_notif(struct task_struct *p)
 }
 EXPORT_SYMBOL(wake_up_process_no_notif);
 
+/*
  * wake_up_lock_sleeper - Wake up a specific process blocked on a "sleeping lock"
  * @p: The process to be woken up.
  *
diff --git a/kernel/msm-3.18/kernel/time/tick-sched.c b/kernel/msm-3.18/kernel/time/tick-sched.c
index d46acce03..15852119a 100644
--- a/kernel/msm-3.18/kernel/time/tick-sched.c
+++ b/kernel/msm-3.18/kernel/time/tick-sched.c
@@ -19,7 +19,6 @@
 #include <linux/percpu.h>
 #include <linux/profile.h>
 #include <linux/sched.h>
-#include <linux/timer.h>
 #include <linux/module.h>
 #include <linux/irq_work.h>
 #include <linux/posix-timers.h>
@@ -53,11 +52,11 @@ u64 jiffy_to_ktime_ns(u64 *now, u64 *jiffy_ktime_ns)
 	unsigned long seq;
 
 	do {
-		seq = read_seqbegin(&jiffies_lock);
+		seq = read_seqcount_begin(&jiffies_seq);
 		*now = ktime_get_ns();
 		*jiffy_ktime_ns = ktime_to_ns(last_jiffies_update);
 		cur_jiffies = get_jiffies_64();
-	} while (read_seqretry(&jiffies_lock, seq));
+	} while (read_seqcount_retry(&jiffies_seq, seq));
 
 	return cur_jiffies;
 }
@@ -827,11 +826,6 @@ static void __tick_nohz_idle_enter(struct tick_sched *ts)
 
 	now = tick_nohz_start_idle(ts);
 
-#ifdef CONFIG_SMP
-	if (check_pending_deferrable_timers(cpu))
-		raise_softirq_irqoff(TIMER_SOFTIRQ);
-#endif
-
 	if (can_stop_idle_tick(cpu, ts)) {
 		int was_stopped = ts->tick_stopped;
 
diff --git a/kernel/msm-3.18/kernel/time/timer.c b/kernel/msm-3.18/kernel/time/timer.c
index 0d9e6817c..0df5cbfbc 100644
--- a/kernel/msm-3.18/kernel/time/timer.c
+++ b/kernel/msm-3.18/kernel/time/timer.c
@@ -49,8 +49,6 @@
 #include <asm/timex.h>
 #include <asm/io.h>
 
-#include "tick-internal.h"
-
 #define CREATE_TRACE_POINTS
 #include <trace/events/timer.h>
 
@@ -98,13 +96,6 @@ struct tvec_base {
 struct tvec_base boot_tvec_bases;
 EXPORT_SYMBOL(boot_tvec_bases);
 static DEFINE_PER_CPU(struct tvec_base *, tvec_bases) = &boot_tvec_bases;
-#ifdef CONFIG_SMP
-struct tvec_base tvec_base_deferrable;
-static atomic_t deferrable_pending;
-#endif
-
-static inline void __run_timers(struct tvec_base *base);
-static inline void __init_timers(struct tvec_base *base);
 
 /* Functions below help us manage 'deferrable' flag */
 static inline unsigned int tbase_get_deferrable(struct tvec_base *base)
@@ -411,8 +402,6 @@ __internal_add_timer(struct tvec_base *base, struct timer_list *timer)
 
 static void internal_add_timer(struct tvec_base *base, struct timer_list *timer)
 {
-	int leftmost = 0;
-
 	(void)catchup_timer_jiffies(base);
 	__internal_add_timer(base, timer);
 	/*
@@ -420,10 +409,8 @@ static void internal_add_timer(struct tvec_base *base, struct timer_list *timer)
 	 */
 	if (!tbase_get_deferrable(timer->base)) {
 		if (!base->active_timers++ ||
-		    time_before(timer->expires, base->next_timer)) {
+		    time_before(timer->expires, base->next_timer))
 			base->next_timer = timer->expires;
-			leftmost = 1;
-		}
 	}
 	base->all_timers++;
 
@@ -440,7 +427,7 @@ static void internal_add_timer(struct tvec_base *base, struct timer_list *timer)
 	 * require special care against races with idle_cpu(), lets deal
 	 * with that later.
 	 */
-	if (leftmost || tick_nohz_full_cpu(base->cpu))
+	if (!tbase_get_deferrable(base) || tick_nohz_full_cpu(base->cpu))
 		wake_up_nohz_cpu(base->cpu);
 }
 
@@ -668,67 +655,10 @@ static inline void debug_assert_init(struct timer_list *timer)
 	debug_timer_assert_init(timer);
 }
 
-#ifdef CONFIG_SMP
-static inline struct tvec_base *__get_timer_base(unsigned int flags)
-{
-	if (flags & TIMER_DEFERRABLE)
-		return &tvec_base_deferrable;
-	else
-		return raw_cpu_read(tvec_bases);
-}
-
-static inline bool is_deferrable_timer_base(struct tvec_base *base)
-{
-	return base == &tvec_base_deferrable;
-}
-
-static inline void __run_deferrable_timers(void)
-{
-	if (time_after_eq(jiffies, tvec_base_deferrable.timer_jiffies)) {
-		if ((atomic_cmpxchg(&deferrable_pending, 1, 0) &&
-			tick_do_timer_cpu == TICK_DO_TIMER_NONE) ||
-			tick_do_timer_cpu == smp_processor_id())
-				__run_timers(&tvec_base_deferrable);
-
-	}
-}
-
-static inline void init_deferrable_timer(void)
-{
-	spin_lock_init(&tvec_base_deferrable.lock);
-	tvec_base_deferrable.cpu = nr_cpu_ids;
-	__init_timers(&tvec_base_deferrable);
-}
-#else
-static inline struct tvec_base *__get_timer_base(unsigned int flags)
-{
-	return raw_cpu_read(tvec_bases);
-}
-
-static inline bool is_deferrable_timer_base(struct tvec_base *base)
-{
-	return false;
-}
-
-static inline void __run_deferrable_timers(void)
-{
-}
-
-static inline void init_deferrable_timer(void)
-{
-	/*
-	 * initialize cpu unbound deferrable timer base only when CONFIG_SMP.
-	 * UP kernel handles the timers with cpu 0 timer base.
-	 */
-}
-#endif
-
 static void do_init_timer(struct timer_list *timer, unsigned int flags,
 			  const char *name, struct lock_class_key *key)
 {
-	struct tvec_base *base;
-
-	base = __get_timer_base(flags);
+	struct tvec_base *base = raw_cpu_read(tvec_bases);
 
 	timer->entry.next = NULL;
 	timer->base = (void *)((unsigned long)base | flags);
@@ -880,21 +810,20 @@ __mod_timer(struct timer_list *timer, unsigned long expires,
 
 	debug_activate(timer, expires);
 
-	if (!is_deferrable_timer_base(base) || pinned == TIMER_PINNED) {
-		cpu = get_nohz_timer_target(pinned);
-		new_base = per_cpu(tvec_bases, cpu);
+	cpu = get_nohz_timer_target(pinned);
+	new_base = per_cpu(tvec_bases, cpu);
 
-		if (base != new_base) {
-			/*
-			 * We are trying to schedule the timer on the local CPU.
-			 * However we can't change timer's base while it is running,
-			 * otherwise del_timer_sync() can't detect that the timer's
-			 * handler yet has not finished. This also guarantees that
-			 * the timer is serialized wrt itself.
-			 */
-			if (likely(base->running_timer != timer))
-				base = switch_timer_base(timer, base, new_base);
-		}
+	if (base != new_base) {
+		/*
+		 * We are trying to schedule the timer on the local CPU.
+		 * However we can't change timer's base while it is running,
+		 * otherwise del_timer_sync() can't detect that the timer's
+		 * handler yet has not finished. This also guarantees that
+		 * the timer is serialized wrt itself.
+		 */
+		if (likely(base->running_timer != timer))
+			base = switch_timer_base(timer, base, new_base);
+	}
 
 	timer->expires = expires;
 	internal_add_timer(base, timer);
@@ -1476,30 +1405,6 @@ static unsigned long cmp_next_hrtimer_event(unsigned long now,
 	return expires;
 }
 
-#ifdef CONFIG_SMP
-/*
- * check_pending_deferrable_timers - Check for unbound deferrable timer expiry.
- * @cpu - Current CPU
- *
- * The function checks whether any global deferrable pending timers
- * are exipired or not. This function does not check cpu bounded
- * diferrable pending timers expiry.
- *
- * The function returns true when a cpu unbounded deferrable timer is expired.
- */
-bool check_pending_deferrable_timers(int cpu)
-{
-	if (cpu == tick_do_timer_cpu ||
-		tick_do_timer_cpu == TICK_DO_TIMER_NONE) {
-		if (time_after_eq(jiffies, tvec_base_deferrable.timer_jiffies)
-			&& !atomic_cmpxchg(&deferrable_pending, 0, 1)) {
-				return true;
-		}
-	}
-	return false;
-}
-#endif
-
 /**
  * get_next_timer_interrupt - return the jiffy of the next pending timer
  * @now: current time (in jiffies)
@@ -1576,7 +1481,6 @@ static void run_timer_softirq(struct softirq_action *h)
 
 	hrtimer_run_pending();
 
-	__run_deferrable_timers();
 	irq_work_tick_soft();
 
 	if (time_after_eq(jiffies, base->timer_jiffies))
@@ -1712,27 +1616,9 @@ signed long __sched schedule_timeout_uninterruptible(signed long timeout)
 }
 EXPORT_SYMBOL(schedule_timeout_uninterruptible);
 
-static inline void __init_timers(struct tvec_base *base)
-{
-	int j;
-
-	for (j = 0; j < TVN_SIZE; j++) {
-		INIT_LIST_HEAD(base->tv5.vec + j);
-		INIT_LIST_HEAD(base->tv4.vec + j);
-		INIT_LIST_HEAD(base->tv3.vec + j);
-		INIT_LIST_HEAD(base->tv2.vec + j);
-	}
-	for (j = 0; j < TVR_SIZE; j++)
-		INIT_LIST_HEAD(base->tv1.vec + j);
-
-	base->timer_jiffies = jiffies;
-	base->next_timer = base->timer_jiffies;
-	base->active_timers = 0;
-	base->all_timers = 0;
-}
-
 static int init_timers_cpu(int cpu)
 {
+	int j;
 	struct tvec_base *base;
 	static char tvec_base_done[NR_CPUS];
 
@@ -1771,11 +1657,23 @@ static int init_timers_cpu(int cpu)
 		base = per_cpu(tvec_bases, cpu);
 	}
 
-	__init_timers(base);
 #ifdef CONFIG_PREEMPT_RT_FULL
 	init_waitqueue_head(&base->wait_for_running_timer);
 #endif
 
+	for (j = 0; j < TVN_SIZE; j++) {
+		INIT_LIST_HEAD(base->tv5.vec + j);
+		INIT_LIST_HEAD(base->tv4.vec + j);
+		INIT_LIST_HEAD(base->tv3.vec + j);
+		INIT_LIST_HEAD(base->tv2.vec + j);
+	}
+	for (j = 0; j < TVR_SIZE; j++)
+		INIT_LIST_HEAD(base->tv1.vec + j);
+
+	base->timer_jiffies = jiffies;
+	base->next_timer = base->timer_jiffies;
+	base->active_timers = 0;
+	base->all_timers = 0;
 	return 0;
 }
 
@@ -1867,8 +1765,6 @@ void __init init_timers(void)
 			       (void *)(long)smp_processor_id());
 	BUG_ON(err != NOTIFY_OK);
 
-	init_deferrable_timer();
-
 	init_timer_stats();
 	register_cpu_notifier(&timers_nb);
 	open_softirq(TIMER_SOFTIRQ, run_timer_softirq);
diff --git a/kernel/msm-3.18/kernel/time/timer.c.rej b/kernel/msm-3.18/kernel/time/timer.c.rej
new file mode 100644
index 000000000..a95b84576
--- /dev/null
+++ b/kernel/msm-3.18/kernel/time/timer.c.rej
@@ -0,0 +1,68 @@
+--- kernel/time/timer.c
++++ kernel/time/timer.c
+@@ -777,26 +716,24 @@ __mod_timer(struct timer_list *timer, unsigned long expires,
+ 
+ 	debug_activate(timer, expires);
+ 
+-	if (!is_deferrable_timer_base(base) || pinned == TIMER_PINNED) {
+-		cpu = get_nohz_timer_target(pinned);
+-		new_base = per_cpu(tvec_bases, cpu);
++	cpu = get_nohz_timer_target(pinned);
++	new_base = per_cpu(tvec_bases, cpu);
+ 
+-		if (base != new_base) {
+-			/*
+-			 * We are trying to schedule the timer on the local CPU.
+-			 * However we can't change timer's base while it is
+-			 * running, otherwise del_timer_sync() can't detect that
+-			 * the timer's handler yet has not finished. This also
+-			 * guarantees that the timer is serialized wrt itself.
+-			 */
+-			if (likely(base->running_timer != timer)) {
+-				/* See the comment in lock_timer_base() */
+-				timer_set_base(timer, NULL);
+-				spin_unlock(&base->lock);
+-				base = new_base;
+-				spin_lock(&base->lock);
+-				timer_set_base(timer, base);
+-			}
++	if (base != new_base) {
++		/*
++		 * We are trying to schedule the timer on the local CPU.
++		 * However we can't change timer's base while it is running,
++		 * otherwise del_timer_sync() can't detect that the timer's
++		 * handler yet has not finished. This also guarantees that
++		 * the timer is serialized wrt itself.
++		 */
++		if (likely(base->running_timer != timer)) {
++			/* See the comment in lock_timer_base() */
++			timer_set_base(timer, NULL);
++			spin_unlock(&base->lock);
++			base = new_base;
++			spin_lock(&base->lock);
++			timer_set_base(timer, base);
+ 		}
+ 	}
+ 
+@@ -1576,8 +1493,20 @@ static int init_timers_cpu(int cpu)
+ 		base = per_cpu(tvec_bases, cpu);
+ 	}
+ 
+-	__init_timers(base);
+ 
++	for (j = 0; j < TVN_SIZE; j++) {
++		INIT_LIST_HEAD(base->tv5.vec + j);
++		INIT_LIST_HEAD(base->tv4.vec + j);
++		INIT_LIST_HEAD(base->tv3.vec + j);
++		INIT_LIST_HEAD(base->tv2.vec + j);
++	}
++	for (j = 0; j < TVR_SIZE; j++)
++		INIT_LIST_HEAD(base->tv1.vec + j);
++
++	base->timer_jiffies = jiffies;
++	base->next_timer = base->timer_jiffies;
++	base->active_timers = 0;
++	base->all_timers = 0;
+ 	return 0;
+ }
+ 
diff --git a/kernel/msm-3.18/lib/rbtree.c b/kernel/msm-3.18/lib/rbtree.c
index 1356454e3..c16c81a3d 100644
--- a/kernel/msm-3.18/lib/rbtree.c
+++ b/kernel/msm-3.18/lib/rbtree.c
@@ -44,30 +44,6 @@
  *  parentheses and have some accompanying text comment.
  */
 
-/*
- * Notes on lockless lookups:
- *
- * All stores to the tree structure (rb_left and rb_right) must be done using
- * WRITE_ONCE(). And we must not inadvertently cause (temporary) loops in the
- * tree structure as seen in program order.
- *
- * These two requirements will allow lockless iteration of the tree -- not
- * correct iteration mind you, tree rotations are not atomic so a lookup might
- * miss entire subtrees.
- *
- * But they do guarantee that any such traversal will only see valid elements
- * and that it will indeed complete -- does not get stuck in a loop.
- *
- * It also guarantees that if the lookup returns an element it is the 'correct'
- * one. But not returning an element does _NOT_ mean it's not present.
- *
- * NOTE:
- *
- * Stores to __rb_parent_color are not important for simple lookups so those
- * are left undone as of now. Nor did I check for loops involving parent
- * pointers.
- */
-
 static inline void rb_set_black(struct rb_node *rb)
 {
 	rb->__rb_parent_color |= RB_BLACK;
@@ -153,9 +129,8 @@ __rb_insert(struct rb_node *node, struct rb_root *root,
 				 * This still leaves us in violation of 4), the
 				 * continuation into Case 3 will fix that.
 				 */
-				tmp = node->rb_left;
-				WRITE_ONCE(parent->rb_right, tmp);
-				WRITE_ONCE(node->rb_left, parent);
+				parent->rb_right = tmp = node->rb_left;
+				node->rb_left = parent;
 				if (tmp)
 					rb_set_parent_color(tmp, parent,
 							    RB_BLACK);
@@ -174,8 +149,8 @@ __rb_insert(struct rb_node *node, struct rb_root *root,
 			 *     /                 \
 			 *    n                   U
 			 */
-			WRITE_ONCE(gparent->rb_left, tmp); /* == parent->rb_right */
-			WRITE_ONCE(parent->rb_right, gparent);
+			gparent->rb_left = tmp;  /* == parent->rb_right */
+			parent->rb_right = gparent;
 			if (tmp)
 				rb_set_parent_color(tmp, gparent, RB_BLACK);
 			__rb_rotate_set_parents(gparent, parent, root, RB_RED);
@@ -196,9 +171,8 @@ __rb_insert(struct rb_node *node, struct rb_root *root,
 			tmp = parent->rb_left;
 			if (node == tmp) {
 				/* Case 2 - right rotate at parent */
-				tmp = node->rb_right;
-				WRITE_ONCE(parent->rb_left, tmp);
-				WRITE_ONCE(node->rb_right, parent);
+				parent->rb_left = tmp = node->rb_right;
+				node->rb_right = parent;
 				if (tmp)
 					rb_set_parent_color(tmp, parent,
 							    RB_BLACK);
@@ -209,8 +183,8 @@ __rb_insert(struct rb_node *node, struct rb_root *root,
 			}
 
 			/* Case 3 - left rotate at gparent */
-			WRITE_ONCE(gparent->rb_right, tmp); /* == parent->rb_left */
-			WRITE_ONCE(parent->rb_left, gparent);
+			gparent->rb_right = tmp;  /* == parent->rb_left */
+			parent->rb_left = gparent;
 			if (tmp)
 				rb_set_parent_color(tmp, gparent, RB_BLACK);
 			__rb_rotate_set_parents(gparent, parent, root, RB_RED);
@@ -250,9 +224,8 @@ ____rb_erase_color(struct rb_node *parent, struct rb_root *root,
 				 *      / \         / \
 				 *     Sl  Sr      N   Sl
 				 */
-				tmp1 = sibling->rb_left;
-				WRITE_ONCE(parent->rb_right, tmp1);
-				WRITE_ONCE(sibling->rb_left, parent);
+				parent->rb_right = tmp1 = sibling->rb_left;
+				sibling->rb_left = parent;
 				rb_set_parent_color(tmp1, parent, RB_BLACK);
 				__rb_rotate_set_parents(parent, sibling, root,
 							RB_RED);
@@ -302,10 +275,9 @@ ____rb_erase_color(struct rb_node *parent, struct rb_root *root,
 				 *                       \
 				 *                        Sr
 				 */
-				tmp1 = tmp2->rb_right;
-				WRITE_ONCE(sibling->rb_left, tmp1);
-				WRITE_ONCE(tmp2->rb_right, sibling);
-				WRITE_ONCE(parent->rb_right, tmp2);
+				sibling->rb_left = tmp1 = tmp2->rb_right;
+				tmp2->rb_right = sibling;
+				parent->rb_right = tmp2;
 				if (tmp1)
 					rb_set_parent_color(tmp1, sibling,
 							    RB_BLACK);
@@ -325,9 +297,8 @@ ____rb_erase_color(struct rb_node *parent, struct rb_root *root,
 			 *        / \         / \
 			 *      (sl) sr      N  (sl)
 			 */
-			tmp2 = sibling->rb_left;
-			WRITE_ONCE(parent->rb_right, tmp2);
-			WRITE_ONCE(sibling->rb_left, parent);
+			parent->rb_right = tmp2 = sibling->rb_left;
+			sibling->rb_left = parent;
 			rb_set_parent_color(tmp1, sibling, RB_BLACK);
 			if (tmp2)
 				rb_set_parent(tmp2, parent);
@@ -339,9 +310,8 @@ ____rb_erase_color(struct rb_node *parent, struct rb_root *root,
 			sibling = parent->rb_left;
 			if (rb_is_red(sibling)) {
 				/* Case 1 - right rotate at parent */
-				tmp1 = sibling->rb_right;
-				WRITE_ONCE(parent->rb_left, tmp1);
-				WRITE_ONCE(sibling->rb_right, parent);
+				parent->rb_left = tmp1 = sibling->rb_right;
+				sibling->rb_right = parent;
 				rb_set_parent_color(tmp1, parent, RB_BLACK);
 				__rb_rotate_set_parents(parent, sibling, root,
 							RB_RED);
@@ -366,10 +336,9 @@ ____rb_erase_color(struct rb_node *parent, struct rb_root *root,
 					break;
 				}
 				/* Case 3 - right rotate at sibling */
-				tmp1 = tmp2->rb_left;
-				WRITE_ONCE(sibling->rb_right, tmp1);
-				WRITE_ONCE(tmp2->rb_left, sibling);
-				WRITE_ONCE(parent->rb_left, tmp2);
+				sibling->rb_right = tmp1 = tmp2->rb_left;
+				tmp2->rb_left = sibling;
+				parent->rb_left = tmp2;
 				if (tmp1)
 					rb_set_parent_color(tmp1, sibling,
 							    RB_BLACK);
@@ -378,9 +347,8 @@ ____rb_erase_color(struct rb_node *parent, struct rb_root *root,
 				sibling = tmp2;
 			}
 			/* Case 4 - left rotate at parent + color flips */
-			tmp2 = sibling->rb_right;
-			WRITE_ONCE(parent->rb_left, tmp2);
-			WRITE_ONCE(sibling->rb_right, parent);
+			parent->rb_left = tmp2 = sibling->rb_right;
+			sibling->rb_right = parent;
 			rb_set_parent_color(tmp1, sibling, RB_BLACK);
 			if (tmp2)
 				rb_set_parent(tmp2, parent);
diff --git a/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6asm.c b/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6asm.c
index fdf8c6fda..25d4c275c 100644
--- a/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6asm.c
+++ b/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6asm.c
@@ -709,7 +709,7 @@ int send_asm_custom_topology(struct audio_client *ac)
 
 	}
 
-	result = wait_event_timeout(ac->mem_wait,
+	result = swait_event_timeout(ac->mem_wait,
 			(atomic_read(&ac->mem_state) >= 0), 5*HZ);
 	if (!result) {
 		pr_err("%s: Set topologies failed timeout\n", __func__);
@@ -1110,9 +1110,9 @@ struct audio_client *q6asm_audio_client_alloc(app_cb cb, void *priv)
 		goto fail_mmap;
 	}
 
-	init_waitqueue_head(&ac->cmd_wait);
-	init_waitqueue_head(&ac->time_wait);
-	init_waitqueue_head(&ac->mem_wait);
+	init_swait_head(&ac->cmd_wait);
+	init_swait_head(&ac->time_wait);
+	init_swait_head(&ac->mem_wait);
 	atomic_set(&ac->time_flag, 1);
 	atomic_set(&ac->reset, 0);
 	INIT_LIST_HEAD(&ac->port[0].mem_map_handle);
@@ -1453,7 +1453,7 @@ static int32_t q6asm_srvc_callback(struct apr_client_data *data, void *priv)
 					atomic_set(&ac->unmap_cb_success, 0);
 
 				atomic_set(&ac->mem_state, payload[1]);
-				wake_up(&ac->mem_wait);
+				swait_wake(&ac->mem_wait);
 			} else {
 				if (payload[0] ==
 				    ASM_CMD_SHARED_MEM_UNMAP_REGIONS)
@@ -1461,7 +1461,7 @@ static int32_t q6asm_srvc_callback(struct apr_client_data *data, void *priv)
 			}
 
 			if (atomic_cmpxchg(&ac->mem_state, -1, 0) == -1)
-				wake_up(&ac->mem_wait);
+				swait_wake(&ac->mem_wait);
 			dev_vdbg(ac->dev, "%s: Payload = [0x%x] status[0x%x]\n",
 					__func__, payload[0], payload[1]);
 			break;
@@ -1483,7 +1483,7 @@ static int32_t q6asm_srvc_callback(struct apr_client_data *data, void *priv)
 		spin_lock_irqsave(&port->dsp_lock, dsp_flags);
 		if (atomic_cmpxchg(&ac->mem_state, -1, 0) == -1) {
 			ac->port[dir].tmp_hdl = payload[0];
-			wake_up(&ac->mem_wait);
+			swait_wake(&ac->mem_wait);
 		}
 		spin_unlock_irqrestore(&port->dsp_lock, dsp_flags);
 		break;
@@ -1493,7 +1493,7 @@ static int32_t q6asm_srvc_callback(struct apr_client_data *data, void *priv)
 					__func__, payload[0], payload[1]);
 		spin_lock_irqsave(&port->dsp_lock, dsp_flags);
 		if (atomic_cmpxchg(&ac->mem_state, -1, 0) == -1)
-			wake_up(&ac->mem_wait);
+			swait_wake(&ac->mem_wait);
 		spin_unlock_irqrestore(&port->dsp_lock, dsp_flags);
 
 		break;
@@ -1558,7 +1558,7 @@ static void q6asm_process_mtmx_get_param_rsp(struct audio_client *ac,
 						     "%s: recv inval tstmp\n",
 						     __func__);
 			if (atomic_cmpxchg(&ac->time_flag, 1, 0))
-				wake_up(&ac->time_wait);
+				swait_wake(&ac->time_wait);
 
 			break;
 		default:
@@ -1631,8 +1631,8 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 		atomic_set(&ac->time_flag, 0);
 		atomic_set(&ac->cmd_state, 0);
 		atomic_set(&ac->cmd_state_pp, 0);
-		wake_up(&ac->time_wait);
-		wake_up(&ac->cmd_wait);
+		swait_wake(&ac->time_wait);
+		swait_wake(&ac->cmd_wait);
 		mutex_unlock(&ac->cmd_lock);
 		return 0;
 	}
@@ -1700,7 +1700,7 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 					else
 						atomic_set(&ac->cmd_state,
 								payload[1]);
-					wake_up(&ac->cmd_wait);
+					swait_wake(&ac->cmd_wait);
 				}
 				return 0;
 			}
@@ -1708,13 +1708,13 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 				if (atomic_read(&ac->cmd_state_pp) &&
 					wakeup_flag) {
 					atomic_set(&ac->cmd_state_pp, 0);
-					wake_up(&ac->cmd_wait);
+					swait_wake(&ac->cmd_wait);
 				}
 			} else {
 				if (atomic_read(&ac->cmd_state) &&
 					wakeup_flag) {
 					atomic_set(&ac->cmd_state, 0);
-					wake_up(&ac->cmd_wait);
+					swait_wake(&ac->cmd_wait);
 				}
 			}
 			if (ac->cb)
@@ -1729,13 +1729,13 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 					 __func__, payload[0], payload[1]);
 				if (wakeup_flag) {
 					atomic_set(&ac->mem_state, payload[1]);
-					wake_up(&ac->mem_wait);
+					swait_wake(&ac->mem_wait);
 				}
 				return 0;
 			}
 			if (atomic_read(&ac->mem_state) && wakeup_flag) {
 				atomic_set(&ac->mem_state, 0);
-				wake_up(&ac->mem_wait);
+				swait_wake(&ac->mem_wait);
 			}
 			if (ac->cb)
 				ac->cb(data->opcode, data->token,
@@ -1834,7 +1834,7 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 			}
 			if (atomic_read(&ac->cmd_state) && wakeup_flag) {
 				atomic_set(&ac->cmd_state, 0);
-				wake_up(&ac->cmd_wait);
+				swait_wake(&ac->cmd_wait);
 			}
 			break;
 		}
@@ -1916,7 +1916,7 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 		ac->time_stamp = (uint64_t)(((uint64_t)payload[2] << 32) |
 				payload[1]);
 		if (atomic_cmpxchg(&ac->time_flag, 1, 0))
-			wake_up(&ac->time_wait);
+			swait_wake(&ac->time_wait);
 		break;
 	case ASM_DATA_EVENT_SR_CM_CHANGE_NOTIFY:
 	case ASM_DATA_EVENT_ENC_SR_CM_CHANGE_NOTIFY:
@@ -1944,7 +1944,7 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 			atomic_set(&ac->cmd_state, payload[0]);
 			ac->path_delay = UINT_MAX;
 		}
-		wake_up(&ac->cmd_wait);
+		swait_wake(&ac->cmd_wait);
 		break;
 	}
 	if (ac->cb)
@@ -2341,7 +2341,7 @@ static int __q6asm_open_read(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for open read\n",
@@ -2470,7 +2470,7 @@ int q6asm_open_write_compressed(struct audio_client *ac, uint32_t format,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 		(atomic_read(&ac->cmd_state) >= 0), 1*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for OPEN_WRITE_COMPR rc[%d]\n",
@@ -2616,7 +2616,7 @@ static int __q6asm_open_write(struct audio_client *ac, uint32_t format,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for open write\n", __func__);
@@ -2841,7 +2841,7 @@ static int __q6asm_open_read_write(struct audio_client *ac, uint32_t rd_format,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for open read-write\n",
@@ -2957,7 +2957,7 @@ int q6asm_open_loopback_v2(struct audio_client *ac, uint16_t bits_per_sample)
 			goto fail_cmd;
 		}
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for open_loopback\n",
@@ -3229,7 +3229,7 @@ int q6asm_open_shared_io(struct audio_client *ac,
 	}
 
 	pr_debug("%s: sent open apr pkt\n", __func__);
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) <= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: Timeout. Waited for open write apr pkt rc[%d]\n",
@@ -3390,7 +3390,7 @@ int q6asm_run(struct audio_client *ac, uint32_t flags,
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for run success",
@@ -3495,7 +3495,7 @@ int q6asm_enc_cfg_blk_aac(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for FORMAT_UPDATE\n",
@@ -3546,7 +3546,7 @@ int q6asm_enc_cfg_blk_g711(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for FORMAT_UPDATE\n",
@@ -3598,7 +3598,7 @@ int q6asm_set_encdec_chan_map(struct audio_client *ac,
 			   ASM_PARAM_ID_DEC_OUTPUT_CHAN_MAP, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 				 (atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout opcode[0x%x]\n", __func__,
@@ -3694,7 +3694,7 @@ int q6asm_enc_cfg_blk_pcm_v3(struct audio_client *ac,
 		pr_err("%s: Comamnd open failed %d\n", __func__, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout opcode[0x%x]\n",
@@ -3774,7 +3774,7 @@ int q6asm_enc_cfg_blk_pcm_v2(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout opcode[0x%x]\n",
@@ -3886,7 +3886,7 @@ int q6asm_enc_cfg_blk_pcm_native(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout opcode[0x%x]\n",
@@ -4009,7 +4009,7 @@ int q6asm_enable_sbrps(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout opcode[0x%x] ", __func__, sbrps.hdr.opcode);
@@ -4057,7 +4057,7 @@ int q6asm_cfg_dual_mono_aac(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout opcode[0x%x]\n", __func__,
@@ -4102,7 +4102,7 @@ int q6asm_cfg_aac_sel_mix_coef(struct audio_client *ac, uint32_t mix_coeff)
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 		(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout opcode[0x%x]\n",
@@ -4157,7 +4157,7 @@ int q6asm_enc_cfg_blk_qcelp(struct audio_client *ac, uint32_t frames_per_buf,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for setencdec v13k resp\n",
@@ -4211,7 +4211,7 @@ int q6asm_enc_cfg_blk_evrc(struct audio_client *ac, uint32_t frames_per_buf,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for encdec evrc\n", __func__);
@@ -4260,7 +4260,7 @@ int q6asm_enc_cfg_blk_amrnb(struct audio_client *ac, uint32_t frames_per_buf,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for set encdec amrnb\n", __func__);
@@ -4309,7 +4309,7 @@ int q6asm_enc_cfg_blk_amrwb(struct audio_client *ac, uint32_t frames_per_buf,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -4385,7 +4385,7 @@ static int __q6asm_media_format_block_pcm(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for format update\n", __func__);
@@ -4466,7 +4466,7 @@ static int __q6asm_media_format_block_pcm_v3(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for format update\n", __func__);
@@ -4597,7 +4597,7 @@ static int __q6asm_media_format_block_multi_ch_pcm(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for format update\n", __func__);
@@ -4666,7 +4666,7 @@ static int __q6asm_media_format_block_multi_ch_pcm_v3(struct audio_client *ac,
 		pr_err("%s: Comamnd open failed %d\n", __func__, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for format update\n", __func__);
@@ -4777,7 +4777,7 @@ static int __q6asm_media_format_block_multi_aac(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -4850,7 +4850,7 @@ int q6asm_media_format_block_wma(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -4910,7 +4910,7 @@ int q6asm_media_format_block_wmapro(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -4958,7 +4958,7 @@ int q6asm_media_format_block_amrwbplus(struct audio_client *ac,
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 				(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -5010,7 +5010,7 @@ int q6asm_stream_media_format_block_flac(struct audio_client *ac,
 				__func__, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 				(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s :timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -5066,7 +5066,7 @@ int q6asm_media_format_block_alac(struct audio_client *ac,
 				__func__, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 				(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s :timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -5118,7 +5118,7 @@ int q6asm_media_format_block_g711(struct audio_client *ac,
 				__func__, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 				(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s :timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -5164,7 +5164,7 @@ int q6asm_stream_media_format_block_vorbis(struct audio_client *ac,
 				__func__, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 				(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s :timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -5218,7 +5218,7 @@ int q6asm_media_format_block_ape(struct audio_client *ac,
 				__func__, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s :timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -5269,7 +5269,7 @@ int q6asm_stream_media_format_block_aptx_dec(struct audio_client *ac,
 				__func__, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 				(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s :timeout. waited for FORMAT_UPDATE\n", __func__);
@@ -5322,7 +5322,7 @@ static int __q6asm_ds1_set_endp_params(struct audio_client *ac, int param_id,
 			__func__, ASM_STREAM_CMD_SET_ENCDEC_PARAM, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 		(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout opcode[0x%x]\n", __func__,
@@ -5426,7 +5426,7 @@ int q6asm_memory_map(struct audio_client *ac, phys_addr_t buf_add, int dir,
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->mem_wait,
+	rc = swait_event_timeout(ac->mem_wait,
 			(atomic_read(&ac->mem_state) >= 0 &&
 			 ac->port[dir].tmp_hdl), 5*HZ);
 	if (!rc) {
@@ -5504,7 +5504,7 @@ int q6asm_memory_unmap(struct audio_client *ac, phys_addr_t buf_add, int dir)
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->mem_wait,
+	rc = swait_event_timeout(ac->mem_wait,
 			(atomic_read(&ac->mem_state) >= 0), 5 * HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for memory_unmap of handle 0x%x\n",
@@ -5648,7 +5648,7 @@ static int q6asm_memory_map_regions(struct audio_client *ac, int dir,
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->mem_wait,
+	rc = swait_event_timeout(ac->mem_wait,
 			(atomic_read(&ac->mem_state) >= 0)
 			 , 5*HZ);
 	if (!rc) {
@@ -5739,7 +5739,7 @@ static int q6asm_memory_unmap_regions(struct audio_client *ac, int dir)
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->mem_wait,
+	rc = swait_event_timeout(ac->mem_wait,
 			(atomic_read(&ac->mem_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for memory_unmap of handle 0x%x\n",
@@ -5819,7 +5819,7 @@ int q6asm_set_lrgain(struct audio_client *ac, int left_gain, int right_gain)
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state_pp) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout, set-params paramid[0x%x]\n", __func__,
@@ -5925,7 +5925,7 @@ int q6asm_set_multich_gain(struct audio_client *ac, uint32_t channels,
 		goto done;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state_pp) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout, set-params paramid[0x%x]\n", __func__,
@@ -5985,7 +5985,7 @@ int q6asm_set_mute(struct audio_client *ac, int muteflag)
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state_pp) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout, set-params paramid[0x%x]\n", __func__,
@@ -6058,7 +6058,7 @@ static int __q6asm_set_volume(struct audio_client *ac, int volume, int instance)
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state_pp) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout, set-params paramid[0x%x]\n", __func__,
@@ -6131,7 +6131,7 @@ int q6asm_set_aptx_dec_bt_addr(struct audio_client *ac,
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout, set-params paramid[0x%x]\n", __func__,
@@ -6200,7 +6200,7 @@ int q6asm_set_softpause(struct audio_client *ac,
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state_pp) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout, set-params paramid[0x%x]\n", __func__,
@@ -6278,7 +6278,7 @@ static int __q6asm_set_softvolume(struct audio_client *ac,
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state_pp) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout, set-params paramid[0x%x]\n", __func__,
@@ -6385,7 +6385,7 @@ int q6asm_equalizer(struct audio_client *ac, void *eq_p)
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state_pp) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout, set-params paramid[0x%x]\n", __func__,
@@ -6898,7 +6898,7 @@ int q6asm_get_session_time(struct audio_client *ac, uint64_t *tstamp)
 		       mtmx_params.hdr.opcode, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->time_wait,
+	rc = swait_event_timeout(ac->time_wait,
 			(atomic_read(&ac->time_flag) == 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout in getting session time from DSP\n",
@@ -6944,7 +6944,7 @@ int q6asm_get_session_time_legacy(struct audio_client *ac, uint64_t *tstamp)
 				__func__, hdr.opcode, rc);
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->time_wait,
+	rc = swait_event_timeout(ac->time_wait,
 			(atomic_read(&ac->time_flag) == 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout in getting session time from DSP\n",
@@ -7010,7 +7010,7 @@ int q6asm_send_audio_effects_params(struct audio_client *ac, char *params,
 		rc = -EINVAL;
 		goto fail_send_param;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 				(atomic_read(&ac->cmd_state_pp) >= 0), 1*HZ);
 	if (!rc) {
 		pr_err("%s: timeout, audio effects set-params\n", __func__);
@@ -7082,7 +7082,7 @@ int q6asm_send_mtmx_strtr_window(struct audio_client *ac,
 		goto fail_cmd;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout, Render window start paramid[0x%x]\n",
@@ -7178,7 +7178,7 @@ static int __q6asm_cmd(struct audio_client *ac, int cmd, uint32_t stream_id)
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait, (atomic_read(state) >= 0), 5*HZ);
+	rc = swait_event_timeout(ac->cmd_wait, (atomic_read(state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for response opcode[0x%x]\n",
 				__func__, hdr.opcode);
@@ -7441,7 +7441,7 @@ int q6asm_reg_tx_overflow(struct audio_client *ac, uint16_t enable)
 		rc = -EINVAL;
 		goto fail_cmd;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 				(atomic_read(&ac->cmd_state) >= 0), 5*HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for tx overflow\n", __func__);
@@ -7524,7 +7524,7 @@ int q6asm_get_path_delay(struct audio_client *ac)
 		return rc;
 	}
 
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 			(atomic_read(&ac->cmd_state) >= 0), 5 * HZ);
 	if (!rc) {
 		pr_err("%s: timeout. waited for response opcode[0x%x]\n",
@@ -7729,7 +7729,7 @@ int q6asm_send_cal(struct audio_client *ac)
 		rc = -EINVAL;
 		goto free;
 	}
-	rc = wait_event_timeout(ac->cmd_wait,
+	rc = swait_event_timeout(ac->cmd_wait,
 				(atomic_read(&ac->cmd_state_pp) >= 0), 5 * HZ);
 	if (!rc) {
 		pr_err("%s: timeout, audio audstrm cal send\n", __func__);
@@ -7951,9 +7951,9 @@ static int __init q6asm_init(void)
 	common_client.session = ASM_CONTROL_SESSION;
 	common_client.port[0].buf = &common_buf[0];
 	common_client.port[1].buf = &common_buf[1];
-	init_waitqueue_head(&common_client.cmd_wait);
-	init_waitqueue_head(&common_client.time_wait);
-	init_waitqueue_head(&common_client.mem_wait);
+	init_swait_head(&common_client.cmd_wait);
+	init_swait_head(&common_client.time_wait);
+	init_swait_head(&common_client.mem_wait);
 	atomic_set(&common_client.time_flag, 1);
 	INIT_LIST_HEAD(&common_client.port[0].mem_map_handle);
 	INIT_LIST_HEAD(&common_client.port[1].mem_map_handle);
diff --git a/kernel/patch-3.18.69-rt75.patch b/kernel/patch-3.18.69-rt75.patch
new file mode 100644
index 000000000..c567cc28e
--- /dev/null
+++ b/kernel/patch-3.18.69-rt75.patch
@@ -0,0 +1,27818 @@
+diff --git a/Documentation/hwlat_detector.txt b/Documentation/hwlat_detector.txt
+new file mode 100644
+index 000000000000..cb61516483d3
+--- /dev/null
++++ b/Documentation/hwlat_detector.txt
+@@ -0,0 +1,64 @@
++Introduction:
++-------------
++
++The module hwlat_detector is a special purpose kernel module that is used to
++detect large system latencies induced by the behavior of certain underlying
++hardware or firmware, independent of Linux itself. The code was developed
++originally to detect SMIs (System Management Interrupts) on x86 systems,
++however there is nothing x86 specific about this patchset. It was
++originally written for use by the "RT" patch since the Real Time
++kernel is highly latency sensitive.
++
++SMIs are usually not serviced by the Linux kernel, which typically does not
++even know that they are occuring. SMIs are instead are set up by BIOS code
++and are serviced by BIOS code, usually for "critical" events such as
++management of thermal sensors and fans. Sometimes though, SMIs are used for
++other tasks and those tasks can spend an inordinate amount of time in the
++handler (sometimes measured in milliseconds). Obviously this is a problem if
++you are trying to keep event service latencies down in the microsecond range.
++
++The hardware latency detector works by hogging all of the cpus for configurable
++amounts of time (by calling stop_machine()), polling the CPU Time Stamp Counter
++for some period, then looking for gaps in the TSC data. Any gap indicates a
++time when the polling was interrupted and since the machine is stopped and
++interrupts turned off the only thing that could do that would be an SMI.
++
++Note that the SMI detector should *NEVER* be used in a production environment.
++It is intended to be run manually to determine if the hardware platform has a
++problem with long system firmware service routines.
++
++Usage:
++------
++
++Loading the module hwlat_detector passing the parameter "enabled=1" (or by
++setting the "enable" entry in "hwlat_detector" debugfs toggled on) is the only
++step required to start the hwlat_detector. It is possible to redefine the
++threshold in microseconds (us) above which latency spikes will be taken
++into account (parameter "threshold=").
++
++Example:
++
++	# modprobe hwlat_detector enabled=1 threshold=100
++
++After the module is loaded, it creates a directory named "hwlat_detector" under
++the debugfs mountpoint, "/debug/hwlat_detector" for this text. It is necessary
++to have debugfs mounted, which might be on /sys/debug on your system.
++
++The /debug/hwlat_detector interface contains the following files:
++
++count			- number of latency spikes observed since last reset
++enable			- a global enable/disable toggle (0/1), resets count
++max			- maximum hardware latency actually observed (usecs)
++sample			- a pipe from which to read current raw sample data
++			  in the format <timestamp> <latency observed usecs>
++			  (can be opened O_NONBLOCK for a single sample)
++threshold		- minimum latency value to be considered (usecs)
++width			- time period to sample with CPUs held (usecs)
++			  must be less than the total window size (enforced)
++window			- total period of sampling, width being inside (usecs)
++
++By default we will set width to 500,000 and window to 1,000,000, meaning that
++we will sample every 1,000,000 usecs (1s) for 500,000 usecs (0.5s). If we
++observe any latencies that exceed the threshold (initially 100 usecs),
++then we write to a global sample ring buffer of 8K samples, which is
++consumed by reading from the "sample" (pipe) debugfs file interface.
+diff --git a/Documentation/sysrq.txt b/Documentation/sysrq.txt
+index 0e307c94809a..6964d0f80ae7 100644
+--- a/Documentation/sysrq.txt
++++ b/Documentation/sysrq.txt
+@@ -59,10 +59,17 @@ On PowerPC - Press 'ALT - Print Screen (or F13) - <command key>,
+ On other - If you know of the key combos for other architectures, please
+            let me know so I can add them to this section.
+ 
+-On all -  write a character to /proc/sysrq-trigger.  e.g.:
+-
++On all -  write a character to /proc/sysrq-trigger, e.g.:
+ 		echo t > /proc/sysrq-trigger
+ 
++On all - Enable network SysRq by writing a cookie to icmp_echo_sysrq, e.g.
++		echo 0x01020304 >/proc/sys/net/ipv4/icmp_echo_sysrq
++	 Send an ICMP echo request with this pattern plus the particular
++	 SysRq command key. Example:
++		# ping -c1 -s57 -p0102030468
++	 will trigger the SysRq-H (help) command.
++
++
+ *  What are the 'command' keys?
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ 'b'     - Will immediately reboot the system without syncing or unmounting
+diff --git a/Documentation/trace/histograms.txt b/Documentation/trace/histograms.txt
+new file mode 100644
+index 000000000000..6f2aeabf7faa
+--- /dev/null
++++ b/Documentation/trace/histograms.txt
+@@ -0,0 +1,186 @@
++		Using the Linux Kernel Latency Histograms
++
++
++This document gives a short explanation how to enable, configure and use
++latency histograms. Latency histograms are primarily relevant in the
++context of real-time enabled kernels (CONFIG_PREEMPT/CONFIG_PREEMPT_RT)
++and are used in the quality management of the Linux real-time
++capabilities.
++
++
++* Purpose of latency histograms
++
++A latency histogram continuously accumulates the frequencies of latency
++data. There are two types of histograms
++- potential sources of latencies
++- effective latencies
++
++
++* Potential sources of latencies
++
++Potential sources of latencies are code segments where interrupts,
++preemption or both are disabled (aka critical sections). To create
++histograms of potential sources of latency, the kernel stores the time
++stamp at the start of a critical section, determines the time elapsed
++when the end of the section is reached, and increments the frequency
++counter of that latency value - irrespective of whether any concurrently
++running process is affected by latency or not.
++- Configuration items (in the Kernel hacking/Tracers submenu)
++  CONFIG_INTERRUPT_OFF_LATENCY
++  CONFIG_PREEMPT_OFF_LATENCY
++
++
++* Effective latencies
++
++Effective latencies are actually occuring during wakeup of a process. To
++determine effective latencies, the kernel stores the time stamp when a
++process is scheduled to be woken up, and determines the duration of the
++wakeup time shortly before control is passed over to this process. Note
++that the apparent latency in user space may be somewhat longer, since the
++process may be interrupted after control is passed over to it but before
++the execution in user space takes place. Simply measuring the interval
++between enqueuing and wakeup may also not appropriate in cases when a
++process is scheduled as a result of a timer expiration. The timer may have
++missed its deadline, e.g. due to disabled interrupts, but this latency
++would not be registered. Therefore, the offsets of missed timers are
++recorded in a separate histogram. If both wakeup latency and missed timer
++offsets are configured and enabled, a third histogram may be enabled that
++records the overall latency as a sum of the timer latency, if any, and the
++wakeup latency. This histogram is called "timerandwakeup".
++- Configuration items (in the Kernel hacking/Tracers submenu)
++  CONFIG_WAKEUP_LATENCY
++  CONFIG_MISSED_TIMER_OFSETS
++
++
++* Usage
++
++The interface to the administration of the latency histograms is located
++in the debugfs file system. To mount it, either enter
++
++mount -t sysfs nodev /sys
++mount -t debugfs nodev /sys/kernel/debug
++
++from shell command line level, or add
++
++nodev	/sys			sysfs	defaults	0 0
++nodev	/sys/kernel/debug	debugfs	defaults	0 0
++
++to the file /etc/fstab. All latency histogram related files are then
++available in the directory /sys/kernel/debug/tracing/latency_hist. A
++particular histogram type is enabled by writing non-zero to the related
++variable in the /sys/kernel/debug/tracing/latency_hist/enable directory.
++Select "preemptirqsoff" for the histograms of potential sources of
++latencies and "wakeup" for histograms of effective latencies etc. The
++histogram data - one per CPU - are available in the files
++
++/sys/kernel/debug/tracing/latency_hist/preemptoff/CPUx
++/sys/kernel/debug/tracing/latency_hist/irqsoff/CPUx
++/sys/kernel/debug/tracing/latency_hist/preemptirqsoff/CPUx
++/sys/kernel/debug/tracing/latency_hist/wakeup/CPUx
++/sys/kernel/debug/tracing/latency_hist/wakeup/sharedprio/CPUx
++/sys/kernel/debug/tracing/latency_hist/missed_timer_offsets/CPUx
++/sys/kernel/debug/tracing/latency_hist/timerandwakeup/CPUx
++
++The histograms are reset by writing non-zero to the file "reset" in a
++particular latency directory. To reset all latency data, use
++
++#!/bin/sh
++
++TRACINGDIR=/sys/kernel/debug/tracing
++HISTDIR=$TRACINGDIR/latency_hist
++
++if test -d $HISTDIR
++then
++  cd $HISTDIR
++  for i in `find . | grep /reset$`
++  do
++    echo 1 >$i
++  done
++fi
++
++
++* Data format
++
++Latency data are stored with a resolution of one microsecond. The
++maximum latency is 10,240 microseconds. The data are only valid, if the
++overflow register is empty. Every output line contains the latency in
++microseconds in the first row and the number of samples in the second
++row. To display only lines with a positive latency count, use, for
++example,
++
++grep -v " 0$" /sys/kernel/debug/tracing/latency_hist/preemptoff/CPU0
++
++#Minimum latency: 0 microseconds.
++#Average latency: 0 microseconds.
++#Maximum latency: 25 microseconds.
++#Total samples: 3104770694
++#There are 0 samples greater or equal than 10240 microseconds
++#usecs	         samples
++    0	      2984486876
++    1	        49843506
++    2	        58219047
++    3	         5348126
++    4	         2187960
++    5	         3388262
++    6	          959289
++    7	          208294
++    8	           40420
++    9	            4485
++   10	           14918
++   11	           18340
++   12	           25052
++   13	           19455
++   14	            5602
++   15	             969
++   16	              47
++   17	              18
++   18	              14
++   19	               1
++   20	               3
++   21	               2
++   22	               5
++   23	               2
++   25	               1
++
++
++* Wakeup latency of a selected process
++
++To only collect wakeup latency data of a particular process, write the
++PID of the requested process to
++
++/sys/kernel/debug/tracing/latency_hist/wakeup/pid
++
++PIDs are not considered, if this variable is set to 0.
++
++
++* Details of the process with the highest wakeup latency so far
++
++Selected data of the process that suffered from the highest wakeup
++latency that occurred in a particular CPU are available in the file
++
++/sys/kernel/debug/tracing/latency_hist/wakeup/max_latency-CPUx.
++
++In addition, other relevant system data at the time when the
++latency occurred are given.
++
++The format of the data is (all in one line):
++<PID> <Priority> <Latency> (<Timeroffset>) <Command> \
++<- <PID> <Priority> <Command> <Timestamp>
++
++The value of <Timeroffset> is only relevant in the combined timer
++and wakeup latency recording. In the wakeup recording, it is
++always 0, in the missed_timer_offsets recording, it is the same
++as <Latency>.
++
++When retrospectively searching for the origin of a latency and
++tracing was not enabled, it may be helpful to know the name and
++some basic data of the task that (finally) was switching to the
++late real-tlme task. In addition to the victim's data, also the
++data of the possible culprit are therefore displayed after the
++"<-" symbol.
++
++Finally, the timestamp of the time when the latency occurred
++in <seconds>.<microseconds> after the most recent system boot
++is provided.
++
++These data are also reset when the wakeup histogram is reset.
+diff --git a/Makefile b/Makefile
+index 49237a0442cd..40fd2c611062 100644
+--- a/Makefile
++++ b/Makefile
+@@ -401,7 +401,7 @@ KBUILD_CPPFLAGS := -D__KERNEL__
+ KBUILD_CFLAGS   := -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs \
+ 		   -fno-strict-aliasing -fno-common \
+ 		   -Werror-implicit-function-declaration \
+-		   -Wno-format-security \
++		   -Wno-format-security -fno-PIE \
+ 		   -std=gnu89
+ 
+ KBUILD_AFLAGS_KERNEL :=
+diff --git a/arch/Kconfig b/arch/Kconfig
+index 05d7a8a458d5..efb75aee313a 100644
+--- a/arch/Kconfig
++++ b/arch/Kconfig
+@@ -6,6 +6,7 @@ config OPROFILE
+ 	tristate "OProfile system profiling"
+ 	depends on PROFILING
+ 	depends on HAVE_OPROFILE
++	depends on !PREEMPT_RT_FULL
+ 	select RING_BUFFER
+ 	select RING_BUFFER_ALLOW_SWAP
+ 	help
+@@ -49,6 +50,7 @@ config KPROBES
+ config JUMP_LABEL
+        bool "Optimize very unlikely/likely branches"
+        depends on HAVE_ARCH_JUMP_LABEL
++       depends on (!INTERRUPT_OFF_HIST && !PREEMPT_OFF_HIST && !WAKEUP_LATENCY_HIST && !MISSED_TIMER_OFFSETS_HIST)
+        help
+          This option enables a transparent branch optimization that
+ 	 makes certain almost-always-true or almost-always-false branch
+diff --git a/arch/alpha/mm/fault.c b/arch/alpha/mm/fault.c
+index 9d0ac091a52a..870f626674e8 100644
+--- a/arch/alpha/mm/fault.c
++++ b/arch/alpha/mm/fault.c
+@@ -107,7 +107,7 @@ do_page_fault(unsigned long address, unsigned long mmcsr,
+ 
+ 	/* If we're in an interrupt context, or have no user context,
+ 	   we must not take the fault.  */
+-	if (!mm || in_atomic())
++	if (!mm || pagefault_disabled())
+ 		goto no_context;
+ 
+ #ifdef CONFIG_ALPHA_LARGE_VMALLOC
+diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
+index 89c4b5ccc68d..d635abf51063 100644
+--- a/arch/arm/Kconfig
++++ b/arch/arm/Kconfig
+@@ -62,6 +62,7 @@ config ARM
+ 	select HAVE_PERF_EVENTS
+ 	select HAVE_PERF_REGS
+ 	select HAVE_PERF_USER_STACK_DUMP
++	select HAVE_PREEMPT_LAZY
+ 	select HAVE_RCU_TABLE_FREE if (SMP && ARM_LPAE)
+ 	select HAVE_REGS_AND_STACK_ACCESS_API
+ 	select HAVE_SYSCALL_TRACEPOINTS
+diff --git a/arch/arm/include/asm/cmpxchg.h b/arch/arm/include/asm/cmpxchg.h
+index abb2c3769b01..2386e9745ba4 100644
+--- a/arch/arm/include/asm/cmpxchg.h
++++ b/arch/arm/include/asm/cmpxchg.h
+@@ -129,6 +129,8 @@ static inline unsigned long __xchg(unsigned long x, volatile void *ptr, int size
+ 
+ #else	/* min ARCH >= ARMv6 */
+ 
++#define __HAVE_ARCH_CMPXCHG 1
++
+ extern void __bad_cmpxchg(volatile void *ptr, int size);
+ 
+ /*
+diff --git a/arch/arm/include/asm/futex.h b/arch/arm/include/asm/futex.h
+index 53e69dae796f..9d861f9f215b 100644
+--- a/arch/arm/include/asm/futex.h
++++ b/arch/arm/include/asm/futex.h
+@@ -93,6 +93,8 @@ futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,
+ 	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
+ 		return -EFAULT;
+ 
++	preempt_disable_rt();
++
+ 	__asm__ __volatile__("@futex_atomic_cmpxchg_inatomic\n"
+ 	"1:	" TUSER(ldr) "	%1, [%4]\n"
+ 	"	teq	%1, %2\n"
+@@ -104,6 +106,8 @@ futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,
+ 	: "cc", "memory");
+ 
+ 	*uval = val;
++
++	preempt_enable_rt();
+ 	return ret;
+ }
+ 
+diff --git a/arch/arm/include/asm/switch_to.h b/arch/arm/include/asm/switch_to.h
+index c99e259469f7..f3e3d800c407 100644
+--- a/arch/arm/include/asm/switch_to.h
++++ b/arch/arm/include/asm/switch_to.h
+@@ -3,6 +3,13 @@
+ 
+ #include <linux/thread_info.h>
+ 
++#if defined CONFIG_PREEMPT_RT_FULL && defined CONFIG_HIGHMEM
++void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p);
++#else
++static inline void
++switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p) { }
++#endif
++
+ /*
+  * For v7 SMP cores running a preemptible kernel we may be pre-empted
+  * during a TLB maintenance operation, so execute an inner-shareable dsb
+@@ -22,6 +29,7 @@ extern struct task_struct *__switch_to(struct task_struct *, struct thread_info
+ 
+ #define switch_to(prev,next,last)					\
+ do {									\
++	switch_kmaps(prev, next);					\
+ 	last = __switch_to(prev,task_thread_info(prev), task_thread_info(next));	\
+ } while (0)
+ 
+diff --git a/arch/arm/include/asm/thread_info.h b/arch/arm/include/asm/thread_info.h
+index ce73ab635414..483c4e7ede2b 100644
+--- a/arch/arm/include/asm/thread_info.h
++++ b/arch/arm/include/asm/thread_info.h
+@@ -51,6 +51,7 @@ struct cpu_context_save {
+ struct thread_info {
+ 	unsigned long		flags;		/* low level flags */
+ 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
++	int			preempt_lazy_count;	/* 0 => preemptable, <0 => bug */
+ 	mm_segment_t		addr_limit;	/* address limit */
+ 	struct task_struct	*task;		/* main task structure */
+ 	struct exec_domain	*exec_domain;	/* execution domain */
+@@ -149,6 +150,7 @@ extern int vfp_restore_user_hwstate(struct user_vfp __user *,
+ #define TIF_SIGPENDING		0
+ #define TIF_NEED_RESCHED	1
+ #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
++#define TIF_NEED_RESCHED_LAZY	3
+ #define TIF_UPROBE		7
+ #define TIF_SYSCALL_TRACE	8
+ #define TIF_SYSCALL_AUDIT	9
+@@ -162,6 +164,7 @@ extern int vfp_restore_user_hwstate(struct user_vfp __user *,
+ #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
+ #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
+ #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
++#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
+ #define _TIF_UPROBE		(1 << TIF_UPROBE)
+ #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
+ #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
+diff --git a/arch/arm/kernel/asm-offsets.c b/arch/arm/kernel/asm-offsets.c
+index 2d2d6087b9b1..32caa773fe0e 100644
+--- a/arch/arm/kernel/asm-offsets.c
++++ b/arch/arm/kernel/asm-offsets.c
+@@ -64,6 +64,7 @@ int main(void)
+   BLANK();
+   DEFINE(TI_FLAGS,		offsetof(struct thread_info, flags));
+   DEFINE(TI_PREEMPT,		offsetof(struct thread_info, preempt_count));
++  DEFINE(TI_PREEMPT_LAZY,	offsetof(struct thread_info, preempt_lazy_count));
+   DEFINE(TI_ADDR_LIMIT,		offsetof(struct thread_info, addr_limit));
+   DEFINE(TI_TASK,		offsetof(struct thread_info, task));
+   DEFINE(TI_EXEC_DOMAIN,	offsetof(struct thread_info, exec_domain));
+diff --git a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
+index 2f5555d307b3..1c4842879e17 100644
+--- a/arch/arm/kernel/entry-armv.S
++++ b/arch/arm/kernel/entry-armv.S
+@@ -207,11 +207,18 @@ __irq_svc:
+ #ifdef CONFIG_PREEMPT
+ 	get_thread_info tsk
+ 	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
+-	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
+ 	teq	r8, #0				@ if preempt count != 0
++	bne	1f				@ return from exeption
++	ldr	r0, [tsk, #TI_FLAGS]		@ get flags
++	tst	r0, #_TIF_NEED_RESCHED		@ if NEED_RESCHED is set
++	blne	svc_preempt			@ preempt!
++
++	ldr	r8, [tsk, #TI_PREEMPT_LAZY]	@ get preempt lazy count
++	teq	r8, #0				@ if preempt lazy count != 0
+ 	movne	r0, #0				@ force flags to 0
+-	tst	r0, #_TIF_NEED_RESCHED
++	tst	r0, #_TIF_NEED_RESCHED_LAZY
+ 	blne	svc_preempt
++1:
+ #endif
+ 
+ 	svc_exit r5, irq = 1			@ return from exception
+@@ -226,8 +233,14 @@ svc_preempt:
+ 1:	bl	preempt_schedule_irq		@ irq en/disable is done inside
+ 	ldr	r0, [tsk, #TI_FLAGS]		@ get new tasks TI_FLAGS
+ 	tst	r0, #_TIF_NEED_RESCHED
++	bne	1b
++	tst	r0, #_TIF_NEED_RESCHED_LAZY
+ 	reteq	r8				@ go again
+-	b	1b
++	ldr	r0, [tsk, #TI_PREEMPT_LAZY]	@ get preempt lazy count
++	teq	r0, #0				@ if preempt lazy count != 0
++	beq	1b
++	ret	r8				@ go again
++
+ #endif
+ 
+ __und_fault:
+diff --git a/arch/arm/kernel/process.c b/arch/arm/kernel/process.c
+index ecefea4e2929..0516ff72fdac 100644
+--- a/arch/arm/kernel/process.c
++++ b/arch/arm/kernel/process.c
+@@ -437,6 +437,30 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
+ }
+ 
+ #ifdef CONFIG_MMU
++/*
++ * CONFIG_SPLIT_PTLOCK_CPUS results in a page->ptl lock.  If the lock is not
++ * initialized by pgtable_page_ctor() then a coredump of the vector page will
++ * fail.
++ */
++static int __init vectors_user_mapping_init_page(void)
++{
++	struct page *page;
++	unsigned long addr = 0xffff0000;
++	pgd_t *pgd;
++	pud_t *pud;
++	pmd_t *pmd;
++
++	pgd = pgd_offset_k(addr);
++	pud = pud_offset(pgd, addr);
++	pmd = pmd_offset(pud, addr);
++	page = pmd_page(*(pmd));
++
++	pgtable_page_ctor(page);
++
++	return 0;
++}
++late_initcall(vectors_user_mapping_init_page);
++
+ #ifdef CONFIG_KUSER_HELPERS
+ /*
+  * The vectors page is always readable from user space for the
+diff --git a/arch/arm/kernel/signal.c b/arch/arm/kernel/signal.c
+index ea6d69125dde..0c66191bebc7 100644
+--- a/arch/arm/kernel/signal.c
++++ b/arch/arm/kernel/signal.c
+@@ -579,7 +579,8 @@ asmlinkage int
+ do_work_pending(struct pt_regs *regs, unsigned int thread_flags, int syscall)
+ {
+ 	do {
+-		if (likely(thread_flags & _TIF_NEED_RESCHED)) {
++		if (likely(thread_flags & (_TIF_NEED_RESCHED |
++					   _TIF_NEED_RESCHED_LAZY))) {
+ 			schedule();
+ 		} else {
+ 			if (unlikely(!user_mode(regs)))
+diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
+index a8e32aaf0383..6e9b81666a23 100644
+--- a/arch/arm/kernel/smp.c
++++ b/arch/arm/kernel/smp.c
+@@ -208,8 +208,6 @@ int __cpu_disable(void)
+ 	flush_cache_louis();
+ 	local_flush_tlb_all();
+ 
+-	clear_tasks_mm_cpumask(cpu);
+-
+ 	return 0;
+ }
+ 
+@@ -225,6 +223,9 @@ void __cpu_die(unsigned int cpu)
+ 		pr_err("CPU%u: cpu didn't die\n", cpu);
+ 		return;
+ 	}
++
++	clear_tasks_mm_cpumask(cpu);
++
+ 	printk(KERN_NOTICE "CPU%u: shutdown\n", cpu);
+ 
+ 	/*
+diff --git a/arch/arm/kernel/unwind.c b/arch/arm/kernel/unwind.c
+index cbb85c5fabf9..5184fe85d167 100644
+--- a/arch/arm/kernel/unwind.c
++++ b/arch/arm/kernel/unwind.c
+@@ -93,7 +93,7 @@ extern const struct unwind_idx __start_unwind_idx[];
+ static const struct unwind_idx *__origin_unwind_idx;
+ extern const struct unwind_idx __stop_unwind_idx[];
+ 
+-static DEFINE_SPINLOCK(unwind_lock);
++static DEFINE_RAW_SPINLOCK(unwind_lock);
+ static LIST_HEAD(unwind_tables);
+ 
+ /* Convert a prel31 symbol to an absolute address */
+@@ -201,7 +201,7 @@ static const struct unwind_idx *unwind_find_idx(unsigned long addr)
+ 		/* module unwind tables */
+ 		struct unwind_table *table;
+ 
+-		spin_lock_irqsave(&unwind_lock, flags);
++		raw_spin_lock_irqsave(&unwind_lock, flags);
+ 		list_for_each_entry(table, &unwind_tables, list) {
+ 			if (addr >= table->begin_addr &&
+ 			    addr < table->end_addr) {
+@@ -213,7 +213,7 @@ static const struct unwind_idx *unwind_find_idx(unsigned long addr)
+ 				break;
+ 			}
+ 		}
+-		spin_unlock_irqrestore(&unwind_lock, flags);
++		raw_spin_unlock_irqrestore(&unwind_lock, flags);
+ 	}
+ 
+ 	pr_debug("%s: idx = %p\n", __func__, idx);
+@@ -530,9 +530,9 @@ struct unwind_table *unwind_table_add(unsigned long start, unsigned long size,
+ 	tab->begin_addr = text_addr;
+ 	tab->end_addr = text_addr + text_size;
+ 
+-	spin_lock_irqsave(&unwind_lock, flags);
++	raw_spin_lock_irqsave(&unwind_lock, flags);
+ 	list_add_tail(&tab->list, &unwind_tables);
+-	spin_unlock_irqrestore(&unwind_lock, flags);
++	raw_spin_unlock_irqrestore(&unwind_lock, flags);
+ 
+ 	return tab;
+ }
+@@ -544,9 +544,9 @@ void unwind_table_del(struct unwind_table *tab)
+ 	if (!tab)
+ 		return;
+ 
+-	spin_lock_irqsave(&unwind_lock, flags);
++	raw_spin_lock_irqsave(&unwind_lock, flags);
+ 	list_del(&tab->list);
+-	spin_unlock_irqrestore(&unwind_lock, flags);
++	raw_spin_unlock_irqrestore(&unwind_lock, flags);
+ 
+ 	kfree(tab);
+ }
+diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c
+index 6c3dc428a881..09b333a68282 100644
+--- a/arch/arm/kvm/arm.c
++++ b/arch/arm/kvm/arm.c
+@@ -454,9 +454,9 @@ static int kvm_vcpu_first_run_init(struct kvm_vcpu *vcpu)
+ 
+ static void vcpu_pause(struct kvm_vcpu *vcpu)
+ {
+-	wait_queue_head_t *wq = kvm_arch_vcpu_wq(vcpu);
++	struct swait_head *wq = kvm_arch_vcpu_wq(vcpu);
+ 
+-	wait_event_interruptible(*wq, !vcpu->arch.pause);
++	swait_event_interruptible(*wq, !vcpu->arch.pause);
+ }
+ 
+ static int kvm_vcpu_initialized(struct kvm_vcpu *vcpu)
+diff --git a/arch/arm/kvm/psci.c b/arch/arm/kvm/psci.c
+index 4d0d89e342f9..e4f4a706758d 100644
+--- a/arch/arm/kvm/psci.c
++++ b/arch/arm/kvm/psci.c
+@@ -67,7 +67,7 @@ static unsigned long kvm_psci_vcpu_on(struct kvm_vcpu *source_vcpu)
+ {
+ 	struct kvm *kvm = source_vcpu->kvm;
+ 	struct kvm_vcpu *vcpu = NULL, *tmp;
+-	wait_queue_head_t *wq;
++	struct swait_head *wq;
+ 	unsigned long cpu_id;
+ 	unsigned long context_id;
+ 	unsigned long mpidr;
+@@ -124,7 +124,7 @@ static unsigned long kvm_psci_vcpu_on(struct kvm_vcpu *source_vcpu)
+ 	smp_mb();		/* Make sure the above is visible */
+ 
+ 	wq = kvm_arch_vcpu_wq(vcpu);
+-	wake_up_interruptible(wq);
++	swait_wake_interruptible(wq);
+ 
+ 	return PSCI_RET_SUCCESS;
+ }
+diff --git a/arch/arm/mach-at91/at91rm9200_time.c b/arch/arm/mach-at91/at91rm9200_time.c
+index 7fd13aef9827..93491b1195fa 100644
+--- a/arch/arm/mach-at91/at91rm9200_time.c
++++ b/arch/arm/mach-at91/at91rm9200_time.c
+@@ -135,6 +135,7 @@ clkevt32k_mode(enum clock_event_mode mode, struct clock_event_device *dev)
+ 		break;
+ 	case CLOCK_EVT_MODE_SHUTDOWN:
+ 	case CLOCK_EVT_MODE_UNUSED:
++		remove_irq(NR_IRQS_LEGACY + AT91_ID_SYS, &at91rm9200_timer_irq);
+ 	case CLOCK_EVT_MODE_RESUME:
+ 		irqmask = 0;
+ 		break;
+diff --git a/arch/arm/mach-exynos/platsmp.c b/arch/arm/mach-exynos/platsmp.c
+index 41ae28d69e6f..9b96bc41b1d3 100644
+--- a/arch/arm/mach-exynos/platsmp.c
++++ b/arch/arm/mach-exynos/platsmp.c
+@@ -137,7 +137,7 @@ static void __iomem *scu_base_addr(void)
+ 	return (void __iomem *)(S5P_VA_SCU);
+ }
+ 
+-static DEFINE_SPINLOCK(boot_lock);
++static DEFINE_RAW_SPINLOCK(boot_lock);
+ 
+ static void exynos_secondary_init(unsigned int cpu)
+ {
+@@ -150,8 +150,8 @@ static void exynos_secondary_init(unsigned int cpu)
+ 	/*
+ 	 * Synchronise with the boot thread.
+ 	 */
+-	spin_lock(&boot_lock);
+-	spin_unlock(&boot_lock);
++	raw_spin_lock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ }
+ 
+ static int exynos_boot_secondary(unsigned int cpu, struct task_struct *idle)
+@@ -165,7 +165,7 @@ static int exynos_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * Set synchronisation state between this boot processor
+ 	 * and the secondary one
+ 	 */
+-	spin_lock(&boot_lock);
++	raw_spin_lock(&boot_lock);
+ 
+ 	/*
+ 	 * The secondary processor is waiting to be released from
+@@ -192,7 +192,7 @@ static int exynos_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 
+ 		if (timeout == 0) {
+ 			printk(KERN_ERR "cpu1 power enable failed");
+-			spin_unlock(&boot_lock);
++			raw_spin_unlock(&boot_lock);
+ 			return -ETIMEDOUT;
+ 		}
+ 	}
+@@ -242,7 +242,7 @@ static int exynos_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * calibrations, then wait for it to finish
+ 	 */
+ fail:
+-	spin_unlock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ 
+ 	return pen_release != -1 ? ret : 0;
+ }
+diff --git a/arch/arm/mach-hisi/platmcpm.c b/arch/arm/mach-hisi/platmcpm.c
+index 280f3f14f77c..bc2ed95c0e62 100644
+--- a/arch/arm/mach-hisi/platmcpm.c
++++ b/arch/arm/mach-hisi/platmcpm.c
+@@ -57,7 +57,7 @@
+ 
+ static void __iomem *sysctrl, *fabric;
+ static int hip04_cpu_table[HIP04_MAX_CLUSTERS][HIP04_MAX_CPUS_PER_CLUSTER];
+-static DEFINE_SPINLOCK(boot_lock);
++static DEFINE_RAW_SPINLOCK(boot_lock);
+ static u32 fabric_phys_addr;
+ /*
+  * [0]: bootwrapper physical address
+@@ -104,7 +104,7 @@ static int hip04_mcpm_power_up(unsigned int cpu, unsigned int cluster)
+ 	if (cluster >= HIP04_MAX_CLUSTERS || cpu >= HIP04_MAX_CPUS_PER_CLUSTER)
+ 		return -EINVAL;
+ 
+-	spin_lock_irq(&boot_lock);
++	raw_spin_lock_irq(&boot_lock);
+ 
+ 	if (hip04_cpu_table[cluster][cpu])
+ 		goto out;
+@@ -133,7 +133,7 @@ static int hip04_mcpm_power_up(unsigned int cpu, unsigned int cluster)
+ 	udelay(20);
+ out:
+ 	hip04_cpu_table[cluster][cpu]++;
+-	spin_unlock_irq(&boot_lock);
++	raw_spin_unlock_irq(&boot_lock);
+ 
+ 	return 0;
+ }
+@@ -149,7 +149,7 @@ static void hip04_mcpm_power_down(void)
+ 
+ 	__mcpm_cpu_going_down(cpu, cluster);
+ 
+-	spin_lock(&boot_lock);
++	raw_spin_lock(&boot_lock);
+ 	BUG_ON(__mcpm_cluster_state(cluster) != CLUSTER_UP);
+ 	hip04_cpu_table[cluster][cpu]--;
+ 	if (hip04_cpu_table[cluster][cpu] == 1) {
+@@ -162,7 +162,7 @@ static void hip04_mcpm_power_down(void)
+ 
+ 	last_man = hip04_cluster_is_down(cluster);
+ 	if (last_man && __mcpm_outbound_enter_critical(cpu, cluster)) {
+-		spin_unlock(&boot_lock);
++		raw_spin_unlock(&boot_lock);
+ 		/* Since it's Cortex A15, disable L2 prefetching. */
+ 		asm volatile(
+ 		"mcr	p15, 1, %0, c15, c0, 3 \n\t"
+@@ -173,7 +173,7 @@ static void hip04_mcpm_power_down(void)
+ 		hip04_set_snoop_filter(cluster, 0);
+ 		__mcpm_outbound_leave_critical(cluster, CLUSTER_DOWN);
+ 	} else {
+-		spin_unlock(&boot_lock);
++		raw_spin_unlock(&boot_lock);
+ 		v7_exit_coherency_flush(louis);
+ 	}
+ 
+@@ -192,7 +192,7 @@ static int hip04_mcpm_wait_for_powerdown(unsigned int cpu, unsigned int cluster)
+ 	       cpu >= HIP04_MAX_CPUS_PER_CLUSTER);
+ 
+ 	count = TIMEOUT_MSEC / POLL_MSEC;
+-	spin_lock_irq(&boot_lock);
++	raw_spin_lock_irq(&boot_lock);
+ 	for (tries = 0; tries < count; tries++) {
+ 		if (hip04_cpu_table[cluster][cpu]) {
+ 			ret = -EBUSY;
+@@ -202,10 +202,10 @@ static int hip04_mcpm_wait_for_powerdown(unsigned int cpu, unsigned int cluster)
+ 		data = readl_relaxed(sysctrl + SC_CPU_RESET_STATUS(cluster));
+ 		if (data & CORE_WFI_STATUS(cpu))
+ 			break;
+-		spin_unlock_irq(&boot_lock);
++		raw_spin_unlock_irq(&boot_lock);
+ 		/* Wait for clean L2 when the whole cluster is down. */
+ 		msleep(POLL_MSEC);
+-		spin_lock_irq(&boot_lock);
++		raw_spin_lock_irq(&boot_lock);
+ 	}
+ 	if (tries >= count)
+ 		goto err;
+@@ -220,10 +220,10 @@ static int hip04_mcpm_wait_for_powerdown(unsigned int cpu, unsigned int cluster)
+ 	}
+ 	if (tries >= count)
+ 		goto err;
+-	spin_unlock_irq(&boot_lock);
++	raw_spin_unlock_irq(&boot_lock);
+ 	return 0;
+ err:
+-	spin_unlock_irq(&boot_lock);
++	raw_spin_unlock_irq(&boot_lock);
+ 	return ret;
+ }
+ 
+@@ -235,10 +235,10 @@ static void hip04_mcpm_powered_up(void)
+ 	cpu = MPIDR_AFFINITY_LEVEL(mpidr, 0);
+ 	cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);
+ 
+-	spin_lock(&boot_lock);
++	raw_spin_lock(&boot_lock);
+ 	if (!hip04_cpu_table[cluster][cpu])
+ 		hip04_cpu_table[cluster][cpu] = 1;
+-	spin_unlock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ }
+ 
+ static void __naked hip04_mcpm_power_up_setup(unsigned int affinity_level)
+diff --git a/arch/arm/mach-omap2/omap-smp.c b/arch/arm/mach-omap2/omap-smp.c
+index 5305ec7341ec..19732b56088b 100644
+--- a/arch/arm/mach-omap2/omap-smp.c
++++ b/arch/arm/mach-omap2/omap-smp.c
+@@ -43,7 +43,7 @@
+ /* SCU base address */
+ static void __iomem *scu_base;
+ 
+-static DEFINE_SPINLOCK(boot_lock);
++static DEFINE_RAW_SPINLOCK(boot_lock);
+ 
+ void __iomem *omap4_get_scu_base(void)
+ {
+@@ -74,8 +74,8 @@ static void omap4_secondary_init(unsigned int cpu)
+ 	/*
+ 	 * Synchronise with the boot thread.
+ 	 */
+-	spin_lock(&boot_lock);
+-	spin_unlock(&boot_lock);
++	raw_spin_lock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ }
+ 
+ static int omap4_boot_secondary(unsigned int cpu, struct task_struct *idle)
+@@ -89,7 +89,7 @@ static int omap4_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * Set synchronisation state between this boot processor
+ 	 * and the secondary one
+ 	 */
+-	spin_lock(&boot_lock);
++	raw_spin_lock(&boot_lock);
+ 
+ 	/*
+ 	 * Update the AuxCoreBoot0 with boot state for secondary core.
+@@ -166,7 +166,7 @@ static int omap4_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * Now the secondary core is starting up let it run its
+ 	 * calibrations, then wait for it to finish
+ 	 */
+-	spin_unlock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ 
+ 	return 0;
+ }
+diff --git a/arch/arm/mach-prima2/platsmp.c b/arch/arm/mach-prima2/platsmp.c
+index 335c12e92262..2bb5796792b2 100644
+--- a/arch/arm/mach-prima2/platsmp.c
++++ b/arch/arm/mach-prima2/platsmp.c
+@@ -23,7 +23,7 @@
+ static void __iomem *scu_base;
+ static void __iomem *rsc_base;
+ 
+-static DEFINE_SPINLOCK(boot_lock);
++static DEFINE_RAW_SPINLOCK(boot_lock);
+ 
+ static struct map_desc scu_io_desc __initdata = {
+ 	.length		= SZ_4K,
+@@ -56,8 +56,8 @@ static void sirfsoc_secondary_init(unsigned int cpu)
+ 	/*
+ 	 * Synchronise with the boot thread.
+ 	 */
+-	spin_lock(&boot_lock);
+-	spin_unlock(&boot_lock);
++	raw_spin_lock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ }
+ 
+ static struct of_device_id rsc_ids[]  = {
+@@ -95,7 +95,7 @@ static int sirfsoc_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	/* make sure write buffer is drained */
+ 	mb();
+ 
+-	spin_lock(&boot_lock);
++	raw_spin_lock(&boot_lock);
+ 
+ 	/*
+ 	 * The secondary processor is waiting to be released from
+@@ -127,7 +127,7 @@ static int sirfsoc_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * now the secondary core is starting up let it run its
+ 	 * calibrations, then wait for it to finish
+ 	 */
+-	spin_unlock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ 
+ 	return pen_release != -1 ? -ENOSYS : 0;
+ }
+diff --git a/arch/arm/mach-qcom/platsmp.c b/arch/arm/mach-qcom/platsmp.c
+index d6908569ecaf..80c5e4fc5910 100644
+--- a/arch/arm/mach-qcom/platsmp.c
++++ b/arch/arm/mach-qcom/platsmp.c
+@@ -46,7 +46,7 @@
+ 
+ extern void secondary_startup(void);
+ 
+-static DEFINE_SPINLOCK(boot_lock);
++static DEFINE_RAW_SPINLOCK(boot_lock);
+ 
+ #ifdef CONFIG_HOTPLUG_CPU
+ static void __ref qcom_cpu_die(unsigned int cpu)
+@@ -60,8 +60,8 @@ static void qcom_secondary_init(unsigned int cpu)
+ 	/*
+ 	 * Synchronise with the boot thread.
+ 	 */
+-	spin_lock(&boot_lock);
+-	spin_unlock(&boot_lock);
++	raw_spin_lock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ }
+ 
+ static int scss_release_secondary(unsigned int cpu)
+@@ -284,7 +284,7 @@ static int qcom_boot_secondary(unsigned int cpu, int (*func)(unsigned int))
+ 	 * set synchronisation state between this boot processor
+ 	 * and the secondary one
+ 	 */
+-	spin_lock(&boot_lock);
++	raw_spin_lock(&boot_lock);
+ 
+ 	/*
+ 	 * Send the secondary CPU a soft interrupt, thereby causing
+@@ -297,7 +297,7 @@ static int qcom_boot_secondary(unsigned int cpu, int (*func)(unsigned int))
+ 	 * now the secondary core is starting up let it run its
+ 	 * calibrations, then wait for it to finish
+ 	 */
+-	spin_unlock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ 
+ 	return ret;
+ }
+diff --git a/arch/arm/mach-spear/platsmp.c b/arch/arm/mach-spear/platsmp.c
+index fd4297713d67..b0553b2c2d53 100644
+--- a/arch/arm/mach-spear/platsmp.c
++++ b/arch/arm/mach-spear/platsmp.c
+@@ -32,7 +32,7 @@ static void write_pen_release(int val)
+ 	sync_cache_w(&pen_release);
+ }
+ 
+-static DEFINE_SPINLOCK(boot_lock);
++static DEFINE_RAW_SPINLOCK(boot_lock);
+ 
+ static void __iomem *scu_base = IOMEM(VA_SCU_BASE);
+ 
+@@ -47,8 +47,8 @@ static void spear13xx_secondary_init(unsigned int cpu)
+ 	/*
+ 	 * Synchronise with the boot thread.
+ 	 */
+-	spin_lock(&boot_lock);
+-	spin_unlock(&boot_lock);
++	raw_spin_lock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ }
+ 
+ static int spear13xx_boot_secondary(unsigned int cpu, struct task_struct *idle)
+@@ -59,7 +59,7 @@ static int spear13xx_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * set synchronisation state between this boot processor
+ 	 * and the secondary one
+ 	 */
+-	spin_lock(&boot_lock);
++	raw_spin_lock(&boot_lock);
+ 
+ 	/*
+ 	 * The secondary processor is waiting to be released from
+@@ -84,7 +84,7 @@ static int spear13xx_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * now the secondary core is starting up let it run its
+ 	 * calibrations, then wait for it to finish
+ 	 */
+-	spin_unlock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ 
+ 	return pen_release != -1 ? -ENOSYS : 0;
+ }
+diff --git a/arch/arm/mach-sti/platsmp.c b/arch/arm/mach-sti/platsmp.c
+index d4b624f8dfcb..56d4028122f5 100644
+--- a/arch/arm/mach-sti/platsmp.c
++++ b/arch/arm/mach-sti/platsmp.c
+@@ -34,7 +34,7 @@ static void write_pen_release(int val)
+ 	sync_cache_w(&pen_release);
+ }
+ 
+-static DEFINE_SPINLOCK(boot_lock);
++static DEFINE_RAW_SPINLOCK(boot_lock);
+ 
+ static void sti_secondary_init(unsigned int cpu)
+ {
+@@ -49,8 +49,8 @@ static void sti_secondary_init(unsigned int cpu)
+ 	/*
+ 	 * Synchronise with the boot thread.
+ 	 */
+-	spin_lock(&boot_lock);
+-	spin_unlock(&boot_lock);
++	raw_spin_lock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ }
+ 
+ static int sti_boot_secondary(unsigned int cpu, struct task_struct *idle)
+@@ -61,7 +61,7 @@ static int sti_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * set synchronisation state between this boot processor
+ 	 * and the secondary one
+ 	 */
+-	spin_lock(&boot_lock);
++	raw_spin_lock(&boot_lock);
+ 
+ 	/*
+ 	 * The secondary processor is waiting to be released from
+@@ -92,7 +92,7 @@ static int sti_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * now the secondary core is starting up let it run its
+ 	 * calibrations, then wait for it to finish
+ 	 */
+-	spin_unlock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ 
+ 	return pen_release != -1 ? -ENOSYS : 0;
+ }
+diff --git a/arch/arm/mach-ux500/platsmp.c b/arch/arm/mach-ux500/platsmp.c
+index a44967f3168c..3af22a4836bf 100644
+--- a/arch/arm/mach-ux500/platsmp.c
++++ b/arch/arm/mach-ux500/platsmp.c
+@@ -51,7 +51,7 @@ static void __iomem *scu_base_addr(void)
+ 	return NULL;
+ }
+ 
+-static DEFINE_SPINLOCK(boot_lock);
++static DEFINE_RAW_SPINLOCK(boot_lock);
+ 
+ static void ux500_secondary_init(unsigned int cpu)
+ {
+@@ -64,8 +64,8 @@ static void ux500_secondary_init(unsigned int cpu)
+ 	/*
+ 	 * Synchronise with the boot thread.
+ 	 */
+-	spin_lock(&boot_lock);
+-	spin_unlock(&boot_lock);
++	raw_spin_lock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ }
+ 
+ static int ux500_boot_secondary(unsigned int cpu, struct task_struct *idle)
+@@ -76,7 +76,7 @@ static int ux500_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * set synchronisation state between this boot processor
+ 	 * and the secondary one
+ 	 */
+-	spin_lock(&boot_lock);
++	raw_spin_lock(&boot_lock);
+ 
+ 	/*
+ 	 * The secondary processor is waiting to be released from
+@@ -97,7 +97,7 @@ static int ux500_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * now the secondary core is starting up let it run its
+ 	 * calibrations, then wait for it to finish
+ 	 */
+-	spin_unlock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ 
+ 	return pen_release != -1 ? -ENOSYS : 0;
+ }
+diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
+index eb8830a4c5ed..c15d2a0826c6 100644
+--- a/arch/arm/mm/fault.c
++++ b/arch/arm/mm/fault.c
+@@ -277,7 +277,7 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
+ 	 * If we're in an interrupt or have no user
+ 	 * context, we must not take the fault..
+ 	 */
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto no_context;
+ 
+ 	if (user_mode(regs))
+@@ -431,6 +431,9 @@ do_translation_fault(unsigned long addr, unsigned int fsr,
+ 	if (addr < TASK_SIZE)
+ 		return do_page_fault(addr, fsr, regs);
+ 
++	if (interrupts_enabled(regs))
++		local_irq_enable();
++
+ 	if (user_mode(regs))
+ 		goto bad_area;
+ 
+@@ -498,6 +501,9 @@ do_translation_fault(unsigned long addr, unsigned int fsr,
+ static int
+ do_sect_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
+ {
++	if (interrupts_enabled(regs))
++		local_irq_enable();
++
+ 	do_bad_area(addr, fsr, regs);
+ 	return 0;
+ }
+diff --git a/arch/arm/mm/highmem.c b/arch/arm/mm/highmem.c
+index e17ed00828d7..f8b4c3bd38ff 100644
+--- a/arch/arm/mm/highmem.c
++++ b/arch/arm/mm/highmem.c
+@@ -53,6 +53,7 @@ EXPORT_SYMBOL(kunmap);
+ 
+ void *kmap_atomic(struct page *page)
+ {
++	pte_t pte = mk_pte(page, kmap_prot);
+ 	unsigned int idx;
+ 	unsigned long vaddr;
+ 	void *kmap;
+@@ -91,7 +92,10 @@ void *kmap_atomic(struct page *page)
+ 	 * in place, so the contained TLB flush ensures the TLB is updated
+ 	 * with the new mapping.
+ 	 */
+-	set_fixmap_pte(idx, mk_pte(page, kmap_prot));
++#ifdef CONFIG_PREEMPT_RT_FULL
++	current->kmap_pte[type] = pte;
++#endif
++	set_fixmap_pte(idx, pte);
+ 
+ 	return (void *)vaddr;
+ }
+@@ -108,12 +112,15 @@ void __kunmap_atomic(void *kvaddr)
+ 
+ 		if (cache_is_vivt())
+ 			__cpuc_flush_dcache_area((void *)vaddr, PAGE_SIZE);
++#ifdef CONFIG_PREEMPT_RT_FULL
++		current->kmap_pte[type] = __pte(0);
++#endif
+ #ifdef CONFIG_DEBUG_HIGHMEM
+ 		BUG_ON(vaddr != __fix_to_virt(idx));
+-		set_fixmap_pte(idx, __pte(0));
+ #else
+ 		(void) idx;  /* to kill a warning */
+ #endif
++		set_fixmap_pte(idx, __pte(0));
+ 		kmap_atomic_idx_pop();
+ 	} else if (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {
+ 		/* this address was obtained through kmap_high_get() */
+@@ -125,6 +132,7 @@ EXPORT_SYMBOL(__kunmap_atomic);
+ 
+ void *kmap_atomic_pfn(unsigned long pfn)
+ {
++	pte_t pte = pfn_pte(pfn, kmap_prot);
+ 	unsigned long vaddr;
+ 	int idx, type;
+ 	struct page *page = pfn_to_page(pfn);
+@@ -139,7 +147,10 @@ void *kmap_atomic_pfn(unsigned long pfn)
+ #ifdef CONFIG_DEBUG_HIGHMEM
+ 	BUG_ON(!pte_none(*(fixmap_page_table + idx)));
+ #endif
+-	set_fixmap_pte(idx, pfn_pte(pfn, kmap_prot));
++#ifdef CONFIG_PREEMPT_RT_FULL
++	current->kmap_pte[type] = pte;
++#endif
++	set_fixmap_pte(idx, pte);
+ 
+ 	return (void *)vaddr;
+ }
+@@ -153,3 +164,28 @@ struct page *kmap_atomic_to_page(const void *ptr)
+ 
+ 	return pte_page(get_fixmap_pte(vaddr));
+ }
++
++#if defined CONFIG_PREEMPT_RT_FULL
++void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p)
++{
++	int i;
++
++	/*
++	 * Clear @prev's kmap_atomic mappings
++	 */
++	for (i = 0; i < prev_p->kmap_idx; i++) {
++		int idx = i + KM_TYPE_NR * smp_processor_id();
++
++		set_fixmap_pte(idx, __pte(0));
++	}
++	/*
++	 * Restore @next_p's kmap_atomic mappings
++	 */
++	for (i = 0; i < next_p->kmap_idx; i++) {
++		int idx = i + KM_TYPE_NR * smp_processor_id();
++
++		if (!pte_none(next_p->kmap_pte[i]))
++			set_fixmap_pte(idx, next_p->kmap_pte[i]);
++	}
++}
++#endif
+diff --git a/arch/arm/plat-versatile/platsmp.c b/arch/arm/plat-versatile/platsmp.c
+index 53feb90c840c..b4a8d54fc3f3 100644
+--- a/arch/arm/plat-versatile/platsmp.c
++++ b/arch/arm/plat-versatile/platsmp.c
+@@ -30,7 +30,7 @@ static void write_pen_release(int val)
+ 	sync_cache_w(&pen_release);
+ }
+ 
+-static DEFINE_SPINLOCK(boot_lock);
++static DEFINE_RAW_SPINLOCK(boot_lock);
+ 
+ void versatile_secondary_init(unsigned int cpu)
+ {
+@@ -43,8 +43,8 @@ void versatile_secondary_init(unsigned int cpu)
+ 	/*
+ 	 * Synchronise with the boot thread.
+ 	 */
+-	spin_lock(&boot_lock);
+-	spin_unlock(&boot_lock);
++	raw_spin_lock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ }
+ 
+ int versatile_boot_secondary(unsigned int cpu, struct task_struct *idle)
+@@ -55,7 +55,7 @@ int versatile_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * Set synchronisation state between this boot processor
+ 	 * and the secondary one
+ 	 */
+-	spin_lock(&boot_lock);
++	raw_spin_lock(&boot_lock);
+ 
+ 	/*
+ 	 * This is really belt and braces; we hold unintended secondary
+@@ -85,7 +85,7 @@ int versatile_boot_secondary(unsigned int cpu, struct task_struct *idle)
+ 	 * now the secondary core is starting up let it run its
+ 	 * calibrations, then wait for it to finish
+ 	 */
+-	spin_unlock(&boot_lock);
++	raw_spin_unlock(&boot_lock);
+ 
+ 	return pen_release != -1 ? -ENOSYS : 0;
+ }
+diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
+index 00b9c4870230..668c6fe55769 100644
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -59,8 +59,10 @@ config ARM64
+ 	select HAVE_PERF_REGS
+ 	select HAVE_PERF_USER_STACK_DUMP
+ 	select HAVE_RCU_TABLE_FREE
++	select HAVE_PREEMPT_LAZY
+ 	select HAVE_SYSCALL_TRACEPOINTS
+ 	select IRQ_DOMAIN
++	select IRQ_FORCED_THREADING
+ 	select MODULES_USE_ELF_RELA
+ 	select NO_BOOTMEM
+ 	select OF
+diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
+index 459bf8e53208..3e24ea15c05c 100644
+--- a/arch/arm64/include/asm/thread_info.h
++++ b/arch/arm64/include/asm/thread_info.h
+@@ -50,6 +50,7 @@ struct thread_info {
+ 	struct exec_domain	*exec_domain;	/* execution domain */
+ 	struct restart_block	restart_block;
+ 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
++	int			preempt_lazy_count;	/* 0 => preemptable, <0 => bug */
+ 	int			cpu;		/* cpu */
+ };
+ 
+@@ -108,6 +109,7 @@ static inline struct thread_info *current_thread_info(void)
+ #define TIF_NEED_RESCHED	1
+ #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
+ #define TIF_FOREIGN_FPSTATE	3	/* CPU's FP state is not current's */
++#define TIF_NEED_RESCHED_LAZY	4
+ #define TIF_NOHZ		7
+ #define TIF_SYSCALL_TRACE	8
+ #define TIF_SYSCALL_AUDIT	9
+@@ -124,6 +126,7 @@ static inline struct thread_info *current_thread_info(void)
+ #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
+ #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
+ #define _TIF_FOREIGN_FPSTATE	(1 << TIF_FOREIGN_FPSTATE)
++#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
+ #define _TIF_NOHZ		(1 << TIF_NOHZ)
+ #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
+ #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
+diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
+index 9a9fce090d58..f77413646dba 100644
+--- a/arch/arm64/kernel/asm-offsets.c
++++ b/arch/arm64/kernel/asm-offsets.c
+@@ -36,6 +36,7 @@ int main(void)
+   BLANK();
+   DEFINE(TI_FLAGS,		offsetof(struct thread_info, flags));
+   DEFINE(TI_PREEMPT,		offsetof(struct thread_info, preempt_count));
++  DEFINE(TI_PREEMPT_LAZY,	offsetof(struct thread_info, preempt_lazy_count));
+   DEFINE(TI_ADDR_LIMIT,		offsetof(struct thread_info, addr_limit));
+   DEFINE(TI_TASK,		offsetof(struct thread_info, task));
+   DEFINE(TI_EXEC_DOMAIN,	offsetof(struct thread_info, exec_domain));
+diff --git a/arch/arm64/kernel/debug-monitors.c b/arch/arm64/kernel/debug-monitors.c
+index 62c91b3b42e8..1e22f0837df4 100644
+--- a/arch/arm64/kernel/debug-monitors.c
++++ b/arch/arm64/kernel/debug-monitors.c
+@@ -183,20 +183,21 @@ static void clear_regs_spsr_ss(struct pt_regs *regs)
+ 
+ /* EL1 Single Step Handler hooks */
+ static LIST_HEAD(step_hook);
+-static DEFINE_RWLOCK(step_hook_lock);
++static DEFINE_SPINLOCK(step_hook_lock);
+ 
+ void register_step_hook(struct step_hook *hook)
+ {
+-	write_lock(&step_hook_lock);
+-	list_add(&hook->node, &step_hook);
+-	write_unlock(&step_hook_lock);
++	spin_lock(&step_hook_lock);
++	list_add_rcu(&hook->node, &step_hook);
++	spin_unlock(&step_hook_lock);
+ }
+ 
+ void unregister_step_hook(struct step_hook *hook)
+ {
+-	write_lock(&step_hook_lock);
+-	list_del(&hook->node);
+-	write_unlock(&step_hook_lock);
++	spin_lock(&step_hook_lock);
++	list_del_rcu(&hook->node);
++	spin_unlock(&step_hook_lock);
++	synchronize_rcu();
+ }
+ 
+ /*
+@@ -210,15 +211,15 @@ static int call_step_hook(struct pt_regs *regs, unsigned int esr)
+ 	struct step_hook *hook;
+ 	int retval = DBG_HOOK_ERROR;
+ 
+-	read_lock(&step_hook_lock);
++	rcu_read_lock();
+ 
+-	list_for_each_entry(hook, &step_hook, node)	{
++	list_for_each_entry_rcu(hook, &step_hook, node)	{
+ 		retval = hook->fn(regs, esr);
+ 		if (retval == DBG_HOOK_HANDLED)
+ 			break;
+ 	}
+ 
+-	read_unlock(&step_hook_lock);
++	rcu_read_unlock();
+ 
+ 	return retval;
+ }
+diff --git a/arch/arm64/kernel/entry.S b/arch/arm64/kernel/entry.S
+index b6bb97026fdd..b8a7b7d9acea 100644
+--- a/arch/arm64/kernel/entry.S
++++ b/arch/arm64/kernel/entry.S
+@@ -369,11 +369,16 @@ el1_irq:
+ #ifdef CONFIG_PREEMPT
+ 	get_thread_info tsk
+ 	ldr	w24, [tsk, #TI_PREEMPT]		// get preempt count
+-	cbnz	w24, 1f				// preempt count != 0
++	cbnz	w24, 2f				// preempt count != 0
+ 	ldr	x0, [tsk, #TI_FLAGS]		// get flags
+-	tbz	x0, #TIF_NEED_RESCHED, 1f	// needs rescheduling?
+-	bl	el1_preempt
++	tbnz	x0, #TIF_NEED_RESCHED, 1f	// needs rescheduling?
++
++	ldr	w24, [tsk, #TI_PREEMPT_LAZY]	// get preempt lazy count
++	cbnz	w24, 2f				// preempt lazy count != 0
++	tbz	x0, #TIF_NEED_RESCHED_LAZY, 2f	// needs rescheduling?
+ 1:
++	bl	el1_preempt
++2:
+ #endif
+ #ifdef CONFIG_TRACE_IRQFLAGS
+ 	bl	trace_hardirqs_on
+@@ -387,6 +392,7 @@ el1_preempt:
+ 1:	bl	preempt_schedule_irq		// irq en/disable is done inside
+ 	ldr	x0, [tsk, #TI_FLAGS]		// get new tasks TI_FLAGS
+ 	tbnz	x0, #TIF_NEED_RESCHED, 1b	// needs rescheduling?
++	tbnz	x0, #TIF_NEED_RESCHED_LAZY, 1b	// needs rescheduling?
+ 	ret	x24
+ #endif
+ 
+@@ -624,6 +630,7 @@ fast_work_pending:
+ 	str	x0, [sp, #S_X0]			// returned x0
+ work_pending:
+ 	tbnz	x1, #TIF_NEED_RESCHED, work_resched
++	tbnz	x1, #TIF_NEED_RESCHED_LAZY, work_resched
+ 	/* TIF_SIGPENDING, TIF_NOTIFY_RESUME or TIF_FOREIGN_FPSTATE case */
+ 	ldr	x2, [sp, #S_PSTATE]
+ 	mov	x0, sp				// 'regs'
+diff --git a/arch/arm64/kernel/perf_event.c b/arch/arm64/kernel/perf_event.c
+index 78a5894b1621..ea465b973599 100644
+--- a/arch/arm64/kernel/perf_event.c
++++ b/arch/arm64/kernel/perf_event.c
+@@ -470,7 +470,7 @@ armpmu_reserve_hardware(struct arm_pmu *armpmu)
+ 			}
+ 
+ 			err = request_irq(irq, armpmu->handle_irq,
+-					IRQF_NOBALANCING,
++					IRQF_NOBALANCING | IRQF_NO_THREAD,
+ 					"arm-pmu", armpmu);
+ 			if (err) {
+ 				pr_err("unable to request IRQ%d for ARM PMU counters\n",
+diff --git a/arch/avr32/mm/fault.c b/arch/avr32/mm/fault.c
+index d223a8b57c1e..8a7fb9343cbf 100644
+--- a/arch/avr32/mm/fault.c
++++ b/arch/avr32/mm/fault.c
+@@ -81,7 +81,7 @@ asmlinkage void do_page_fault(unsigned long ecr, struct pt_regs *regs)
+ 	 * If we're in an interrupt or have no user context, we must
+ 	 * not take the fault...
+ 	 */
+-	if (in_atomic() || !mm || regs->sr & SYSREG_BIT(GM))
++	if (!mm || regs->sr & SYSREG_BIT(GM) || pagefault_disabled())
+ 		goto no_context;
+ 
+ 	local_irq_enable();
+diff --git a/arch/cris/mm/fault.c b/arch/cris/mm/fault.c
+index 2686a7aa8ec8..b1a5631d95bf 100644
+--- a/arch/cris/mm/fault.c
++++ b/arch/cris/mm/fault.c
+@@ -113,7 +113,7 @@ do_page_fault(unsigned long address, struct pt_regs *regs,
+ 	 * user context, we must not take the fault.
+ 	 */
+ 
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto no_context;
+ 
+ 	if (user_mode(regs))
+diff --git a/arch/frv/mm/fault.c b/arch/frv/mm/fault.c
+index ec4917ddf678..c90d23f50579 100644
+--- a/arch/frv/mm/fault.c
++++ b/arch/frv/mm/fault.c
+@@ -78,7 +78,7 @@ asmlinkage void do_page_fault(int datammu, unsigned long esr0, unsigned long ear
+ 	 * If we're in an interrupt or have no user
+ 	 * context, we must not take the fault..
+ 	 */
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto no_context;
+ 
+ 	if (user_mode(__frame))
+diff --git a/arch/ia64/mm/fault.c b/arch/ia64/mm/fault.c
+index ba5ba7accd0d..f9f7d083cfb4 100644
+--- a/arch/ia64/mm/fault.c
++++ b/arch/ia64/mm/fault.c
+@@ -96,7 +96,7 @@ ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *re
+ 	/*
+ 	 * If we're in an interrupt or have no user context, we must not take the fault..
+ 	 */
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto no_context;
+ 
+ #ifdef CONFIG_VIRTUAL_MEM_MAP
+diff --git a/arch/m32r/mm/fault.c b/arch/m32r/mm/fault.c
+index e3d4d4890104..7069ad371f72 100644
+--- a/arch/m32r/mm/fault.c
++++ b/arch/m32r/mm/fault.c
+@@ -114,7 +114,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code,
+ 	 * If we're in an interrupt or have no user context or are running in an
+ 	 * atomic region then we must not take the fault..
+ 	 */
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto bad_area_nosemaphore;
+ 
+ 	if (error_code & ACE_USERMODE)
+diff --git a/arch/m68k/mm/fault.c b/arch/m68k/mm/fault.c
+index b2f04aee46ec..5de2621aab37 100644
+--- a/arch/m68k/mm/fault.c
++++ b/arch/m68k/mm/fault.c
+@@ -81,7 +81,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,
+ 	 * If we're in an interrupt or have no user
+ 	 * context, we must not take the fault..
+ 	 */
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto no_context;
+ 
+ 	if (user_mode(regs))
+diff --git a/arch/microblaze/mm/fault.c b/arch/microblaze/mm/fault.c
+index d46a5ebb7570..914ae86b8c96 100644
+--- a/arch/microblaze/mm/fault.c
++++ b/arch/microblaze/mm/fault.c
+@@ -107,7 +107,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long address,
+ 	if ((error_code & 0x13) == 0x13 || (error_code & 0x11) == 0x11)
+ 		is_write = 0;
+ 
+-	if (unlikely(in_atomic() || !mm)) {
++	if (unlikely(!mm || pagefault_disabled())) {
+ 		if (kernel_mode(regs))
+ 			goto bad_area_nosemaphore;
+ 
+diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
+index 9536ef912f59..d196a00b89f5 100644
+--- a/arch/mips/Kconfig
++++ b/arch/mips/Kconfig
+@@ -2196,7 +2196,7 @@ config CPU_R4400_WORKAROUNDS
+ #
+ config HIGHMEM
+ 	bool "High Memory Support"
+-	depends on 32BIT && CPU_SUPPORTS_HIGHMEM && SYS_SUPPORTS_HIGHMEM && !CPU_MIPS32_3_5_EVA
++	depends on 32BIT && CPU_SUPPORTS_HIGHMEM && SYS_SUPPORTS_HIGHMEM && !CPU_MIPS32_3_5_EVA && !PREEMPT_RT_FULL
+ 
+ config CPU_SUPPORTS_HIGHMEM
+ 	bool
+diff --git a/arch/mips/kernel/signal.c b/arch/mips/kernel/signal.c
+index 16f1e4f2bf3c..00ede39a386b 100644
+--- a/arch/mips/kernel/signal.c
++++ b/arch/mips/kernel/signal.c
+@@ -613,6 +613,7 @@ asmlinkage void do_notify_resume(struct pt_regs *regs, void *unused,
+ 	__u32 thread_info_flags)
+ {
+ 	local_irq_enable();
++	preempt_check_resched();
+ 
+ 	user_exit();
+ 
+diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c
+index 70ab5d664332..5b41b84d9092 100644
+--- a/arch/mips/mm/fault.c
++++ b/arch/mips/mm/fault.c
+@@ -89,7 +89,7 @@ static void __kprobes __do_page_fault(struct pt_regs *regs, unsigned long write,
+ 	 * If we're in an interrupt or have no user
+ 	 * context, we must not take the fault..
+ 	 */
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto bad_area_nosemaphore;
+ 
+ 	if (user_mode(regs))
+diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
+index f42e35e42790..e09dae6ce80d 100644
+--- a/arch/mips/mm/init.c
++++ b/arch/mips/mm/init.c
+@@ -90,7 +90,7 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
+ 
+ 	BUG_ON(Page_dcache_dirty(page));
+ 
+-	pagefault_disable();
++	raw_pagefault_disable();
+ 	idx = (addr >> PAGE_SHIFT) & (FIX_N_COLOURS - 1);
+ 	idx += in_interrupt() ? FIX_N_COLOURS : 0;
+ 	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
+@@ -146,7 +146,7 @@ void kunmap_coherent(void)
+ 	tlbw_use_hazard();
+ 	write_c0_entryhi(old_ctx);
+ 	local_irq_restore(flags);
+-	pagefault_enable();
++	raw_pagefault_enable();
+ }
+ 
+ void copy_user_highpage(struct page *to, struct page *from,
+diff --git a/arch/mn10300/mm/fault.c b/arch/mn10300/mm/fault.c
+index 0c2cc5d39c8e..54305d255ced 100644
+--- a/arch/mn10300/mm/fault.c
++++ b/arch/mn10300/mm/fault.c
+@@ -168,7 +168,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long fault_code,
+ 	 * If we're in an interrupt or have no user
+ 	 * context, we must not take the fault..
+ 	 */
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto no_context;
+ 
+ 	if ((fault_code & MMUFCR_xFC_ACCESS) == MMUFCR_xFC_ACCESS_USR)
+diff --git a/arch/parisc/mm/fault.c b/arch/parisc/mm/fault.c
+index 3b7c02f9b726..95475ce74310 100644
+--- a/arch/parisc/mm/fault.c
++++ b/arch/parisc/mm/fault.c
+@@ -208,7 +208,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long code,
+ 	int fault;
+ 	unsigned int flags;
+ 
+-	if (in_atomic())
++	if (pagefault_disabled())
+ 		goto no_context;
+ 
+ 	tsk = current;
+diff --git a/arch/powerpc/Kconfig b/arch/powerpc/Kconfig
+index 88eace4e28c3..5e0337b11d52 100644
+--- a/arch/powerpc/Kconfig
++++ b/arch/powerpc/Kconfig
+@@ -60,10 +60,11 @@ config LOCKDEP_SUPPORT
+ 
+ config RWSEM_GENERIC_SPINLOCK
+ 	bool
++	default y if PREEMPT_RT_FULL
+ 
+ config RWSEM_XCHGADD_ALGORITHM
+ 	bool
+-	default y
++	default y if !PREEMPT_RT_FULL
+ 
+ config GENERIC_LOCKBREAK
+ 	bool
+@@ -136,6 +137,7 @@ config PPC
+ 	select ARCH_HAS_TICK_BROADCAST if GENERIC_CLOCKEVENTS_BROADCAST
+ 	select GENERIC_STRNCPY_FROM_USER
+ 	select GENERIC_STRNLEN_USER
++	select HAVE_PREEMPT_LAZY
+ 	select HAVE_MOD_ARCH_SPECIFIC
+ 	select MODULES_USE_ELF_RELA
+ 	select CLONE_BACKWARDS
+@@ -303,7 +305,7 @@ menu "Kernel options"
+ 
+ config HIGHMEM
+ 	bool "High memory support"
+-	depends on PPC32
++	depends on PPC32 && !PREEMPT_RT_FULL
+ 
+ source kernel/Kconfig.hz
+ source kernel/Kconfig.preempt
+diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h
+index 9bb51b9e03c6..8f6428c12180 100644
+--- a/arch/powerpc/include/asm/kvm_host.h
++++ b/arch/powerpc/include/asm/kvm_host.h
+@@ -296,7 +296,7 @@ struct kvmppc_vcore {
+ 	u8 in_guest;
+ 	struct list_head runnable_threads;
+ 	spinlock_t lock;
+-	wait_queue_head_t wq;
++	struct swait_head wq;
+ 	u64 stolen_tb;
+ 	u64 preempt_tb;
+ 	struct kvm_vcpu *runner;
+@@ -619,7 +619,7 @@ struct kvm_vcpu_arch {
+ 	u8 prodded;
+ 	u32 last_inst;
+ 
+-	wait_queue_head_t *wqp;
++	struct swait_head *wqp;
+ 	struct kvmppc_vcore *vcore;
+ 	int ret;
+ 	int trap;
+diff --git a/arch/powerpc/include/asm/thread_info.h b/arch/powerpc/include/asm/thread_info.h
+index b034ecdb7c74..16ff509e28df 100644
+--- a/arch/powerpc/include/asm/thread_info.h
++++ b/arch/powerpc/include/asm/thread_info.h
+@@ -43,6 +43,8 @@ struct thread_info {
+ 	int		cpu;			/* cpu we're on */
+ 	int		preempt_count;		/* 0 => preemptable,
+ 						   <0 => BUG */
++	int		preempt_lazy_count;	/* 0 => preemptable,
++						   <0 => BUG */
+ 	struct restart_block restart_block;
+ 	unsigned long	local_flags;		/* private flags for thread */
+ 
+@@ -88,8 +90,7 @@ static inline struct thread_info *current_thread_info(void)
+ #define TIF_SYSCALL_TRACE	0	/* syscall trace active */
+ #define TIF_SIGPENDING		1	/* signal pending */
+ #define TIF_NEED_RESCHED	2	/* rescheduling necessary */
+-#define TIF_POLLING_NRFLAG	3	/* true if poll_idle() is polling
+-					   TIF_NEED_RESCHED */
++#define TIF_NEED_RESCHED_LAZY	3	/* lazy rescheduling necessary */
+ #define TIF_32BIT		4	/* 32 bit binary */
+ #define TIF_RESTORE_TM		5	/* need to restore TM FP/VEC/VSX */
+ #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
+@@ -107,6 +108,8 @@ static inline struct thread_info *current_thread_info(void)
+ #if defined(CONFIG_PPC64)
+ #define TIF_ELF2ABI		18	/* function descriptors must die! */
+ #endif
++#define TIF_POLLING_NRFLAG	19	/* true if poll_idle() is polling
++					   TIF_NEED_RESCHED */
+ 
+ /* as above, but as bit values */
+ #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
+@@ -125,14 +128,16 @@ static inline struct thread_info *current_thread_info(void)
+ #define _TIF_SYSCALL_TRACEPOINT	(1<<TIF_SYSCALL_TRACEPOINT)
+ #define _TIF_EMULATE_STACK_STORE	(1<<TIF_EMULATE_STACK_STORE)
+ #define _TIF_NOHZ		(1<<TIF_NOHZ)
++#define _TIF_NEED_RESCHED_LAZY	(1<<TIF_NEED_RESCHED_LAZY)
+ #define _TIF_SYSCALL_T_OR_A	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
+ 				 _TIF_SECCOMP | _TIF_SYSCALL_TRACEPOINT | \
+ 				 _TIF_NOHZ)
+ 
+ #define _TIF_USER_WORK_MASK	(_TIF_SIGPENDING | _TIF_NEED_RESCHED | \
+ 				 _TIF_NOTIFY_RESUME | _TIF_UPROBE | \
+-				 _TIF_RESTORE_TM)
++				 _TIF_RESTORE_TM | _TIF_NEED_RESCHED_LAZY)
+ #define _TIF_PERSYSCALL_MASK	(_TIF_RESTOREALL|_TIF_NOERROR)
++#define _TIF_NEED_RESCHED_MASK	(_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
+ 
+ /* Bits in local_flags */
+ /* Don't move TLF_NAPPING without adjusting the code in entry_32.S */
+diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
+index a892c651ac2d..5184b38f5b5e 100644
+--- a/arch/powerpc/kernel/asm-offsets.c
++++ b/arch/powerpc/kernel/asm-offsets.c
+@@ -159,6 +159,7 @@ int main(void)
+ 	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
+ 	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
+ 	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
++	DEFINE(TI_PREEMPT_LAZY, offsetof(struct thread_info, preempt_lazy_count));
+ 	DEFINE(TI_TASK, offsetof(struct thread_info, task));
+ 	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
+ 
+diff --git a/arch/powerpc/kernel/entry_32.S b/arch/powerpc/kernel/entry_32.S
+index 22b45a4955cd..081f926193f2 100644
+--- a/arch/powerpc/kernel/entry_32.S
++++ b/arch/powerpc/kernel/entry_32.S
+@@ -890,7 +890,14 @@ resume_kernel:
+ 	cmpwi	0,r0,0		/* if non-zero, just restore regs and return */
+ 	bne	restore
+ 	andi.	r8,r8,_TIF_NEED_RESCHED
++	bne+	1f
++	lwz	r0,TI_PREEMPT_LAZY(r9)
++	cmpwi	0,r0,0		/* if non-zero, just restore regs and return */
++	bne	restore
++	lwz	r0,TI_FLAGS(r9)
++	andi.	r0,r0,_TIF_NEED_RESCHED_LAZY
+ 	beq+	restore
++1:
+ 	lwz	r3,_MSR(r1)
+ 	andi.	r0,r3,MSR_EE	/* interrupts off? */
+ 	beq	restore		/* don't schedule if so */
+@@ -901,11 +908,11 @@ resume_kernel:
+ 	 */
+ 	bl	trace_hardirqs_off
+ #endif
+-1:	bl	preempt_schedule_irq
++2:	bl	preempt_schedule_irq
+ 	CURRENT_THREAD_INFO(r9, r1)
+ 	lwz	r3,TI_FLAGS(r9)
+-	andi.	r0,r3,_TIF_NEED_RESCHED
+-	bne-	1b
++	andi.	r0,r3,_TIF_NEED_RESCHED_MASK
++	bne-	2b
+ #ifdef CONFIG_TRACE_IRQFLAGS
+ 	/* And now, to properly rebalance the above, we tell lockdep they
+ 	 * are being turned back on, which will happen when we return
+@@ -1226,7 +1233,7 @@ global_dbcr0:
+ #endif /* !(CONFIG_4xx || CONFIG_BOOKE) */
+ 
+ do_work:			/* r10 contains MSR_KERNEL here */
+-	andi.	r0,r9,_TIF_NEED_RESCHED
++	andi.	r0,r9,_TIF_NEED_RESCHED_MASK
+ 	beq	do_user_signal
+ 
+ do_resched:			/* r10 contains MSR_KERNEL here */
+@@ -1247,7 +1254,7 @@ recheck:
+ 	MTMSRD(r10)		/* disable interrupts */
+ 	CURRENT_THREAD_INFO(r9, r1)
+ 	lwz	r9,TI_FLAGS(r9)
+-	andi.	r0,r9,_TIF_NEED_RESCHED
++	andi.	r0,r9,_TIF_NEED_RESCHED_MASK
+ 	bne-	do_resched
+ 	andi.	r0,r9,_TIF_USER_WORK_MASK
+ 	beq	restore_user
+diff --git a/arch/powerpc/kernel/entry_64.S b/arch/powerpc/kernel/entry_64.S
+index e233c0fec524..b7b673a578bd 100644
+--- a/arch/powerpc/kernel/entry_64.S
++++ b/arch/powerpc/kernel/entry_64.S
+@@ -644,7 +644,7 @@ _GLOBAL(ret_from_except_lite)
+ #else
+ 	beq	restore
+ #endif
+-1:	andi.	r0,r4,_TIF_NEED_RESCHED
++1:	andi.	r0,r4,_TIF_NEED_RESCHED_MASK
+ 	beq	2f
+ 	bl	restore_interrupts
+ 	SCHEDULE_USER
+@@ -706,10 +706,18 @@ resume_kernel:
+ 
+ #ifdef CONFIG_PREEMPT
+ 	/* Check if we need to preempt */
++	lwz	r8,TI_PREEMPT(r9)
++	cmpwi	0,r8,0		/* if non-zero, just restore regs and return */
++	bne	restore
+ 	andi.	r0,r4,_TIF_NEED_RESCHED
++	bne+	check_count
++
++	andi.	r0,r4,_TIF_NEED_RESCHED_LAZY
+ 	beq+	restore
++	lwz	r8,TI_PREEMPT_LAZY(r9)
++
+ 	/* Check that preempt_count() == 0 and interrupts are enabled */
+-	lwz	r8,TI_PREEMPT(r9)
++check_count:
+ 	cmpwi	cr1,r8,0
+ 	ld	r0,SOFTE(r1)
+ 	cmpdi	r0,0
+@@ -726,7 +734,7 @@ resume_kernel:
+ 	/* Re-test flags and eventually loop */
+ 	CURRENT_THREAD_INFO(r9, r1)
+ 	ld	r4,TI_FLAGS(r9)
+-	andi.	r0,r4,_TIF_NEED_RESCHED
++	andi.	r0,r4,_TIF_NEED_RESCHED_MASK
+ 	bne	1b
+ 
+ 	/*
+diff --git a/arch/powerpc/kernel/irq.c b/arch/powerpc/kernel/irq.c
+index c14383575fe8..ec023fd7bbde 100644
+--- a/arch/powerpc/kernel/irq.c
++++ b/arch/powerpc/kernel/irq.c
+@@ -615,6 +615,7 @@ void irq_ctx_init(void)
+ 	}
+ }
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ void do_softirq_own_stack(void)
+ {
+ 	struct thread_info *curtp, *irqtp;
+@@ -632,6 +633,7 @@ void do_softirq_own_stack(void)
+ 	if (irqtp->flags)
+ 		set_bits(irqtp->flags, &curtp->flags);
+ }
++#endif
+ 
+ irq_hw_number_t virq_to_hw(unsigned int virq)
+ {
+diff --git a/arch/powerpc/kernel/misc_32.S b/arch/powerpc/kernel/misc_32.S
+index 7c6bb4b17b49..e9dfe2270e93 100644
+--- a/arch/powerpc/kernel/misc_32.S
++++ b/arch/powerpc/kernel/misc_32.S
+@@ -40,6 +40,7 @@
+  * We store the saved ksp_limit in the unused part
+  * of the STACK_FRAME_OVERHEAD
+  */
++#ifndef CONFIG_PREEMPT_RT_FULL
+ _GLOBAL(call_do_softirq)
+ 	mflr	r0
+ 	stw	r0,4(r1)
+@@ -56,6 +57,7 @@ _GLOBAL(call_do_softirq)
+ 	stw	r10,THREAD+KSP_LIMIT(r2)
+ 	mtlr	r0
+ 	blr
++#endif
+ 
+ /*
+  * void call_do_irq(struct pt_regs *regs, struct thread_info *irqtp);
+diff --git a/arch/powerpc/kernel/misc_64.S b/arch/powerpc/kernel/misc_64.S
+index 4e314b90c75d..8a7238dd2f4b 100644
+--- a/arch/powerpc/kernel/misc_64.S
++++ b/arch/powerpc/kernel/misc_64.S
+@@ -29,6 +29,7 @@
+ 
+ 	.text
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ _GLOBAL(call_do_softirq)
+ 	mflr	r0
+ 	std	r0,16(r1)
+@@ -39,6 +40,7 @@ _GLOBAL(call_do_softirq)
+ 	ld	r0,16(r1)
+ 	mtlr	r0
+ 	blr
++#endif
+ 
+ _GLOBAL(call_do_irq)
+ 	mflr	r0
+diff --git a/arch/powerpc/kernel/time.c b/arch/powerpc/kernel/time.c
+index 7505599c2593..f4de021815ae 100644
+--- a/arch/powerpc/kernel/time.c
++++ b/arch/powerpc/kernel/time.c
+@@ -424,7 +424,7 @@ unsigned long profile_pc(struct pt_regs *regs)
+ EXPORT_SYMBOL(profile_pc);
+ #endif
+ 
+-#ifdef CONFIG_IRQ_WORK
++#if defined(CONFIG_IRQ_WORK)
+ 
+ /*
+  * 64-bit uses a byte in the PACA, 32-bit uses a per-cpu variable...
+diff --git a/arch/powerpc/kvm/Kconfig b/arch/powerpc/kvm/Kconfig
+index 602eb51d20bc..60fc1adab19f 100644
+--- a/arch/powerpc/kvm/Kconfig
++++ b/arch/powerpc/kvm/Kconfig
+@@ -157,6 +157,7 @@ config KVM_E500MC
+ config KVM_MPIC
+ 	bool "KVM in-kernel MPIC emulation"
+ 	depends on KVM && E500
++	depends on !PREEMPT_RT_FULL
+ 	select HAVE_KVM_IRQCHIP
+ 	select HAVE_KVM_IRQFD
+ 	select HAVE_KVM_IRQ_ROUTING
+diff --git a/arch/powerpc/kvm/book3s_hv.c b/arch/powerpc/kvm/book3s_hv.c
+index 060880ac7826..f114c926f6c0 100644
+--- a/arch/powerpc/kvm/book3s_hv.c
++++ b/arch/powerpc/kvm/book3s_hv.c
+@@ -84,11 +84,11 @@ static void kvmppc_fast_vcpu_kick_hv(struct kvm_vcpu *vcpu)
+ {
+ 	int me;
+ 	int cpu = vcpu->cpu;
+-	wait_queue_head_t *wqp;
++	struct swait_head *wqp;
+ 
+ 	wqp = kvm_arch_vcpu_wq(vcpu);
+-	if (waitqueue_active(wqp)) {
+-		wake_up_interruptible(wqp);
++	if (swaitqueue_active(wqp)) {
++		swait_wake_interruptible(wqp);
+ 		++vcpu->stat.halt_wakeup;
+ 	}
+ 
+@@ -639,8 +639,8 @@ int kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu)
+ 		tvcpu->arch.prodded = 1;
+ 		smp_mb();
+ 		if (vcpu->arch.ceded) {
+-			if (waitqueue_active(&vcpu->wq)) {
+-				wake_up_interruptible(&vcpu->wq);
++			if (swaitqueue_active(&vcpu->wq)) {
++				swait_wake_interruptible(&vcpu->wq);
+ 				vcpu->stat.halt_wakeup++;
+ 			}
+ 		}
+@@ -1363,7 +1363,7 @@ static struct kvmppc_vcore *kvmppc_vcore_create(struct kvm *kvm, int core)
+ 
+ 	INIT_LIST_HEAD(&vcore->runnable_threads);
+ 	spin_lock_init(&vcore->lock);
+-	init_waitqueue_head(&vcore->wq);
++	init_swait_head(&vcore->wq);
+ 	vcore->preempt_tb = TB_NIL;
+ 	vcore->lpcr = kvm->arch.lpcr;
+ 	vcore->first_vcpuid = core * threads_per_subcore;
+@@ -1832,13 +1832,13 @@ static void kvmppc_wait_for_exec(struct kvm_vcpu *vcpu, int wait_state)
+  */
+ static void kvmppc_vcore_blocked(struct kvmppc_vcore *vc)
+ {
+-	DEFINE_WAIT(wait);
++	DEFINE_SWAITER(wait);
+ 
+-	prepare_to_wait(&vc->wq, &wait, TASK_INTERRUPTIBLE);
++	swait_prepare(&vc->wq, &wait, TASK_INTERRUPTIBLE);
+ 	vc->vcore_state = VCORE_SLEEPING;
+ 	spin_unlock(&vc->lock);
+ 	schedule();
+-	finish_wait(&vc->wq, &wait);
++	swait_finish(&vc->wq, &wait);
+ 	spin_lock(&vc->lock);
+ 	vc->vcore_state = VCORE_INACTIVE;
+ }
+@@ -1879,7 +1879,7 @@ static int kvmppc_run_vcpu(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu)
+ 			kvmppc_create_dtl_entry(vcpu, vc);
+ 			kvmppc_start_thread(vcpu);
+ 		} else if (vc->vcore_state == VCORE_SLEEPING) {
+-			wake_up(&vc->wq);
++			swait_wake(&vc->wq);
+ 		}
+ 
+ 	}
+diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c
+index f06b56baf0b3..1e949bd22909 100644
+--- a/arch/powerpc/mm/fault.c
++++ b/arch/powerpc/mm/fault.c
+@@ -273,7 +273,7 @@ int __kprobes do_page_fault(struct pt_regs *regs, unsigned long address,
+ 	if (!arch_irq_disabled_regs(regs))
+ 		local_irq_enable();
+ 
+-	if (in_atomic() || mm == NULL) {
++	if (in_atomic() || mm == NULL || pagefault_disabled()) {
+ 		if (!user_mode(regs)) {
+ 			rc = SIGSEGV;
+ 			goto bail;
+diff --git a/arch/s390/include/asm/kvm_host.h b/arch/s390/include/asm/kvm_host.h
+index 2175f911a73a..884ec9b27b3c 100644
+--- a/arch/s390/include/asm/kvm_host.h
++++ b/arch/s390/include/asm/kvm_host.h
+@@ -311,7 +311,7 @@ struct kvm_s390_local_interrupt {
+ 	struct list_head list;
+ 	atomic_t active;
+ 	struct kvm_s390_float_interrupt *float_int;
+-	wait_queue_head_t *wq;
++	struct swait_head *wq;
+ 	atomic_t *cpuflags;
+ 	unsigned int action_bits;
+ };
+diff --git a/arch/s390/kvm/interrupt.c b/arch/s390/kvm/interrupt.c
+index cd6344d334cb..ca578e5dbfa6 100644
+--- a/arch/s390/kvm/interrupt.c
++++ b/arch/s390/kvm/interrupt.c
+@@ -620,13 +620,13 @@ no_timer:
+ 
+ void kvm_s390_vcpu_wakeup(struct kvm_vcpu *vcpu)
+ {
+-	if (waitqueue_active(&vcpu->wq)) {
++	if (swaitqueue_active(&vcpu->wq)) {
+ 		/*
+ 		 * The vcpu gave up the cpu voluntarily, mark it as a good
+ 		 * yield-candidate.
+ 		 */
+ 		vcpu->preempted = true;
+-		wake_up_interruptible(&vcpu->wq);
++		swait_wake_interruptible(&vcpu->wq);
+ 		vcpu->stat.halt_wakeup++;
+ 	}
+ }
+@@ -747,7 +747,7 @@ int kvm_s390_inject_program_int(struct kvm_vcpu *vcpu, u16 code)
+ 	spin_lock(&li->lock);
+ 	list_add(&inti->list, &li->list);
+ 	atomic_set(&li->active, 1);
+-	BUG_ON(waitqueue_active(li->wq));
++	BUG_ON(swaitqueue_active(li->wq));
+ 	spin_unlock(&li->lock);
+ 	return 0;
+ }
+@@ -772,7 +772,7 @@ int kvm_s390_inject_prog_irq(struct kvm_vcpu *vcpu,
+ 	spin_lock(&li->lock);
+ 	list_add(&inti->list, &li->list);
+ 	atomic_set(&li->active, 1);
+-	BUG_ON(waitqueue_active(li->wq));
++	BUG_ON(swaitqueue_active(li->wq));
+ 	spin_unlock(&li->lock);
+ 	return 0;
+ }
+diff --git a/arch/s390/mm/fault.c b/arch/s390/mm/fault.c
+index fbe8f2cf9245..43ec237a17e2 100644
+--- a/arch/s390/mm/fault.c
++++ b/arch/s390/mm/fault.c
+@@ -435,7 +435,8 @@ static inline int do_exception(struct pt_regs *regs, int access)
+ 	 * user context.
+ 	 */
+ 	fault = VM_FAULT_BADCONTEXT;
+-	if (unlikely(!user_space_fault(regs) || in_atomic() || !mm))
++	if (unlikely(!user_space_fault(regs) || !mm ||
++		     tsk->pagefault_disabled))
+ 		goto out;
+ 
+ 	address = trans_exc_code & __FAIL_ADDR_MASK;
+diff --git a/arch/score/mm/fault.c b/arch/score/mm/fault.c
+index 6860beb2a280..b946291e60d1 100644
+--- a/arch/score/mm/fault.c
++++ b/arch/score/mm/fault.c
+@@ -73,7 +73,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
+ 	* If we're in an interrupt or have no user
+ 	* context, we must not take the fault..
+ 	*/
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto bad_area_nosemaphore;
+ 
+ 	if (user_mode(regs))
+diff --git a/arch/sh/kernel/irq.c b/arch/sh/kernel/irq.c
+index 65a1ecd77f96..d5dd95e66791 100644
+--- a/arch/sh/kernel/irq.c
++++ b/arch/sh/kernel/irq.c
+@@ -149,6 +149,7 @@ void irq_ctx_exit(int cpu)
+ 	hardirq_ctx[cpu] = NULL;
+ }
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ void do_softirq_own_stack(void)
+ {
+ 	struct thread_info *curctx;
+@@ -176,6 +177,7 @@ void do_softirq_own_stack(void)
+ 		  "r5", "r6", "r7", "r8", "r9", "r15", "t", "pr"
+ 	);
+ }
++#endif
+ #else
+ static inline void handle_one_irq(unsigned int irq)
+ {
+diff --git a/arch/sh/mm/fault.c b/arch/sh/mm/fault.c
+index a58fec9b55e0..364cae5806ce 100644
+--- a/arch/sh/mm/fault.c
++++ b/arch/sh/mm/fault.c
+@@ -440,7 +440,7 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,
+ 	 * If we're in an interrupt, have no user context or are running
+ 	 * in an atomic region then we must not take the fault:
+ 	 */
+-	if (unlikely(in_atomic() || !mm)) {
++	if (unlikely(!mm || pagefault_disabled())) {
+ 		bad_area_nosemaphore(regs, error_code, address);
+ 		return;
+ 	}
+diff --git a/arch/sparc/Kconfig b/arch/sparc/Kconfig
+index 96ac69c5eba0..950b5733fdff 100644
+--- a/arch/sparc/Kconfig
++++ b/arch/sparc/Kconfig
+@@ -182,12 +182,10 @@ config NR_CPUS
+ source kernel/Kconfig.hz
+ 
+ config RWSEM_GENERIC_SPINLOCK
+-	bool
+-	default y if SPARC32
++	def_bool PREEMPT_RT_FULL
+ 
+ config RWSEM_XCHGADD_ALGORITHM
+-	bool
+-	default y if SPARC64
++	def_bool !RWSEM_GENERIC_SPINLOCK && !PREEMPT_RT_FULL
+ 
+ config GENERIC_HWEIGHT
+ 	bool
+@@ -528,6 +526,10 @@ menu "Executable file formats"
+ 
+ source "fs/Kconfig.binfmt"
+ 
++config EARLY_PRINTK
++	bool
++	default y
++
+ config COMPAT
+ 	bool
+ 	depends on SPARC64
+diff --git a/arch/sparc/kernel/irq_64.c b/arch/sparc/kernel/irq_64.c
+index 4033c23bdfa6..763cd88b4e92 100644
+--- a/arch/sparc/kernel/irq_64.c
++++ b/arch/sparc/kernel/irq_64.c
+@@ -849,6 +849,7 @@ void __irq_entry handler_irq(int pil, struct pt_regs *regs)
+ 	set_irq_regs(old_regs);
+ }
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ void do_softirq_own_stack(void)
+ {
+ 	void *orig_sp, *sp = softirq_stack[smp_processor_id()];
+@@ -863,6 +864,7 @@ void do_softirq_own_stack(void)
+ 	__asm__ __volatile__("mov %0, %%sp"
+ 			     : : "r" (orig_sp));
+ }
++#endif
+ 
+ #ifdef CONFIG_HOTPLUG_CPU
+ void fixup_irqs(void)
+diff --git a/arch/sparc/kernel/setup_32.c b/arch/sparc/kernel/setup_32.c
+index baef495c06bd..3548e8591416 100644
+--- a/arch/sparc/kernel/setup_32.c
++++ b/arch/sparc/kernel/setup_32.c
+@@ -309,6 +309,7 @@ void __init setup_arch(char **cmdline_p)
+ 
+ 	boot_flags_init(*cmdline_p);
+ 
++	early_console = &prom_early_console;
+ 	register_console(&prom_early_console);
+ 
+ 	printk("ARCH: ");
+diff --git a/arch/sparc/kernel/setup_64.c b/arch/sparc/kernel/setup_64.c
+index c38d19fc27ba..5e78950e2cbd 100644
+--- a/arch/sparc/kernel/setup_64.c
++++ b/arch/sparc/kernel/setup_64.c
+@@ -563,6 +563,12 @@ static void __init init_sparc64_elf_hwcap(void)
+ 		pause_patch();
+ }
+ 
++static inline void register_prom_console(void)
++{
++	early_console = &prom_early_console;
++	register_console(&prom_early_console);
++}
++
+ void __init setup_arch(char **cmdline_p)
+ {
+ 	/* Initialize PROM console and command line. */
+@@ -574,7 +580,7 @@ void __init setup_arch(char **cmdline_p)
+ #ifdef CONFIG_EARLYFB
+ 	if (btext_find_display())
+ #endif
+-		register_console(&prom_early_console);
++		register_prom_console();
+ 
+ 	if (tlb_type == hypervisor)
+ 		printk("ARCH: SUN4V\n");
+diff --git a/arch/sparc/mm/fault_32.c b/arch/sparc/mm/fault_32.c
+index 70d817154fe8..7c04a49477af 100644
+--- a/arch/sparc/mm/fault_32.c
++++ b/arch/sparc/mm/fault_32.c
+@@ -196,7 +196,7 @@ asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,
+ 	 * If we're in an interrupt or have no user
+ 	 * context, we must not take the fault..
+ 	 */
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto no_context;
+ 
+ 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
+diff --git a/arch/sparc/mm/fault_64.c b/arch/sparc/mm/fault_64.c
+index 479823249429..28afbdf069e1 100644
+--- a/arch/sparc/mm/fault_64.c
++++ b/arch/sparc/mm/fault_64.c
+@@ -330,7 +330,7 @@ asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)
+ 	 * If we're in an interrupt or have no user
+ 	 * context, we must not take the fault..
+ 	 */
+-	if (in_atomic() || !mm)
++	if (!mm || pagefault_disabled())
+ 		goto intr_or_no_mm;
+ 
+ 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
+diff --git a/arch/tile/mm/fault.c b/arch/tile/mm/fault.c
+index c6d2a76d91a8..4cdab266bcc3 100644
+--- a/arch/tile/mm/fault.c
++++ b/arch/tile/mm/fault.c
+@@ -357,7 +357,7 @@ static int handle_page_fault(struct pt_regs *regs,
+ 	 * If we're in an interrupt, have no user context or are running in an
+ 	 * atomic region then we must not take the fault.
+ 	 */
+-	if (in_atomic() || !mm) {
++	if (!mm || pagefault_disabled()) {
+ 		vma = NULL;  /* happy compiler */
+ 		goto bad_area_nosemaphore;
+ 	}
+diff --git a/arch/um/kernel/trap.c b/arch/um/kernel/trap.c
+index 209617302df8..6126d3a23382 100644
+--- a/arch/um/kernel/trap.c
++++ b/arch/um/kernel/trap.c
+@@ -38,7 +38,7 @@ int handle_page_fault(unsigned long address, unsigned long ip,
+ 	 * If the fault was during atomic operation, don't take the fault, just
+ 	 * fail.
+ 	 */
+-	if (in_atomic())
++	if (pagefault_disabled())
+ 		goto out_nosemaphore;
+ 
+ 	if (is_user)
+diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
+index c9148e268512..627abc68a218 100644
+--- a/arch/x86/Kconfig
++++ b/arch/x86/Kconfig
+@@ -21,6 +21,7 @@ config X86_64
+ ### Arch settings
+ config X86
+ 	def_bool y
++	select HAVE_PREEMPT_LAZY
+ 	select ARCH_MIGHT_HAVE_ACPI_PDC if ACPI
+ 	select ARCH_HAS_DEBUG_STRICT_USER_COPY_CHECKS
+ 	select ARCH_HAS_FAST_MULTIPLIER
+@@ -197,8 +198,11 @@ config ARCH_MAY_HAVE_PC_FDC
+ 	def_bool y
+ 	depends on ISA_DMA_API
+ 
++config RWSEM_GENERIC_SPINLOCK
++	def_bool PREEMPT_RT_FULL
++
+ config RWSEM_XCHGADD_ALGORITHM
+-	def_bool y
++	def_bool !RWSEM_GENERIC_SPINLOCK && !PREEMPT_RT_FULL
+ 
+ config GENERIC_CALIBRATE_DELAY
+ 	def_bool y
+@@ -811,7 +815,7 @@ config IOMMU_HELPER
+ config MAXSMP
+ 	bool "Enable Maximum number of SMP Processors and NUMA Nodes"
+ 	depends on X86_64 && SMP && DEBUG_KERNEL
+-	select CPUMASK_OFFSTACK
++	select CPUMASK_OFFSTACK if !PREEMPT_RT_FULL
+ 	---help---
+ 	  Enable maximum number of CPUS and NUMA Nodes for this architecture.
+ 	  If unsure, say N.
+diff --git a/arch/x86/crypto/aesni-intel_glue.c b/arch/x86/crypto/aesni-intel_glue.c
+index 5a93783a8a0d..d828fd40d0d0 100644
+--- a/arch/x86/crypto/aesni-intel_glue.c
++++ b/arch/x86/crypto/aesni-intel_glue.c
+@@ -381,14 +381,14 @@ static int ecb_encrypt(struct blkcipher_desc *desc,
+ 	err = blkcipher_walk_virt(desc, &walk);
+ 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
+ 
+-	kernel_fpu_begin();
+ 	while ((nbytes = walk.nbytes)) {
++		kernel_fpu_begin();
+ 		aesni_ecb_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,
+-			      nbytes & AES_BLOCK_MASK);
++				nbytes & AES_BLOCK_MASK);
++		kernel_fpu_end();
+ 		nbytes &= AES_BLOCK_SIZE - 1;
+ 		err = blkcipher_walk_done(desc, &walk, nbytes);
+ 	}
+-	kernel_fpu_end();
+ 
+ 	return err;
+ }
+@@ -405,14 +405,14 @@ static int ecb_decrypt(struct blkcipher_desc *desc,
+ 	err = blkcipher_walk_virt(desc, &walk);
+ 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
+ 
+-	kernel_fpu_begin();
+ 	while ((nbytes = walk.nbytes)) {
++		kernel_fpu_begin();
+ 		aesni_ecb_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,
+ 			      nbytes & AES_BLOCK_MASK);
++		kernel_fpu_end();
+ 		nbytes &= AES_BLOCK_SIZE - 1;
+ 		err = blkcipher_walk_done(desc, &walk, nbytes);
+ 	}
+-	kernel_fpu_end();
+ 
+ 	return err;
+ }
+@@ -429,14 +429,14 @@ static int cbc_encrypt(struct blkcipher_desc *desc,
+ 	err = blkcipher_walk_virt(desc, &walk);
+ 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
+ 
+-	kernel_fpu_begin();
+ 	while ((nbytes = walk.nbytes)) {
++		kernel_fpu_begin();
+ 		aesni_cbc_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,
+ 			      nbytes & AES_BLOCK_MASK, walk.iv);
++		kernel_fpu_end();
+ 		nbytes &= AES_BLOCK_SIZE - 1;
+ 		err = blkcipher_walk_done(desc, &walk, nbytes);
+ 	}
+-	kernel_fpu_end();
+ 
+ 	return err;
+ }
+@@ -453,14 +453,14 @@ static int cbc_decrypt(struct blkcipher_desc *desc,
+ 	err = blkcipher_walk_virt(desc, &walk);
+ 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
+ 
+-	kernel_fpu_begin();
+ 	while ((nbytes = walk.nbytes)) {
++		kernel_fpu_begin();
+ 		aesni_cbc_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,
+ 			      nbytes & AES_BLOCK_MASK, walk.iv);
++		kernel_fpu_end();
+ 		nbytes &= AES_BLOCK_SIZE - 1;
+ 		err = blkcipher_walk_done(desc, &walk, nbytes);
+ 	}
+-	kernel_fpu_end();
+ 
+ 	return err;
+ }
+@@ -512,18 +512,20 @@ static int ctr_crypt(struct blkcipher_desc *desc,
+ 	err = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);
+ 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
+ 
+-	kernel_fpu_begin();
+ 	while ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {
++		kernel_fpu_begin();
+ 		aesni_ctr_enc_tfm(ctx, walk.dst.virt.addr, walk.src.virt.addr,
+ 				  nbytes & AES_BLOCK_MASK, walk.iv);
++		kernel_fpu_end();
+ 		nbytes &= AES_BLOCK_SIZE - 1;
+ 		err = blkcipher_walk_done(desc, &walk, nbytes);
+ 	}
+ 	if (walk.nbytes) {
++		kernel_fpu_begin();
+ 		ctr_crypt_final(ctx, &walk);
++		kernel_fpu_end();
+ 		err = blkcipher_walk_done(desc, &walk, 0);
+ 	}
+-	kernel_fpu_end();
+ 
+ 	return err;
+ }
+diff --git a/arch/x86/crypto/cast5_avx_glue.c b/arch/x86/crypto/cast5_avx_glue.c
+index 60ada677a928..7344a356e7a0 100644
+--- a/arch/x86/crypto/cast5_avx_glue.c
++++ b/arch/x86/crypto/cast5_avx_glue.c
+@@ -60,7 +60,7 @@ static inline void cast5_fpu_end(bool fpu_enabled)
+ static int ecb_crypt(struct blkcipher_desc *desc, struct blkcipher_walk *walk,
+ 		     bool enc)
+ {
+-	bool fpu_enabled = false;
++	bool fpu_enabled;
+ 	struct cast5_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);
+ 	const unsigned int bsize = CAST5_BLOCK_SIZE;
+ 	unsigned int nbytes;
+@@ -76,7 +76,7 @@ static int ecb_crypt(struct blkcipher_desc *desc, struct blkcipher_walk *walk,
+ 		u8 *wsrc = walk->src.virt.addr;
+ 		u8 *wdst = walk->dst.virt.addr;
+ 
+-		fpu_enabled = cast5_fpu_begin(fpu_enabled, nbytes);
++		fpu_enabled = cast5_fpu_begin(false, nbytes);
+ 
+ 		/* Process multi-block batch */
+ 		if (nbytes >= bsize * CAST5_PARALLEL_BLOCKS) {
+@@ -104,10 +104,9 @@ static int ecb_crypt(struct blkcipher_desc *desc, struct blkcipher_walk *walk,
+ 		} while (nbytes >= bsize);
+ 
+ done:
++		cast5_fpu_end(fpu_enabled);
+ 		err = blkcipher_walk_done(desc, walk, nbytes);
+ 	}
+-
+-	cast5_fpu_end(fpu_enabled);
+ 	return err;
+ }
+ 
+@@ -228,7 +227,7 @@ done:
+ static int cbc_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,
+ 		       struct scatterlist *src, unsigned int nbytes)
+ {
+-	bool fpu_enabled = false;
++	bool fpu_enabled;
+ 	struct blkcipher_walk walk;
+ 	int err;
+ 
+@@ -237,12 +236,11 @@ static int cbc_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,
+ 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
+ 
+ 	while ((nbytes = walk.nbytes)) {
+-		fpu_enabled = cast5_fpu_begin(fpu_enabled, nbytes);
++		fpu_enabled = cast5_fpu_begin(false, nbytes);
+ 		nbytes = __cbc_decrypt(desc, &walk);
++		cast5_fpu_end(fpu_enabled);
+ 		err = blkcipher_walk_done(desc, &walk, nbytes);
+ 	}
+-
+-	cast5_fpu_end(fpu_enabled);
+ 	return err;
+ }
+ 
+@@ -312,7 +310,7 @@ done:
+ static int ctr_crypt(struct blkcipher_desc *desc, struct scatterlist *dst,
+ 		     struct scatterlist *src, unsigned int nbytes)
+ {
+-	bool fpu_enabled = false;
++	bool fpu_enabled;
+ 	struct blkcipher_walk walk;
+ 	int err;
+ 
+@@ -321,13 +319,12 @@ static int ctr_crypt(struct blkcipher_desc *desc, struct scatterlist *dst,
+ 	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
+ 
+ 	while ((nbytes = walk.nbytes) >= CAST5_BLOCK_SIZE) {
+-		fpu_enabled = cast5_fpu_begin(fpu_enabled, nbytes);
++		fpu_enabled = cast5_fpu_begin(false, nbytes);
+ 		nbytes = __ctr_crypt(desc, &walk);
++		cast5_fpu_end(fpu_enabled);
+ 		err = blkcipher_walk_done(desc, &walk, nbytes);
+ 	}
+ 
+-	cast5_fpu_end(fpu_enabled);
+-
+ 	if (walk.nbytes) {
+ 		ctr_crypt_final(desc, &walk);
+ 		err = blkcipher_walk_done(desc, &walk, 0);
+diff --git a/arch/x86/crypto/glue_helper.c b/arch/x86/crypto/glue_helper.c
+index 432f1d76ceb8..4a2bd21c2137 100644
+--- a/arch/x86/crypto/glue_helper.c
++++ b/arch/x86/crypto/glue_helper.c
+@@ -39,7 +39,7 @@ static int __glue_ecb_crypt_128bit(const struct common_glue_ctx *gctx,
+ 	void *ctx = crypto_blkcipher_ctx(desc->tfm);
+ 	const unsigned int bsize = 128 / 8;
+ 	unsigned int nbytes, i, func_bytes;
+-	bool fpu_enabled = false;
++	bool fpu_enabled;
+ 	int err;
+ 
+ 	err = blkcipher_walk_virt(desc, walk);
+@@ -49,7 +49,7 @@ static int __glue_ecb_crypt_128bit(const struct common_glue_ctx *gctx,
+ 		u8 *wdst = walk->dst.virt.addr;
+ 
+ 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
+-					     desc, fpu_enabled, nbytes);
++					     desc, false, nbytes);
+ 
+ 		for (i = 0; i < gctx->num_funcs; i++) {
+ 			func_bytes = bsize * gctx->funcs[i].num_blocks;
+@@ -71,10 +71,10 @@ static int __glue_ecb_crypt_128bit(const struct common_glue_ctx *gctx,
+ 		}
+ 
+ done:
++		glue_fpu_end(fpu_enabled);
+ 		err = blkcipher_walk_done(desc, walk, nbytes);
+ 	}
+ 
+-	glue_fpu_end(fpu_enabled);
+ 	return err;
+ }
+ 
+@@ -194,7 +194,7 @@ int glue_cbc_decrypt_128bit(const struct common_glue_ctx *gctx,
+ 			    struct scatterlist *src, unsigned int nbytes)
+ {
+ 	const unsigned int bsize = 128 / 8;
+-	bool fpu_enabled = false;
++	bool fpu_enabled;
+ 	struct blkcipher_walk walk;
+ 	int err;
+ 
+@@ -203,12 +203,12 @@ int glue_cbc_decrypt_128bit(const struct common_glue_ctx *gctx,
+ 
+ 	while ((nbytes = walk.nbytes)) {
+ 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
+-					     desc, fpu_enabled, nbytes);
++					     desc, false, nbytes);
+ 		nbytes = __glue_cbc_decrypt_128bit(gctx, desc, &walk);
++		glue_fpu_end(fpu_enabled);
+ 		err = blkcipher_walk_done(desc, &walk, nbytes);
+ 	}
+ 
+-	glue_fpu_end(fpu_enabled);
+ 	return err;
+ }
+ EXPORT_SYMBOL_GPL(glue_cbc_decrypt_128bit);
+@@ -278,7 +278,7 @@ int glue_ctr_crypt_128bit(const struct common_glue_ctx *gctx,
+ 			  struct scatterlist *src, unsigned int nbytes)
+ {
+ 	const unsigned int bsize = 128 / 8;
+-	bool fpu_enabled = false;
++	bool fpu_enabled;
+ 	struct blkcipher_walk walk;
+ 	int err;
+ 
+@@ -287,13 +287,12 @@ int glue_ctr_crypt_128bit(const struct common_glue_ctx *gctx,
+ 
+ 	while ((nbytes = walk.nbytes) >= bsize) {
+ 		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
+-					     desc, fpu_enabled, nbytes);
++					     desc, false, nbytes);
+ 		nbytes = __glue_ctr_crypt_128bit(gctx, desc, &walk);
++		glue_fpu_end(fpu_enabled);
+ 		err = blkcipher_walk_done(desc, &walk, nbytes);
+ 	}
+ 
+-	glue_fpu_end(fpu_enabled);
+-
+ 	if (walk.nbytes) {
+ 		glue_ctr_crypt_final_128bit(
+ 			gctx->funcs[gctx->num_funcs - 1].fn_u.ctr, desc, &walk);
+@@ -348,7 +347,7 @@ int glue_xts_crypt_128bit(const struct common_glue_ctx *gctx,
+ 			  void *tweak_ctx, void *crypt_ctx)
+ {
+ 	const unsigned int bsize = 128 / 8;
+-	bool fpu_enabled = false;
++	bool fpu_enabled;
+ 	struct blkcipher_walk walk;
+ 	int err;
+ 
+@@ -361,21 +360,21 @@ int glue_xts_crypt_128bit(const struct common_glue_ctx *gctx,
+ 
+ 	/* set minimum length to bsize, for tweak_fn */
+ 	fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
+-				     desc, fpu_enabled,
++				     desc, false,
+ 				     nbytes < bsize ? bsize : nbytes);
+-
+ 	/* calculate first value of T */
+ 	tweak_fn(tweak_ctx, walk.iv, walk.iv);
++	glue_fpu_end(fpu_enabled);
+ 
+ 	while (nbytes) {
++		fpu_enabled = glue_fpu_begin(bsize, gctx->fpu_blocks_limit,
++				desc, false, nbytes);
+ 		nbytes = __glue_xts_crypt_128bit(gctx, crypt_ctx, desc, &walk);
+ 
++		glue_fpu_end(fpu_enabled);
+ 		err = blkcipher_walk_done(desc, &walk, nbytes);
+ 		nbytes = walk.nbytes;
+ 	}
+-
+-	glue_fpu_end(fpu_enabled);
+-
+ 	return err;
+ }
+ EXPORT_SYMBOL_GPL(glue_xts_crypt_128bit);
+diff --git a/arch/x86/include/asm/preempt.h b/arch/x86/include/asm/preempt.h
+index 400873450e33..ba4976828a18 100644
+--- a/arch/x86/include/asm/preempt.h
++++ b/arch/x86/include/asm/preempt.h
+@@ -85,17 +85,46 @@ static __always_inline void __preempt_count_sub(int val)
+  * a decrement which hits zero means we have no preempt_count and should
+  * reschedule.
+  */
+-static __always_inline bool __preempt_count_dec_and_test(void)
++static __always_inline bool ____preempt_count_dec_and_test(void)
+ {
+ 	GEN_UNARY_RMWcc("decl", __preempt_count, __percpu_arg(0), "e");
+ }
+ 
++static __always_inline bool __preempt_count_dec_and_test(void)
++{
++	if (____preempt_count_dec_and_test())
++		return true;
++#ifdef CONFIG_PREEMPT_LAZY
++	if (current_thread_info()->preempt_lazy_count)
++		return false;
++	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
++#else
++	return false;
++#endif
++}
++
+ /*
+  * Returns true when we need to resched and can (barring IRQ state).
+  */
+ static __always_inline bool should_resched(void)
+ {
++#ifdef CONFIG_PREEMPT_LAZY
++	u32 tmp;
++
++	tmp = raw_cpu_read_4(__preempt_count);
++	if (!tmp)
++		return true;
++
++	/* preempt count == 0 ? */
++	tmp &= ~PREEMPT_NEED_RESCHED;
++	if (tmp)
++		return false;
++	if (current_thread_info()->preempt_lazy_count)
++		return false;
++	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
++#else
+ 	return unlikely(!raw_cpu_read_4(__preempt_count));
++#endif
+ }
+ 
+ #ifdef CONFIG_PREEMPT
+diff --git a/arch/x86/include/asm/signal.h b/arch/x86/include/asm/signal.h
+index 31eab867e6d3..b1b08a28c72a 100644
+--- a/arch/x86/include/asm/signal.h
++++ b/arch/x86/include/asm/signal.h
+@@ -23,6 +23,19 @@ typedef struct {
+ 	unsigned long sig[_NSIG_WORDS];
+ } sigset_t;
+ 
++/*
++ * Because some traps use the IST stack, we must keep preemption
++ * disabled while calling do_trap(), but do_trap() may call
++ * force_sig_info() which will grab the signal spin_locks for the
++ * task, which in PREEMPT_RT_FULL are mutexes.  By defining
++ * ARCH_RT_DELAYS_SIGNAL_SEND the force_sig_info() will set
++ * TIF_NOTIFY_RESUME and set up the signal to be sent on exit of the
++ * trap.
++ */
++#if defined(CONFIG_PREEMPT_RT_FULL) && defined(CONFIG_X86_64)
++#define ARCH_RT_DELAYS_SIGNAL_SEND
++#endif
++
+ #ifndef CONFIG_COMPAT
+ typedef sigset_t compat_sigset_t;
+ #endif
+diff --git a/arch/x86/include/asm/stackprotector.h b/arch/x86/include/asm/stackprotector.h
+index 6a998598f172..64fb5cbe54fa 100644
+--- a/arch/x86/include/asm/stackprotector.h
++++ b/arch/x86/include/asm/stackprotector.h
+@@ -57,7 +57,7 @@
+  */
+ static __always_inline void boot_init_stack_canary(void)
+ {
+-	u64 canary;
++	u64 uninitialized_var(canary);
+ 	u64 tsc;
+ 
+ #ifdef CONFIG_X86_64
+@@ -68,8 +68,16 @@ static __always_inline void boot_init_stack_canary(void)
+ 	 * of randomness. The TSC only matters for very early init,
+ 	 * there it already has some randomness on most systems. Later
+ 	 * on during the bootup the random pool has true entropy too.
++	 *
++	 * For preempt-rt we need to weaken the randomness a bit, as
++	 * we can't call into the random generator from atomic context
++	 * due to locking constraints. We just leave canary
++	 * uninitialized and use the TSC based randomness on top of
++	 * it.
+ 	 */
++#ifndef CONFIG_PREEMPT_RT_FULL
+ 	get_random_bytes(&canary, sizeof(canary));
++#endif
+ 	tsc = __native_read_tsc();
+ 	canary += tsc + (tsc << 32UL);
+ 
+diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
+index c4d96943e666..766fa5cc15b4 100644
+--- a/arch/x86/include/asm/thread_info.h
++++ b/arch/x86/include/asm/thread_info.h
+@@ -57,6 +57,8 @@ struct thread_info {
+ 	__u32			status;		/* thread synchronous flags */
+ 	__u32			cpu;		/* current CPU */
+ 	int			saved_preempt_count;
++	int			preempt_lazy_count;	/* 0 => lazy preemptable
++							   <0 => BUG */
+ 	mm_segment_t		addr_limit;
+ 	struct restart_block    restart_block;
+ 	void __user		*sysenter_return;
+@@ -102,6 +104,7 @@ struct thread_info {
+ #define TIF_SYSCALL_EMU		6	/* syscall emulation active */
+ #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
+ #define TIF_SECCOMP		8	/* secure computing */
++#define TIF_NEED_RESCHED_LAZY	9	/* lazy rescheduling necessary */
+ #define TIF_MCE_NOTIFY		10	/* notify userspace of an MCE */
+ #define TIF_USER_RETURN_NOTIFY	11	/* notify kernel of userspace return */
+ #define TIF_UPROBE		12	/* breakpointed or singlestepping */
+@@ -127,6 +130,7 @@ struct thread_info {
+ #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
+ #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
+ #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
++#define _TIF_NEED_RESCHED_LAZY	(1 << TIF_NEED_RESCHED_LAZY)
+ #define _TIF_MCE_NOTIFY		(1 << TIF_MCE_NOTIFY)
+ #define _TIF_USER_RETURN_NOTIFY	(1 << TIF_USER_RETURN_NOTIFY)
+ #define _TIF_UPROBE		(1 << TIF_UPROBE)
+@@ -177,6 +181,8 @@ struct thread_info {
+ #define _TIF_WORK_CTXSW_PREV (_TIF_WORK_CTXSW|_TIF_USER_RETURN_NOTIFY)
+ #define _TIF_WORK_CTXSW_NEXT (_TIF_WORK_CTXSW)
+ 
++#define _TIF_NEED_RESCHED_MASK (_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_LAZY)
++
+ #define STACK_WARN		(THREAD_SIZE/8)
+ #define KERNEL_STACK_OFFSET	(5*(BITS_PER_LONG/8))
+ 
+diff --git a/arch/x86/include/asm/uv/uv_bau.h b/arch/x86/include/asm/uv/uv_bau.h
+index 2d60a7813dfe..ddc4c9e3d09d 100644
+--- a/arch/x86/include/asm/uv/uv_bau.h
++++ b/arch/x86/include/asm/uv/uv_bau.h
+@@ -615,9 +615,9 @@ struct bau_control {
+ 	cycles_t		send_message;
+ 	cycles_t		period_end;
+ 	cycles_t		period_time;
+-	spinlock_t		uvhub_lock;
+-	spinlock_t		queue_lock;
+-	spinlock_t		disable_lock;
++	raw_spinlock_t		uvhub_lock;
++	raw_spinlock_t		queue_lock;
++	raw_spinlock_t		disable_lock;
+ 	/* tunables */
+ 	int			max_concurr;
+ 	int			max_concurr_const;
+@@ -776,15 +776,15 @@ static inline int atom_asr(short i, struct atomic_short *v)
+  * to be lowered below the current 'v'.  atomic_add_unless can only stop
+  * on equal.
+  */
+-static inline int atomic_inc_unless_ge(spinlock_t *lock, atomic_t *v, int u)
++static inline int atomic_inc_unless_ge(raw_spinlock_t *lock, atomic_t *v, int u)
+ {
+-	spin_lock(lock);
++	raw_spin_lock(lock);
+ 	if (atomic_read(v) >= u) {
+-		spin_unlock(lock);
++		raw_spin_unlock(lock);
+ 		return 0;
+ 	}
+ 	atomic_inc(v);
+-	spin_unlock(lock);
++	raw_spin_unlock(lock);
+ 	return 1;
+ }
+ 
+diff --git a/arch/x86/include/asm/uv/uv_hub.h b/arch/x86/include/asm/uv/uv_hub.h
+index a00ad8f2a657..c2729abe02bc 100644
+--- a/arch/x86/include/asm/uv/uv_hub.h
++++ b/arch/x86/include/asm/uv/uv_hub.h
+@@ -492,7 +492,7 @@ struct uv_blade_info {
+ 	unsigned short	nr_online_cpus;
+ 	unsigned short	pnode;
+ 	short		memory_nid;
+-	spinlock_t	nmi_lock;	/* obsolete, see uv_hub_nmi */
++	raw_spinlock_t	nmi_lock;	/* obsolete, see uv_hub_nmi */
+ 	unsigned long	nmi_count;	/* obsolete, see uv_hub_nmi */
+ };
+ extern struct uv_blade_info *uv_blade_info;
+diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c
+index 1183d545da1e..791ffdafd674 100644
+--- a/arch/x86/kernel/apic/io_apic.c
++++ b/arch/x86/kernel/apic/io_apic.c
+@@ -2494,7 +2494,8 @@ static bool io_apic_level_ack_pending(struct irq_cfg *cfg)
+ static inline bool ioapic_irqd_mask(struct irq_data *data, struct irq_cfg *cfg)
+ {
+ 	/* If we are moving the irq we need to mask it */
+-	if (unlikely(irqd_is_setaffinity_pending(data))) {
++	if (unlikely(irqd_is_setaffinity_pending(data) &&
++		     !irqd_irq_inprogress(data))) {
+ 		mask_ioapic(cfg);
+ 		return true;
+ 	}
+diff --git a/arch/x86/kernel/apic/x2apic_uv_x.c b/arch/x86/kernel/apic/x2apic_uv_x.c
+index 8e9dcfd630e4..2cfedcae31b9 100644
+--- a/arch/x86/kernel/apic/x2apic_uv_x.c
++++ b/arch/x86/kernel/apic/x2apic_uv_x.c
+@@ -918,7 +918,7 @@ void __init uv_system_init(void)
+ 			uv_blade_info[blade].pnode = pnode;
+ 			uv_blade_info[blade].nr_possible_cpus = 0;
+ 			uv_blade_info[blade].nr_online_cpus = 0;
+-			spin_lock_init(&uv_blade_info[blade].nmi_lock);
++			raw_spin_lock_init(&uv_blade_info[blade].nmi_lock);
+ 			min_pnode = min(pnode, min_pnode);
+ 			max_pnode = max(pnode, max_pnode);
+ 			blade++;
+diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c
+index 9f6b9341950f..5701b507510b 100644
+--- a/arch/x86/kernel/asm-offsets.c
++++ b/arch/x86/kernel/asm-offsets.c
+@@ -32,6 +32,7 @@ void common(void) {
+ 	OFFSET(TI_flags, thread_info, flags);
+ 	OFFSET(TI_status, thread_info, status);
+ 	OFFSET(TI_addr_limit, thread_info, addr_limit);
++	OFFSET(TI_preempt_lazy_count, thread_info, preempt_lazy_count);
+ 
+ 	BLANK();
+ 	OFFSET(crypto_tfm_ctx_offset, crypto_tfm, __crt_ctx);
+@@ -71,4 +72,5 @@ void common(void) {
+ 
+ 	BLANK();
+ 	DEFINE(PTREGS_SIZE, sizeof(struct pt_regs));
++	DEFINE(_PREEMPT_ENABLED, PREEMPT_ENABLED);
+ }
+diff --git a/arch/x86/kernel/cpu/mcheck/mce.c b/arch/x86/kernel/cpu/mcheck/mce.c
+index bf44e45a2a76..1492d3adfdac 100644
+--- a/arch/x86/kernel/cpu/mcheck/mce.c
++++ b/arch/x86/kernel/cpu/mcheck/mce.c
+@@ -41,6 +41,8 @@
+ #include <linux/debugfs.h>
+ #include <linux/irq_work.h>
+ #include <linux/export.h>
++#include <linux/jiffies.h>
++#include <linux/work-simple.h>
+ 
+ #include <asm/processor.h>
+ #include <asm/tlbflush.h>
+@@ -1272,7 +1274,7 @@ void mce_log_therm_throt_event(__u64 status)
+ static unsigned long check_interval = 5 * 60; /* 5 minutes */
+ 
+ static DEFINE_PER_CPU(unsigned long, mce_next_interval); /* in jiffies */
+-static DEFINE_PER_CPU(struct timer_list, mce_timer);
++static DEFINE_PER_CPU(struct hrtimer, mce_timer);
+ 
+ static unsigned long mce_adjust_timer_default(unsigned long interval)
+ {
+@@ -1289,14 +1291,11 @@ static int cmc_error_seen(void)
+ 	return test_and_clear_bit(0, v);
+ }
+ 
+-static void mce_timer_fn(unsigned long data)
++static enum hrtimer_restart mce_timer_fn(struct hrtimer *timer)
+ {
+-	struct timer_list *t = this_cpu_ptr(&mce_timer);
+ 	unsigned long iv;
+ 	int notify;
+ 
+-	WARN_ON(smp_processor_id() != data);
+-
+ 	if (mce_available(this_cpu_ptr(&cpu_info))) {
+ 		machine_check_poll(MCP_TIMESTAMP,
+ 				this_cpu_ptr(&mce_poll_banks));
+@@ -1319,9 +1318,11 @@ static void mce_timer_fn(unsigned long data)
+ 	__this_cpu_write(mce_next_interval, iv);
+ 	/* Might have become 0 after CMCI storm subsided */
+ 	if (iv) {
+-		t->expires = jiffies + iv;
+-		add_timer_on(t, smp_processor_id());
++		hrtimer_forward_now(timer, ns_to_ktime(
++					jiffies_to_usecs(iv) * 1000ULL));
++		return HRTIMER_RESTART;
+ 	}
++	return HRTIMER_NORESTART;
+ }
+ 
+ /*
+@@ -1329,28 +1330,37 @@ static void mce_timer_fn(unsigned long data)
+  */
+ void mce_timer_kick(unsigned long interval)
+ {
+-	struct timer_list *t = this_cpu_ptr(&mce_timer);
+-	unsigned long when = jiffies + interval;
++	struct hrtimer *t = this_cpu_ptr(&mce_timer);
+ 	unsigned long iv = __this_cpu_read(mce_next_interval);
+ 
+-	if (timer_pending(t)) {
+-		if (time_before(when, t->expires))
+-			mod_timer_pinned(t, when);
++	if (hrtimer_active(t)) {
++		s64 exp;
++		s64 intv_us;
++
++		intv_us = jiffies_to_usecs(interval);
++		exp = ktime_to_us(hrtimer_expires_remaining(t));
++		if (intv_us < exp) {
++			hrtimer_cancel(t);
++			hrtimer_start_range_ns(t,
++					ns_to_ktime(intv_us * 1000),
++					0, HRTIMER_MODE_REL_PINNED);
++		}
+ 	} else {
+-		t->expires = round_jiffies(when);
+-		add_timer_on(t, smp_processor_id());
++		hrtimer_start_range_ns(t,
++			ns_to_ktime(jiffies_to_usecs(interval) * 1000ULL),
++				0, HRTIMER_MODE_REL_PINNED);
+ 	}
+ 	if (interval < iv)
+ 		__this_cpu_write(mce_next_interval, interval);
+ }
+ 
+-/* Must not be called in IRQ context where del_timer_sync() can deadlock */
++/* Must not be called in IRQ context where hrtimer_cancel() can deadlock */
+ static void mce_timer_delete_all(void)
+ {
+ 	int cpu;
+ 
+ 	for_each_online_cpu(cpu)
+-		del_timer_sync(&per_cpu(mce_timer, cpu));
++		hrtimer_cancel(&per_cpu(mce_timer, cpu));
+ }
+ 
+ static void mce_do_trigger(struct work_struct *work)
+@@ -1360,6 +1370,56 @@ static void mce_do_trigger(struct work_struct *work)
+ 
+ static DECLARE_WORK(mce_trigger_work, mce_do_trigger);
+ 
++static void __mce_notify_work(struct swork_event *event)
++{
++	/* Not more than two messages every minute */
++	static DEFINE_RATELIMIT_STATE(ratelimit, 60*HZ, 2);
++
++	/* wake processes polling /dev/mcelog */
++	wake_up_interruptible(&mce_chrdev_wait);
++
++	/*
++	 * There is no risk of missing notifications because
++	 * work_pending is always cleared before the function is
++	 * executed.
++	 */
++	if (mce_helper[0] && !work_pending(&mce_trigger_work))
++		schedule_work(&mce_trigger_work);
++
++	if (__ratelimit(&ratelimit))
++		pr_info(HW_ERR "Machine check events logged\n");
++}
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++static bool notify_work_ready __read_mostly;
++static struct swork_event notify_work;
++
++static int mce_notify_work_init(void)
++{
++	int err;
++
++	err = swork_get();
++	if (err)
++		return err;
++
++	INIT_SWORK(&notify_work, __mce_notify_work);
++	notify_work_ready = true;
++	return 0;
++}
++
++static void mce_notify_work(void)
++{
++	if (notify_work_ready)
++		swork_queue(&notify_work);
++}
++#else
++static void mce_notify_work(void)
++{
++	__mce_notify_work(NULL);
++}
++static inline int mce_notify_work_init(void) { return 0; }
++#endif
++
+ /*
+  * Notify the user(s) about new machine check events.
+  * Can be called from interrupt context, but not from machine check/NMI
+@@ -1367,19 +1427,8 @@ static DECLARE_WORK(mce_trigger_work, mce_do_trigger);
+  */
+ int mce_notify_irq(void)
+ {
+-	/* Not more than two messages every minute */
+-	static DEFINE_RATELIMIT_STATE(ratelimit, 60*HZ, 2);
+-
+ 	if (test_and_clear_bit(0, &mce_need_notify)) {
+-		/* wake processes polling /dev/mcelog */
+-		wake_up_interruptible(&mce_chrdev_wait);
+-
+-		if (mce_helper[0])
+-			schedule_work(&mce_trigger_work);
+-
+-		if (__ratelimit(&ratelimit))
+-			pr_info(HW_ERR "Machine check events logged\n");
+-
++		mce_notify_work();
+ 		return 1;
+ 	}
+ 	return 0;
+@@ -1650,7 +1699,7 @@ static void __mcheck_cpu_init_vendor(struct cpuinfo_x86 *c)
+ 	}
+ }
+ 
+-static void mce_start_timer(unsigned int cpu, struct timer_list *t)
++static void mce_start_timer(unsigned int cpu, struct hrtimer *t)
+ {
+ 	unsigned long iv = check_interval * HZ;
+ 
+@@ -1659,16 +1708,17 @@ static void mce_start_timer(unsigned int cpu, struct timer_list *t)
+ 
+ 	per_cpu(mce_next_interval, cpu) = iv;
+ 
+-	t->expires = round_jiffies(jiffies + iv);
+-	add_timer_on(t, cpu);
++	hrtimer_start_range_ns(t, ns_to_ktime(jiffies_to_usecs(iv) * 1000ULL),
++			0, HRTIMER_MODE_REL_PINNED);
+ }
+ 
+ static void __mcheck_cpu_init_timer(void)
+ {
+-	struct timer_list *t = this_cpu_ptr(&mce_timer);
++	struct hrtimer *t = this_cpu_ptr(&mce_timer);
+ 	unsigned int cpu = smp_processor_id();
+ 
+-	setup_timer(t, mce_timer_fn, cpu);
++	hrtimer_init(t, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
++	t->function = mce_timer_fn;
+ 	mce_start_timer(cpu, t);
+ }
+ 
+@@ -2345,6 +2395,8 @@ static void mce_disable_cpu(void *h)
+ 	if (!mce_available(raw_cpu_ptr(&cpu_info)))
+ 		return;
+ 
++	hrtimer_cancel(this_cpu_ptr(&mce_timer));
++
+ 	if (!(action & CPU_TASKS_FROZEN))
+ 		cmci_clear();
+ 	for (i = 0; i < mca_cfg.banks; i++) {
+@@ -2371,6 +2423,7 @@ static void mce_reenable_cpu(void *h)
+ 		if (b->init)
+ 			wrmsrl(MSR_IA32_MCx_CTL(i), b->ctl);
+ 	}
++	__mcheck_cpu_init_timer();
+ }
+ 
+ /* Get notified when a cpu comes on/off. Be hotplug friendly. */
+@@ -2378,7 +2431,6 @@ static int
+ mce_cpu_callback(struct notifier_block *nfb, unsigned long action, void *hcpu)
+ {
+ 	unsigned int cpu = (unsigned long)hcpu;
+-	struct timer_list *t = &per_cpu(mce_timer, cpu);
+ 
+ 	switch (action & ~CPU_TASKS_FROZEN) {
+ 	case CPU_ONLINE:
+@@ -2398,11 +2450,9 @@ mce_cpu_callback(struct notifier_block *nfb, unsigned long action, void *hcpu)
+ 		break;
+ 	case CPU_DOWN_PREPARE:
+ 		smp_call_function_single(cpu, mce_disable_cpu, &action, 1);
+-		del_timer_sync(t);
+ 		break;
+ 	case CPU_DOWN_FAILED:
+ 		smp_call_function_single(cpu, mce_reenable_cpu, &action, 1);
+-		mce_start_timer(cpu, t);
+ 		break;
+ 	}
+ 
+@@ -2441,6 +2491,10 @@ static __init int mcheck_init_device(void)
+ 		goto err_out;
+ 	}
+ 
++	err = mce_notify_work_init();
++	if (err)
++		goto err_out;
++
+ 	if (!zalloc_cpumask_var(&mce_device_initialized, GFP_KERNEL)) {
+ 		err = -ENOMEM;
+ 		goto err_out;
+diff --git a/arch/x86/kernel/cpu/perf_event_intel_rapl.c b/arch/x86/kernel/cpu/perf_event_intel_rapl.c
+index 611d821eac1a..f35cf3f095fe 100644
+--- a/arch/x86/kernel/cpu/perf_event_intel_rapl.c
++++ b/arch/x86/kernel/cpu/perf_event_intel_rapl.c
+@@ -104,7 +104,7 @@ static struct kobj_attribute format_attr_##_var =		\
+ #define RAPL_CNTR_WIDTH 32 /* 32-bit rapl counters */
+ 
+ struct rapl_pmu {
+-	spinlock_t	 lock;
++	raw_spinlock_t	 lock;
+ 	int		 hw_unit;  /* 1/2^hw_unit Joule */
+ 	int		 n_active; /* number of active events */
+ 	struct list_head active_list;
+@@ -194,13 +194,13 @@ static enum hrtimer_restart rapl_hrtimer_handle(struct hrtimer *hrtimer)
+ 	if (!pmu->n_active)
+ 		return HRTIMER_NORESTART;
+ 
+-	spin_lock_irqsave(&pmu->lock, flags);
++	raw_spin_lock_irqsave(&pmu->lock, flags);
+ 
+ 	list_for_each_entry(event, &pmu->active_list, active_entry) {
+ 		rapl_event_update(event);
+ 	}
+ 
+-	spin_unlock_irqrestore(&pmu->lock, flags);
++	raw_spin_unlock_irqrestore(&pmu->lock, flags);
+ 
+ 	hrtimer_forward_now(hrtimer, pmu->timer_interval);
+ 
+@@ -237,9 +237,9 @@ static void rapl_pmu_event_start(struct perf_event *event, int mode)
+ 	struct rapl_pmu *pmu = __this_cpu_read(rapl_pmu);
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&pmu->lock, flags);
++	raw_spin_lock_irqsave(&pmu->lock, flags);
+ 	__rapl_pmu_event_start(pmu, event);
+-	spin_unlock_irqrestore(&pmu->lock, flags);
++	raw_spin_unlock_irqrestore(&pmu->lock, flags);
+ }
+ 
+ static void rapl_pmu_event_stop(struct perf_event *event, int mode)
+@@ -248,7 +248,7 @@ static void rapl_pmu_event_stop(struct perf_event *event, int mode)
+ 	struct hw_perf_event *hwc = &event->hw;
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&pmu->lock, flags);
++	raw_spin_lock_irqsave(&pmu->lock, flags);
+ 
+ 	/* mark event as deactivated and stopped */
+ 	if (!(hwc->state & PERF_HES_STOPPED)) {
+@@ -273,7 +273,7 @@ static void rapl_pmu_event_stop(struct perf_event *event, int mode)
+ 		hwc->state |= PERF_HES_UPTODATE;
+ 	}
+ 
+-	spin_unlock_irqrestore(&pmu->lock, flags);
++	raw_spin_unlock_irqrestore(&pmu->lock, flags);
+ }
+ 
+ static int rapl_pmu_event_add(struct perf_event *event, int mode)
+@@ -282,14 +282,14 @@ static int rapl_pmu_event_add(struct perf_event *event, int mode)
+ 	struct hw_perf_event *hwc = &event->hw;
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&pmu->lock, flags);
++	raw_spin_lock_irqsave(&pmu->lock, flags);
+ 
+ 	hwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;
+ 
+ 	if (mode & PERF_EF_START)
+ 		__rapl_pmu_event_start(pmu, event);
+ 
+-	spin_unlock_irqrestore(&pmu->lock, flags);
++	raw_spin_unlock_irqrestore(&pmu->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -551,7 +551,7 @@ static int rapl_cpu_prepare(int cpu)
+ 	if (!pmu)
+ 		return -1;
+ 
+-	spin_lock_init(&pmu->lock);
++	raw_spin_lock_init(&pmu->lock);
+ 
+ 	INIT_LIST_HEAD(&pmu->active_list);
+ 
+diff --git a/arch/x86/kernel/dumpstack_32.c b/arch/x86/kernel/dumpstack_32.c
+index 5abd4cd4230c..1282817bb4c3 100644
+--- a/arch/x86/kernel/dumpstack_32.c
++++ b/arch/x86/kernel/dumpstack_32.c
+@@ -42,7 +42,7 @@ void dump_trace(struct task_struct *task, struct pt_regs *regs,
+ 		unsigned long *stack, unsigned long bp,
+ 		const struct stacktrace_ops *ops, void *data)
+ {
+-	const unsigned cpu = get_cpu();
++	const unsigned cpu = get_cpu_light();
+ 	int graph = 0;
+ 	u32 *prev_esp;
+ 
+@@ -86,7 +86,7 @@ void dump_trace(struct task_struct *task, struct pt_regs *regs,
+ 			break;
+ 		touch_nmi_watchdog();
+ 	}
+-	put_cpu();
++	put_cpu_light();
+ }
+ EXPORT_SYMBOL(dump_trace);
+ 
+diff --git a/arch/x86/kernel/dumpstack_64.c b/arch/x86/kernel/dumpstack_64.c
+index ff86f19b5758..4821f291890f 100644
+--- a/arch/x86/kernel/dumpstack_64.c
++++ b/arch/x86/kernel/dumpstack_64.c
+@@ -152,7 +152,7 @@ void dump_trace(struct task_struct *task, struct pt_regs *regs,
+ 		unsigned long *stack, unsigned long bp,
+ 		const struct stacktrace_ops *ops, void *data)
+ {
+-	const unsigned cpu = get_cpu();
++	const unsigned cpu = get_cpu_light();
+ 	struct thread_info *tinfo;
+ 	unsigned long *irq_stack = (unsigned long *)per_cpu(irq_stack_ptr, cpu);
+ 	unsigned long dummy;
+@@ -241,7 +241,7 @@ void dump_trace(struct task_struct *task, struct pt_regs *regs,
+ 	 * This handles the process stack:
+ 	 */
+ 	bp = ops->walk_stack(tinfo, stack, bp, ops, data, NULL, &graph);
+-	put_cpu();
++	put_cpu_light();
+ }
+ EXPORT_SYMBOL(dump_trace);
+ 
+@@ -255,7 +255,7 @@ show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,
+ 	int cpu;
+ 	int i;
+ 
+-	preempt_disable();
++	migrate_disable();
+ 	cpu = smp_processor_id();
+ 
+ 	irq_stack_end	= (unsigned long *)(per_cpu(irq_stack_ptr, cpu));
+@@ -288,7 +288,7 @@ show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,
+ 		pr_cont(" %016lx", *stack++);
+ 		touch_nmi_watchdog();
+ 	}
+-	preempt_enable();
++	migrate_enable();
+ 
+ 	pr_cont("\n");
+ 	show_trace_log_lvl(task, regs, sp, bp, log_lvl);
+diff --git a/arch/x86/kernel/entry_32.S b/arch/x86/kernel/entry_32.S
+index fe611c4ae3ff..4c1a1b5bfaab 100644
+--- a/arch/x86/kernel/entry_32.S
++++ b/arch/x86/kernel/entry_32.S
+@@ -359,8 +359,24 @@ END(ret_from_exception)
+ ENTRY(resume_kernel)
+ 	DISABLE_INTERRUPTS(CLBR_ANY)
+ need_resched:
++	# preempt count == 0 + NEED_RS set?
+ 	cmpl $0,PER_CPU_VAR(__preempt_count)
++#ifndef CONFIG_PREEMPT_LAZY
+ 	jnz restore_all
++#else
++	jz test_int_off
++
++	# atleast preempt count == 0 ?
++	cmpl $_PREEMPT_ENABLED,PER_CPU_VAR(__preempt_count)
++	jne restore_all
++
++	cmpl $0,TI_preempt_lazy_count(%ebp)	# non-zero preempt_lazy_count ?
++	jnz restore_all
++
++	testl $_TIF_NEED_RESCHED_LAZY, TI_flags(%ebp)
++	jz restore_all
++test_int_off:
++#endif
+ 	testl $X86_EFLAGS_IF,PT_EFLAGS(%esp)	# interrupts off (exception path) ?
+ 	jz restore_all
+ 	call preempt_schedule_irq
+@@ -591,7 +607,7 @@ ENDPROC(system_call)
+ 	ALIGN
+ 	RING0_PTREGS_FRAME		# can't unwind into user space anyway
+ work_pending:
+-	testb $_TIF_NEED_RESCHED, %cl
++	testl $_TIF_NEED_RESCHED_MASK, %ecx
+ 	jz work_notifysig
+ work_resched:
+ 	call schedule
+@@ -604,7 +620,7 @@ work_resched:
+ 	andl $_TIF_WORK_MASK, %ecx	# is there any work to be done other
+ 					# than syscall tracing?
+ 	jz restore_all
+-	testb $_TIF_NEED_RESCHED, %cl
++	testl $_TIF_NEED_RESCHED_MASK, %ecx
+ 	jnz work_resched
+ 
+ work_notifysig:				# deal with pending signals and
+diff --git a/arch/x86/kernel/entry_64.S b/arch/x86/kernel/entry_64.S
+index a3255ca219ea..f4cbb1272e27 100644
+--- a/arch/x86/kernel/entry_64.S
++++ b/arch/x86/kernel/entry_64.S
+@@ -454,8 +454,8 @@ sysret_check:
+ 	/* Handle reschedules */
+ 	/* edx:	work, edi: workmask */
+ sysret_careful:
+-	bt $TIF_NEED_RESCHED,%edx
+-	jnc sysret_signal
++	testl $_TIF_NEED_RESCHED_MASK,%edx
++	jz sysret_signal
+ 	TRACE_IRQS_ON
+ 	ENABLE_INTERRUPTS(CLBR_NONE)
+ 	pushq_cfi %rdi
+@@ -554,8 +554,8 @@ GLOBAL(int_with_check)
+ 	/* First do a reschedule test. */
+ 	/* edx:	work, edi: workmask */
+ int_careful:
+-	bt $TIF_NEED_RESCHED,%edx
+-	jnc  int_very_careful
++	testl $_TIF_NEED_RESCHED_MASK,%edx
++	jz  int_very_careful
+ 	TRACE_IRQS_ON
+ 	ENABLE_INTERRUPTS(CLBR_NONE)
+ 	pushq_cfi %rdi
+@@ -870,8 +870,8 @@ native_irq_return_ldt:
+ 	/* edi: workmask, edx: work */
+ retint_careful:
+ 	CFI_RESTORE_STATE
+-	bt    $TIF_NEED_RESCHED,%edx
+-	jnc   retint_signal
++	testl $_TIF_NEED_RESCHED_MASK,%edx
++	jz   retint_signal
+ 	TRACE_IRQS_ON
+ 	ENABLE_INTERRUPTS(CLBR_NONE)
+ 	pushq_cfi %rdi
+@@ -903,7 +903,22 @@ retint_signal:
+ 	/* rcx:	 threadinfo. interrupts off. */
+ ENTRY(retint_kernel)
+ 	cmpl $0,PER_CPU_VAR(__preempt_count)
++#ifndef CONFIG_PREEMPT_LAZY
+ 	jnz  retint_restore_args
++#else
++	jz  check_int_off
++
++	# atleast preempt count == 0 ?
++	cmpl $_PREEMPT_ENABLED,PER_CPU_VAR(__preempt_count)
++	jnz retint_restore_args
++
++	cmpl $0, TI_preempt_lazy_count(%rcx)
++	jnz retint_restore_args
++
++	bt $TIF_NEED_RESCHED_LAZY,TI_flags(%rcx)
++	jnc  retint_restore_args
++check_int_off:
++#endif
+ 	bt   $9,EFLAGS-ARGOFFSET(%rsp)	/* interrupts off? */
+ 	jnc  retint_restore_args
+ 	call preempt_schedule_irq
+@@ -1119,6 +1134,7 @@ bad_gs:
+ 	jmp  2b
+ 	.previous
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ /* Call softirq on interrupt stack. Interrupts are off. */
+ ENTRY(do_softirq_own_stack)
+ 	CFI_STARTPROC
+@@ -1138,6 +1154,7 @@ ENTRY(do_softirq_own_stack)
+ 	ret
+ 	CFI_ENDPROC
+ END(do_softirq_own_stack)
++#endif
+ 
+ #ifdef CONFIG_XEN
+ idtentry xen_hypervisor_callback xen_do_hypervisor_callback has_error_code=0
+@@ -1305,7 +1322,7 @@ paranoid_userspace:
+ 	movq %rsp,%rdi			/* &pt_regs */
+ 	call sync_regs
+ 	movq %rax,%rsp			/* switch stack for scheduling */
+-	testl $_TIF_NEED_RESCHED,%ebx
++	testl $_TIF_NEED_RESCHED_MASK,%ebx
+ 	jnz paranoid_schedule
+ 	movl %ebx,%edx			/* arg3: thread flags */
+ 	TRACE_IRQS_ON
+diff --git a/arch/x86/kernel/irq_32.c b/arch/x86/kernel/irq_32.c
+index 63ce838e5a54..b889f5ba4fa8 100644
+--- a/arch/x86/kernel/irq_32.c
++++ b/arch/x86/kernel/irq_32.c
+@@ -142,6 +142,7 @@ void irq_ctx_init(int cpu)
+ 	       cpu, per_cpu(hardirq_stack, cpu),  per_cpu(softirq_stack, cpu));
+ }
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ void do_softirq_own_stack(void)
+ {
+ 	struct thread_info *curstk;
+@@ -160,6 +161,7 @@ void do_softirq_own_stack(void)
+ 
+ 	call_on_stack(__do_softirq, isp);
+ }
++#endif
+ 
+ bool handle_irq(unsigned irq, struct pt_regs *regs)
+ {
+diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
+index dc3d52a245cf..66ada0bb5710 100644
+--- a/arch/x86/kernel/kvm.c
++++ b/arch/x86/kernel/kvm.c
+@@ -36,6 +36,7 @@
+ #include <linux/kprobes.h>
+ #include <linux/debugfs.h>
+ #include <linux/nmi.h>
++#include <linux/wait-simple.h>
+ #include <asm/timer.h>
+ #include <asm/cpu.h>
+ #include <asm/traps.h>
+@@ -91,14 +92,14 @@ static void kvm_io_delay(void)
+ 
+ struct kvm_task_sleep_node {
+ 	struct hlist_node link;
+-	wait_queue_head_t wq;
++	struct swait_head wq;
+ 	u32 token;
+ 	int cpu;
+ 	bool halted;
+ };
+ 
+ static struct kvm_task_sleep_head {
+-	spinlock_t lock;
++	raw_spinlock_t lock;
+ 	struct hlist_head list;
+ } async_pf_sleepers[KVM_TASK_SLEEP_HASHSIZE];
+ 
+@@ -122,17 +123,17 @@ void kvm_async_pf_task_wait(u32 token)
+ 	u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);
+ 	struct kvm_task_sleep_head *b = &async_pf_sleepers[key];
+ 	struct kvm_task_sleep_node n, *e;
+-	DEFINE_WAIT(wait);
++	DEFINE_SWAITER(wait);
+ 
+ 	rcu_irq_enter();
+ 
+-	spin_lock(&b->lock);
++	raw_spin_lock(&b->lock);
+ 	e = _find_apf_task(b, token);
+ 	if (e) {
+ 		/* dummy entry exist -> wake up was delivered ahead of PF */
+ 		hlist_del(&e->link);
+ 		kfree(e);
+-		spin_unlock(&b->lock);
++		raw_spin_unlock(&b->lock);
+ 
+ 		rcu_irq_exit();
+ 		return;
+@@ -141,13 +142,13 @@ void kvm_async_pf_task_wait(u32 token)
+ 	n.token = token;
+ 	n.cpu = smp_processor_id();
+ 	n.halted = is_idle_task(current) || preempt_count() > 1;
+-	init_waitqueue_head(&n.wq);
++	init_swait_head(&n.wq);
+ 	hlist_add_head(&n.link, &b->list);
+-	spin_unlock(&b->lock);
++	raw_spin_unlock(&b->lock);
+ 
+ 	for (;;) {
+ 		if (!n.halted)
+-			prepare_to_wait(&n.wq, &wait, TASK_UNINTERRUPTIBLE);
++			swait_prepare(&n.wq, &wait, TASK_UNINTERRUPTIBLE);
+ 		if (hlist_unhashed(&n.link))
+ 			break;
+ 
+@@ -168,7 +169,7 @@ void kvm_async_pf_task_wait(u32 token)
+ 		rcu_irq_enter();
+ 	}
+ 	if (!n.halted)
+-		finish_wait(&n.wq, &wait);
++		swait_finish(&n.wq, &wait);
+ 
+ 	rcu_irq_exit();
+ 	return;
+@@ -180,8 +181,8 @@ static void apf_task_wake_one(struct kvm_task_sleep_node *n)
+ 	hlist_del_init(&n->link);
+ 	if (n->halted)
+ 		smp_send_reschedule(n->cpu);
+-	else if (waitqueue_active(&n->wq))
+-		wake_up(&n->wq);
++	else if (swaitqueue_active(&n->wq))
++		swait_wake(&n->wq);
+ }
+ 
+ static void apf_task_wake_all(void)
+@@ -191,14 +192,14 @@ static void apf_task_wake_all(void)
+ 	for (i = 0; i < KVM_TASK_SLEEP_HASHSIZE; i++) {
+ 		struct hlist_node *p, *next;
+ 		struct kvm_task_sleep_head *b = &async_pf_sleepers[i];
+-		spin_lock(&b->lock);
++		raw_spin_lock(&b->lock);
+ 		hlist_for_each_safe(p, next, &b->list) {
+ 			struct kvm_task_sleep_node *n =
+ 				hlist_entry(p, typeof(*n), link);
+ 			if (n->cpu == smp_processor_id())
+ 				apf_task_wake_one(n);
+ 		}
+-		spin_unlock(&b->lock);
++		raw_spin_unlock(&b->lock);
+ 	}
+ }
+ 
+@@ -214,7 +215,7 @@ void kvm_async_pf_task_wake(u32 token)
+ 	}
+ 
+ again:
+-	spin_lock(&b->lock);
++	raw_spin_lock(&b->lock);
+ 	n = _find_apf_task(b, token);
+ 	if (!n) {
+ 		/*
+@@ -227,17 +228,17 @@ again:
+ 			 * Allocation failed! Busy wait while other cpu
+ 			 * handles async PF.
+ 			 */
+-			spin_unlock(&b->lock);
++			raw_spin_unlock(&b->lock);
+ 			cpu_relax();
+ 			goto again;
+ 		}
+ 		n->token = token;
+ 		n->cpu = smp_processor_id();
+-		init_waitqueue_head(&n->wq);
++		init_swait_head(&n->wq);
+ 		hlist_add_head(&n->link, &b->list);
+ 	} else
+ 		apf_task_wake_one(n);
+-	spin_unlock(&b->lock);
++	raw_spin_unlock(&b->lock);
+ 	return;
+ }
+ EXPORT_SYMBOL_GPL(kvm_async_pf_task_wake);
+@@ -488,7 +489,7 @@ void __init kvm_guest_init(void)
+ 	paravirt_ops_setup();
+ 	register_reboot_notifier(&kvm_pv_reboot_nb);
+ 	for (i = 0; i < KVM_TASK_SLEEP_HASHSIZE; i++)
+-		spin_lock_init(&async_pf_sleepers[i].lock);
++		raw_spin_lock_init(&async_pf_sleepers[i].lock);
+ 	if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF))
+ 		x86_init.irqs.trap_init = kvm_apf_trap_init;
+ 
+diff --git a/arch/x86/kernel/process_32.c b/arch/x86/kernel/process_32.c
+index 603c4f99cb5a..ae3ac253f181 100644
+--- a/arch/x86/kernel/process_32.c
++++ b/arch/x86/kernel/process_32.c
+@@ -35,6 +35,7 @@
+ #include <linux/uaccess.h>
+ #include <linux/io.h>
+ #include <linux/kdebug.h>
++#include <linux/highmem.h>
+ 
+ #include <asm/pgtable.h>
+ #include <asm/ldt.h>
+@@ -214,6 +215,35 @@ start_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)
+ }
+ EXPORT_SYMBOL_GPL(start_thread);
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++static void switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p)
++{
++	int i;
++
++	/*
++	 * Clear @prev's kmap_atomic mappings
++	 */
++	for (i = 0; i < prev_p->kmap_idx; i++) {
++		int idx = i + KM_TYPE_NR * smp_processor_id();
++		pte_t *ptep = kmap_pte - idx;
++
++		kpte_clear_flush(ptep, __fix_to_virt(FIX_KMAP_BEGIN + idx));
++	}
++	/*
++	 * Restore @next_p's kmap_atomic mappings
++	 */
++	for (i = 0; i < next_p->kmap_idx; i++) {
++		int idx = i + KM_TYPE_NR * smp_processor_id();
++
++		if (!pte_none(next_p->kmap_pte[i]))
++			set_pte(kmap_pte - idx, next_p->kmap_pte[i]);
++	}
++}
++#else
++static inline void
++switch_kmaps(struct task_struct *prev_p, struct task_struct *next_p) { }
++#endif
++
+ 
+ /*
+  *	switch_to(x,y) should switch tasks from x to y.
+@@ -301,6 +331,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
+ 		     task_thread_info(next_p)->flags & _TIF_WORK_CTXSW_NEXT))
+ 		__switch_to_xtra(prev_p, next_p, tss);
+ 
++	switch_kmaps(prev_p, next_p);
++
+ 	/*
+ 	 * Leave lazy mode, flushing any hypercalls made here.
+ 	 * This must be done before restoring TLS segments so
+diff --git a/arch/x86/kernel/signal.c b/arch/x86/kernel/signal.c
+index ed37a768d0fc..37d2cc072f1b 100644
+--- a/arch/x86/kernel/signal.c
++++ b/arch/x86/kernel/signal.c
+@@ -746,6 +746,14 @@ do_notify_resume(struct pt_regs *regs, void *unused, __u32 thread_info_flags)
+ 		mce_notify_process();
+ #endif /* CONFIG_X86_64 && CONFIG_X86_MCE */
+ 
++#ifdef ARCH_RT_DELAYS_SIGNAL_SEND
++	if (unlikely(current->forced_info.si_signo)) {
++		struct task_struct *t = current;
++		force_sig_info(t->forced_info.si_signo,	&t->forced_info, t);
++		t->forced_info.si_signo = 0;
++	}
++#endif
++
+ 	if (thread_info_flags & _TIF_UPROBE)
+ 		uprobe_notify_resume(regs);
+ 
+diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
+index 07ab8e9733c5..4f0662bb6705 100644
+--- a/arch/x86/kernel/traps.c
++++ b/arch/x86/kernel/traps.c
+@@ -87,9 +87,21 @@ static inline void conditional_sti(struct pt_regs *regs)
+ 		local_irq_enable();
+ }
+ 
+-static inline void preempt_conditional_sti(struct pt_regs *regs)
++static inline void conditional_sti_ist(struct pt_regs *regs)
+ {
++#ifdef CONFIG_X86_64
++	/*
++	 * X86_64 uses a per CPU stack on the IST for certain traps
++	 * like int3. The task can not be preempted when using one
++	 * of these stacks, thus preemption must be disabled, otherwise
++	 * the stack can be corrupted if the task is scheduled out,
++	 * and another task comes in and uses this stack.
++	 *
++	 * On x86_32 the task keeps its own stack and it is OK if the
++	 * task schedules out.
++	 */
+ 	preempt_count_inc();
++#endif
+ 	if (regs->flags & X86_EFLAGS_IF)
+ 		local_irq_enable();
+ }
+@@ -100,11 +112,13 @@ static inline void conditional_cli(struct pt_regs *regs)
+ 		local_irq_disable();
+ }
+ 
+-static inline void preempt_conditional_cli(struct pt_regs *regs)
++static inline void conditional_cli_ist(struct pt_regs *regs)
+ {
+ 	if (regs->flags & X86_EFLAGS_IF)
+ 		local_irq_disable();
++#ifdef CONFIG_X86_64
+ 	preempt_count_dec();
++#endif
+ }
+ 
+ static nokprobe_inline int
+@@ -372,9 +386,9 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
+ 	 * as we may switch to the interrupt stack.
+ 	 */
+ 	debug_stack_usage_inc();
+-	preempt_conditional_sti(regs);
++	conditional_sti_ist(regs);
+ 	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, NULL);
+-	preempt_conditional_cli(regs);
++	conditional_cli_ist(regs);
+ 	debug_stack_usage_dec();
+ exit:
+ 	exception_exit(prev_state);
+@@ -517,12 +531,12 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
+ 	debug_stack_usage_inc();
+ 
+ 	/* It's safe to allow irq's after DR6 has been saved */
+-	preempt_conditional_sti(regs);
++	conditional_sti_ist(regs);
+ 
+ 	if (regs->flags & X86_VM_MASK) {
+ 		handle_vm86_trap((struct kernel_vm86_regs *) regs, error_code,
+ 					X86_TRAP_DB);
+-		preempt_conditional_cli(regs);
++		conditional_cli_ist(regs);
+ 		debug_stack_usage_dec();
+ 		goto exit;
+ 	}
+@@ -542,7 +556,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
+ 	si_code = get_si_code(tsk->thread.debugreg6);
+ 	if (tsk->thread.debugreg6 & (DR_STEP | DR_TRAP_BITS) || user_icebp)
+ 		send_sigtrap(tsk, regs, error_code, si_code);
+-	preempt_conditional_cli(regs);
++	conditional_cli_ist(regs);
+ 	debug_stack_usage_dec();
+ 
+ exit:
+diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
+index de8e50040124..33caf685e978 100644
+--- a/arch/x86/kvm/lapic.c
++++ b/arch/x86/kvm/lapic.c
+@@ -1034,8 +1034,38 @@ static void update_divide_count(struct kvm_lapic *apic)
+ 				   apic->divide_count);
+ }
+ 
++
++static enum hrtimer_restart apic_timer_fn(struct hrtimer *data);
++
++static void apic_timer_expired(struct hrtimer *data)
++{
++	int ret, i = 0;
++	enum hrtimer_restart r;
++	struct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);
++
++	r = apic_timer_fn(data);
++
++	if (r == HRTIMER_RESTART) {
++		do {
++			ret = hrtimer_start_expires(data, HRTIMER_MODE_ABS);
++			if (ret == -ETIME)
++				hrtimer_add_expires_ns(&ktimer->timer,
++							ktimer->period);
++			i++;
++		} while (ret == -ETIME && i < 10);
++
++		if (ret == -ETIME) {
++			printk_once(KERN_ERR "%s: failed to reprogram timer\n",
++			       __func__);
++			WARN_ON_ONCE(1);
++		}
++	}
++}
++
++
+ static void start_apic_timer(struct kvm_lapic *apic)
+ {
++	int ret;
+ 	ktime_t now;
+ 	atomic_set(&apic->lapic_timer.pending, 0);
+ 
+@@ -1065,9 +1095,11 @@ static void start_apic_timer(struct kvm_lapic *apic)
+ 			}
+ 		}
+ 
+-		hrtimer_start(&apic->lapic_timer.timer,
++		ret = hrtimer_start(&apic->lapic_timer.timer,
+ 			      ktime_add_ns(now, apic->lapic_timer.period),
+ 			      HRTIMER_MODE_ABS);
++		if (ret == -ETIME)
++			apic_timer_expired(&apic->lapic_timer.timer);
+ 
+ 		apic_debug("%s: bus cycle is %" PRId64 "ns, now 0x%016"
+ 			   PRIx64 ", "
+@@ -1097,8 +1129,10 @@ static void start_apic_timer(struct kvm_lapic *apic)
+ 			ns = (tscdeadline - guest_tsc) * 1000000ULL;
+ 			do_div(ns, this_tsc_khz);
+ 		}
+-		hrtimer_start(&apic->lapic_timer.timer,
++		ret = hrtimer_start(&apic->lapic_timer.timer,
+ 			ktime_add_ns(now, ns), HRTIMER_MODE_ABS);
++		if (ret == -ETIME)
++			apic_timer_expired(&apic->lapic_timer.timer);
+ 
+ 		local_irq_restore(flags);
+ 	}
+@@ -1539,7 +1573,7 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
+ 	struct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);
+ 	struct kvm_lapic *apic = container_of(ktimer, struct kvm_lapic, lapic_timer);
+ 	struct kvm_vcpu *vcpu = apic->vcpu;
+-	wait_queue_head_t *q = &vcpu->wq;
++	struct swait_head *q = &vcpu->wq;
+ 
+ 	/*
+ 	 * There is a race window between reading and incrementing, but we do
+@@ -1553,8 +1587,8 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
+ 		kvm_make_request(KVM_REQ_PENDING_TIMER, vcpu);
+ 	}
+ 
+-	if (waitqueue_active(q))
+-		wake_up_interruptible(q);
++	if (swaitqueue_active(q))
++		swait_wake_interruptible(q);
+ 
+ 	if (lapic_is_periodic(apic)) {
+ 		hrtimer_add_expires_ns(&ktimer->timer, ktimer->period);
+@@ -1587,6 +1621,7 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu)
+ 	hrtimer_init(&apic->lapic_timer.timer, CLOCK_MONOTONIC,
+ 		     HRTIMER_MODE_ABS);
+ 	apic->lapic_timer.timer.function = apic_timer_fn;
++	apic->lapic_timer.timer.irqsafe = 1;
+ 
+ 	/*
+ 	 * APIC is created enabled. This will prevent kvm_lapic_set_base from
+@@ -1708,7 +1743,8 @@ void __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)
+ 
+ 	timer = &vcpu->arch.apic->lapic_timer.timer;
+ 	if (hrtimer_cancel(timer))
+-		hrtimer_start_expires(timer, HRTIMER_MODE_ABS);
++		if (hrtimer_start_expires(timer, HRTIMER_MODE_ABS) == -ETIME)
++			apic_timer_expired(timer);
+ }
+ 
+ /*
+diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
+index 1e839d801055..27c96bde44a5 100644
+--- a/arch/x86/kvm/x86.c
++++ b/arch/x86/kvm/x86.c
+@@ -5773,6 +5773,13 @@ int kvm_arch_init(void *opaque)
+ 		goto out;
+ 	}
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++	if (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {
++		printk(KERN_ERR "RT requires X86_FEATURE_CONSTANT_TSC\n");
++		return -EOPNOTSUPP;
++	}
++#endif
++
+ 	r = kvm_mmu_module_init();
+ 	if (r)
+ 		goto out_free_percpu;
+diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
+index 6fa245ae52c5..bc77c3ce4a9c 100644
+--- a/arch/x86/mm/fault.c
++++ b/arch/x86/mm/fault.c
+@@ -1128,7 +1128,7 @@ __do_page_fault(struct pt_regs *regs, unsigned long error_code,
+ 	 * If we're in an interrupt, have no user context or are running
+ 	 * in an atomic region then we must not take the fault:
+ 	 */
+-	if (unlikely(in_atomic() || !mm)) {
++	if (unlikely(!mm || pagefault_disabled())) {
+ 		bad_area_nosemaphore(regs, error_code, address);
+ 		return;
+ 	}
+diff --git a/arch/x86/mm/highmem_32.c b/arch/x86/mm/highmem_32.c
+index 4500142bc4aa..7f96844472bb 100644
+--- a/arch/x86/mm/highmem_32.c
++++ b/arch/x86/mm/highmem_32.c
+@@ -32,6 +32,7 @@ EXPORT_SYMBOL(kunmap);
+  */
+ void *kmap_atomic_prot(struct page *page, pgprot_t prot)
+ {
++	pte_t pte = mk_pte(page, prot);
+ 	unsigned long vaddr;
+ 	int idx, type;
+ 
+@@ -45,7 +46,10 @@ void *kmap_atomic_prot(struct page *page, pgprot_t prot)
+ 	idx = type + KM_TYPE_NR*smp_processor_id();
+ 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+ 	BUG_ON(!pte_none(*(kmap_pte-idx)));
+-	set_pte(kmap_pte-idx, mk_pte(page, prot));
++#ifdef CONFIG_PREEMPT_RT_FULL
++	current->kmap_pte[type] = pte;
++#endif
++	set_pte(kmap_pte-idx, pte);
+ 	arch_flush_lazy_mmu_mode();
+ 
+ 	return (void *)vaddr;
+@@ -88,6 +92,9 @@ void __kunmap_atomic(void *kvaddr)
+ 		 * is a bad idea also, in case the page changes cacheability
+ 		 * attributes or becomes a protected page in a hypervisor.
+ 		 */
++#ifdef CONFIG_PREEMPT_RT_FULL
++		current->kmap_pte[type] = __pte(0);
++#endif
+ 		kpte_clear_flush(kmap_pte-idx, vaddr);
+ 		kmap_atomic_idx_pop();
+ 		arch_flush_lazy_mmu_mode();
+diff --git a/arch/x86/mm/iomap_32.c b/arch/x86/mm/iomap_32.c
+index 7b179b499fa3..62377d67ab07 100644
+--- a/arch/x86/mm/iomap_32.c
++++ b/arch/x86/mm/iomap_32.c
+@@ -56,6 +56,7 @@ EXPORT_SYMBOL_GPL(iomap_free);
+ 
+ void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
+ {
++	pte_t pte = pfn_pte(pfn, prot);
+ 	unsigned long vaddr;
+ 	int idx, type;
+ 
+@@ -64,7 +65,12 @@ void *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)
+ 	type = kmap_atomic_idx_push();
+ 	idx = type + KM_TYPE_NR * smp_processor_id();
+ 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
+-	set_pte(kmap_pte - idx, pfn_pte(pfn, prot));
++	WARN_ON(!pte_none(*(kmap_pte - idx)));
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++	current->kmap_pte[type] = pte;
++#endif
++	set_pte(kmap_pte - idx, pte);
+ 	arch_flush_lazy_mmu_mode();
+ 
+ 	return (void *)vaddr;
+@@ -110,6 +116,9 @@ iounmap_atomic(void __iomem *kvaddr)
+ 		 * is a bad idea also, in case the page changes cacheability
+ 		 * attributes or becomes a protected page in a hypervisor.
+ 		 */
++#ifdef CONFIG_PREEMPT_RT_FULL
++		current->kmap_pte[type] = __pte(0);
++#endif
+ 		kpte_clear_flush(kmap_pte-idx, vaddr);
+ 		kmap_atomic_idx_pop();
+ 	}
+diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c
+index e5545f2105f6..3c4b0318a363 100644
+--- a/arch/x86/mm/pageattr.c
++++ b/arch/x86/mm/pageattr.c
+@@ -211,7 +211,15 @@ static void cpa_flush_array(unsigned long *start, int numpages, int cache,
+ 			    int in_flags, struct page **pages)
+ {
+ 	unsigned int i, level;
++#ifdef CONFIG_PREEMPT
++	/*
++	 * Avoid wbinvd() because it causes latencies on all CPUs,
++	 * regardless of any CPU isolation that may be in effect.
++	 */
++	unsigned long do_wbinvd = 0;
++#else
+ 	unsigned long do_wbinvd = cache && numpages >= 1024; /* 4M threshold */
++#endif
+ 
+ 	BUG_ON(irqs_disabled());
+ 
+diff --git a/arch/x86/platform/uv/tlb_uv.c b/arch/x86/platform/uv/tlb_uv.c
+index 3968d67d366b..7d444650bdd4 100644
+--- a/arch/x86/platform/uv/tlb_uv.c
++++ b/arch/x86/platform/uv/tlb_uv.c
+@@ -714,9 +714,9 @@ static void destination_plugged(struct bau_desc *bau_desc,
+ 
+ 		quiesce_local_uvhub(hmaster);
+ 
+-		spin_lock(&hmaster->queue_lock);
++		raw_spin_lock(&hmaster->queue_lock);
+ 		reset_with_ipi(&bau_desc->distribution, bcp);
+-		spin_unlock(&hmaster->queue_lock);
++		raw_spin_unlock(&hmaster->queue_lock);
+ 
+ 		end_uvhub_quiesce(hmaster);
+ 
+@@ -736,9 +736,9 @@ static void destination_timeout(struct bau_desc *bau_desc,
+ 
+ 		quiesce_local_uvhub(hmaster);
+ 
+-		spin_lock(&hmaster->queue_lock);
++		raw_spin_lock(&hmaster->queue_lock);
+ 		reset_with_ipi(&bau_desc->distribution, bcp);
+-		spin_unlock(&hmaster->queue_lock);
++		raw_spin_unlock(&hmaster->queue_lock);
+ 
+ 		end_uvhub_quiesce(hmaster);
+ 
+@@ -759,7 +759,7 @@ static void disable_for_period(struct bau_control *bcp, struct ptc_stats *stat)
+ 	cycles_t tm1;
+ 
+ 	hmaster = bcp->uvhub_master;
+-	spin_lock(&hmaster->disable_lock);
++	raw_spin_lock(&hmaster->disable_lock);
+ 	if (!bcp->baudisabled) {
+ 		stat->s_bau_disabled++;
+ 		tm1 = get_cycles();
+@@ -772,7 +772,7 @@ static void disable_for_period(struct bau_control *bcp, struct ptc_stats *stat)
+ 			}
+ 		}
+ 	}
+-	spin_unlock(&hmaster->disable_lock);
++	raw_spin_unlock(&hmaster->disable_lock);
+ }
+ 
+ static void count_max_concurr(int stat, struct bau_control *bcp,
+@@ -835,7 +835,7 @@ static void record_send_stats(cycles_t time1, cycles_t time2,
+  */
+ static void uv1_throttle(struct bau_control *hmaster, struct ptc_stats *stat)
+ {
+-	spinlock_t *lock = &hmaster->uvhub_lock;
++	raw_spinlock_t *lock = &hmaster->uvhub_lock;
+ 	atomic_t *v;
+ 
+ 	v = &hmaster->active_descriptor_count;
+@@ -968,7 +968,7 @@ static int check_enable(struct bau_control *bcp, struct ptc_stats *stat)
+ 	struct bau_control *hmaster;
+ 
+ 	hmaster = bcp->uvhub_master;
+-	spin_lock(&hmaster->disable_lock);
++	raw_spin_lock(&hmaster->disable_lock);
+ 	if (bcp->baudisabled && (get_cycles() >= bcp->set_bau_on_time)) {
+ 		stat->s_bau_reenabled++;
+ 		for_each_present_cpu(tcpu) {
+@@ -980,10 +980,10 @@ static int check_enable(struct bau_control *bcp, struct ptc_stats *stat)
+ 				tbcp->period_giveups = 0;
+ 			}
+ 		}
+-		spin_unlock(&hmaster->disable_lock);
++		raw_spin_unlock(&hmaster->disable_lock);
+ 		return 0;
+ 	}
+-	spin_unlock(&hmaster->disable_lock);
++	raw_spin_unlock(&hmaster->disable_lock);
+ 	return -1;
+ }
+ 
+@@ -1899,9 +1899,9 @@ static void __init init_per_cpu_tunables(void)
+ 		bcp->cong_reps			= congested_reps;
+ 		bcp->disabled_period =		sec_2_cycles(disabled_period);
+ 		bcp->giveup_limit =		giveup_limit;
+-		spin_lock_init(&bcp->queue_lock);
+-		spin_lock_init(&bcp->uvhub_lock);
+-		spin_lock_init(&bcp->disable_lock);
++		raw_spin_lock_init(&bcp->queue_lock);
++		raw_spin_lock_init(&bcp->uvhub_lock);
++		raw_spin_lock_init(&bcp->disable_lock);
+ 	}
+ }
+ 
+diff --git a/arch/x86/platform/uv/uv_time.c b/arch/x86/platform/uv/uv_time.c
+index a244237f3cfa..a718fe0d2e73 100644
+--- a/arch/x86/platform/uv/uv_time.c
++++ b/arch/x86/platform/uv/uv_time.c
+@@ -58,7 +58,7 @@ static DEFINE_PER_CPU(struct clock_event_device, cpu_ced);
+ 
+ /* There is one of these allocated per node */
+ struct uv_rtc_timer_head {
+-	spinlock_t	lock;
++	raw_spinlock_t	lock;
+ 	/* next cpu waiting for timer, local node relative: */
+ 	int		next_cpu;
+ 	/* number of cpus on this node: */
+@@ -178,7 +178,7 @@ static __init int uv_rtc_allocate_timers(void)
+ 				uv_rtc_deallocate_timers();
+ 				return -ENOMEM;
+ 			}
+-			spin_lock_init(&head->lock);
++			raw_spin_lock_init(&head->lock);
+ 			head->ncpus = uv_blade_nr_possible_cpus(bid);
+ 			head->next_cpu = -1;
+ 			blade_info[bid] = head;
+@@ -232,7 +232,7 @@ static int uv_rtc_set_timer(int cpu, u64 expires)
+ 	unsigned long flags;
+ 	int next_cpu;
+ 
+-	spin_lock_irqsave(&head->lock, flags);
++	raw_spin_lock_irqsave(&head->lock, flags);
+ 
+ 	next_cpu = head->next_cpu;
+ 	*t = expires;
+@@ -244,12 +244,12 @@ static int uv_rtc_set_timer(int cpu, u64 expires)
+ 		if (uv_setup_intr(cpu, expires)) {
+ 			*t = ULLONG_MAX;
+ 			uv_rtc_find_next_timer(head, pnode);
+-			spin_unlock_irqrestore(&head->lock, flags);
++			raw_spin_unlock_irqrestore(&head->lock, flags);
+ 			return -ETIME;
+ 		}
+ 	}
+ 
+-	spin_unlock_irqrestore(&head->lock, flags);
++	raw_spin_unlock_irqrestore(&head->lock, flags);
+ 	return 0;
+ }
+ 
+@@ -268,7 +268,7 @@ static int uv_rtc_unset_timer(int cpu, int force)
+ 	unsigned long flags;
+ 	int rc = 0;
+ 
+-	spin_lock_irqsave(&head->lock, flags);
++	raw_spin_lock_irqsave(&head->lock, flags);
+ 
+ 	if ((head->next_cpu == bcpu && uv_read_rtc(NULL) >= *t) || force)
+ 		rc = 1;
+@@ -280,7 +280,7 @@ static int uv_rtc_unset_timer(int cpu, int force)
+ 			uv_rtc_find_next_timer(head, pnode);
+ 	}
+ 
+-	spin_unlock_irqrestore(&head->lock, flags);
++	raw_spin_unlock_irqrestore(&head->lock, flags);
+ 
+ 	return rc;
+ }
+@@ -300,13 +300,18 @@ static int uv_rtc_unset_timer(int cpu, int force)
+ static cycle_t uv_read_rtc(struct clocksource *cs)
+ {
+ 	unsigned long offset;
++	cycle_t cycles;
+ 
++	preempt_disable();
+ 	if (uv_get_min_hub_revision_id() == 1)
+ 		offset = 0;
+ 	else
+ 		offset = (uv_blade_processor_id() * L1_CACHE_BYTES) % PAGE_SIZE;
+ 
+-	return (cycle_t)uv_read_local_mmr(UVH_RTC | offset);
++	cycles = (cycle_t)uv_read_local_mmr(UVH_RTC | offset);
++	preempt_enable();
++
++	return cycles;
+ }
+ 
+ /*
+diff --git a/arch/xtensa/mm/fault.c b/arch/xtensa/mm/fault.c
+index 9e3571a6535c..9696ab258ee6 100644
+--- a/arch/xtensa/mm/fault.c
++++ b/arch/xtensa/mm/fault.c
+@@ -57,7 +57,7 @@ void do_page_fault(struct pt_regs *regs)
+ 	/* If we're in an interrupt or have no user
+ 	 * context, we must not take the fault..
+ 	 */
+-	if (in_atomic() || !mm) {
++	if (!mm || pagefault_disabled()) {
+ 		bad_page_fault(regs, address, SIGSEGV);
+ 		return;
+ 	}
+diff --git a/block/blk-core.c b/block/blk-core.c
+index 93f9152fc271..e88517d92075 100644
+--- a/block/blk-core.c
++++ b/block/blk-core.c
+@@ -100,6 +100,9 @@ void blk_rq_init(struct request_queue *q, struct request *rq)
+ 
+ 	INIT_LIST_HEAD(&rq->queuelist);
+ 	INIT_LIST_HEAD(&rq->timeout_list);
++#if CONFIG_PREEMPT_RT_FULL
++	INIT_WORK(&rq->work, __blk_mq_complete_request_remote_work);
++#endif
+ 	rq->cpu = -1;
+ 	rq->q = q;
+ 	rq->__sector = (sector_t) -1;
+@@ -194,7 +197,7 @@ EXPORT_SYMBOL(blk_delay_queue);
+  **/
+ void blk_start_queue(struct request_queue *q)
+ {
+-	WARN_ON(!irqs_disabled());
++	WARN_ON_NONRT(!irqs_disabled());
+ 
+ 	queue_flag_clear(QUEUE_FLAG_STOPPED, q);
+ 	__blk_run_queue(q);
+@@ -627,7 +630,7 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
+ 	q->bypass_depth = 1;
+ 	__set_bit(QUEUE_FLAG_BYPASS, &q->queue_flags);
+ 
+-	init_waitqueue_head(&q->mq_freeze_wq);
++	init_swait_head(&q->mq_freeze_wq);
+ 
+ 	if (blkcg_init_queue(q))
+ 		goto fail_bdi;
+@@ -3037,7 +3040,7 @@ static void queue_unplugged(struct request_queue *q, unsigned int depth,
+ 		blk_run_queue_async(q);
+ 	else
+ 		__blk_run_queue(q);
+-	spin_unlock(q->queue_lock);
++	spin_unlock_irq(q->queue_lock);
+ }
+ 
+ static void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)
+@@ -3085,7 +3088,6 @@ EXPORT_SYMBOL(blk_check_plugged);
+ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
+ {
+ 	struct request_queue *q;
+-	unsigned long flags;
+ 	struct request *rq;
+ 	LIST_HEAD(list);
+ 	unsigned int depth;
+@@ -3105,11 +3107,6 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
+ 	q = NULL;
+ 	depth = 0;
+ 
+-	/*
+-	 * Save and disable interrupts here, to avoid doing it for every
+-	 * queue lock we have to take.
+-	 */
+-	local_irq_save(flags);
+ 	while (!list_empty(&list)) {
+ 		rq = list_entry_rq(list.next);
+ 		list_del_init(&rq->queuelist);
+@@ -3122,7 +3119,7 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
+ 				queue_unplugged(q, depth, from_schedule);
+ 			q = rq->q;
+ 			depth = 0;
+-			spin_lock(q->queue_lock);
++			spin_lock_irq(q->queue_lock);
+ 		}
+ 
+ 		/*
+@@ -3149,8 +3146,6 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
+ 	 */
+ 	if (q)
+ 		queue_unplugged(q, depth, from_schedule);
+-
+-	local_irq_restore(flags);
+ }
+ 
+ void blk_finish_plug(struct blk_plug *plug)
+diff --git a/block/blk-ioc.c b/block/blk-ioc.c
+index 1a27f45ec776..28f467e636cc 100644
+--- a/block/blk-ioc.c
++++ b/block/blk-ioc.c
+@@ -7,6 +7,7 @@
+ #include <linux/bio.h>
+ #include <linux/blkdev.h>
+ #include <linux/slab.h>
++#include <linux/delay.h>
+ 
+ #include "blk.h"
+ 
+@@ -109,7 +110,7 @@ static void ioc_release_fn(struct work_struct *work)
+ 			spin_unlock(q->queue_lock);
+ 		} else {
+ 			spin_unlock_irqrestore(&ioc->lock, flags);
+-			cpu_relax();
++			cpu_chill();
+ 			spin_lock_irqsave_nested(&ioc->lock, flags, 1);
+ 		}
+ 	}
+@@ -187,7 +188,7 @@ retry:
+ 			spin_unlock(icq->q->queue_lock);
+ 		} else {
+ 			spin_unlock_irqrestore(&ioc->lock, flags);
+-			cpu_relax();
++			cpu_chill();
+ 			goto retry;
+ 		}
+ 	}
+diff --git a/block/blk-iopoll.c b/block/blk-iopoll.c
+index 0736729d6494..3e21e31d0d7e 100644
+--- a/block/blk-iopoll.c
++++ b/block/blk-iopoll.c
+@@ -35,6 +35,7 @@ void blk_iopoll_sched(struct blk_iopoll *iop)
+ 	list_add_tail(&iop->list, this_cpu_ptr(&blk_cpu_iopoll));
+ 	__raise_softirq_irqoff(BLOCK_IOPOLL_SOFTIRQ);
+ 	local_irq_restore(flags);
++	preempt_check_resched_rt();
+ }
+ EXPORT_SYMBOL(blk_iopoll_sched);
+ 
+@@ -132,6 +133,7 @@ static void blk_iopoll_softirq(struct softirq_action *h)
+ 		__raise_softirq_irqoff(BLOCK_IOPOLL_SOFTIRQ);
+ 
+ 	local_irq_enable();
++	preempt_check_resched_rt();
+ }
+ 
+ /**
+@@ -201,6 +203,7 @@ static int blk_iopoll_cpu_notify(struct notifier_block *self,
+ 				 this_cpu_ptr(&blk_cpu_iopoll));
+ 		__raise_softirq_irqoff(BLOCK_IOPOLL_SOFTIRQ);
+ 		local_irq_enable();
++		preempt_check_resched_rt();
+ 	}
+ 
+ 	return NOTIFY_OK;
+diff --git a/block/blk-mq-cpu.c b/block/blk-mq-cpu.c
+index bb3ed488f7b5..628c6c13c482 100644
+--- a/block/blk-mq-cpu.c
++++ b/block/blk-mq-cpu.c
+@@ -16,7 +16,7 @@
+ #include "blk-mq.h"
+ 
+ static LIST_HEAD(blk_mq_cpu_notify_list);
+-static DEFINE_RAW_SPINLOCK(blk_mq_cpu_notify_lock);
++static DEFINE_SPINLOCK(blk_mq_cpu_notify_lock);
+ 
+ static int blk_mq_main_cpu_notify(struct notifier_block *self,
+ 				  unsigned long action, void *hcpu)
+@@ -25,7 +25,10 @@ static int blk_mq_main_cpu_notify(struct notifier_block *self,
+ 	struct blk_mq_cpu_notifier *notify;
+ 	int ret = NOTIFY_OK;
+ 
+-	raw_spin_lock(&blk_mq_cpu_notify_lock);
++	if (action != CPU_POST_DEAD)
++		return NOTIFY_OK;
++
++	spin_lock(&blk_mq_cpu_notify_lock);
+ 
+ 	list_for_each_entry(notify, &blk_mq_cpu_notify_list, list) {
+ 		ret = notify->notify(notify->data, action, cpu);
+@@ -33,7 +36,7 @@ static int blk_mq_main_cpu_notify(struct notifier_block *self,
+ 			break;
+ 	}
+ 
+-	raw_spin_unlock(&blk_mq_cpu_notify_lock);
++	spin_unlock(&blk_mq_cpu_notify_lock);
+ 	return ret;
+ }
+ 
+@@ -41,16 +44,16 @@ void blk_mq_register_cpu_notifier(struct blk_mq_cpu_notifier *notifier)
+ {
+ 	BUG_ON(!notifier->notify);
+ 
+-	raw_spin_lock(&blk_mq_cpu_notify_lock);
++	spin_lock(&blk_mq_cpu_notify_lock);
+ 	list_add_tail(&notifier->list, &blk_mq_cpu_notify_list);
+-	raw_spin_unlock(&blk_mq_cpu_notify_lock);
++	spin_unlock(&blk_mq_cpu_notify_lock);
+ }
+ 
+ void blk_mq_unregister_cpu_notifier(struct blk_mq_cpu_notifier *notifier)
+ {
+-	raw_spin_lock(&blk_mq_cpu_notify_lock);
++	spin_lock(&blk_mq_cpu_notify_lock);
+ 	list_del(&notifier->list);
+-	raw_spin_unlock(&blk_mq_cpu_notify_lock);
++	spin_unlock(&blk_mq_cpu_notify_lock);
+ }
+ 
+ void blk_mq_init_cpu_notifier(struct blk_mq_cpu_notifier *notifier,
+diff --git a/block/blk-mq.c b/block/blk-mq.c
+index 691959ecb80f..2c666222b289 100644
+--- a/block/blk-mq.c
++++ b/block/blk-mq.c
+@@ -85,7 +85,7 @@ static int blk_mq_queue_enter(struct request_queue *q)
+ 		if (percpu_ref_tryget_live(&q->mq_usage_counter))
+ 			return 0;
+ 
+-		ret = wait_event_interruptible(q->mq_freeze_wq,
++		ret = swait_event_interruptible(q->mq_freeze_wq,
+ 				!q->mq_freeze_depth || blk_queue_dying(q));
+ 		if (blk_queue_dying(q))
+ 			return -ENODEV;
+@@ -104,7 +104,7 @@ static void blk_mq_usage_counter_release(struct percpu_ref *ref)
+ 	struct request_queue *q =
+ 		container_of(ref, struct request_queue, mq_usage_counter);
+ 
+-	wake_up_all(&q->mq_freeze_wq);
++	swait_wake_all(&q->mq_freeze_wq);
+ }
+ 
+ static void blk_mq_freeze_queue_start(struct request_queue *q)
+@@ -123,7 +123,7 @@ static void blk_mq_freeze_queue_start(struct request_queue *q)
+ 
+ static void blk_mq_freeze_queue_wait(struct request_queue *q)
+ {
+-	wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->mq_usage_counter));
++	swait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->mq_usage_counter));
+ }
+ 
+ /*
+@@ -146,7 +146,7 @@ static void blk_mq_unfreeze_queue(struct request_queue *q)
+ 	spin_unlock_irq(q->queue_lock);
+ 	if (wake) {
+ 		percpu_ref_reinit(&q->mq_usage_counter);
+-		wake_up_all(&q->mq_freeze_wq);
++		swait_wake_all(&q->mq_freeze_wq);
+ 	}
+ }
+ 
+@@ -194,6 +194,9 @@ static void blk_mq_rq_ctx_init(struct request_queue *q, struct blk_mq_ctx *ctx,
+ 	rq->resid_len = 0;
+ 	rq->sense = NULL;
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++	INIT_WORK(&rq->work, __blk_mq_complete_request_remote_work);
++#endif
+ 	INIT_LIST_HEAD(&rq->timeout_list);
+ 	rq->timeout = 0;
+ 
+@@ -313,6 +316,17 @@ void blk_mq_end_request(struct request *rq, int error)
+ }
+ EXPORT_SYMBOL(blk_mq_end_request);
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++
++void __blk_mq_complete_request_remote_work(struct work_struct *work)
++{
++	struct request *rq = container_of(work, struct request, work);
++
++	rq->q->softirq_done_fn(rq);
++}
++
++#else
++
+ static void __blk_mq_complete_request_remote(void *data)
+ {
+ 	struct request *rq = data;
+@@ -320,6 +334,8 @@ static void __blk_mq_complete_request_remote(void *data)
+ 	rq->q->softirq_done_fn(rq);
+ }
+ 
++#endif
++
+ static void blk_mq_ipi_complete_request(struct request *rq)
+ {
+ 	struct blk_mq_ctx *ctx = rq->mq_ctx;
+@@ -331,19 +347,23 @@ static void blk_mq_ipi_complete_request(struct request *rq)
+ 		return;
+ 	}
+ 
+-	cpu = get_cpu();
++	cpu = get_cpu_light();
+ 	if (!test_bit(QUEUE_FLAG_SAME_FORCE, &rq->q->queue_flags))
+ 		shared = cpus_share_cache(cpu, ctx->cpu);
+ 
+ 	if (cpu != ctx->cpu && !shared && cpu_online(ctx->cpu)) {
++#ifdef CONFIG_PREEMPT_RT_FULL
++		schedule_work_on(ctx->cpu, &rq->work);
++#else
+ 		rq->csd.func = __blk_mq_complete_request_remote;
+ 		rq->csd.info = rq;
+ 		rq->csd.flags = 0;
+ 		smp_call_function_single_async(ctx->cpu, &rq->csd);
++#endif
+ 	} else {
+ 		rq->q->softirq_done_fn(rq);
+ 	}
+-	put_cpu();
++	put_cpu_light();
+ }
+ 
+ void __blk_mq_complete_request(struct request *rq)
+@@ -814,9 +834,9 @@ void blk_mq_run_queues(struct request_queue *q, bool async)
+ 		    test_bit(BLK_MQ_S_STOPPED, &hctx->state))
+ 			continue;
+ 
+-		preempt_disable();
++		migrate_disable();
+ 		blk_mq_run_hw_queue(hctx, async);
+-		preempt_enable();
++		migrate_enable();
+ 	}
+ }
+ EXPORT_SYMBOL(blk_mq_run_queues);
+@@ -843,9 +863,9 @@ void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)
+ {
+ 	clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+ 
+-	preempt_disable();
++	migrate_disable();
+ 	blk_mq_run_hw_queue(hctx, false);
+-	preempt_enable();
++	migrate_enable();
+ }
+ EXPORT_SYMBOL(blk_mq_start_hw_queue);
+ 
+@@ -870,9 +890,9 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
+ 			continue;
+ 
+ 		clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+-		preempt_disable();
++		migrate_disable();
+ 		blk_mq_run_hw_queue(hctx, async);
+-		preempt_enable();
++		migrate_enable();
+ 	}
+ }
+ EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
+@@ -1478,7 +1498,7 @@ static int blk_mq_hctx_notify(void *data, unsigned long action,
+ {
+ 	struct blk_mq_hw_ctx *hctx = data;
+ 
+-	if (action == CPU_DEAD || action == CPU_DEAD_FROZEN)
++	if (action == CPU_POST_DEAD)
+ 		return blk_mq_hctx_cpu_offline(hctx, cpu);
+ 
+ 	/*
+diff --git a/block/blk-mq.h b/block/blk-mq.h
+index d567d5283ffa..d1d78dfe4123 100644
+--- a/block/blk-mq.h
++++ b/block/blk-mq.h
+@@ -73,7 +73,10 @@ struct blk_align_bitmap {
+ static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
+ 					   unsigned int cpu)
+ {
+-	return per_cpu_ptr(q->queue_ctx, cpu);
++	struct blk_mq_ctx *ctx;
++
++	ctx = per_cpu_ptr(q->queue_ctx, cpu);
++	return ctx;
+ }
+ 
+ /*
+@@ -84,12 +87,12 @@ static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
+  */
+ static inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
+ {
+-	return __blk_mq_get_ctx(q, get_cpu());
++	return __blk_mq_get_ctx(q, get_cpu_light());
+ }
+ 
+ static inline void blk_mq_put_ctx(struct blk_mq_ctx *ctx)
+ {
+-	put_cpu();
++	put_cpu_light();
+ }
+ 
+ struct blk_mq_alloc_data {
+diff --git a/block/blk-softirq.c b/block/blk-softirq.c
+index 53b1737e978d..81c3c0a62edf 100644
+--- a/block/blk-softirq.c
++++ b/block/blk-softirq.c
+@@ -51,6 +51,7 @@ static void trigger_softirq(void *data)
+ 		raise_softirq_irqoff(BLOCK_SOFTIRQ);
+ 
+ 	local_irq_restore(flags);
++	preempt_check_resched_rt();
+ }
+ 
+ /*
+@@ -93,6 +94,7 @@ static int blk_cpu_notify(struct notifier_block *self, unsigned long action,
+ 				 this_cpu_ptr(&blk_cpu_done));
+ 		raise_softirq_irqoff(BLOCK_SOFTIRQ);
+ 		local_irq_enable();
++		preempt_check_resched_rt();
+ 	}
+ 
+ 	return NOTIFY_OK;
+@@ -150,6 +152,7 @@ do_local:
+ 		goto do_local;
+ 
+ 	local_irq_restore(flags);
++	preempt_check_resched_rt();
+ }
+ 
+ /**
+diff --git a/block/bounce.c b/block/bounce.c
+index ab21ba203d5c..97b734aed81a 100644
+--- a/block/bounce.c
++++ b/block/bounce.c
+@@ -54,11 +54,11 @@ static void bounce_copy_vec(struct bio_vec *to, unsigned char *vfrom)
+ 	unsigned long flags;
+ 	unsigned char *vto;
+ 
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	vto = kmap_atomic(to->bv_page);
+ 	memcpy(vto + to->bv_offset, vfrom, to->bv_len);
+ 	kunmap_atomic(vto);
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ }
+ 
+ #else /* CONFIG_HIGHMEM */
+diff --git a/crypto/algapi.c b/crypto/algapi.c
+index 314cc745f2f8..0269c5ed43f9 100644
+--- a/crypto/algapi.c
++++ b/crypto/algapi.c
+@@ -698,13 +698,13 @@ EXPORT_SYMBOL_GPL(crypto_spawn_tfm2);
+ 
+ int crypto_register_notifier(struct notifier_block *nb)
+ {
+-	return blocking_notifier_chain_register(&crypto_chain, nb);
++	return srcu_notifier_chain_register(&crypto_chain, nb);
+ }
+ EXPORT_SYMBOL_GPL(crypto_register_notifier);
+ 
+ int crypto_unregister_notifier(struct notifier_block *nb)
+ {
+-	return blocking_notifier_chain_unregister(&crypto_chain, nb);
++	return srcu_notifier_chain_unregister(&crypto_chain, nb);
+ }
+ EXPORT_SYMBOL_GPL(crypto_unregister_notifier);
+ 
+diff --git a/crypto/api.c b/crypto/api.c
+index 7db2e89a3114..25a77b2c2c34 100644
+--- a/crypto/api.c
++++ b/crypto/api.c
+@@ -31,7 +31,7 @@ EXPORT_SYMBOL_GPL(crypto_alg_list);
+ DECLARE_RWSEM(crypto_alg_sem);
+ EXPORT_SYMBOL_GPL(crypto_alg_sem);
+ 
+-BLOCKING_NOTIFIER_HEAD(crypto_chain);
++SRCU_NOTIFIER_HEAD(crypto_chain);
+ EXPORT_SYMBOL_GPL(crypto_chain);
+ 
+ static struct crypto_alg *crypto_larval_wait(struct crypto_alg *alg);
+@@ -236,10 +236,10 @@ int crypto_probing_notify(unsigned long val, void *v)
+ {
+ 	int ok;
+ 
+-	ok = blocking_notifier_call_chain(&crypto_chain, val, v);
++	ok = srcu_notifier_call_chain(&crypto_chain, val, v);
+ 	if (ok == NOTIFY_DONE) {
+ 		request_module("cryptomgr");
+-		ok = blocking_notifier_call_chain(&crypto_chain, val, v);
++		ok = srcu_notifier_call_chain(&crypto_chain, val, v);
+ 	}
+ 
+ 	return ok;
+diff --git a/crypto/internal.h b/crypto/internal.h
+index bd39bfc92eab..a5db167cba84 100644
+--- a/crypto/internal.h
++++ b/crypto/internal.h
+@@ -48,7 +48,7 @@ struct crypto_larval {
+ 
+ extern struct list_head crypto_alg_list;
+ extern struct rw_semaphore crypto_alg_sem;
+-extern struct blocking_notifier_head crypto_chain;
++extern struct srcu_notifier_head crypto_chain;
+ 
+ #ifdef CONFIG_PROC_FS
+ void __init crypto_init_proc(void);
+@@ -142,7 +142,7 @@ static inline int crypto_is_moribund(struct crypto_alg *alg)
+ 
+ static inline void crypto_notify(unsigned long val, void *v)
+ {
+-	blocking_notifier_call_chain(&crypto_chain, val, v);
++	srcu_notifier_call_chain(&crypto_chain, val, v);
+ }
+ 
+ #endif	/* _CRYPTO_INTERNAL_H */
+diff --git a/drivers/acpi/acpica/acglobal.h b/drivers/acpi/acpica/acglobal.h
+index ebf02cc10a43..f15ef3dc468c 100644
+--- a/drivers/acpi/acpica/acglobal.h
++++ b/drivers/acpi/acpica/acglobal.h
+@@ -112,7 +112,7 @@ ACPI_GLOBAL(u8, acpi_gbl_global_lock_pending);
+  * interrupt level
+  */
+ ACPI_GLOBAL(acpi_spinlock, acpi_gbl_gpe_lock);	/* For GPE data structs and registers */
+-ACPI_GLOBAL(acpi_spinlock, acpi_gbl_hardware_lock);	/* For ACPI H/W except GPE registers */
++ACPI_GLOBAL(acpi_raw_spinlock, acpi_gbl_hardware_lock);	/* For ACPI H/W except GPE registers */
+ ACPI_GLOBAL(acpi_spinlock, acpi_gbl_reference_count_lock);
+ 
+ /* Mutex for _OSI support */
+diff --git a/drivers/acpi/acpica/hwregs.c b/drivers/acpi/acpica/hwregs.c
+index a4c34d2c556b..b40826b27624 100644
+--- a/drivers/acpi/acpica/hwregs.c
++++ b/drivers/acpi/acpica/hwregs.c
+@@ -269,14 +269,14 @@ acpi_status acpi_hw_clear_acpi_status(void)
+ 			  ACPI_BITMASK_ALL_FIXED_STATUS,
+ 			  ACPI_FORMAT_UINT64(acpi_gbl_xpm1a_status.address)));
+ 
+-	lock_flags = acpi_os_acquire_lock(acpi_gbl_hardware_lock);
++	raw_spin_lock_irqsave(acpi_gbl_hardware_lock, lock_flags);
+ 
+ 	/* Clear the fixed events in PM1 A/B */
+ 
+ 	status = acpi_hw_register_write(ACPI_REGISTER_PM1_STATUS,
+ 					ACPI_BITMASK_ALL_FIXED_STATUS);
+ 
+-	acpi_os_release_lock(acpi_gbl_hardware_lock, lock_flags);
++	raw_spin_unlock_irqrestore(acpi_gbl_hardware_lock, lock_flags);
+ 
+ 	if (ACPI_FAILURE(status)) {
+ 		goto exit;
+diff --git a/drivers/acpi/acpica/hwxface.c b/drivers/acpi/acpica/hwxface.c
+index 96d007df65ec..e57a03ed379c 100644
+--- a/drivers/acpi/acpica/hwxface.c
++++ b/drivers/acpi/acpica/hwxface.c
+@@ -374,7 +374,7 @@ acpi_status acpi_write_bit_register(u32 register_id, u32 value)
+ 		return_ACPI_STATUS(AE_BAD_PARAMETER);
+ 	}
+ 
+-	lock_flags = acpi_os_acquire_lock(acpi_gbl_hardware_lock);
++	raw_spin_lock_irqsave(acpi_gbl_hardware_lock, lock_flags);
+ 
+ 	/*
+ 	 * At this point, we know that the parent register is one of the
+@@ -435,7 +435,7 @@ acpi_status acpi_write_bit_register(u32 register_id, u32 value)
+ 
+ unlock_and_exit:
+ 
+-	acpi_os_release_lock(acpi_gbl_hardware_lock, lock_flags);
++	raw_spin_unlock_irqrestore(acpi_gbl_hardware_lock, lock_flags);
+ 	return_ACPI_STATUS(status);
+ }
+ 
+diff --git a/drivers/acpi/acpica/utmutex.c b/drivers/acpi/acpica/utmutex.c
+index 82717fff9ffc..508c5770ec71 100644
+--- a/drivers/acpi/acpica/utmutex.c
++++ b/drivers/acpi/acpica/utmutex.c
+@@ -88,7 +88,7 @@ acpi_status acpi_ut_mutex_initialize(void)
+ 		return_ACPI_STATUS (status);
+ 	}
+ 
+-	status = acpi_os_create_lock (&acpi_gbl_hardware_lock);
++	status = acpi_os_create_raw_lock (&acpi_gbl_hardware_lock);
+ 	if (ACPI_FAILURE (status)) {
+ 		return_ACPI_STATUS (status);
+ 	}
+@@ -141,7 +141,7 @@ void acpi_ut_mutex_terminate(void)
+ 	/* Delete the spinlocks */
+ 
+ 	acpi_os_delete_lock(acpi_gbl_gpe_lock);
+-	acpi_os_delete_lock(acpi_gbl_hardware_lock);
++	acpi_os_delete_raw_lock(acpi_gbl_hardware_lock);
+ 	acpi_os_delete_lock(acpi_gbl_reference_count_lock);
+ 
+ 	/* Delete the reader/writer lock */
+diff --git a/drivers/ata/libata-sff.c b/drivers/ata/libata-sff.c
+index 12d337754e4a..a6f7a089f4a9 100644
+--- a/drivers/ata/libata-sff.c
++++ b/drivers/ata/libata-sff.c
+@@ -678,9 +678,9 @@ unsigned int ata_sff_data_xfer_noirq(struct ata_device *dev, unsigned char *buf,
+ 	unsigned long flags;
+ 	unsigned int consumed;
+ 
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	consumed = ata_sff_data_xfer32(dev, buf, buflen, rw);
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 
+ 	return consumed;
+ }
+@@ -719,7 +719,7 @@ static void ata_pio_sector(struct ata_queued_cmd *qc)
+ 		unsigned long flags;
+ 
+ 		/* FIXME: use a bounce buffer */
+-		local_irq_save(flags);
++		local_irq_save_nort(flags);
+ 		buf = kmap_atomic(page);
+ 
+ 		/* do the actual data transfer */
+@@ -727,7 +727,7 @@ static void ata_pio_sector(struct ata_queued_cmd *qc)
+ 				       do_write);
+ 
+ 		kunmap_atomic(buf);
+-		local_irq_restore(flags);
++		local_irq_restore_nort(flags);
+ 	} else {
+ 		buf = page_address(page);
+ 		ap->ops->sff_data_xfer(qc->dev, buf + offset, qc->sect_size,
+@@ -864,7 +864,7 @@ next_sg:
+ 		unsigned long flags;
+ 
+ 		/* FIXME: use bounce buffer */
+-		local_irq_save(flags);
++		local_irq_save_nort(flags);
+ 		buf = kmap_atomic(page);
+ 
+ 		/* do the actual data transfer */
+@@ -872,7 +872,7 @@ next_sg:
+ 								count, rw);
+ 
+ 		kunmap_atomic(buf);
+-		local_irq_restore(flags);
++		local_irq_restore_nort(flags);
+ 	} else {
+ 		buf = page_address(page);
+ 		consumed = ap->ops->sff_data_xfer(dev,  buf + offset,
+diff --git a/drivers/block/zram/zram_drv.c b/drivers/block/zram/zram_drv.c
+index 7e94459a489a..d6b91472f357 100644
+--- a/drivers/block/zram/zram_drv.c
++++ b/drivers/block/zram/zram_drv.c
+@@ -336,6 +336,8 @@ static struct zram_meta *zram_meta_alloc(u64 disksize)
+ 		goto free_table;
+ 	}
+ 
++	zram_meta_init_table_locks(meta, disksize);
++
+ 	return meta;
+ 
+ free_table:
+@@ -425,12 +427,12 @@ static int zram_decompress_page(struct zram *zram, char *mem, u32 index)
+ 	unsigned long handle;
+ 	size_t size;
+ 
+-	bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
++	zram_lock_table(&meta->table[index]);
+ 	handle = meta->table[index].handle;
+ 	size = zram_get_obj_size(meta, index);
+ 
+ 	if (!handle || zram_test_flag(meta, index, ZRAM_ZERO)) {
+-		bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
++		zram_unlock_table(&meta->table[index]);
+ 		memset(mem, 0, PAGE_SIZE);
+ 		return 0;
+ 	}
+@@ -441,7 +443,7 @@ static int zram_decompress_page(struct zram *zram, char *mem, u32 index)
+ 	else
+ 		ret = zcomp_decompress(zram->comp, cmem, size, mem);
+ 	zs_unmap_object(meta->mem_pool, handle);
+-	bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
++	zram_unlock_table(&meta->table[index]);
+ 
+ 	/* Should NEVER happen. Return bio error if it does. */
+ 	if (unlikely(ret)) {
+@@ -461,14 +463,14 @@ static int zram_bvec_read(struct zram *zram, struct bio_vec *bvec,
+ 	struct zram_meta *meta = zram->meta;
+ 	page = bvec->bv_page;
+ 
+-	bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
++	zram_lock_table(&meta->table[index]);
+ 	if (unlikely(!meta->table[index].handle) ||
+ 			zram_test_flag(meta, index, ZRAM_ZERO)) {
+-		bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
++		zram_unlock_table(&meta->table[index]);
+ 		handle_zero_page(bvec);
+ 		return 0;
+ 	}
+-	bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
++	zram_unlock_table(&meta->table[index]);
+ 
+ 	if (is_partial_io(bvec))
+ 		/* Use  a temporary buffer to decompress the page */
+@@ -563,10 +565,10 @@ static int zram_bvec_write(struct zram *zram, struct bio_vec *bvec, u32 index,
+ 		if (user_mem)
+ 			kunmap_atomic(user_mem);
+ 		/* Free memory associated with this sector now. */
+-		bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
++		zram_lock_table(&meta->table[index]);
+ 		zram_free_page(zram, index);
+ 		zram_set_flag(meta, index, ZRAM_ZERO);
+-		bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
++		zram_unlock_table(&meta->table[index]);
+ 
+ 		atomic64_inc(&zram->stats.zero_pages);
+ 		ret = 0;
+@@ -626,12 +628,12 @@ static int zram_bvec_write(struct zram *zram, struct bio_vec *bvec, u32 index,
+ 	 * Free memory associated with this sector
+ 	 * before overwriting unused sectors.
+ 	 */
+-	bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
++	zram_lock_table(&meta->table[index]);
+ 	zram_free_page(zram, index);
+ 
+ 	meta->table[index].handle = handle;
+ 	zram_set_obj_size(meta, index, clen);
+-	bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
++	zram_unlock_table(&meta->table[index]);
+ 
+ 	/* Update stats */
+ 	atomic64_add(clen, &zram->stats.compr_data_size);
+@@ -698,9 +700,9 @@ static void zram_bio_discard(struct zram *zram, u32 index,
+ 	}
+ 
+ 	while (n >= PAGE_SIZE) {
+-		bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
++		zram_lock_table(&meta->table[index]);
+ 		zram_free_page(zram, index);
+-		bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
++		zram_unlock_table(&meta->table[index]);
+ 		atomic64_inc(&zram->stats.notify_free);
+ 		index++;
+ 		n -= PAGE_SIZE;
+@@ -939,9 +941,9 @@ static void zram_slot_free_notify(struct block_device *bdev,
+ 	zram = bdev->bd_disk->private_data;
+ 	meta = zram->meta;
+ 
+-	bit_spin_lock(ZRAM_ACCESS, &meta->table[index].value);
++	zram_lock_table(&meta->table[index]);
+ 	zram_free_page(zram, index);
+-	bit_spin_unlock(ZRAM_ACCESS, &meta->table[index].value);
++	zram_unlock_table(&meta->table[index]);
+ 	atomic64_inc(&zram->stats.notify_free);
+ }
+ 
+diff --git a/drivers/block/zram/zram_drv.h b/drivers/block/zram/zram_drv.h
+index c6ee271317f5..73e7e70d9a8d 100644
+--- a/drivers/block/zram/zram_drv.h
++++ b/drivers/block/zram/zram_drv.h
+@@ -78,6 +78,9 @@ enum zram_pageflags {
+ struct zram_table_entry {
+ 	unsigned long handle;
+ 	unsigned long value;
++#ifdef CONFIG_PREEMPT_RT_BASE
++	spinlock_t lock;
++#endif
+ };
+ 
+ struct zram_stats {
+@@ -120,4 +123,42 @@ struct zram {
+ 
+ 	char compressor[10];
+ };
++
++#ifndef CONFIG_PREEMPT_RT_BASE
++static inline void zram_lock_table(struct zram_table_entry *table)
++{
++	bit_spin_lock(ZRAM_ACCESS, &table->value);
++}
++
++static inline void zram_unlock_table(struct zram_table_entry *table)
++{
++	bit_spin_unlock(ZRAM_ACCESS, &table->value);
++}
++
++static inline void zram_meta_init_table_locks(struct zram_meta *meta, u64 disksize) { }
++#else /* CONFIG_PREEMPT_RT_BASE */
++static inline void zram_lock_table(struct zram_table_entry *table)
++{
++	spin_lock(&table->lock);
++	__set_bit(ZRAM_ACCESS, &table->value);
++}
++
++static inline void zram_unlock_table(struct zram_table_entry *table)
++{
++	__clear_bit(ZRAM_ACCESS, &table->value);
++	spin_unlock(&table->lock);
++}
++
++static inline void zram_meta_init_table_locks(struct zram_meta *meta, u64 disksize)
++{
++        size_t num_pages = disksize >> PAGE_SHIFT;
++        size_t index;
++
++        for (index = 0; index < num_pages; index++) {
++		spinlock_t *lock = &meta->table[index].lock;
++		spin_lock_init(lock);
++        }
++}
++#endif /* CONFIG_PREEMPT_RT_BASE */
++
+ #endif
+diff --git a/drivers/char/random.c b/drivers/char/random.c
+index d55156fc064d..acb8e7c218d1 100644
+--- a/drivers/char/random.c
++++ b/drivers/char/random.c
+@@ -776,8 +776,6 @@ static void add_timer_randomness(struct timer_rand_state *state, unsigned num)
+ 	} sample;
+ 	long delta, delta2, delta3;
+ 
+-	preempt_disable();
+-
+ 	sample.jiffies = jiffies;
+ 	sample.cycles = random_get_entropy();
+ 	sample.num = num;
+@@ -818,7 +816,6 @@ static void add_timer_randomness(struct timer_rand_state *state, unsigned num)
+ 		 */
+ 		credit_entropy_bits(r, min_t(int, fls(delta>>1), 11));
+ 	}
+-	preempt_enable();
+ }
+ 
+ void add_input_randomness(unsigned int type, unsigned int code,
+@@ -871,28 +868,27 @@ static __u32 get_reg(struct fast_pool *f, struct pt_regs *regs)
+ 	return *(ptr + f->reg_idx++);
+ }
+ 
+-void add_interrupt_randomness(int irq, int irq_flags)
++void add_interrupt_randomness(int irq, int irq_flags, __u64 ip)
+ {
+ 	struct entropy_store	*r;
+ 	struct fast_pool	*fast_pool = this_cpu_ptr(&irq_randomness);
+-	struct pt_regs		*regs = get_irq_regs();
+ 	unsigned long		now = jiffies;
+ 	cycles_t		cycles = random_get_entropy();
+ 	__u32			c_high, j_high;
+-	__u64			ip;
+ 	unsigned long		seed;
+ 	int			credit = 0;
+ 
+ 	if (cycles == 0)
+-		cycles = get_reg(fast_pool, regs);
++		cycles = get_reg(fast_pool, NULL);
+ 	c_high = (sizeof(cycles) > 4) ? cycles >> 32 : 0;
+ 	j_high = (sizeof(now) > 4) ? now >> 32 : 0;
+ 	fast_pool->pool[0] ^= cycles ^ j_high ^ irq;
+ 	fast_pool->pool[1] ^= now ^ c_high;
+-	ip = regs ? instruction_pointer(regs) : _RET_IP_;
++	if (!ip)
++		ip = _RET_IP_;
+ 	fast_pool->pool[2] ^= ip;
+ 	fast_pool->pool[3] ^= (sizeof(ip) > 4) ? ip >> 32 :
+-		get_reg(fast_pool, regs);
++		get_reg(fast_pool, NULL);
+ 
+ 	fast_mix(fast_pool);
+ 	add_interrupt_bench(cycles);
+diff --git a/drivers/clocksource/tcb_clksrc.c b/drivers/clocksource/tcb_clksrc.c
+index 8bdbc45c6dad..43f1c6bc6e28 100644
+--- a/drivers/clocksource/tcb_clksrc.c
++++ b/drivers/clocksource/tcb_clksrc.c
+@@ -23,8 +23,7 @@
+  *     this 32 bit free-running counter. the second channel is not used.
+  *
+  *   - The third channel may be used to provide a 16-bit clockevent
+- *     source, used in either periodic or oneshot mode.  This runs
+- *     at 32 KiHZ, and can handle delays of up to two seconds.
++ *     source, used in either periodic or oneshot mode.
+  *
+  * A boot clocksource and clockevent source are also currently needed,
+  * unless the relevant platforms (ARM/AT91, AVR32/AT32) are changed so
+@@ -74,6 +73,7 @@ static struct clocksource clksrc = {
+ struct tc_clkevt_device {
+ 	struct clock_event_device	clkevt;
+ 	struct clk			*clk;
++	u32				freq;
+ 	void __iomem			*regs;
+ };
+ 
+@@ -82,13 +82,6 @@ static struct tc_clkevt_device *to_tc_clkevt(struct clock_event_device *clkevt)
+ 	return container_of(clkevt, struct tc_clkevt_device, clkevt);
+ }
+ 
+-/* For now, we always use the 32K clock ... this optimizes for NO_HZ,
+- * because using one of the divided clocks would usually mean the
+- * tick rate can never be less than several dozen Hz (vs 0.5 Hz).
+- *
+- * A divided clock could be good for high resolution timers, since
+- * 30.5 usec resolution can seem "low".
+- */
+ static u32 timer_clock;
+ 
+ static void tc_mode(enum clock_event_mode m, struct clock_event_device *d)
+@@ -111,11 +104,12 @@ static void tc_mode(enum clock_event_mode m, struct clock_event_device *d)
+ 	case CLOCK_EVT_MODE_PERIODIC:
+ 		clk_enable(tcd->clk);
+ 
+-		/* slow clock, count up to RC, then irq and restart */
++		/* count up to RC, then irq and restart */
+ 		__raw_writel(timer_clock
+ 				| ATMEL_TC_WAVE | ATMEL_TC_WAVESEL_UP_AUTO,
+ 				regs + ATMEL_TC_REG(2, CMR));
+-		__raw_writel((32768 + HZ/2) / HZ, tcaddr + ATMEL_TC_REG(2, RC));
++		__raw_writel((tcd->freq + HZ / 2) / HZ,
++			     tcaddr + ATMEL_TC_REG(2, RC));
+ 
+ 		/* Enable clock and interrupts on RC compare */
+ 		__raw_writel(ATMEL_TC_CPCS, regs + ATMEL_TC_REG(2, IER));
+@@ -128,7 +122,7 @@ static void tc_mode(enum clock_event_mode m, struct clock_event_device *d)
+ 	case CLOCK_EVT_MODE_ONESHOT:
+ 		clk_enable(tcd->clk);
+ 
+-		/* slow clock, count up to RC, then irq and stop */
++		/* count up to RC, then irq and stop */
+ 		__raw_writel(timer_clock | ATMEL_TC_CPCSTOP
+ 				| ATMEL_TC_WAVE | ATMEL_TC_WAVESEL_UP_AUTO,
+ 				regs + ATMEL_TC_REG(2, CMR));
+@@ -157,8 +151,12 @@ static struct tc_clkevt_device clkevt = {
+ 		.name		= "tc_clkevt",
+ 		.features	= CLOCK_EVT_FEAT_PERIODIC
+ 					| CLOCK_EVT_FEAT_ONESHOT,
++#ifdef CONFIG_ATMEL_TCB_CLKSRC_USE_SLOW_CLOCK
+ 		/* Should be lower than at91rm9200's system timer */
+ 		.rating		= 125,
++#else
++		.rating		= 200,
++#endif
+ 		.set_next_event	= tc_next_event,
+ 		.set_mode	= tc_mode,
+ 	},
+@@ -178,8 +176,9 @@ static irqreturn_t ch2_irq(int irq, void *handle)
+ 	return IRQ_NONE;
+ }
+ 
+-static int __init setup_clkevents(struct atmel_tc *tc, int clk32k_divisor_idx)
++static int __init setup_clkevents(struct atmel_tc *tc, int divisor_idx)
+ {
++	unsigned divisor = atmel_tc_divisors[divisor_idx];
+ 	int ret;
+ 	struct clk *t2_clk = tc->clk[2];
+ 	int irq = tc->irq[2];
+@@ -193,7 +192,11 @@ static int __init setup_clkevents(struct atmel_tc *tc, int clk32k_divisor_idx)
+ 	clkevt.regs = tc->regs;
+ 	clkevt.clk = t2_clk;
+ 
+-	timer_clock = clk32k_divisor_idx;
++	timer_clock = divisor_idx;
++	if (!divisor)
++		clkevt.freq = 32768;
++	else
++		clkevt.freq = clk_get_rate(t2_clk) / divisor;
+ 
+ 	clkevt.clkevt.cpumask = cpumask_of(0);
+ 
+@@ -203,7 +206,7 @@ static int __init setup_clkevents(struct atmel_tc *tc, int clk32k_divisor_idx)
+ 		return ret;
+ 	}
+ 
+-	clockevents_config_and_register(&clkevt.clkevt, 32768, 1, 0xffff);
++	clockevents_config_and_register(&clkevt.clkevt, clkevt.freq, 1, 0xffff);
+ 
+ 	return ret;
+ }
+@@ -340,7 +343,11 @@ static int __init tcb_clksrc_init(void)
+ 		goto err_disable_t1;
+ 
+ 	/* channel 2:  periodic and oneshot timer support */
++#ifdef CONFIG_ATMEL_TCB_CLKSRC_USE_SLOW_CLOCK
+ 	ret = setup_clkevents(tc, clk32k_divisor_idx);
++#else
++	ret = setup_clkevents(tc, best_divisor_idx);
++#endif
+ 	if (ret)
+ 		goto err_unregister_clksrc;
+ 
+diff --git a/drivers/clocksource/timer-atmel-pit.c b/drivers/clocksource/timer-atmel-pit.c
+index d5289098b3df..109ad45835a0 100644
+--- a/drivers/clocksource/timer-atmel-pit.c
++++ b/drivers/clocksource/timer-atmel-pit.c
+@@ -90,6 +90,7 @@ static cycle_t read_pit_clk(struct clocksource *cs)
+ 	return elapsed;
+ }
+ 
++static struct irqaction at91sam926x_pit_irq;
+ /*
+  * Clockevent device:  interrupts every 1/HZ (== pit_cycles * MCK/16)
+  */
+@@ -100,6 +101,8 @@ pit_clkevt_mode(enum clock_event_mode mode, struct clock_event_device *dev)
+ 
+ 	switch (mode) {
+ 	case CLOCK_EVT_MODE_PERIODIC:
++		/* Set up irq handler */
++		setup_irq(at91sam926x_pit_irq.irq, &at91sam926x_pit_irq);
+ 		/* update clocksource counter */
+ 		data->cnt += data->cycle * PIT_PICNT(pit_read(data->base, AT91_PIT_PIVR));
+ 		pit_write(data->base, AT91_PIT_MR,
+@@ -113,6 +116,7 @@ pit_clkevt_mode(enum clock_event_mode mode, struct clock_event_device *dev)
+ 		/* disable irq, leaving the clocksource active */
+ 		pit_write(data->base, AT91_PIT_MR,
+ 			  (data->cycle - 1) | AT91_PIT_PITEN);
++		remove_irq(at91sam926x_pit_irq.irq, &at91sam926x_pit_irq);
+ 		break;
+ 	case CLOCK_EVT_MODE_RESUME:
+ 		break;
+diff --git a/drivers/cpufreq/Kconfig.x86 b/drivers/cpufreq/Kconfig.x86
+index 89ae88f91895..e3108a67e671 100644
+--- a/drivers/cpufreq/Kconfig.x86
++++ b/drivers/cpufreq/Kconfig.x86
+@@ -113,7 +113,7 @@ config X86_POWERNOW_K7_ACPI
+ 
+ config X86_POWERNOW_K8
+ 	tristate "AMD Opteron/Athlon64 PowerNow!"
+-	depends on ACPI && ACPI_PROCESSOR && X86_ACPI_CPUFREQ
++	depends on ACPI && ACPI_PROCESSOR && X86_ACPI_CPUFREQ && !PREEMPT_RT_BASE
+ 	help
+ 	  This adds the CPUFreq driver for K8/early Opteron/Athlon64 processors.
+ 	  Support for K10 and newer processors is now in acpi-cpufreq.
+diff --git a/drivers/cpufreq/cpufreq.c b/drivers/cpufreq/cpufreq.c
+index 90e8deb6c15e..7a9c1a7ecfe5 100644
+--- a/drivers/cpufreq/cpufreq.c
++++ b/drivers/cpufreq/cpufreq.c
+@@ -53,12 +53,6 @@ static inline bool has_target(void)
+ 	return cpufreq_driver->target_index || cpufreq_driver->target;
+ }
+ 
+-/*
+- * rwsem to guarantee that cpufreq driver module doesn't unload during critical
+- * sections
+- */
+-static DECLARE_RWSEM(cpufreq_rwsem);
+-
+ /* internal prototypes */
+ static int __cpufreq_governor(struct cpufreq_policy *policy,
+ 		unsigned int event);
+@@ -205,9 +199,6 @@ struct cpufreq_policy *cpufreq_cpu_get(unsigned int cpu)
+ 	if (cpufreq_disabled() || (cpu >= nr_cpu_ids))
+ 		return NULL;
+ 
+-	if (!down_read_trylock(&cpufreq_rwsem))
+-		return NULL;
+-
+ 	/* get the cpufreq driver */
+ 	read_lock_irqsave(&cpufreq_driver_lock, flags);
+ 
+@@ -220,9 +211,6 @@ struct cpufreq_policy *cpufreq_cpu_get(unsigned int cpu)
+ 
+ 	read_unlock_irqrestore(&cpufreq_driver_lock, flags);
+ 
+-	if (!policy)
+-		up_read(&cpufreq_rwsem);
+-
+ 	return policy;
+ }
+ EXPORT_SYMBOL_GPL(cpufreq_cpu_get);
+@@ -233,7 +221,6 @@ void cpufreq_cpu_put(struct cpufreq_policy *policy)
+ 		return;
+ 
+ 	kobject_put(&policy->kobj);
+-	up_read(&cpufreq_rwsem);
+ }
+ EXPORT_SYMBOL_GPL(cpufreq_cpu_put);
+ 
+@@ -762,9 +749,6 @@ static ssize_t show(struct kobject *kobj, struct attribute *attr, char *buf)
+ 	struct freq_attr *fattr = to_attr(attr);
+ 	ssize_t ret;
+ 
+-	if (!down_read_trylock(&cpufreq_rwsem))
+-		return -EINVAL;
+-
+ 	down_read(&policy->rwsem);
+ 
+ 	if (fattr->show)
+@@ -773,7 +757,6 @@ static ssize_t show(struct kobject *kobj, struct attribute *attr, char *buf)
+ 		ret = -EIO;
+ 
+ 	up_read(&policy->rwsem);
+-	up_read(&cpufreq_rwsem);
+ 
+ 	return ret;
+ }
+@@ -790,9 +773,6 @@ static ssize_t store(struct kobject *kobj, struct attribute *attr,
+ 	if (!cpu_online(policy->cpu))
+ 		goto unlock;
+ 
+-	if (!down_read_trylock(&cpufreq_rwsem))
+-		goto unlock;
+-
+ 	down_write(&policy->rwsem);
+ 
+ 	if (fattr->store)
+@@ -801,8 +781,6 @@ static ssize_t store(struct kobject *kobj, struct attribute *attr,
+ 		ret = -EIO;
+ 
+ 	up_write(&policy->rwsem);
+-
+-	up_read(&cpufreq_rwsem);
+ unlock:
+ 	put_online_cpus();
+ 
+@@ -1142,9 +1120,6 @@ static int __cpufreq_add_dev(struct device *dev, struct subsys_interface *sif)
+ 	}
+ #endif
+ 
+-	if (!down_read_trylock(&cpufreq_rwsem))
+-		return 0;
+-
+ #ifdef CONFIG_HOTPLUG_CPU
+ 	/* Check if this cpu was hot-unplugged earlier and has siblings */
+ 	read_lock_irqsave(&cpufreq_driver_lock, flags);
+@@ -1152,7 +1127,6 @@ static int __cpufreq_add_dev(struct device *dev, struct subsys_interface *sif)
+ 		if (cpumask_test_cpu(cpu, tpolicy->related_cpus)) {
+ 			read_unlock_irqrestore(&cpufreq_driver_lock, flags);
+ 			ret = cpufreq_add_policy_cpu(tpolicy, cpu, dev);
+-			up_read(&cpufreq_rwsem);
+ 			return ret;
+ 		}
+ 	}
+@@ -1288,7 +1262,6 @@ static int __cpufreq_add_dev(struct device *dev, struct subsys_interface *sif)
+ 	up_write(&policy->rwsem);
+ 
+ 	kobject_uevent(&policy->kobj, KOBJ_ADD);
+-	up_read(&cpufreq_rwsem);
+ 
+ 	pr_debug("initialization complete\n");
+ 
+@@ -1314,8 +1287,6 @@ err_set_policy_cpu:
+ 	cpufreq_policy_free(policy);
+ 
+ nomem_out:
+-	up_read(&cpufreq_rwsem);
+-
+ 	return ret;
+ }
+ 
+@@ -2528,19 +2499,20 @@ int cpufreq_unregister_driver(struct cpufreq_driver *driver)
+ 
+ 	pr_debug("unregistering driver %s\n", driver->name);
+ 
++	/* Protect against concurrent cpu hotplug */
++	get_online_cpus();
+ 	subsys_interface_unregister(&cpufreq_interface);
+ 	if (cpufreq_boost_supported())
+ 		cpufreq_sysfs_remove_file(&boost.attr);
+ 
+ 	unregister_hotcpu_notifier(&cpufreq_cpu_notifier);
+ 
+-	down_write(&cpufreq_rwsem);
+ 	write_lock_irqsave(&cpufreq_driver_lock, flags);
+ 
+ 	cpufreq_driver = NULL;
+ 
+ 	write_unlock_irqrestore(&cpufreq_driver_lock, flags);
+-	up_write(&cpufreq_rwsem);
++	put_online_cpus();
+ 
+ 	return 0;
+ }
+diff --git a/drivers/gpio/gpio-omap.c b/drivers/gpio/gpio-omap.c
+index 415682f69214..da96b23ae203 100644
+--- a/drivers/gpio/gpio-omap.c
++++ b/drivers/gpio/gpio-omap.c
+@@ -57,7 +57,7 @@ struct gpio_bank {
+ 	u32 saved_datain;
+ 	u32 level_mask;
+ 	u32 toggle_mask;
+-	spinlock_t lock;
++	raw_spinlock_t lock;
+ 	struct gpio_chip chip;
+ 	struct clk *dbck;
+ 	u32 mod_usage;
+@@ -503,19 +503,19 @@ static int omap_gpio_irq_type(struct irq_data *d, unsigned type)
+ 		(type & (IRQ_TYPE_LEVEL_LOW|IRQ_TYPE_LEVEL_HIGH)))
+ 		return -EINVAL;
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	offset = GPIO_INDEX(bank, gpio);
+ 	retval = omap_set_gpio_triggering(bank, offset, type);
+ 	if (!LINE_USED(bank->mod_usage, offset)) {
+ 		omap_enable_gpio_module(bank, offset);
+ 		omap_set_gpio_direction(bank, offset, 1);
+ 	} else if (!omap_gpio_is_input(bank, BIT(offset))) {
+-		spin_unlock_irqrestore(&bank->lock, flags);
++		raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 		return -EINVAL;
+ 	}
+ 
+ 	bank->irq_usage |= BIT(GPIO_INDEX(bank, gpio));
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 
+ 	if (type & (IRQ_TYPE_LEVEL_LOW | IRQ_TYPE_LEVEL_HIGH))
+ 		__irq_set_handler_locked(d->irq, handle_level_irq);
+@@ -633,14 +633,14 @@ static int omap_set_gpio_wakeup(struct gpio_bank *bank, int gpio, int enable)
+ 		return -EINVAL;
+ 	}
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	if (enable)
+ 		bank->context.wake_en |= gpio_bit;
+ 	else
+ 		bank->context.wake_en &= ~gpio_bit;
+ 
+ 	writel_relaxed(bank->context.wake_en, bank->base + bank->regs->wkup_en);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -675,7 +675,7 @@ static int omap_gpio_request(struct gpio_chip *chip, unsigned offset)
+ 	if (!BANK_USED(bank))
+ 		pm_runtime_get_sync(bank->dev);
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	/* Set trigger to none. You need to enable the desired trigger with
+ 	 * request_irq() or set_irq_type(). Only do this if the IRQ line has
+ 	 * not already been requested.
+@@ -685,7 +685,7 @@ static int omap_gpio_request(struct gpio_chip *chip, unsigned offset)
+ 		omap_enable_gpio_module(bank, offset);
+ 	}
+ 	bank->mod_usage |= BIT(offset);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -695,11 +695,11 @@ static void omap_gpio_free(struct gpio_chip *chip, unsigned offset)
+ 	struct gpio_bank *bank = container_of(chip, struct gpio_bank, chip);
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	bank->mod_usage &= ~(BIT(offset));
+ 	omap_disable_gpio_module(bank, offset);
+ 	omap_reset_gpio(bank, bank->chip.base + offset);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 
+ 	/*
+ 	 * If this is the last gpio to be freed in the bank,
+@@ -799,12 +799,12 @@ static void omap_gpio_irq_shutdown(struct irq_data *d)
+ 	unsigned long flags;
+ 	unsigned offset = GPIO_INDEX(bank, gpio);
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	gpio_unlock_as_irq(&bank->chip, offset);
+ 	bank->irq_usage &= ~(BIT(offset));
+ 	omap_disable_gpio_module(bank, offset);
+ 	omap_reset_gpio(bank, gpio);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 
+ 	/*
+ 	 * If this is the last IRQ to be freed in the bank,
+@@ -828,10 +828,10 @@ static void omap_gpio_mask_irq(struct irq_data *d)
+ 	unsigned int gpio = omap_irq_to_gpio(bank, d->hwirq);
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	omap_set_gpio_irqenable(bank, gpio, 0);
+ 	omap_set_gpio_triggering(bank, GPIO_INDEX(bank, gpio), IRQ_TYPE_NONE);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ }
+ 
+ static void omap_gpio_unmask_irq(struct irq_data *d)
+@@ -842,7 +842,7 @@ static void omap_gpio_unmask_irq(struct irq_data *d)
+ 	u32 trigger = irqd_get_trigger_type(d);
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	if (trigger)
+ 		omap_set_gpio_triggering(bank, GPIO_INDEX(bank, gpio), trigger);
+ 
+@@ -854,7 +854,7 @@ static void omap_gpio_unmask_irq(struct irq_data *d)
+ 	}
+ 
+ 	omap_set_gpio_irqenable(bank, gpio, 1);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ }
+ 
+ /*---------------------------------------------------------------------*/
+@@ -867,9 +867,9 @@ static int omap_mpuio_suspend_noirq(struct device *dev)
+ 					OMAP_MPUIO_GPIO_MASKIT / bank->stride;
+ 	unsigned long		flags;
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	writel_relaxed(0xffff & ~bank->context.wake_en, mask_reg);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -882,9 +882,9 @@ static int omap_mpuio_resume_noirq(struct device *dev)
+ 					OMAP_MPUIO_GPIO_MASKIT / bank->stride;
+ 	unsigned long		flags;
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	writel_relaxed(bank->context.wake_en, mask_reg);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -930,9 +930,9 @@ static int omap_gpio_get_direction(struct gpio_chip *chip, unsigned offset)
+ 
+ 	bank = container_of(chip, struct gpio_bank, chip);
+ 	reg = bank->base + bank->regs->direction;
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	dir = !!(readl_relaxed(reg) & BIT(offset));
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 	return dir;
+ }
+ 
+@@ -942,9 +942,9 @@ static int omap_gpio_input(struct gpio_chip *chip, unsigned offset)
+ 	unsigned long flags;
+ 
+ 	bank = container_of(chip, struct gpio_bank, chip);
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	omap_set_gpio_direction(bank, offset, 1);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 	return 0;
+ }
+ 
+@@ -968,10 +968,10 @@ static int omap_gpio_output(struct gpio_chip *chip, unsigned offset, int value)
+ 	unsigned long flags;
+ 
+ 	bank = container_of(chip, struct gpio_bank, chip);
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	bank->set_dataout(bank, offset, value);
+ 	omap_set_gpio_direction(bank, offset, 0);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 	return 0;
+ }
+ 
+@@ -983,9 +983,9 @@ static int omap_gpio_debounce(struct gpio_chip *chip, unsigned offset,
+ 
+ 	bank = container_of(chip, struct gpio_bank, chip);
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	omap2_set_gpio_debounce(bank, offset, debounce);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -996,9 +996,9 @@ static void omap_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
+ 	unsigned long flags;
+ 
+ 	bank = container_of(chip, struct gpio_bank, chip);
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 	bank->set_dataout(bank, offset, value);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ }
+ 
+ /*---------------------------------------------------------------------*/
+@@ -1223,7 +1223,7 @@ static int omap_gpio_probe(struct platform_device *pdev)
+ 	else
+ 		bank->set_dataout = omap_set_gpio_dataout_mask;
+ 
+-	spin_lock_init(&bank->lock);
++	raw_spin_lock_init(&bank->lock);
+ 
+ 	/* Static mapping, never released */
+ 	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+@@ -1270,7 +1270,7 @@ static int omap_gpio_runtime_suspend(struct device *dev)
+ 	unsigned long flags;
+ 	u32 wake_low, wake_hi;
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 
+ 	/*
+ 	 * Only edges can generate a wakeup event to the PRCM.
+@@ -1323,7 +1323,7 @@ update_gpio_context_count:
+ 				bank->get_context_loss_count(bank->dev);
+ 
+ 	omap_gpio_dbck_disable(bank);
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -1338,7 +1338,7 @@ static int omap_gpio_runtime_resume(struct device *dev)
+ 	unsigned long flags;
+ 	int c;
+ 
+-	spin_lock_irqsave(&bank->lock, flags);
++	raw_spin_lock_irqsave(&bank->lock, flags);
+ 
+ 	/*
+ 	 * On the first resume during the probe, the context has not
+@@ -1374,14 +1374,14 @@ static int omap_gpio_runtime_resume(struct device *dev)
+ 			if (c != bank->context_loss_count) {
+ 				omap_gpio_restore_context(bank);
+ 			} else {
+-				spin_unlock_irqrestore(&bank->lock, flags);
++				raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 				return 0;
+ 			}
+ 		}
+ 	}
+ 
+ 	if (!bank->workaround_enabled) {
+-		spin_unlock_irqrestore(&bank->lock, flags);
++		raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 		return 0;
+ 	}
+ 
+@@ -1436,7 +1436,7 @@ static int omap_gpio_runtime_resume(struct device *dev)
+ 	}
+ 
+ 	bank->workaround_enabled = false;
+-	spin_unlock_irqrestore(&bank->lock, flags);
++	raw_spin_unlock_irqrestore(&bank->lock, flags);
+ 
+ 	return 0;
+ }
+diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
+index d88dbedeaa77..68560152b721 100644
+--- a/drivers/gpu/drm/i915/i915_gem.c
++++ b/drivers/gpu/drm/i915/i915_gem.c
+@@ -5144,7 +5144,7 @@ static bool mutex_is_locked_by(struct mutex *mutex, struct task_struct *task)
+ 	if (!mutex_is_locked(mutex))
+ 		return false;
+ 
+-#if defined(CONFIG_SMP) && !defined(CONFIG_DEBUG_MUTEXES)
++#if defined(CONFIG_SMP) && !defined(CONFIG_DEBUG_MUTEXES) && !defined(CONFIG_PREEMPT_RT_BASE)
+ 	return mutex->owner == task;
+ #else
+ 	/* Since UP may be pre-empted, we cannot assume that we own the lock */
+diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+index 1a0611bb576b..8ea32c1616c6 100644
+--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c
++++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+@@ -1170,7 +1170,9 @@ i915_gem_ringbuffer_submission(struct drm_device *dev, struct drm_file *file,
+ 			return ret;
+ 	}
+ 
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	trace_i915_gem_ring_dispatch(ring, intel_ring_get_seqno(ring), flags);
++#endif
+ 
+ 	i915_gem_execbuffer_move_to_active(vmas, ring);
+ 	i915_gem_execbuffer_retire_commands(dev, file, ring, batch_obj);
+diff --git a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
+index 23329b48766f..763e9c004f23 100644
+--- a/drivers/gpu/drm/i915/i915_irq.c
++++ b/drivers/gpu/drm/i915/i915_irq.c
+@@ -943,6 +943,7 @@ static int i915_get_crtc_scanoutpos(struct drm_device *dev, int pipe,
+ 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+ 
+ 	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
++	preempt_disable_rt();
+ 
+ 	/* Get optional system timestamp before query. */
+ 	if (stime)
+@@ -994,6 +995,7 @@ static int i915_get_crtc_scanoutpos(struct drm_device *dev, int pipe,
+ 		*etime = ktime_get();
+ 
+ 	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
++	preempt_enable_rt();
+ 
+ 	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+ 
+diff --git a/drivers/gpu/drm/i915/intel_sprite.c b/drivers/gpu/drm/i915/intel_sprite.c
+index 4edebce7f213..c7c118476758 100644
+--- a/drivers/gpu/drm/i915/intel_sprite.c
++++ b/drivers/gpu/drm/i915/intel_sprite.c
+@@ -36,6 +36,7 @@
+ #include "intel_drv.h"
+ #include <drm/i915_drm.h>
+ #include "i915_drv.h"
++#include <linux/locallock.h>
+ 
+ static int usecs_to_scanlines(const struct drm_display_mode *mode, int usecs)
+ {
+@@ -46,6 +47,8 @@ static int usecs_to_scanlines(const struct drm_display_mode *mode, int usecs)
+ 	return DIV_ROUND_UP(usecs * mode->crtc_clock, 1000 * mode->crtc_htotal);
+ }
+ 
++static DEFINE_LOCAL_IRQ_LOCK(pipe_update_lock);
++
+ static bool intel_pipe_update_start(struct intel_crtc *crtc, uint32_t *start_vbl_count)
+ {
+ 	struct drm_device *dev = crtc->base.dev;
+@@ -72,7 +75,7 @@ static bool intel_pipe_update_start(struct intel_crtc *crtc, uint32_t *start_vbl
+ 	if (WARN_ON(drm_vblank_get(dev, pipe)))
+ 		return false;
+ 
+-	local_irq_disable();
++	local_lock_irq(pipe_update_lock);
+ 
+ 	trace_i915_pipe_update_start(crtc, min, max);
+ 
+@@ -94,11 +97,11 @@ static bool intel_pipe_update_start(struct intel_crtc *crtc, uint32_t *start_vbl
+ 			break;
+ 		}
+ 
+-		local_irq_enable();
++		local_unlock_irq(pipe_update_lock);
+ 
+ 		timeout = schedule_timeout(timeout);
+ 
+-		local_irq_disable();
++		local_lock_irq(pipe_update_lock);
+ 	}
+ 
+ 	finish_wait(wq, &wait);
+@@ -120,7 +123,7 @@ static void intel_pipe_update_end(struct intel_crtc *crtc, u32 start_vbl_count)
+ 
+ 	trace_i915_pipe_update_end(crtc, end_vbl_count);
+ 
+-	local_irq_enable();
++	local_unlock_irq(pipe_update_lock);
+ 
+ 	if (start_vbl_count != end_vbl_count)
+ 		DRM_ERROR("Atomic update failure on pipe %c (start=%u end=%u)\n",
+diff --git a/drivers/gpu/drm/radeon/radeon_display.c b/drivers/gpu/drm/radeon/radeon_display.c
+index 21e6e9745d00..7acf4536c827 100644
+--- a/drivers/gpu/drm/radeon/radeon_display.c
++++ b/drivers/gpu/drm/radeon/radeon_display.c
+@@ -1784,6 +1784,7 @@ int radeon_get_crtc_scanoutpos(struct drm_device *dev, int crtc, unsigned int fl
+ 	struct radeon_device *rdev = dev->dev_private;
+ 
+ 	/* preempt_disable_rt() should go right here in PREEMPT_RT patchset. */
++	preempt_disable_rt();
+ 
+ 	/* Get optional system timestamp before query. */
+ 	if (stime)
+@@ -1876,6 +1877,7 @@ int radeon_get_crtc_scanoutpos(struct drm_device *dev, int crtc, unsigned int fl
+ 		*etime = ktime_get();
+ 
+ 	/* preempt_enable_rt() should go right here in PREEMPT_RT patchset. */
++	preempt_enable_rt();
+ 
+ 	/* Decode into vertical and horizontal scanout position. */
+ 	*vpos = position & 0x1fff;
+diff --git a/drivers/i2c/busses/i2c-omap.c b/drivers/i2c/busses/i2c-omap.c
+index 277a2288d4a8..6d1c2960f9fa 100644
+--- a/drivers/i2c/busses/i2c-omap.c
++++ b/drivers/i2c/busses/i2c-omap.c
+@@ -875,15 +875,12 @@ omap_i2c_isr(int irq, void *dev_id)
+ 	u16 mask;
+ 	u16 stat;
+ 
+-	spin_lock(&dev->lock);
+-	mask = omap_i2c_read_reg(dev, OMAP_I2C_IE_REG);
+ 	stat = omap_i2c_read_reg(dev, OMAP_I2C_STAT_REG);
++	mask = omap_i2c_read_reg(dev, OMAP_I2C_IE_REG);
+ 
+ 	if (stat & mask)
+ 		ret = IRQ_WAKE_THREAD;
+ 
+-	spin_unlock(&dev->lock);
+-
+ 	return ret;
+ }
+ 
+diff --git a/drivers/ide/alim15x3.c b/drivers/ide/alim15x3.c
+index 36f76e28a0bf..394f142f90c7 100644
+--- a/drivers/ide/alim15x3.c
++++ b/drivers/ide/alim15x3.c
+@@ -234,7 +234,7 @@ static int init_chipset_ali15x3(struct pci_dev *dev)
+ 
+ 	isa_dev = pci_get_device(PCI_VENDOR_ID_AL, PCI_DEVICE_ID_AL_M1533, NULL);
+ 
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 
+ 	if (m5229_revision < 0xC2) {
+ 		/*
+@@ -325,7 +325,7 @@ out:
+ 	}
+ 	pci_dev_put(north);
+ 	pci_dev_put(isa_dev);
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 	return 0;
+ }
+ 
+diff --git a/drivers/ide/hpt366.c b/drivers/ide/hpt366.c
+index 696b6c1ec940..0d0a96629b73 100644
+--- a/drivers/ide/hpt366.c
++++ b/drivers/ide/hpt366.c
+@@ -1241,7 +1241,7 @@ static int init_dma_hpt366(ide_hwif_t *hwif,
+ 
+ 	dma_old = inb(base + 2);
+ 
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 
+ 	dma_new = dma_old;
+ 	pci_read_config_byte(dev, hwif->channel ? 0x4b : 0x43, &masterdma);
+@@ -1252,7 +1252,7 @@ static int init_dma_hpt366(ide_hwif_t *hwif,
+ 	if (dma_new != dma_old)
+ 		outb(dma_new, base + 2);
+ 
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 
+ 	printk(KERN_INFO "    %s: BM-DMA at 0x%04lx-0x%04lx\n",
+ 			 hwif->name, base, base + 7);
+diff --git a/drivers/ide/ide-io-std.c b/drivers/ide/ide-io-std.c
+index 19763977568c..4169433faab5 100644
+--- a/drivers/ide/ide-io-std.c
++++ b/drivers/ide/ide-io-std.c
+@@ -175,7 +175,7 @@ void ide_input_data(ide_drive_t *drive, struct ide_cmd *cmd, void *buf,
+ 		unsigned long uninitialized_var(flags);
+ 
+ 		if ((io_32bit & 2) && !mmio) {
+-			local_irq_save(flags);
++			local_irq_save_nort(flags);
+ 			ata_vlb_sync(io_ports->nsect_addr);
+ 		}
+ 
+@@ -186,7 +186,7 @@ void ide_input_data(ide_drive_t *drive, struct ide_cmd *cmd, void *buf,
+ 			insl(data_addr, buf, words);
+ 
+ 		if ((io_32bit & 2) && !mmio)
+-			local_irq_restore(flags);
++			local_irq_restore_nort(flags);
+ 
+ 		if (((len + 1) & 3) < 2)
+ 			return;
+@@ -219,7 +219,7 @@ void ide_output_data(ide_drive_t *drive, struct ide_cmd *cmd, void *buf,
+ 		unsigned long uninitialized_var(flags);
+ 
+ 		if ((io_32bit & 2) && !mmio) {
+-			local_irq_save(flags);
++			local_irq_save_nort(flags);
+ 			ata_vlb_sync(io_ports->nsect_addr);
+ 		}
+ 
+@@ -230,7 +230,7 @@ void ide_output_data(ide_drive_t *drive, struct ide_cmd *cmd, void *buf,
+ 			outsl(data_addr, buf, words);
+ 
+ 		if ((io_32bit & 2) && !mmio)
+-			local_irq_restore(flags);
++			local_irq_restore_nort(flags);
+ 
+ 		if (((len + 1) & 3) < 2)
+ 			return;
+diff --git a/drivers/ide/ide-io.c b/drivers/ide/ide-io.c
+index 177db6d5b2f5..079ae6bebf18 100644
+--- a/drivers/ide/ide-io.c
++++ b/drivers/ide/ide-io.c
+@@ -659,7 +659,7 @@ void ide_timer_expiry (unsigned long data)
+ 		/* disable_irq_nosync ?? */
+ 		disable_irq(hwif->irq);
+ 		/* local CPU only, as if we were handling an interrupt */
+-		local_irq_disable();
++		local_irq_disable_nort();
+ 		if (hwif->polling) {
+ 			startstop = handler(drive);
+ 		} else if (drive_is_ready(drive)) {
+diff --git a/drivers/ide/ide-iops.c b/drivers/ide/ide-iops.c
+index 376f2dc410c5..f014dd1b73dc 100644
+--- a/drivers/ide/ide-iops.c
++++ b/drivers/ide/ide-iops.c
+@@ -129,12 +129,12 @@ int __ide_wait_stat(ide_drive_t *drive, u8 good, u8 bad,
+ 				if ((stat & ATA_BUSY) == 0)
+ 					break;
+ 
+-				local_irq_restore(flags);
++				local_irq_restore_nort(flags);
+ 				*rstat = stat;
+ 				return -EBUSY;
+ 			}
+ 		}
+-		local_irq_restore(flags);
++		local_irq_restore_nort(flags);
+ 	}
+ 	/*
+ 	 * Allow status to settle, then read it again.
+diff --git a/drivers/ide/ide-probe.c b/drivers/ide/ide-probe.c
+index a3d3b1733c49..3eff5828ecfc 100644
+--- a/drivers/ide/ide-probe.c
++++ b/drivers/ide/ide-probe.c
+@@ -196,10 +196,10 @@ static void do_identify(ide_drive_t *drive, u8 cmd, u16 *id)
+ 	int bswap = 1;
+ 
+ 	/* local CPU only; some systems need this */
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	/* read 512 bytes of id info */
+ 	hwif->tp_ops->input_data(drive, NULL, id, SECTOR_SIZE);
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 
+ 	drive->dev_flags |= IDE_DFLAG_ID_READ;
+ #ifdef DEBUG
+diff --git a/drivers/ide/ide-taskfile.c b/drivers/ide/ide-taskfile.c
+index dabb88b1cbec..2cecea72520a 100644
+--- a/drivers/ide/ide-taskfile.c
++++ b/drivers/ide/ide-taskfile.c
+@@ -250,7 +250,7 @@ void ide_pio_bytes(ide_drive_t *drive, struct ide_cmd *cmd,
+ 
+ 		page_is_high = PageHighMem(page);
+ 		if (page_is_high)
+-			local_irq_save(flags);
++			local_irq_save_nort(flags);
+ 
+ 		buf = kmap_atomic(page) + offset;
+ 
+@@ -271,7 +271,7 @@ void ide_pio_bytes(ide_drive_t *drive, struct ide_cmd *cmd,
+ 		kunmap_atomic(buf);
+ 
+ 		if (page_is_high)
+-			local_irq_restore(flags);
++			local_irq_restore_nort(flags);
+ 
+ 		len -= nr_bytes;
+ 	}
+@@ -414,7 +414,7 @@ static ide_startstop_t pre_task_out_intr(ide_drive_t *drive,
+ 	}
+ 
+ 	if ((drive->dev_flags & IDE_DFLAG_UNMASK) == 0)
+-		local_irq_disable();
++		local_irq_disable_nort();
+ 
+ 	ide_set_handler(drive, &task_pio_intr, WAIT_WORSTCASE);
+ 
+diff --git a/drivers/infiniband/ulp/ipoib/ipoib_multicast.c b/drivers/infiniband/ulp/ipoib/ipoib_multicast.c
+index 6391ed0fe449..b95e7761603e 100644
+--- a/drivers/infiniband/ulp/ipoib/ipoib_multicast.c
++++ b/drivers/infiniband/ulp/ipoib/ipoib_multicast.c
+@@ -799,7 +799,7 @@ void ipoib_mcast_restart_task(struct work_struct *work)
+ 
+ 	ipoib_mcast_stop_thread(dev, 0);
+ 
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	netif_addr_lock(dev);
+ 	spin_lock(&priv->lock);
+ 
+@@ -881,7 +881,7 @@ void ipoib_mcast_restart_task(struct work_struct *work)
+ 
+ 	spin_unlock(&priv->lock);
+ 	netif_addr_unlock(dev);
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 
+ 	/* We have to cancel outside of the spinlock */
+ 	list_for_each_entry_safe(mcast, tmcast, &remove_list, list) {
+diff --git a/drivers/input/gameport/gameport.c b/drivers/input/gameport/gameport.c
+index e29c04e2aff4..412ab00c7534 100644
+--- a/drivers/input/gameport/gameport.c
++++ b/drivers/input/gameport/gameport.c
+@@ -124,12 +124,12 @@ static int old_gameport_measure_speed(struct gameport *gameport)
+ 	tx = 1 << 30;
+ 
+ 	for(i = 0; i < 50; i++) {
+-		local_irq_save(flags);
++		local_irq_save_nort(flags);
+ 		GET_TIME(t1);
+ 		for (t = 0; t < 50; t++) gameport_read(gameport);
+ 		GET_TIME(t2);
+ 		GET_TIME(t3);
+-		local_irq_restore(flags);
++		local_irq_restore_nort(flags);
+ 		udelay(i * 10);
+ 		if ((t = DELTA(t2,t1) - DELTA(t3,t2)) < tx) tx = t;
+ 	}
+@@ -148,11 +148,11 @@ static int old_gameport_measure_speed(struct gameport *gameport)
+ 	tx = 1 << 30;
+ 
+ 	for(i = 0; i < 50; i++) {
+-		local_irq_save(flags);
++		local_irq_save_nort(flags);
+ 		rdtscl(t1);
+ 		for (t = 0; t < 50; t++) gameport_read(gameport);
+ 		rdtscl(t2);
+-		local_irq_restore(flags);
++		local_irq_restore_nort(flags);
+ 		udelay(i * 10);
+ 		if (t2 - t1 < tx) tx = t2 - t1;
+ 	}
+diff --git a/drivers/leds/trigger/Kconfig b/drivers/leds/trigger/Kconfig
+index 49794b47b51c..3d7245d6b2f8 100644
+--- a/drivers/leds/trigger/Kconfig
++++ b/drivers/leds/trigger/Kconfig
+@@ -61,7 +61,7 @@ config LEDS_TRIGGER_BACKLIGHT
+ 
+ config LEDS_TRIGGER_CPU
+ 	bool "LED CPU Trigger"
+-	depends on LEDS_TRIGGERS
++	depends on LEDS_TRIGGERS && !PREEMPT_RT_BASE
+ 	help
+ 	  This allows LEDs to be controlled by active CPUs. This shows
+ 	  the active CPUs across an array of LEDs so you can see which
+diff --git a/drivers/md/bcache/Kconfig b/drivers/md/bcache/Kconfig
+index 4d200883c505..98b64ed5cb81 100644
+--- a/drivers/md/bcache/Kconfig
++++ b/drivers/md/bcache/Kconfig
+@@ -1,6 +1,7 @@
+ 
+ config BCACHE
+ 	tristate "Block device as cache"
++	depends on !PREEMPT_RT_FULL
+ 	---help---
+ 	Allows a block device to be used as cache for other devices; uses
+ 	a btree for indexing and the layout is optimized for SSDs.
+diff --git a/drivers/md/dm.c b/drivers/md/dm.c
+index fb07be386287..16dbf36da10d 100644
+--- a/drivers/md/dm.c
++++ b/drivers/md/dm.c
+@@ -1953,14 +1953,14 @@ static void dm_request_fn(struct request_queue *q)
+ 		if (map_request(ti, clone, md))
+ 			goto requeued;
+ 
+-		BUG_ON(!irqs_disabled());
++		BUG_ON_NONRT(!irqs_disabled());
+ 		spin_lock(q->queue_lock);
+ 	}
+ 
+ 	goto out;
+ 
+ requeued:
+-	BUG_ON(!irqs_disabled());
++	BUG_ON_NONRT(!irqs_disabled());
+ 	spin_lock(q->queue_lock);
+ 
+ delay_and_out:
+diff --git a/drivers/md/raid5.c b/drivers/md/raid5.c
+index 094f36064ff0..930fcf6e5999 100644
+--- a/drivers/md/raid5.c
++++ b/drivers/md/raid5.c
+@@ -1649,8 +1649,9 @@ static void raid_run_ops(struct stripe_head *sh, unsigned long ops_request)
+ 	struct raid5_percpu *percpu;
+ 	unsigned long cpu;
+ 
+-	cpu = get_cpu();
++	cpu = get_cpu_light();
+ 	percpu = per_cpu_ptr(conf->percpu, cpu);
++	spin_lock(&percpu->lock);
+ 	if (test_bit(STRIPE_OP_BIOFILL, &ops_request)) {
+ 		ops_run_biofill(sh);
+ 		overlap_clear++;
+@@ -1702,7 +1703,8 @@ static void raid_run_ops(struct stripe_head *sh, unsigned long ops_request)
+ 			if (test_and_clear_bit(R5_Overlap, &dev->flags))
+ 				wake_up(&sh->raid_conf->wait_for_overlap);
+ 		}
+-	put_cpu();
++	spin_unlock(&percpu->lock);
++	put_cpu_light();
+ }
+ 
+ static int grow_one_stripe(struct r5conf *conf, int hash)
+@@ -5716,6 +5718,7 @@ static int raid5_alloc_percpu(struct r5conf *conf)
+ 			       __func__, cpu);
+ 			break;
+ 		}
++		spin_lock_init(&per_cpu_ptr(conf->percpu, cpu)->lock);
+ 	}
+ 	put_online_cpus();
+ 
+diff --git a/drivers/md/raid5.h b/drivers/md/raid5.h
+index d59f5ca743cd..9aa9af4ab1aa 100644
+--- a/drivers/md/raid5.h
++++ b/drivers/md/raid5.h
+@@ -457,6 +457,7 @@ struct r5conf {
+ 	int			recovery_disabled;
+ 	/* per cpu variables */
+ 	struct raid5_percpu {
++		spinlock_t	lock;	     /* Protection for -RT */
+ 		struct page	*spare_page; /* Used when checking P/Q in raid6 */
+ 		void		*scribble;   /* space for constructing buffer
+ 					      * lists and performing address
+diff --git a/drivers/misc/Kconfig b/drivers/misc/Kconfig
+index 878d5430973c..1e23b08640b6 100644
+--- a/drivers/misc/Kconfig
++++ b/drivers/misc/Kconfig
+@@ -54,6 +54,7 @@ config AD525X_DPOT_SPI
+ config ATMEL_TCLIB
+ 	bool "Atmel AT32/AT91 Timer/Counter Library"
+ 	depends on (AVR32 || ARCH_AT91)
++	default y if PREEMPT_RT_FULL
+ 	help
+ 	  Select this if you want a library to allocate the Timer/Counter
+ 	  blocks found on many Atmel processors.  This facilitates using
+@@ -69,8 +70,7 @@ config ATMEL_TCB_CLKSRC
+ 	  are combined to make a single 32-bit timer.
+ 
+ 	  When GENERIC_CLOCKEVENTS is defined, the third timer channel
+-	  may be used as a clock event device supporting oneshot mode
+-	  (delays of up to two seconds) based on the 32 KiHz clock.
++	  may be used as a clock event device supporting oneshot mode.
+ 
+ config ATMEL_TCB_CLKSRC_BLOCK
+ 	int
+@@ -84,6 +84,15 @@ config ATMEL_TCB_CLKSRC_BLOCK
+ 	  TC can be used for other purposes, such as PWM generation and
+ 	  interval timing.
+ 
++config ATMEL_TCB_CLKSRC_USE_SLOW_CLOCK
++	bool "TC Block use 32 KiHz clock"
++	depends on ATMEL_TCB_CLKSRC
++	default y if !PREEMPT_RT_FULL
++	help
++	  Select this to use 32 KiHz base clock rate as TC block clock
++	  source for clock events.
++
++
+ config DUMMY_IRQ
+ 	tristate "Dummy IRQ handler"
+ 	default n
+@@ -113,6 +122,35 @@ config IBM_ASM
+ 	  for information on the specific driver level and support statement
+ 	  for your IBM server.
+ 
++config HWLAT_DETECTOR
++	tristate "Testing module to detect hardware-induced latencies"
++	depends on DEBUG_FS
++	depends on RING_BUFFER
++	default m
++	---help---
++	  A simple hardware latency detector. Use this module to detect
++	  large latencies introduced by the behavior of the underlying
++	  system firmware external to Linux. We do this using periodic
++	  use of stop_machine to grab all available CPUs and measure
++	  for unexplainable gaps in the CPU timestamp counter(s). By
++	  default, the module is not enabled until the "enable" file
++	  within the "hwlat_detector" debugfs directory is toggled.
++
++	  This module is often used to detect SMI (System Management
++	  Interrupts) on x86 systems, though is not x86 specific. To
++	  this end, we default to using a sample window of 1 second,
++	  during which we will sample for 0.5 seconds. If an SMI or
++	  similar event occurs during that time, it is recorded
++	  into an 8K samples global ring buffer until retreived.
++
++	  WARNING: This software should never be enabled (it can be built
++	  but should not be turned on after it is loaded) in a production
++	  environment where high latencies are a concern since the
++	  sampling mechanism actually introduces latencies for
++	  regular tasks while the CPU(s) are being held.
++
++	  If unsure, say N
++
+ config PHANTOM
+ 	tristate "Sensable PHANToM (PCI)"
+ 	depends on PCI
+diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
+index 7d5c4cd118c4..6a8e39388cf9 100644
+--- a/drivers/misc/Makefile
++++ b/drivers/misc/Makefile
+@@ -38,6 +38,7 @@ obj-$(CONFIG_C2PORT)		+= c2port/
+ obj-$(CONFIG_HMC6352)		+= hmc6352.o
+ obj-y				+= eeprom/
+ obj-y				+= cb710/
++obj-$(CONFIG_HWLAT_DETECTOR)	+= hwlat_detector.o
+ obj-$(CONFIG_SPEAR13XX_PCIE_GADGET)	+= spear13xx_pcie_gadget.o
+ obj-$(CONFIG_VMWARE_BALLOON)	+= vmw_balloon.o
+ obj-$(CONFIG_ARM_CHARLCD)	+= arm-charlcd.o
+diff --git a/drivers/misc/hwlat_detector.c b/drivers/misc/hwlat_detector.c
+new file mode 100644
+index 000000000000..2429c4331e68
+--- /dev/null
++++ b/drivers/misc/hwlat_detector.c
+@@ -0,0 +1,1240 @@
++/*
++ * hwlat_detector.c - A simple Hardware Latency detector.
++ *
++ * Use this module to detect large system latencies induced by the behavior of
++ * certain underlying system hardware or firmware, independent of Linux itself.
++ * The code was developed originally to detect the presence of SMIs on Intel
++ * and AMD systems, although there is no dependency upon x86 herein.
++ *
++ * The classical example usage of this module is in detecting the presence of
++ * SMIs or System Management Interrupts on Intel and AMD systems. An SMI is a
++ * somewhat special form of hardware interrupt spawned from earlier CPU debug
++ * modes in which the (BIOS/EFI/etc.) firmware arranges for the South Bridge
++ * LPC (or other device) to generate a special interrupt under certain
++ * circumstances, for example, upon expiration of a special SMI timer device,
++ * due to certain external thermal readings, on certain I/O address accesses,
++ * and other situations. An SMI hits a special CPU pin, triggers a special
++ * SMI mode (complete with special memory map), and the OS is unaware.
++ *
++ * Although certain hardware-inducing latencies are necessary (for example,
++ * a modern system often requires an SMI handler for correct thermal control
++ * and remote management) they can wreak havoc upon any OS-level performance
++ * guarantees toward low-latency, especially when the OS is not even made
++ * aware of the presence of these interrupts. For this reason, we need a
++ * somewhat brute force mechanism to detect these interrupts. In this case,
++ * we do it by hogging all of the CPU(s) for configurable timer intervals,
++ * sampling the built-in CPU timer, looking for discontiguous readings.
++ *
++ * WARNING: This implementation necessarily introduces latencies. Therefore,
++ *          you should NEVER use this module in a production environment
++ *          requiring any kind of low-latency performance guarantee(s).
++ *
++ * Copyright (C) 2008-2009 Jon Masters, Red Hat, Inc. <jcm@redhat.com>
++ *
++ * Includes useful feedback from Clark Williams <clark@redhat.com>
++ *
++ * This file is licensed under the terms of the GNU General Public
++ * License version 2. This program is licensed "as is" without any
++ * warranty of any kind, whether express or implied.
++ */
++
++#include <linux/module.h>
++#include <linux/init.h>
++#include <linux/ring_buffer.h>
++#include <linux/time.h>
++#include <linux/hrtimer.h>
++#include <linux/kthread.h>
++#include <linux/debugfs.h>
++#include <linux/seq_file.h>
++#include <linux/uaccess.h>
++#include <linux/version.h>
++#include <linux/delay.h>
++#include <linux/slab.h>
++#include <linux/trace_clock.h>
++
++#define BUF_SIZE_DEFAULT	262144UL		/* 8K*(sizeof(entry)) */
++#define BUF_FLAGS		(RB_FL_OVERWRITE)	/* no block on full */
++#define U64STR_SIZE		22			/* 20 digits max */
++
++#define VERSION			"1.0.0"
++#define BANNER			"hwlat_detector: "
++#define DRVNAME			"hwlat_detector"
++#define DEFAULT_SAMPLE_WINDOW	1000000			/* 1s */
++#define DEFAULT_SAMPLE_WIDTH	500000			/* 0.5s */
++#define DEFAULT_LAT_THRESHOLD	10			/* 10us */
++
++/* Module metadata */
++
++MODULE_LICENSE("GPL");
++MODULE_AUTHOR("Jon Masters <jcm@redhat.com>");
++MODULE_DESCRIPTION("A simple hardware latency detector");
++MODULE_VERSION(VERSION);
++
++/* Module parameters */
++
++static int debug;
++static int enabled;
++static int threshold;
++
++module_param(debug, int, 0);			/* enable debug */
++module_param(enabled, int, 0);			/* enable detector */
++module_param(threshold, int, 0);		/* latency threshold */
++
++/* Buffering and sampling */
++
++static struct ring_buffer *ring_buffer;		/* sample buffer */
++static DEFINE_MUTEX(ring_buffer_mutex);		/* lock changes */
++static unsigned long buf_size = BUF_SIZE_DEFAULT;
++static struct task_struct *kthread;		/* sampling thread */
++
++/* DebugFS filesystem entries */
++
++static struct dentry *debug_dir;		/* debugfs directory */
++static struct dentry *debug_max;		/* maximum TSC delta */
++static struct dentry *debug_count;		/* total detect count */
++static struct dentry *debug_sample_width;	/* sample width us */
++static struct dentry *debug_sample_window;	/* sample window us */
++static struct dentry *debug_sample;		/* raw samples us */
++static struct dentry *debug_threshold;		/* threshold us */
++static struct dentry *debug_enable;		/* enable/disable */
++
++/* Individual samples and global state */
++
++struct sample;					/* latency sample */
++struct data;					/* Global state */
++
++/* Sampling functions */
++static int __buffer_add_sample(struct sample *sample);
++static struct sample *buffer_get_sample(struct sample *sample);
++
++/* Threading and state */
++static int kthread_fn(void *unused);
++static int start_kthread(void);
++static int stop_kthread(void);
++static void __reset_stats(void);
++static int init_stats(void);
++
++/* Debugfs interface */
++static ssize_t simple_data_read(struct file *filp, char __user *ubuf,
++				size_t cnt, loff_t *ppos, const u64 *entry);
++static ssize_t simple_data_write(struct file *filp, const char __user *ubuf,
++				 size_t cnt, loff_t *ppos, u64 *entry);
++static int debug_sample_fopen(struct inode *inode, struct file *filp);
++static ssize_t debug_sample_fread(struct file *filp, char __user *ubuf,
++				  size_t cnt, loff_t *ppos);
++static int debug_sample_release(struct inode *inode, struct file *filp);
++static int debug_enable_fopen(struct inode *inode, struct file *filp);
++static ssize_t debug_enable_fread(struct file *filp, char __user *ubuf,
++				  size_t cnt, loff_t *ppos);
++static ssize_t debug_enable_fwrite(struct file *file,
++				   const char __user *user_buffer,
++				   size_t user_size, loff_t *offset);
++
++/* Initialization functions */
++static int init_debugfs(void);
++static void free_debugfs(void);
++static int detector_init(void);
++static void detector_exit(void);
++
++/* Individual latency samples are stored here when detected and packed into
++ * the ring_buffer circular buffer, where they are overwritten when
++ * more than buf_size/sizeof(sample) samples are received. */
++struct sample {
++	u64		seqnum;		/* unique sequence */
++	u64		duration;	/* ktime delta */
++	u64		outer_duration;	/* ktime delta (outer loop) */
++	struct timespec	timestamp;	/* wall time */
++	unsigned long   lost;
++};
++
++/* keep the global state somewhere. */
++static struct data {
++
++	struct mutex lock;		/* protect changes */
++
++	u64	count;			/* total since reset */
++	u64	max_sample;		/* max hardware latency */
++	u64	threshold;		/* sample threshold level */
++
++	u64	sample_window;		/* total sampling window (on+off) */
++	u64	sample_width;		/* active sampling portion of window */
++
++	atomic_t sample_open;		/* whether the sample file is open */
++
++	wait_queue_head_t wq;		/* waitqeue for new sample values */
++
++} data;
++
++/**
++ * __buffer_add_sample - add a new latency sample recording to the ring buffer
++ * @sample: The new latency sample value
++ *
++ * This receives a new latency sample and records it in a global ring buffer.
++ * No additional locking is used in this case.
++ */
++static int __buffer_add_sample(struct sample *sample)
++{
++	return ring_buffer_write(ring_buffer,
++				 sizeof(struct sample), sample);
++}
++
++/**
++ * buffer_get_sample - remove a hardware latency sample from the ring buffer
++ * @sample: Pre-allocated storage for the sample
++ *
++ * This retrieves a hardware latency sample from the global circular buffer
++ */
++static struct sample *buffer_get_sample(struct sample *sample)
++{
++	struct ring_buffer_event *e = NULL;
++	struct sample *s = NULL;
++	unsigned int cpu = 0;
++
++	if (!sample)
++		return NULL;
++
++	mutex_lock(&ring_buffer_mutex);
++	for_each_online_cpu(cpu) {
++		e = ring_buffer_consume(ring_buffer, cpu, NULL, &sample->lost);
++		if (e)
++			break;
++	}
++
++	if (e) {
++		s = ring_buffer_event_data(e);
++		memcpy(sample, s, sizeof(struct sample));
++	} else
++		sample = NULL;
++	mutex_unlock(&ring_buffer_mutex);
++
++	return sample;
++}
++
++#ifndef CONFIG_TRACING
++#define time_type	ktime_t
++#define time_get()	ktime_get()
++#define time_to_us(x)	ktime_to_us(x)
++#define time_sub(a, b)	ktime_sub(a, b)
++#define init_time(a, b)	(a).tv64 = b
++#define time_u64(a)	((a).tv64)
++#else
++#define time_type	u64
++#define time_get()	trace_clock_local()
++#define time_to_us(x)	div_u64(x, 1000)
++#define time_sub(a, b)	((a) - (b))
++#define init_time(a, b)	(a = b)
++#define time_u64(a)	a
++#endif
++/**
++ * get_sample - sample the CPU TSC and look for likely hardware latencies
++ *
++ * Used to repeatedly capture the CPU TSC (or similar), looking for potential
++ * hardware-induced latency. Called with interrupts disabled and with
++ * data.lock held.
++ */
++static int get_sample(void)
++{
++	time_type start, t1, t2, last_t2;
++	s64 diff, total = 0;
++	u64 sample = 0;
++	u64 outer_sample = 0;
++	int ret = -1;
++
++	init_time(last_t2, 0);
++	start = time_get(); /* start timestamp */
++
++	do {
++
++		t1 = time_get();	/* we'll look for a discontinuity */
++		t2 = time_get();
++
++		if (time_u64(last_t2)) {
++			/* Check the delta from outer loop (t2 to next t1) */
++			diff = time_to_us(time_sub(t1, last_t2));
++			/* This shouldn't happen */
++			if (diff < 0) {
++				pr_err(BANNER "time running backwards\n");
++				goto out;
++			}
++			if (diff > outer_sample)
++				outer_sample = diff;
++		}
++		last_t2 = t2;
++
++		total = time_to_us(time_sub(t2, start)); /* sample width */
++
++		/* This checks the inner loop (t1 to t2) */
++		diff = time_to_us(time_sub(t2, t1));     /* current diff */
++
++		/* This shouldn't happen */
++		if (diff < 0) {
++			pr_err(BANNER "time running backwards\n");
++			goto out;
++		}
++
++		if (diff > sample)
++			sample = diff; /* only want highest value */
++
++	} while (total <= data.sample_width);
++
++	ret = 0;
++
++	/* If we exceed the threshold value, we have found a hardware latency */
++	if (sample > data.threshold || outer_sample > data.threshold) {
++		struct sample s;
++
++		ret = 1;
++
++		data.count++;
++		s.seqnum = data.count;
++		s.duration = sample;
++		s.outer_duration = outer_sample;
++		s.timestamp = CURRENT_TIME;
++		__buffer_add_sample(&s);
++
++		/* Keep a running maximum ever recorded hardware latency */
++		if (sample > data.max_sample)
++			data.max_sample = sample;
++	}
++
++out:
++	return ret;
++}
++
++/*
++ * kthread_fn - The CPU time sampling/hardware latency detection kernel thread
++ * @unused: A required part of the kthread API.
++ *
++ * Used to periodically sample the CPU TSC via a call to get_sample. We
++ * disable interrupts, which does (intentionally) introduce latency since we
++ * need to ensure nothing else might be running (and thus pre-empting).
++ * Obviously this should never be used in production environments.
++ *
++ * Currently this runs on which ever CPU it was scheduled on, but most
++ * real-worald hardware latency situations occur across several CPUs,
++ * but we might later generalize this if we find there are any actualy
++ * systems with alternate SMI delivery or other hardware latencies.
++ */
++static int kthread_fn(void *unused)
++{
++	int ret;
++	u64 interval;
++
++	while (!kthread_should_stop()) {
++
++		mutex_lock(&data.lock);
++
++		local_irq_disable();
++		ret = get_sample();
++		local_irq_enable();
++
++		if (ret > 0)
++			wake_up(&data.wq); /* wake up reader(s) */
++
++		interval = data.sample_window - data.sample_width;
++		do_div(interval, USEC_PER_MSEC); /* modifies interval value */
++
++		mutex_unlock(&data.lock);
++
++		if (msleep_interruptible(interval))
++			break;
++	}
++
++	return 0;
++}
++
++/**
++ * start_kthread - Kick off the hardware latency sampling/detector kthread
++ *
++ * This starts a kernel thread that will sit and sample the CPU timestamp
++ * counter (TSC or similar) and look for potential hardware latencies.
++ */
++static int start_kthread(void)
++{
++	kthread = kthread_run(kthread_fn, NULL,
++					DRVNAME);
++	if (IS_ERR(kthread)) {
++		pr_err(BANNER "could not start sampling thread\n");
++		enabled = 0;
++		return -ENOMEM;
++	}
++
++	return 0;
++}
++
++/**
++ * stop_kthread - Inform the hardware latency samping/detector kthread to stop
++ *
++ * This kicks the running hardware latency sampling/detector kernel thread and
++ * tells it to stop sampling now. Use this on unload and at system shutdown.
++ */
++static int stop_kthread(void)
++{
++	int ret;
++
++	ret = kthread_stop(kthread);
++
++	return ret;
++}
++
++/**
++ * __reset_stats - Reset statistics for the hardware latency detector
++ *
++ * We use data to store various statistics and global state. We call this
++ * function in order to reset those when "enable" is toggled on or off, and
++ * also at initialization. Should be called with data.lock held.
++ */
++static void __reset_stats(void)
++{
++	data.count = 0;
++	data.max_sample = 0;
++	ring_buffer_reset(ring_buffer); /* flush out old sample entries */
++}
++
++/**
++ * init_stats - Setup global state statistics for the hardware latency detector
++ *
++ * We use data to store various statistics and global state. We also use
++ * a global ring buffer (ring_buffer) to keep raw samples of detected hardware
++ * induced system latencies. This function initializes these structures and
++ * allocates the global ring buffer also.
++ */
++static int init_stats(void)
++{
++	int ret = -ENOMEM;
++
++	mutex_init(&data.lock);
++	init_waitqueue_head(&data.wq);
++	atomic_set(&data.sample_open, 0);
++
++	ring_buffer = ring_buffer_alloc(buf_size, BUF_FLAGS);
++
++	if (WARN(!ring_buffer, KERN_ERR BANNER
++			       "failed to allocate ring buffer!\n"))
++		goto out;
++
++	__reset_stats();
++	data.threshold = threshold ?: DEFAULT_LAT_THRESHOLD; /* threshold us */
++	data.sample_window = DEFAULT_SAMPLE_WINDOW; /* window us */
++	data.sample_width = DEFAULT_SAMPLE_WIDTH;   /* width us */
++
++	ret = 0;
++
++out:
++	return ret;
++
++}
++
++/*
++ * simple_data_read - Wrapper read function for global state debugfs entries
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The userspace provided buffer to read value into
++ * @cnt: The maximum number of bytes to read
++ * @ppos: The current "file" position
++ * @entry: The entry to read from
++ *
++ * This function provides a generic read implementation for the global state
++ * "data" structure debugfs filesystem entries. It would be nice to use
++ * simple_attr_read directly, but we need to make sure that the data.lock
++ * is held during the actual read.
++ */
++static ssize_t simple_data_read(struct file *filp, char __user *ubuf,
++				size_t cnt, loff_t *ppos, const u64 *entry)
++{
++	char buf[U64STR_SIZE];
++	u64 val = 0;
++	int len = 0;
++
++	memset(buf, 0, sizeof(buf));
++
++	if (!entry)
++		return -EFAULT;
++
++	mutex_lock(&data.lock);
++	val = *entry;
++	mutex_unlock(&data.lock);
++
++	len = snprintf(buf, sizeof(buf), "%llu\n", (unsigned long long)val);
++
++	return simple_read_from_buffer(ubuf, cnt, ppos, buf, len);
++
++}
++
++/*
++ * simple_data_write - Wrapper write function for global state debugfs entries
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The userspace provided buffer to write value from
++ * @cnt: The maximum number of bytes to write
++ * @ppos: The current "file" position
++ * @entry: The entry to write to
++ *
++ * This function provides a generic write implementation for the global state
++ * "data" structure debugfs filesystem entries. It would be nice to use
++ * simple_attr_write directly, but we need to make sure that the data.lock
++ * is held during the actual write.
++ */
++static ssize_t simple_data_write(struct file *filp, const char __user *ubuf,
++				 size_t cnt, loff_t *ppos, u64 *entry)
++{
++	char buf[U64STR_SIZE];
++	int csize = min(cnt, sizeof(buf));
++	u64 val = 0;
++	int err = 0;
++
++	memset(buf, '\0', sizeof(buf));
++	if (copy_from_user(buf, ubuf, csize))
++		return -EFAULT;
++
++	buf[U64STR_SIZE-1] = '\0';			/* just in case */
++	err = kstrtoull(buf, 10, &val);
++	if (err)
++		return -EINVAL;
++
++	mutex_lock(&data.lock);
++	*entry = val;
++	mutex_unlock(&data.lock);
++
++	return csize;
++}
++
++/**
++ * debug_count_fopen - Open function for "count" debugfs entry
++ * @inode: The in-kernel inode representation of the debugfs "file"
++ * @filp: The active open file structure for the debugfs "file"
++ *
++ * This function provides an open implementation for the "count" debugfs
++ * interface to the hardware latency detector.
++ */
++static int debug_count_fopen(struct inode *inode, struct file *filp)
++{
++	return 0;
++}
++
++/**
++ * debug_count_fread - Read function for "count" debugfs entry
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The userspace provided buffer to read value into
++ * @cnt: The maximum number of bytes to read
++ * @ppos: The current "file" position
++ *
++ * This function provides a read implementation for the "count" debugfs
++ * interface to the hardware latency detector. Can be used to read the
++ * number of latency readings exceeding the configured threshold since
++ * the detector was last reset (e.g. by writing a zero into "count").
++ */
++static ssize_t debug_count_fread(struct file *filp, char __user *ubuf,
++				     size_t cnt, loff_t *ppos)
++{
++	return simple_data_read(filp, ubuf, cnt, ppos, &data.count);
++}
++
++/**
++ * debug_count_fwrite - Write function for "count" debugfs entry
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The user buffer that contains the value to write
++ * @cnt: The maximum number of bytes to write to "file"
++ * @ppos: The current position in the debugfs "file"
++ *
++ * This function provides a write implementation for the "count" debugfs
++ * interface to the hardware latency detector. Can be used to write a
++ * desired value, especially to zero the total count.
++ */
++static ssize_t  debug_count_fwrite(struct file *filp,
++				       const char __user *ubuf,
++				       size_t cnt,
++				       loff_t *ppos)
++{
++	return simple_data_write(filp, ubuf, cnt, ppos, &data.count);
++}
++
++/**
++ * debug_enable_fopen - Dummy open function for "enable" debugfs interface
++ * @inode: The in-kernel inode representation of the debugfs "file"
++ * @filp: The active open file structure for the debugfs "file"
++ *
++ * This function provides an open implementation for the "enable" debugfs
++ * interface to the hardware latency detector.
++ */
++static int debug_enable_fopen(struct inode *inode, struct file *filp)
++{
++	return 0;
++}
++
++/**
++ * debug_enable_fread - Read function for "enable" debugfs interface
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The userspace provided buffer to read value into
++ * @cnt: The maximum number of bytes to read
++ * @ppos: The current "file" position
++ *
++ * This function provides a read implementation for the "enable" debugfs
++ * interface to the hardware latency detector. Can be used to determine
++ * whether the detector is currently enabled ("0\n" or "1\n" returned).
++ */
++static ssize_t debug_enable_fread(struct file *filp, char __user *ubuf,
++				      size_t cnt, loff_t *ppos)
++{
++	char buf[4];
++
++	if ((cnt < sizeof(buf)) || (*ppos))
++		return 0;
++
++	buf[0] = enabled ? '1' : '0';
++	buf[1] = '\n';
++	buf[2] = '\0';
++	if (copy_to_user(ubuf, buf, strlen(buf)))
++		return -EFAULT;
++	return *ppos = strlen(buf);
++}
++
++/**
++ * debug_enable_fwrite - Write function for "enable" debugfs interface
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The user buffer that contains the value to write
++ * @cnt: The maximum number of bytes to write to "file"
++ * @ppos: The current position in the debugfs "file"
++ *
++ * This function provides a write implementation for the "enable" debugfs
++ * interface to the hardware latency detector. Can be used to enable or
++ * disable the detector, which will have the side-effect of possibly
++ * also resetting the global stats and kicking off the measuring
++ * kthread (on an enable) or the converse (upon a disable).
++ */
++static ssize_t  debug_enable_fwrite(struct file *filp,
++					const char __user *ubuf,
++					size_t cnt,
++					loff_t *ppos)
++{
++	char buf[4];
++	int csize = min(cnt, sizeof(buf));
++	long val = 0;
++	int err = 0;
++
++	memset(buf, '\0', sizeof(buf));
++	if (copy_from_user(buf, ubuf, csize))
++		return -EFAULT;
++
++	buf[sizeof(buf)-1] = '\0';			/* just in case */
++	err = kstrtoul(buf, 10, &val);
++	if (0 != err)
++		return -EINVAL;
++
++	if (val) {
++		if (enabled)
++			goto unlock;
++		enabled = 1;
++		__reset_stats();
++		if (start_kthread())
++			return -EFAULT;
++	} else {
++		if (!enabled)
++			goto unlock;
++		enabled = 0;
++		err = stop_kthread();
++		if (err) {
++			pr_err(BANNER "cannot stop kthread\n");
++			return -EFAULT;
++		}
++		wake_up(&data.wq);		/* reader(s) should return */
++	}
++unlock:
++	return csize;
++}
++
++/**
++ * debug_max_fopen - Open function for "max" debugfs entry
++ * @inode: The in-kernel inode representation of the debugfs "file"
++ * @filp: The active open file structure for the debugfs "file"
++ *
++ * This function provides an open implementation for the "max" debugfs
++ * interface to the hardware latency detector.
++ */
++static int debug_max_fopen(struct inode *inode, struct file *filp)
++{
++	return 0;
++}
++
++/**
++ * debug_max_fread - Read function for "max" debugfs entry
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The userspace provided buffer to read value into
++ * @cnt: The maximum number of bytes to read
++ * @ppos: The current "file" position
++ *
++ * This function provides a read implementation for the "max" debugfs
++ * interface to the hardware latency detector. Can be used to determine
++ * the maximum latency value observed since it was last reset.
++ */
++static ssize_t debug_max_fread(struct file *filp, char __user *ubuf,
++				   size_t cnt, loff_t *ppos)
++{
++	return simple_data_read(filp, ubuf, cnt, ppos, &data.max_sample);
++}
++
++/**
++ * debug_max_fwrite - Write function for "max" debugfs entry
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The user buffer that contains the value to write
++ * @cnt: The maximum number of bytes to write to "file"
++ * @ppos: The current position in the debugfs "file"
++ *
++ * This function provides a write implementation for the "max" debugfs
++ * interface to the hardware latency detector. Can be used to reset the
++ * maximum or set it to some other desired value - if, then, subsequent
++ * measurements exceed this value, the maximum will be updated.
++ */
++static ssize_t  debug_max_fwrite(struct file *filp,
++				     const char __user *ubuf,
++				     size_t cnt,
++				     loff_t *ppos)
++{
++	return simple_data_write(filp, ubuf, cnt, ppos, &data.max_sample);
++}
++
++
++/**
++ * debug_sample_fopen - An open function for "sample" debugfs interface
++ * @inode: The in-kernel inode representation of this debugfs "file"
++ * @filp: The active open file structure for the debugfs "file"
++ *
++ * This function handles opening the "sample" file within the hardware
++ * latency detector debugfs directory interface. This file is used to read
++ * raw samples from the global ring_buffer and allows the user to see a
++ * running latency history. Can be opened blocking or non-blocking,
++ * affecting whether it behaves as a buffer read pipe, or does not.
++ * Implements simple locking to prevent multiple simultaneous use.
++ */
++static int debug_sample_fopen(struct inode *inode, struct file *filp)
++{
++	if (!atomic_add_unless(&data.sample_open, 1, 1))
++		return -EBUSY;
++	else
++		return 0;
++}
++
++/**
++ * debug_sample_fread - A read function for "sample" debugfs interface
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The user buffer that will contain the samples read
++ * @cnt: The maximum bytes to read from the debugfs "file"
++ * @ppos: The current position in the debugfs "file"
++ *
++ * This function handles reading from the "sample" file within the hardware
++ * latency detector debugfs directory interface. This file is used to read
++ * raw samples from the global ring_buffer and allows the user to see a
++ * running latency history. By default this will block pending a new
++ * value written into the sample buffer, unless there are already a
++ * number of value(s) waiting in the buffer, or the sample file was
++ * previously opened in a non-blocking mode of operation.
++ */
++static ssize_t debug_sample_fread(struct file *filp, char __user *ubuf,
++					size_t cnt, loff_t *ppos)
++{
++	int len = 0;
++	char buf[64];
++	struct sample *sample = NULL;
++
++	if (!enabled)
++		return 0;
++
++	sample = kzalloc(sizeof(struct sample), GFP_KERNEL);
++	if (!sample)
++		return -ENOMEM;
++
++	while (!buffer_get_sample(sample)) {
++
++		DEFINE_WAIT(wait);
++
++		if (filp->f_flags & O_NONBLOCK) {
++			len = -EAGAIN;
++			goto out;
++		}
++
++		prepare_to_wait(&data.wq, &wait, TASK_INTERRUPTIBLE);
++		schedule();
++		finish_wait(&data.wq, &wait);
++
++		if (signal_pending(current)) {
++			len = -EINTR;
++			goto out;
++		}
++
++		if (!enabled) {			/* enable was toggled */
++			len = 0;
++			goto out;
++		}
++	}
++
++	len = snprintf(buf, sizeof(buf), "%010lu.%010lu\t%llu\t%llu\n",
++		       sample->timestamp.tv_sec,
++		       sample->timestamp.tv_nsec,
++		       sample->duration,
++		       sample->outer_duration);
++
++
++	/* handling partial reads is more trouble than it's worth */
++	if (len > cnt)
++		goto out;
++
++	if (copy_to_user(ubuf, buf, len))
++		len = -EFAULT;
++
++out:
++	kfree(sample);
++	return len;
++}
++
++/**
++ * debug_sample_release - Release function for "sample" debugfs interface
++ * @inode: The in-kernel inode represenation of the debugfs "file"
++ * @filp: The active open file structure for the debugfs "file"
++ *
++ * This function completes the close of the debugfs interface "sample" file.
++ * Frees the sample_open "lock" so that other users may open the interface.
++ */
++static int debug_sample_release(struct inode *inode, struct file *filp)
++{
++	atomic_dec(&data.sample_open);
++
++	return 0;
++}
++
++/**
++ * debug_threshold_fopen - Open function for "threshold" debugfs entry
++ * @inode: The in-kernel inode representation of the debugfs "file"
++ * @filp: The active open file structure for the debugfs "file"
++ *
++ * This function provides an open implementation for the "threshold" debugfs
++ * interface to the hardware latency detector.
++ */
++static int debug_threshold_fopen(struct inode *inode, struct file *filp)
++{
++	return 0;
++}
++
++/**
++ * debug_threshold_fread - Read function for "threshold" debugfs entry
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The userspace provided buffer to read value into
++ * @cnt: The maximum number of bytes to read
++ * @ppos: The current "file" position
++ *
++ * This function provides a read implementation for the "threshold" debugfs
++ * interface to the hardware latency detector. It can be used to determine
++ * the current threshold level at which a latency will be recorded in the
++ * global ring buffer, typically on the order of 10us.
++ */
++static ssize_t debug_threshold_fread(struct file *filp, char __user *ubuf,
++					 size_t cnt, loff_t *ppos)
++{
++	return simple_data_read(filp, ubuf, cnt, ppos, &data.threshold);
++}
++
++/**
++ * debug_threshold_fwrite - Write function for "threshold" debugfs entry
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The user buffer that contains the value to write
++ * @cnt: The maximum number of bytes to write to "file"
++ * @ppos: The current position in the debugfs "file"
++ *
++ * This function provides a write implementation for the "threshold" debugfs
++ * interface to the hardware latency detector. It can be used to configure
++ * the threshold level at which any subsequently detected latencies will
++ * be recorded into the global ring buffer.
++ */
++static ssize_t  debug_threshold_fwrite(struct file *filp,
++					const char __user *ubuf,
++					size_t cnt,
++					loff_t *ppos)
++{
++	int ret;
++
++	ret = simple_data_write(filp, ubuf, cnt, ppos, &data.threshold);
++
++	if (enabled)
++		wake_up_process(kthread);
++
++	return ret;
++}
++
++/**
++ * debug_width_fopen - Open function for "width" debugfs entry
++ * @inode: The in-kernel inode representation of the debugfs "file"
++ * @filp: The active open file structure for the debugfs "file"
++ *
++ * This function provides an open implementation for the "width" debugfs
++ * interface to the hardware latency detector.
++ */
++static int debug_width_fopen(struct inode *inode, struct file *filp)
++{
++	return 0;
++}
++
++/**
++ * debug_width_fread - Read function for "width" debugfs entry
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The userspace provided buffer to read value into
++ * @cnt: The maximum number of bytes to read
++ * @ppos: The current "file" position
++ *
++ * This function provides a read implementation for the "width" debugfs
++ * interface to the hardware latency detector. It can be used to determine
++ * for how many us of the total window us we will actively sample for any
++ * hardware-induced latecy periods. Obviously, it is not possible to
++ * sample constantly and have the system respond to a sample reader, or,
++ * worse, without having the system appear to have gone out to lunch.
++ */
++static ssize_t debug_width_fread(struct file *filp, char __user *ubuf,
++				     size_t cnt, loff_t *ppos)
++{
++	return simple_data_read(filp, ubuf, cnt, ppos, &data.sample_width);
++}
++
++/**
++ * debug_width_fwrite - Write function for "width" debugfs entry
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The user buffer that contains the value to write
++ * @cnt: The maximum number of bytes to write to "file"
++ * @ppos: The current position in the debugfs "file"
++ *
++ * This function provides a write implementation for the "width" debugfs
++ * interface to the hardware latency detector. It can be used to configure
++ * for how many us of the total window us we will actively sample for any
++ * hardware-induced latency periods. Obviously, it is not possible to
++ * sample constantly and have the system respond to a sample reader, or,
++ * worse, without having the system appear to have gone out to lunch. It
++ * is enforced that width is less that the total window size.
++ */
++static ssize_t  debug_width_fwrite(struct file *filp,
++				       const char __user *ubuf,
++				       size_t cnt,
++				       loff_t *ppos)
++{
++	char buf[U64STR_SIZE];
++	int csize = min(cnt, sizeof(buf));
++	u64 val = 0;
++	int err = 0;
++
++	memset(buf, '\0', sizeof(buf));
++	if (copy_from_user(buf, ubuf, csize))
++		return -EFAULT;
++
++	buf[U64STR_SIZE-1] = '\0';			/* just in case */
++	err = kstrtoull(buf, 10, &val);
++	if (0 != err)
++		return -EINVAL;
++
++	mutex_lock(&data.lock);
++	if (val < data.sample_window)
++		data.sample_width = val;
++	else {
++		mutex_unlock(&data.lock);
++		return -EINVAL;
++	}
++	mutex_unlock(&data.lock);
++
++	if (enabled)
++		wake_up_process(kthread);
++
++	return csize;
++}
++
++/**
++ * debug_window_fopen - Open function for "window" debugfs entry
++ * @inode: The in-kernel inode representation of the debugfs "file"
++ * @filp: The active open file structure for the debugfs "file"
++ *
++ * This function provides an open implementation for the "window" debugfs
++ * interface to the hardware latency detector. The window is the total time
++ * in us that will be considered one sample period. Conceptually, windows
++ * occur back-to-back and contain a sample width period during which
++ * actual sampling occurs.
++ */
++static int debug_window_fopen(struct inode *inode, struct file *filp)
++{
++	return 0;
++}
++
++/**
++ * debug_window_fread - Read function for "window" debugfs entry
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The userspace provided buffer to read value into
++ * @cnt: The maximum number of bytes to read
++ * @ppos: The current "file" position
++ *
++ * This function provides a read implementation for the "window" debugfs
++ * interface to the hardware latency detector. The window is the total time
++ * in us that will be considered one sample period. Conceptually, windows
++ * occur back-to-back and contain a sample width period during which
++ * actual sampling occurs. Can be used to read the total window size.
++ */
++static ssize_t debug_window_fread(struct file *filp, char __user *ubuf,
++				      size_t cnt, loff_t *ppos)
++{
++	return simple_data_read(filp, ubuf, cnt, ppos, &data.sample_window);
++}
++
++/**
++ * debug_window_fwrite - Write function for "window" debugfs entry
++ * @filp: The active open file structure for the debugfs "file"
++ * @ubuf: The user buffer that contains the value to write
++ * @cnt: The maximum number of bytes to write to "file"
++ * @ppos: The current position in the debugfs "file"
++ *
++ * This function provides a write implementation for the "window" debufds
++ * interface to the hardware latency detetector. The window is the total time
++ * in us that will be considered one sample period. Conceptually, windows
++ * occur back-to-back and contain a sample width period during which
++ * actual sampling occurs. Can be used to write a new total window size. It
++ * is enfoced that any value written must be greater than the sample width
++ * size, or an error results.
++ */
++static ssize_t  debug_window_fwrite(struct file *filp,
++					const char __user *ubuf,
++					size_t cnt,
++					loff_t *ppos)
++{
++	char buf[U64STR_SIZE];
++	int csize = min(cnt, sizeof(buf));
++	u64 val = 0;
++	int err = 0;
++
++	memset(buf, '\0', sizeof(buf));
++	if (copy_from_user(buf, ubuf, csize))
++		return -EFAULT;
++
++	buf[U64STR_SIZE-1] = '\0';			/* just in case */
++	err = kstrtoull(buf, 10, &val);
++	if (0 != err)
++		return -EINVAL;
++
++	mutex_lock(&data.lock);
++	if (data.sample_width < val)
++		data.sample_window = val;
++	else {
++		mutex_unlock(&data.lock);
++		return -EINVAL;
++	}
++	mutex_unlock(&data.lock);
++
++	return csize;
++}
++
++/*
++ * Function pointers for the "count" debugfs file operations
++ */
++static const struct file_operations count_fops = {
++	.open		= debug_count_fopen,
++	.read		= debug_count_fread,
++	.write		= debug_count_fwrite,
++	.owner		= THIS_MODULE,
++};
++
++/*
++ * Function pointers for the "enable" debugfs file operations
++ */
++static const struct file_operations enable_fops = {
++	.open		= debug_enable_fopen,
++	.read		= debug_enable_fread,
++	.write		= debug_enable_fwrite,
++	.owner		= THIS_MODULE,
++};
++
++/*
++ * Function pointers for the "max" debugfs file operations
++ */
++static const struct file_operations max_fops = {
++	.open		= debug_max_fopen,
++	.read		= debug_max_fread,
++	.write		= debug_max_fwrite,
++	.owner		= THIS_MODULE,
++};
++
++/*
++ * Function pointers for the "sample" debugfs file operations
++ */
++static const struct file_operations sample_fops = {
++	.open		= debug_sample_fopen,
++	.read		= debug_sample_fread,
++	.release	= debug_sample_release,
++	.owner		= THIS_MODULE,
++};
++
++/*
++ * Function pointers for the "threshold" debugfs file operations
++ */
++static const struct file_operations threshold_fops = {
++	.open		= debug_threshold_fopen,
++	.read		= debug_threshold_fread,
++	.write		= debug_threshold_fwrite,
++	.owner		= THIS_MODULE,
++};
++
++/*
++ * Function pointers for the "width" debugfs file operations
++ */
++static const struct file_operations width_fops = {
++	.open		= debug_width_fopen,
++	.read		= debug_width_fread,
++	.write		= debug_width_fwrite,
++	.owner		= THIS_MODULE,
++};
++
++/*
++ * Function pointers for the "window" debugfs file operations
++ */
++static const struct file_operations window_fops = {
++	.open		= debug_window_fopen,
++	.read		= debug_window_fread,
++	.write		= debug_window_fwrite,
++	.owner		= THIS_MODULE,
++};
++
++/**
++ * init_debugfs - A function to initialize the debugfs interface files
++ *
++ * This function creates entries in debugfs for "hwlat_detector", including
++ * files to read values from the detector, current samples, and the
++ * maximum sample that has been captured since the hardware latency
++ * dectector was started.
++ */
++static int init_debugfs(void)
++{
++	int ret = -ENOMEM;
++
++	debug_dir = debugfs_create_dir(DRVNAME, NULL);
++	if (!debug_dir)
++		goto err_debug_dir;
++
++	debug_sample = debugfs_create_file("sample", 0444,
++					       debug_dir, NULL,
++					       &sample_fops);
++	if (!debug_sample)
++		goto err_sample;
++
++	debug_count = debugfs_create_file("count", 0444,
++					      debug_dir, NULL,
++					      &count_fops);
++	if (!debug_count)
++		goto err_count;
++
++	debug_max = debugfs_create_file("max", 0444,
++					    debug_dir, NULL,
++					    &max_fops);
++	if (!debug_max)
++		goto err_max;
++
++	debug_sample_window = debugfs_create_file("window", 0644,
++						      debug_dir, NULL,
++						      &window_fops);
++	if (!debug_sample_window)
++		goto err_window;
++
++	debug_sample_width = debugfs_create_file("width", 0644,
++						     debug_dir, NULL,
++						     &width_fops);
++	if (!debug_sample_width)
++		goto err_width;
++
++	debug_threshold = debugfs_create_file("threshold", 0644,
++						  debug_dir, NULL,
++						  &threshold_fops);
++	if (!debug_threshold)
++		goto err_threshold;
++
++	debug_enable = debugfs_create_file("enable", 0644,
++					       debug_dir, &enabled,
++					       &enable_fops);
++	if (!debug_enable)
++		goto err_enable;
++
++	else {
++		ret = 0;
++		goto out;
++	}
++
++err_enable:
++	debugfs_remove(debug_threshold);
++err_threshold:
++	debugfs_remove(debug_sample_width);
++err_width:
++	debugfs_remove(debug_sample_window);
++err_window:
++	debugfs_remove(debug_max);
++err_max:
++	debugfs_remove(debug_count);
++err_count:
++	debugfs_remove(debug_sample);
++err_sample:
++	debugfs_remove(debug_dir);
++err_debug_dir:
++out:
++	return ret;
++}
++
++/**
++ * free_debugfs - A function to cleanup the debugfs file interface
++ */
++static void free_debugfs(void)
++{
++	/* could also use a debugfs_remove_recursive */
++	debugfs_remove(debug_enable);
++	debugfs_remove(debug_threshold);
++	debugfs_remove(debug_sample_width);
++	debugfs_remove(debug_sample_window);
++	debugfs_remove(debug_max);
++	debugfs_remove(debug_count);
++	debugfs_remove(debug_sample);
++	debugfs_remove(debug_dir);
++}
++
++/**
++ * detector_init - Standard module initialization code
++ */
++static int detector_init(void)
++{
++	int ret = -ENOMEM;
++
++	pr_info(BANNER "version %s\n", VERSION);
++
++	ret = init_stats();
++	if (0 != ret)
++		goto out;
++
++	ret = init_debugfs();
++	if (0 != ret)
++		goto err_stats;
++
++	if (enabled)
++		ret = start_kthread();
++
++	goto out;
++
++err_stats:
++	ring_buffer_free(ring_buffer);
++out:
++	return ret;
++
++}
++
++/**
++ * detector_exit - Standard module cleanup code
++ */
++static void detector_exit(void)
++{
++	int err;
++
++	if (enabled) {
++		enabled = 0;
++		err = stop_kthread();
++		if (err)
++			pr_err(BANNER "cannot stop kthread\n");
++	}
++
++	free_debugfs();
++	ring_buffer_free(ring_buffer);	/* free up the ring buffer */
++
++}
++
++module_init(detector_init);
++module_exit(detector_exit);
+diff --git a/drivers/mmc/host/mmci.c b/drivers/mmc/host/mmci.c
+index 3ec7e330f7e5..139f4d096849 100644
+--- a/drivers/mmc/host/mmci.c
++++ b/drivers/mmc/host/mmci.c
+@@ -1153,15 +1153,12 @@ static irqreturn_t mmci_pio_irq(int irq, void *dev_id)
+ 	struct sg_mapping_iter *sg_miter = &host->sg_miter;
+ 	struct variant_data *variant = host->variant;
+ 	void __iomem *base = host->base;
+-	unsigned long flags;
+ 	u32 status;
+ 
+ 	status = readl(base + MMCISTATUS);
+ 
+ 	dev_dbg(mmc_dev(host->mmc), "irq1 (pio) %08x\n", status);
+ 
+-	local_irq_save(flags);
+-
+ 	do {
+ 		unsigned int remain, len;
+ 		char *buffer;
+@@ -1201,8 +1198,6 @@ static irqreturn_t mmci_pio_irq(int irq, void *dev_id)
+ 
+ 	sg_miter_stop(sg_miter);
+ 
+-	local_irq_restore(flags);
+-
+ 	/*
+ 	 * If we have less than the fifo 'half-full' threshold to transfer,
+ 	 * trigger a PIO interrupt as soon as any data is available.
+diff --git a/drivers/net/ethernet/3com/3c59x.c b/drivers/net/ethernet/3com/3c59x.c
+index 41095ebad97f..b0a0cb22aec4 100644
+--- a/drivers/net/ethernet/3com/3c59x.c
++++ b/drivers/net/ethernet/3com/3c59x.c
+@@ -842,9 +842,9 @@ static void poll_vortex(struct net_device *dev)
+ {
+ 	struct vortex_private *vp = netdev_priv(dev);
+ 	unsigned long flags;
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	(vp->full_bus_master_rx ? boomerang_interrupt:vortex_interrupt)(dev->irq,dev);
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ }
+ #endif
+ 
+@@ -1916,12 +1916,12 @@ static void vortex_tx_timeout(struct net_device *dev)
+ 			 * Block interrupts because vortex_interrupt does a bare spin_lock()
+ 			 */
+ 			unsigned long flags;
+-			local_irq_save(flags);
++			local_irq_save_nort(flags);
+ 			if (vp->full_bus_master_tx)
+ 				boomerang_interrupt(dev->irq, dev);
+ 			else
+ 				vortex_interrupt(dev->irq, dev);
+-			local_irq_restore(flags);
++			local_irq_restore_nort(flags);
+ 		}
+ 	}
+ 
+diff --git a/drivers/net/ethernet/atheros/atl1c/atl1c_main.c b/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
+index 067f2cb9b215..c2010fcacee3 100644
+--- a/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
++++ b/drivers/net/ethernet/atheros/atl1c/atl1c_main.c
+@@ -2212,11 +2212,7 @@ static netdev_tx_t atl1c_xmit_frame(struct sk_buff *skb,
+ 	}
+ 
+ 	tpd_req = atl1c_cal_tpd_req(skb);
+-	if (!spin_trylock_irqsave(&adapter->tx_lock, flags)) {
+-		if (netif_msg_pktdata(adapter))
+-			dev_info(&adapter->pdev->dev, "tx locked\n");
+-		return NETDEV_TX_LOCKED;
+-	}
++	spin_lock_irqsave(&adapter->tx_lock, flags);
+ 
+ 	if (atl1c_tpd_avail(adapter, type) < tpd_req) {
+ 		/* no enough descriptor, just stop queue */
+diff --git a/drivers/net/ethernet/atheros/atl1e/atl1e_main.c b/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
+index 2326579f9454..a042658ff8af 100644
+--- a/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
++++ b/drivers/net/ethernet/atheros/atl1e/atl1e_main.c
+@@ -1880,8 +1880,7 @@ static netdev_tx_t atl1e_xmit_frame(struct sk_buff *skb,
+ 		return NETDEV_TX_OK;
+ 	}
+ 	tpd_req = atl1e_cal_tdp_req(skb);
+-	if (!spin_trylock_irqsave(&adapter->tx_lock, flags))
+-		return NETDEV_TX_LOCKED;
++	spin_lock_irqsave(&adapter->tx_lock, flags);
+ 
+ 	if (atl1e_tpd_avail(adapter) < tpd_req) {
+ 		/* no enough descriptor, just stop queue */
+diff --git a/drivers/net/ethernet/chelsio/cxgb/sge.c b/drivers/net/ethernet/chelsio/cxgb/sge.c
+index 4c5879389003..1adb83c01741 100644
+--- a/drivers/net/ethernet/chelsio/cxgb/sge.c
++++ b/drivers/net/ethernet/chelsio/cxgb/sge.c
+@@ -1663,8 +1663,7 @@ static int t1_sge_tx(struct sk_buff *skb, struct adapter *adapter,
+ 	struct cmdQ *q = &sge->cmdQ[qid];
+ 	unsigned int credits, pidx, genbit, count, use_sched_skb = 0;
+ 
+-	if (!spin_trylock(&q->lock))
+-		return NETDEV_TX_LOCKED;
++	spin_lock(&q->lock);
+ 
+ 	reclaim_completed_tx(sge, q);
+ 
+diff --git a/drivers/net/ethernet/freescale/gianfar.c b/drivers/net/ethernet/freescale/gianfar.c
+index 4fdf0aa16978..bb7aaf0d9a3f 100644
+--- a/drivers/net/ethernet/freescale/gianfar.c
++++ b/drivers/net/ethernet/freescale/gianfar.c
+@@ -1483,7 +1483,7 @@ static int gfar_suspend(struct device *dev)
+ 
+ 	if (netif_running(ndev)) {
+ 
+-		local_irq_save(flags);
++		local_irq_save_nort(flags);
+ 		lock_tx_qs(priv);
+ 
+ 		gfar_halt_nodisable(priv);
+@@ -1499,7 +1499,7 @@ static int gfar_suspend(struct device *dev)
+ 		gfar_write(&regs->maccfg1, tempval);
+ 
+ 		unlock_tx_qs(priv);
+-		local_irq_restore(flags);
++		local_irq_restore_nort(flags);
+ 
+ 		disable_napi(priv);
+ 
+@@ -1541,7 +1541,7 @@ static int gfar_resume(struct device *dev)
+ 	/* Disable Magic Packet mode, in case something
+ 	 * else woke us up.
+ 	 */
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	lock_tx_qs(priv);
+ 
+ 	tempval = gfar_read(&regs->maccfg2);
+@@ -1551,7 +1551,7 @@ static int gfar_resume(struct device *dev)
+ 	gfar_start(priv);
+ 
+ 	unlock_tx_qs(priv);
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 
+ 	netif_device_attach(ndev);
+ 
+@@ -3307,14 +3307,14 @@ static irqreturn_t gfar_error(int irq, void *grp_id)
+ 			dev->stats.tx_dropped++;
+ 			atomic64_inc(&priv->extra_stats.tx_underrun);
+ 
+-			local_irq_save(flags);
++			local_irq_save_nort(flags);
+ 			lock_tx_qs(priv);
+ 
+ 			/* Reactivate the Tx Queues */
+ 			gfar_write(&regs->tstat, gfargrp->tstat);
+ 
+ 			unlock_tx_qs(priv);
+-			local_irq_restore(flags);
++			local_irq_restore_nort(flags);
+ 		}
+ 		netif_dbg(priv, tx_err, dev, "Transmit Error\n");
+ 	}
+diff --git a/drivers/net/ethernet/neterion/s2io.c b/drivers/net/ethernet/neterion/s2io.c
+index f5e4b820128b..631175b5df3b 100644
+--- a/drivers/net/ethernet/neterion/s2io.c
++++ b/drivers/net/ethernet/neterion/s2io.c
+@@ -4084,12 +4084,7 @@ static netdev_tx_t s2io_xmit(struct sk_buff *skb, struct net_device *dev)
+ 			[skb->priority & (MAX_TX_FIFOS - 1)];
+ 	fifo = &mac_control->fifos[queue];
+ 
+-	if (do_spin_lock)
+-		spin_lock_irqsave(&fifo->tx_lock, flags);
+-	else {
+-		if (unlikely(!spin_trylock_irqsave(&fifo->tx_lock, flags)))
+-			return NETDEV_TX_LOCKED;
+-	}
++	spin_lock_irqsave(&fifo->tx_lock, flags);
+ 
+ 	if (sp->config.multiq) {
+ 		if (__netif_subqueue_stopped(dev, fifo->fifo_no)) {
+diff --git a/drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c b/drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
+index 3b98b263bad0..ca4add749410 100644
+--- a/drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
++++ b/drivers/net/ethernet/oki-semi/pch_gbe/pch_gbe_main.c
+@@ -2137,10 +2137,8 @@ static int pch_gbe_xmit_frame(struct sk_buff *skb, struct net_device *netdev)
+ 	struct pch_gbe_tx_ring *tx_ring = adapter->tx_ring;
+ 	unsigned long flags;
+ 
+-	if (!spin_trylock_irqsave(&tx_ring->tx_lock, flags)) {
+-		/* Collision - tell upper layer to requeue */
+-		return NETDEV_TX_LOCKED;
+-	}
++	spin_lock_irqsave(&tx_ring->tx_lock, flags);
++
+ 	if (unlikely(!PCH_GBE_DESC_UNUSED(tx_ring))) {
+ 		netif_stop_queue(netdev);
+ 		spin_unlock_irqrestore(&tx_ring->tx_lock, flags);
+diff --git a/drivers/net/ethernet/realtek/8139too.c b/drivers/net/ethernet/realtek/8139too.c
+index 007b38cce69a..7858f2bd894d 100644
+--- a/drivers/net/ethernet/realtek/8139too.c
++++ b/drivers/net/ethernet/realtek/8139too.c
+@@ -2215,7 +2215,7 @@ static void rtl8139_poll_controller(struct net_device *dev)
+ 	struct rtl8139_private *tp = netdev_priv(dev);
+ 	const int irq = tp->pci_dev->irq;
+ 
+-	disable_irq(irq);
++	disable_irq_nosync(irq);
+ 	rtl8139_interrupt(irq, dev);
+ 	enable_irq(irq);
+ }
+diff --git a/drivers/net/ethernet/tehuti/tehuti.c b/drivers/net/ethernet/tehuti/tehuti.c
+index 6ab36d9ff2ab..6b7d989c4630 100644
+--- a/drivers/net/ethernet/tehuti/tehuti.c
++++ b/drivers/net/ethernet/tehuti/tehuti.c
+@@ -1629,13 +1629,8 @@ static netdev_tx_t bdx_tx_transmit(struct sk_buff *skb,
+ 	unsigned long flags;
+ 
+ 	ENTER;
+-	local_irq_save(flags);
+-	if (!spin_trylock(&priv->tx_lock)) {
+-		local_irq_restore(flags);
+-		DBG("%s[%s]: TX locked, returning NETDEV_TX_LOCKED\n",
+-		    BDX_DRV_NAME, ndev->name);
+-		return NETDEV_TX_LOCKED;
+-	}
++
++	spin_lock_irqsave(&priv->tx_lock, flags);
+ 
+ 	/* build tx descriptor */
+ 	BDX_ASSERT(f->m.wptr >= f->m.memsz);	/* started with valid wptr */
+diff --git a/drivers/net/rionet.c b/drivers/net/rionet.c
+index 18cc2c8d5447..a5e0ef3c02d1 100644
+--- a/drivers/net/rionet.c
++++ b/drivers/net/rionet.c
+@@ -174,11 +174,7 @@ static int rionet_start_xmit(struct sk_buff *skb, struct net_device *ndev)
+ 	unsigned long flags;
+ 	int add_num = 1;
+ 
+-	local_irq_save(flags);
+-	if (!spin_trylock(&rnet->tx_lock)) {
+-		local_irq_restore(flags);
+-		return NETDEV_TX_LOCKED;
+-	}
++	spin_lock_irqsave(&rnet->tx_lock, flags);
+ 
+ 	if (is_multicast_ether_addr(eth->h_dest))
+ 		add_num = nets[rnet->mport->id].nact;
+diff --git a/drivers/net/wireless/orinoco/orinoco_usb.c b/drivers/net/wireless/orinoco/orinoco_usb.c
+index 995846422dc0..dfda85005038 100644
+--- a/drivers/net/wireless/orinoco/orinoco_usb.c
++++ b/drivers/net/wireless/orinoco/orinoco_usb.c
+@@ -699,7 +699,7 @@ static void ezusb_req_ctx_wait(struct ezusb_priv *upriv,
+ 			while (!ctx->done.done && msecs--)
+ 				udelay(1000);
+ 		} else {
+-			wait_event_interruptible(ctx->done.wait,
++			swait_event_interruptible(ctx->done.wait,
+ 						 ctx->done.done);
+ 		}
+ 		break;
+diff --git a/drivers/pci/access.c b/drivers/pci/access.c
+index 7f249b9ab2ce..3e0b802d80de 100644
+--- a/drivers/pci/access.c
++++ b/drivers/pci/access.c
+@@ -493,7 +493,7 @@ void pci_cfg_access_unlock(struct pci_dev *dev)
+ 	WARN_ON(!dev->block_cfg_access);
+ 
+ 	dev->block_cfg_access = 0;
+-	wake_up_all(&pci_cfg_wait);
++	wake_up_all_locked(&pci_cfg_wait);
+ 	raw_spin_unlock_irqrestore(&pci_lock, flags);
+ }
+ EXPORT_SYMBOL_GPL(pci_cfg_access_unlock);
+diff --git a/drivers/pinctrl/qcom/pinctrl-msm.c b/drivers/pinctrl/qcom/pinctrl-msm.c
+index db4c22dd07fa..95fe9a4e61ef 100644
+--- a/drivers/pinctrl/qcom/pinctrl-msm.c
++++ b/drivers/pinctrl/qcom/pinctrl-msm.c
+@@ -59,7 +59,7 @@ struct msm_pinctrl {
+ 	struct notifier_block restart_nb;
+ 	int irq;
+ 
+-	spinlock_t lock;
++	raw_spinlock_t lock;
+ 
+ 	DECLARE_BITMAP(dual_edge_irqs, MAX_NR_GPIO);
+ 	DECLARE_BITMAP(enabled_irqs, MAX_NR_GPIO);
+@@ -155,14 +155,14 @@ static int msm_pinmux_set_mux(struct pinctrl_dev *pctldev,
+ 	if (WARN_ON(i == g->nfuncs))
+ 		return -EINVAL;
+ 
+-	spin_lock_irqsave(&pctrl->lock, flags);
++	raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 
+ 	val = readl(pctrl->regs + g->ctl_reg);
+ 	val &= ~(0x7 << g->mux_bit);
+ 	val |= i << g->mux_bit;
+ 	writel(val, pctrl->regs + g->ctl_reg);
+ 
+-	spin_unlock_irqrestore(&pctrl->lock, flags);
++	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -336,14 +336,14 @@ static int msm_config_group_set(struct pinctrl_dev *pctldev,
+ 			break;
+ 		case PIN_CONFIG_OUTPUT:
+ 			/* set output value */
+-			spin_lock_irqsave(&pctrl->lock, flags);
++			raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 			val = readl(pctrl->regs + g->io_reg);
+ 			if (arg)
+ 				val |= BIT(g->out_bit);
+ 			else
+ 				val &= ~BIT(g->out_bit);
+ 			writel(val, pctrl->regs + g->io_reg);
+-			spin_unlock_irqrestore(&pctrl->lock, flags);
++			raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ 
+ 			/* enable output */
+ 			arg = 1;
+@@ -360,12 +360,12 @@ static int msm_config_group_set(struct pinctrl_dev *pctldev,
+ 			return -EINVAL;
+ 		}
+ 
+-		spin_lock_irqsave(&pctrl->lock, flags);
++		raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 		val = readl(pctrl->regs + g->ctl_reg);
+ 		val &= ~(mask << bit);
+ 		val |= arg << bit;
+ 		writel(val, pctrl->regs + g->ctl_reg);
+-		spin_unlock_irqrestore(&pctrl->lock, flags);
++		raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ 	}
+ 
+ 	return 0;
+@@ -394,13 +394,13 @@ static int msm_gpio_direction_input(struct gpio_chip *chip, unsigned offset)
+ 
+ 	g = &pctrl->soc->groups[offset];
+ 
+-	spin_lock_irqsave(&pctrl->lock, flags);
++	raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 
+ 	val = readl(pctrl->regs + g->ctl_reg);
+ 	val &= ~BIT(g->oe_bit);
+ 	writel(val, pctrl->regs + g->ctl_reg);
+ 
+-	spin_unlock_irqrestore(&pctrl->lock, flags);
++	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -414,7 +414,7 @@ static int msm_gpio_direction_output(struct gpio_chip *chip, unsigned offset, in
+ 
+ 	g = &pctrl->soc->groups[offset];
+ 
+-	spin_lock_irqsave(&pctrl->lock, flags);
++	raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 
+ 	val = readl(pctrl->regs + g->io_reg);
+ 	if (value)
+@@ -427,7 +427,7 @@ static int msm_gpio_direction_output(struct gpio_chip *chip, unsigned offset, in
+ 	val |= BIT(g->oe_bit);
+ 	writel(val, pctrl->regs + g->ctl_reg);
+ 
+-	spin_unlock_irqrestore(&pctrl->lock, flags);
++	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -453,7 +453,7 @@ static void msm_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
+ 
+ 	g = &pctrl->soc->groups[offset];
+ 
+-	spin_lock_irqsave(&pctrl->lock, flags);
++	raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 
+ 	val = readl(pctrl->regs + g->io_reg);
+ 	if (value)
+@@ -462,7 +462,7 @@ static void msm_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
+ 		val &= ~BIT(g->out_bit);
+ 	writel(val, pctrl->regs + g->io_reg);
+ 
+-	spin_unlock_irqrestore(&pctrl->lock, flags);
++	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ }
+ 
+ static int msm_gpio_request(struct gpio_chip *chip, unsigned offset)
+@@ -593,7 +593,7 @@ static void msm_gpio_irq_mask(struct irq_data *d)
+ 
+ 	g = &pctrl->soc->groups[d->hwirq];
+ 
+-	spin_lock_irqsave(&pctrl->lock, flags);
++	raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 
+ 	val = readl(pctrl->regs + g->intr_cfg_reg);
+ 	val &= ~BIT(g->intr_enable_bit);
+@@ -601,7 +601,7 @@ static void msm_gpio_irq_mask(struct irq_data *d)
+ 
+ 	clear_bit(d->hwirq, pctrl->enabled_irqs);
+ 
+-	spin_unlock_irqrestore(&pctrl->lock, flags);
++	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ }
+ 
+ static void msm_gpio_irq_unmask(struct irq_data *d)
+@@ -614,7 +614,7 @@ static void msm_gpio_irq_unmask(struct irq_data *d)
+ 
+ 	g = &pctrl->soc->groups[d->hwirq];
+ 
+-	spin_lock_irqsave(&pctrl->lock, flags);
++	raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 
+ 	val = readl(pctrl->regs + g->intr_cfg_reg);
+ 	val |= BIT(g->intr_enable_bit);
+@@ -622,7 +622,7 @@ static void msm_gpio_irq_unmask(struct irq_data *d)
+ 
+ 	set_bit(d->hwirq, pctrl->enabled_irqs);
+ 
+-	spin_unlock_irqrestore(&pctrl->lock, flags);
++	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ }
+ 
+ static void msm_gpio_irq_ack(struct irq_data *d)
+@@ -635,7 +635,7 @@ static void msm_gpio_irq_ack(struct irq_data *d)
+ 
+ 	g = &pctrl->soc->groups[d->hwirq];
+ 
+-	spin_lock_irqsave(&pctrl->lock, flags);
++	raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 
+ 	val = readl(pctrl->regs + g->intr_status_reg);
+ 	if (g->intr_ack_high)
+@@ -647,7 +647,7 @@ static void msm_gpio_irq_ack(struct irq_data *d)
+ 	if (test_bit(d->hwirq, pctrl->dual_edge_irqs))
+ 		msm_gpio_update_dual_edge_pos(pctrl, g, d);
+ 
+-	spin_unlock_irqrestore(&pctrl->lock, flags);
++	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ }
+ 
+ static int msm_gpio_irq_set_type(struct irq_data *d, unsigned int type)
+@@ -660,7 +660,7 @@ static int msm_gpio_irq_set_type(struct irq_data *d, unsigned int type)
+ 
+ 	g = &pctrl->soc->groups[d->hwirq];
+ 
+-	spin_lock_irqsave(&pctrl->lock, flags);
++	raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 
+ 	/*
+ 	 * For hw without possibility of detecting both edges
+@@ -734,7 +734,7 @@ static int msm_gpio_irq_set_type(struct irq_data *d, unsigned int type)
+ 	if (test_bit(d->hwirq, pctrl->dual_edge_irqs))
+ 		msm_gpio_update_dual_edge_pos(pctrl, g, d);
+ 
+-	spin_unlock_irqrestore(&pctrl->lock, flags);
++	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ 
+ 	if (type & (IRQ_TYPE_LEVEL_LOW | IRQ_TYPE_LEVEL_HIGH))
+ 		__irq_set_handler_locked(d->irq, handle_level_irq);
+@@ -750,11 +750,11 @@ static int msm_gpio_irq_set_wake(struct irq_data *d, unsigned int on)
+ 	struct msm_pinctrl *pctrl = to_msm_pinctrl(gc);
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&pctrl->lock, flags);
++	raw_spin_lock_irqsave(&pctrl->lock, flags);
+ 
+ 	irq_set_irq_wake(pctrl->irq, on);
+ 
+-	spin_unlock_irqrestore(&pctrl->lock, flags);
++	raw_spin_unlock_irqrestore(&pctrl->lock, flags);
+ 
+ 	return 0;
+ }
+@@ -891,7 +891,7 @@ int msm_pinctrl_probe(struct platform_device *pdev,
+ 	pctrl->soc = soc_data;
+ 	pctrl->chip = msm_gpio_template;
+ 
+-	spin_lock_init(&pctrl->lock);
++	raw_spin_lock_init(&pctrl->lock);
+ 
+ 	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+ 	pctrl->regs = devm_ioremap_resource(&pdev->dev, res);
+diff --git a/drivers/scsi/fcoe/fcoe.c b/drivers/scsi/fcoe/fcoe.c
+index 4a8ac7d8c76b..8933c02b6729 100644
+--- a/drivers/scsi/fcoe/fcoe.c
++++ b/drivers/scsi/fcoe/fcoe.c
+@@ -1286,7 +1286,7 @@ static void fcoe_percpu_thread_destroy(unsigned int cpu)
+ 	struct sk_buff *skb;
+ #ifdef CONFIG_SMP
+ 	struct fcoe_percpu_s *p0;
+-	unsigned targ_cpu = get_cpu();
++	unsigned targ_cpu = get_cpu_light();
+ #endif /* CONFIG_SMP */
+ 
+ 	FCOE_DBG("Destroying receive thread for CPU %d\n", cpu);
+@@ -1342,7 +1342,7 @@ static void fcoe_percpu_thread_destroy(unsigned int cpu)
+ 			kfree_skb(skb);
+ 		spin_unlock_bh(&p->fcoe_rx_list.lock);
+ 	}
+-	put_cpu();
++	put_cpu_light();
+ #else
+ 	/*
+ 	 * This a non-SMP scenario where the singular Rx thread is
+@@ -1566,11 +1566,11 @@ err2:
+ static int fcoe_alloc_paged_crc_eof(struct sk_buff *skb, int tlen)
+ {
+ 	struct fcoe_percpu_s *fps;
+-	int rc;
++	int rc, cpu = get_cpu_light();
+ 
+-	fps = &get_cpu_var(fcoe_percpu);
++	fps = &per_cpu(fcoe_percpu, cpu);
+ 	rc = fcoe_get_paged_crc_eof(skb, tlen, fps);
+-	put_cpu_var(fcoe_percpu);
++	put_cpu_light();
+ 
+ 	return rc;
+ }
+@@ -1768,11 +1768,11 @@ static inline int fcoe_filter_frames(struct fc_lport *lport,
+ 		return 0;
+ 	}
+ 
+-	stats = per_cpu_ptr(lport->stats, get_cpu());
++	stats = per_cpu_ptr(lport->stats, get_cpu_light());
+ 	stats->InvalidCRCCount++;
+ 	if (stats->InvalidCRCCount < 5)
+ 		printk(KERN_WARNING "fcoe: dropping frame with CRC error\n");
+-	put_cpu();
++	put_cpu_light();
+ 	return -EINVAL;
+ }
+ 
+@@ -1816,7 +1816,7 @@ static void fcoe_recv_frame(struct sk_buff *skb)
+ 	 */
+ 	hp = (struct fcoe_hdr *) skb_network_header(skb);
+ 
+-	stats = per_cpu_ptr(lport->stats, get_cpu());
++	stats = per_cpu_ptr(lport->stats, get_cpu_light());
+ 	if (unlikely(FC_FCOE_DECAPS_VER(hp) != FC_FCOE_VER)) {
+ 		if (stats->ErrorFrames < 5)
+ 			printk(KERN_WARNING "fcoe: FCoE version "
+@@ -1848,13 +1848,13 @@ static void fcoe_recv_frame(struct sk_buff *skb)
+ 		goto drop;
+ 
+ 	if (!fcoe_filter_frames(lport, fp)) {
+-		put_cpu();
++		put_cpu_light();
+ 		fc_exch_recv(lport, fp);
+ 		return;
+ 	}
+ drop:
+ 	stats->ErrorFrames++;
+-	put_cpu();
++	put_cpu_light();
+ 	kfree_skb(skb);
+ }
+ 
+diff --git a/drivers/scsi/fcoe/fcoe_ctlr.c b/drivers/scsi/fcoe/fcoe_ctlr.c
+index 34a1b1f333b4..d91131210695 100644
+--- a/drivers/scsi/fcoe/fcoe_ctlr.c
++++ b/drivers/scsi/fcoe/fcoe_ctlr.c
+@@ -831,7 +831,7 @@ static unsigned long fcoe_ctlr_age_fcfs(struct fcoe_ctlr *fip)
+ 
+ 	INIT_LIST_HEAD(&del_list);
+ 
+-	stats = per_cpu_ptr(fip->lp->stats, get_cpu());
++	stats = per_cpu_ptr(fip->lp->stats, get_cpu_light());
+ 
+ 	list_for_each_entry_safe(fcf, next, &fip->fcfs, list) {
+ 		deadline = fcf->time + fcf->fka_period + fcf->fka_period / 2;
+@@ -867,7 +867,7 @@ static unsigned long fcoe_ctlr_age_fcfs(struct fcoe_ctlr *fip)
+ 				sel_time = fcf->time;
+ 		}
+ 	}
+-	put_cpu();
++	put_cpu_light();
+ 
+ 	list_for_each_entry_safe(fcf, next, &del_list, list) {
+ 		/* Removes fcf from current list */
+diff --git a/drivers/scsi/libfc/fc_exch.c b/drivers/scsi/libfc/fc_exch.c
+index 30f9ef0c0d4f..6c686bc01a82 100644
+--- a/drivers/scsi/libfc/fc_exch.c
++++ b/drivers/scsi/libfc/fc_exch.c
+@@ -814,10 +814,10 @@ static struct fc_exch *fc_exch_em_alloc(struct fc_lport *lport,
+ 	}
+ 	memset(ep, 0, sizeof(*ep));
+ 
+-	cpu = get_cpu();
++	cpu = get_cpu_light();
+ 	pool = per_cpu_ptr(mp->pool, cpu);
+ 	spin_lock_bh(&pool->lock);
+-	put_cpu();
++	put_cpu_light();
+ 
+ 	/* peek cache of free slot */
+ 	if (pool->left != FC_XID_UNKNOWN) {
+diff --git a/drivers/scsi/libsas/sas_ata.c b/drivers/scsi/libsas/sas_ata.c
+index 3f0c3e0b5838..35293453642c 100644
+--- a/drivers/scsi/libsas/sas_ata.c
++++ b/drivers/scsi/libsas/sas_ata.c
+@@ -191,7 +191,7 @@ static unsigned int sas_ata_qc_issue(struct ata_queued_cmd *qc)
+ 	/* TODO: audit callers to ensure they are ready for qc_issue to
+ 	 * unconditionally re-enable interrupts
+ 	 */
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	spin_unlock(ap->lock);
+ 
+ 	/* If the device fell off, no sense in issuing commands */
+@@ -261,7 +261,7 @@ static unsigned int sas_ata_qc_issue(struct ata_queued_cmd *qc)
+ 
+  out:
+ 	spin_lock(ap->lock);
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 	return ret;
+ }
+ 
+diff --git a/drivers/scsi/qla2xxx/qla_inline.h b/drivers/scsi/qla2xxx/qla_inline.h
+index fee9eb7c8a60..b42d4adc42dc 100644
+--- a/drivers/scsi/qla2xxx/qla_inline.h
++++ b/drivers/scsi/qla2xxx/qla_inline.h
+@@ -59,12 +59,12 @@ qla2x00_poll(struct rsp_que *rsp)
+ {
+ 	unsigned long flags;
+ 	struct qla_hw_data *ha = rsp->hw;
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	if (IS_P3P_TYPE(ha))
+ 		qla82xx_poll(0, rsp);
+ 	else
+ 		ha->isp_ops->intr_handler(0, rsp);
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ }
+ 
+ static inline uint8_t *
+diff --git a/drivers/thermal/x86_pkg_temp_thermal.c b/drivers/thermal/x86_pkg_temp_thermal.c
+index 9ea3d9d49ffc..e3c2663e0b1f 100644
+--- a/drivers/thermal/x86_pkg_temp_thermal.c
++++ b/drivers/thermal/x86_pkg_temp_thermal.c
+@@ -29,6 +29,7 @@
+ #include <linux/pm.h>
+ #include <linux/thermal.h>
+ #include <linux/debugfs.h>
++#include <linux/work-simple.h>
+ #include <asm/cpu_device_id.h>
+ #include <asm/mce.h>
+ 
+@@ -352,7 +353,7 @@ static void pkg_temp_thermal_threshold_work_fn(struct work_struct *work)
+ 	}
+ }
+ 
+-static int pkg_temp_thermal_platform_thermal_notify(__u64 msr_val)
++static void platform_thermal_notify_work(struct swork_event *event)
+ {
+ 	unsigned long flags;
+ 	int cpu = smp_processor_id();
+@@ -369,7 +370,7 @@ static int pkg_temp_thermal_platform_thermal_notify(__u64 msr_val)
+ 			pkg_work_scheduled[phy_id]) {
+ 		disable_pkg_thres_interrupt();
+ 		spin_unlock_irqrestore(&pkg_work_lock, flags);
+-		return -EINVAL;
++		return;
+ 	}
+ 	pkg_work_scheduled[phy_id] = 1;
+ 	spin_unlock_irqrestore(&pkg_work_lock, flags);
+@@ -378,9 +379,48 @@ static int pkg_temp_thermal_platform_thermal_notify(__u64 msr_val)
+ 	schedule_delayed_work_on(cpu,
+ 				&per_cpu(pkg_temp_thermal_threshold_work, cpu),
+ 				msecs_to_jiffies(notify_delay_ms));
++}
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++static struct swork_event notify_work;
++
++static int thermal_notify_work_init(void)
++{
++	int err;
++
++	err = swork_get();
++	if (err)
++		return err;
++
++	INIT_SWORK(&notify_work, platform_thermal_notify_work);
+ 	return 0;
+ }
+ 
++static void thermal_notify_work_cleanup(void)
++{
++	swork_put();
++}
++
++static int pkg_temp_thermal_platform_thermal_notify(__u64 msr_val)
++{
++	swork_queue(&notify_work);
++	return 0;
++}
++
++#else  /* !CONFIG_PREEMPT_RT_FULL */
++
++static int thermal_notify_work_init(void) { return 0; }
++
++static int thermal_notify_work_cleanup(void) {  }
++
++static int pkg_temp_thermal_platform_thermal_notify(__u64 msr_val)
++{
++	platform_thermal_notify_work(NULL);
++
++	return 0;
++}
++#endif /* CONFIG_PREEMPT_RT_FULL */
++
+ static int find_siblings_cpu(int cpu)
+ {
+ 	int i;
+@@ -584,6 +624,9 @@ static int __init pkg_temp_thermal_init(void)
+ 	if (!x86_match_cpu(pkg_temp_thermal_ids))
+ 		return -ENODEV;
+ 
++	if (!thermal_notify_work_init())
++		return -ENODEV;
++
+ 	spin_lock_init(&pkg_work_lock);
+ 	platform_thermal_package_notify =
+ 			pkg_temp_thermal_platform_thermal_notify;
+@@ -608,7 +651,7 @@ err_ret:
+ 	kfree(pkg_work_scheduled);
+ 	platform_thermal_package_notify = NULL;
+ 	platform_thermal_package_rate_control = NULL;
+-
++	thermal_notify_work_cleanup();
+ 	return -ENODEV;
+ }
+ 
+@@ -633,6 +676,7 @@ static void __exit pkg_temp_thermal_exit(void)
+ 	mutex_unlock(&phy_dev_list_mutex);
+ 	platform_thermal_package_notify = NULL;
+ 	platform_thermal_package_rate_control = NULL;
++	thermal_notify_work_cleanup();
+ 	for_each_online_cpu(i)
+ 		cancel_delayed_work_sync(
+ 			&per_cpu(pkg_temp_thermal_threshold_work, i));
+diff --git a/drivers/tty/serial/8250/8250_core.c b/drivers/tty/serial/8250/8250_core.c
+index 04da6f0e3326..37ea80c69bd5 100644
+--- a/drivers/tty/serial/8250/8250_core.c
++++ b/drivers/tty/serial/8250/8250_core.c
+@@ -37,6 +37,7 @@
+ #include <linux/nmi.h>
+ #include <linux/mutex.h>
+ #include <linux/slab.h>
++#include <linux/kdb.h>
+ #include <linux/uaccess.h>
+ #include <linux/pm_runtime.h>
+ #ifdef CONFIG_SPARC
+@@ -81,7 +82,16 @@ static unsigned int skip_txen_test; /* force skip of txen test at init time */
+ #define DEBUG_INTR(fmt...)	do { } while (0)
+ #endif
+ 
+-#define PASS_LIMIT	512
++/*
++ * On -rt we can have a more delays, and legitimately
++ * so - so don't drop work spuriously and spam the
++ * syslog:
++ */
++#ifdef CONFIG_PREEMPT_RT_FULL
++# define PASS_LIMIT	1000000
++#else
++# define PASS_LIMIT	512
++#endif
+ 
+ #define BOTH_EMPTY 	(UART_LSR_TEMT | UART_LSR_THRE)
+ 
+@@ -3191,7 +3201,7 @@ serial8250_console_write(struct console *co, const char *s, unsigned int count)
+ 
+ 	serial8250_rpm_get(up);
+ 
+-	if (port->sysrq || oops_in_progress)
++	if (port->sysrq || oops_in_progress || in_kdb_printk())
+ 		locked = spin_trylock_irqsave(&port->lock, flags);
+ 	else
+ 		spin_lock_irqsave(&port->lock, flags);
+diff --git a/drivers/tty/serial/amba-pl011.c b/drivers/tty/serial/amba-pl011.c
+index 02016fcd91b8..5ef2c62bb904 100644
+--- a/drivers/tty/serial/amba-pl011.c
++++ b/drivers/tty/serial/amba-pl011.c
+@@ -1935,13 +1935,19 @@ pl011_console_write(struct console *co, const char *s, unsigned int count)
+ 
+ 	clk_enable(uap->clk);
+ 
+-	local_irq_save(flags);
++	/*
++	 * local_irq_save(flags);
++	 *
++	 * This local_irq_save() is nonsense. If we come in via sysrq
++	 * handling then interrupts are already disabled. Aside of
++	 * that the port.sysrq check is racy on SMP regardless.
++	*/
+ 	if (uap->port.sysrq)
+ 		locked = 0;
+ 	else if (oops_in_progress)
+-		locked = spin_trylock(&uap->port.lock);
++		locked = spin_trylock_irqsave(&uap->port.lock, flags);
+ 	else
+-		spin_lock(&uap->port.lock);
++		spin_lock_irqsave(&uap->port.lock, flags);
+ 
+ 	/*
+ 	 *	First save the CR then disable the interrupts
+@@ -1963,8 +1969,7 @@ pl011_console_write(struct console *co, const char *s, unsigned int count)
+ 	writew(old_cr, uap->port.membase + UART011_CR);
+ 
+ 	if (locked)
+-		spin_unlock(&uap->port.lock);
+-	local_irq_restore(flags);
++		spin_unlock_irqrestore(&uap->port.lock, flags);
+ 
+ 	clk_disable(uap->clk);
+ }
+diff --git a/drivers/tty/serial/omap-serial.c b/drivers/tty/serial/omap-serial.c
+index ef46bbb1086f..10c1c260839c 100644
+--- a/drivers/tty/serial/omap-serial.c
++++ b/drivers/tty/serial/omap-serial.c
+@@ -1270,13 +1270,10 @@ serial_omap_console_write(struct console *co, const char *s,
+ 
+ 	pm_runtime_get_sync(up->dev);
+ 
+-	local_irq_save(flags);
+-	if (up->port.sysrq)
+-		locked = 0;
+-	else if (oops_in_progress)
+-		locked = spin_trylock(&up->port.lock);
++	if (up->port.sysrq || oops_in_progress)
++		locked = spin_trylock_irqsave(&up->port.lock, flags);
+ 	else
+-		spin_lock(&up->port.lock);
++		spin_lock_irqsave(&up->port.lock, flags);
+ 
+ 	/*
+ 	 * First save the IER then disable the interrupts
+@@ -1305,8 +1302,7 @@ serial_omap_console_write(struct console *co, const char *s,
+ 	pm_runtime_mark_last_busy(up->dev);
+ 	pm_runtime_put_autosuspend(up->dev);
+ 	if (locked)
+-		spin_unlock(&up->port.lock);
+-	local_irq_restore(flags);
++		spin_unlock_irqrestore(&up->port.lock, flags);
+ }
+ 
+ static int __init
+diff --git a/drivers/usb/core/hcd.c b/drivers/usb/core/hcd.c
+index 826b8fb9eb59..11d6a91e53ea 100644
+--- a/drivers/usb/core/hcd.c
++++ b/drivers/usb/core/hcd.c
+@@ -1684,9 +1684,9 @@ static void __usb_hcd_giveback_urb(struct urb *urb)
+ 	 * and no one may trigger the above deadlock situation when
+ 	 * running complete() in tasklet.
+ 	 */
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	urb->complete(urb);
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 
+ 	usb_anchor_resume_wakeups(anchor);
+ 	atomic_dec(&urb->use_count);
+diff --git a/drivers/usb/gadget/function/f_fs.c b/drivers/usb/gadget/function/f_fs.c
+index a5d34216e962..6b8e39909e8a 100644
+--- a/drivers/usb/gadget/function/f_fs.c
++++ b/drivers/usb/gadget/function/f_fs.c
+@@ -1428,7 +1428,7 @@ static void ffs_data_put(struct ffs_data *ffs)
+ 		pr_info("%s(): freeing\n", __func__);
+ 		ffs_data_clear(ffs);
+ 		BUG_ON(waitqueue_active(&ffs->ev.waitq) ||
+-		       waitqueue_active(&ffs->ep0req_completion.wait));
++		       swaitqueue_active(&ffs->ep0req_completion.wait));
+ 		kfree(ffs->dev_name);
+ 		kfree(ffs);
+ 	}
+diff --git a/drivers/usb/gadget/legacy/inode.c b/drivers/usb/gadget/legacy/inode.c
+index fe45311f243e..a4c365d4a959 100644
+--- a/drivers/usb/gadget/legacy/inode.c
++++ b/drivers/usb/gadget/legacy/inode.c
+@@ -339,7 +339,7 @@ ep_io (struct ep_data *epdata, void *buf, unsigned len)
+ 	spin_unlock_irq (&epdata->dev->lock);
+ 
+ 	if (likely (value == 0)) {
+-		value = wait_event_interruptible (done.wait, done.done);
++		value = swait_event_interruptible (done.wait, done.done);
+ 		if (value != 0) {
+ 			spin_lock_irq (&epdata->dev->lock);
+ 			if (likely (epdata->ep != NULL)) {
+@@ -348,7 +348,7 @@ ep_io (struct ep_data *epdata, void *buf, unsigned len)
+ 				usb_ep_dequeue (epdata->ep, epdata->req);
+ 				spin_unlock_irq (&epdata->dev->lock);
+ 
+-				wait_event (done.wait, done.done);
++				swait_event (done.wait, done.done);
+ 				if (epdata->status == -ECONNRESET)
+ 					epdata->status = -EINTR;
+ 			} else {
+diff --git a/fs/aio.c b/fs/aio.c
+index 58caa7e5d81c..d8af0bfbb0e3 100644
+--- a/fs/aio.c
++++ b/fs/aio.c
+@@ -40,6 +40,7 @@
+ #include <linux/ramfs.h>
+ #include <linux/percpu-refcount.h>
+ #include <linux/mount.h>
++#include <linux/work-simple.h>
+ 
+ #include <asm/kmap_types.h>
+ #include <asm/uaccess.h>
+@@ -110,7 +111,7 @@ struct kioctx {
+ 	struct page		**ring_pages;
+ 	long			nr_pages;
+ 
+-	struct work_struct	free_work;
++	struct swork_event	free_work;
+ 
+ 	/*
+ 	 * signals when all in-flight requests are done
+@@ -226,6 +227,7 @@ static int __init aio_setup(void)
+ 		.mount		= aio_mount,
+ 		.kill_sb	= kill_anon_super,
+ 	};
++	BUG_ON(swork_get());
+ 	aio_mnt = kern_mount(&aio_fs);
+ 	if (IS_ERR(aio_mnt))
+ 		panic("Failed to create aio fs mount.");
+@@ -505,9 +507,9 @@ static int kiocb_cancel(struct kiocb *kiocb)
+ 	return cancel(kiocb);
+ }
+ 
+-static void free_ioctx(struct work_struct *work)
++static void free_ioctx(struct swork_event *sev)
+ {
+-	struct kioctx *ctx = container_of(work, struct kioctx, free_work);
++	struct kioctx *ctx = container_of(sev, struct kioctx, free_work);
+ 
+ 	pr_debug("freeing %p\n", ctx);
+ 
+@@ -526,8 +528,8 @@ static void free_ioctx_reqs(struct percpu_ref *ref)
+ 	if (ctx->requests_done)
+ 		complete(ctx->requests_done);
+ 
+-	INIT_WORK(&ctx->free_work, free_ioctx);
+-	schedule_work(&ctx->free_work);
++	INIT_SWORK(&ctx->free_work, free_ioctx);
++	swork_queue(&ctx->free_work);
+ }
+ 
+ /*
+@@ -535,9 +537,9 @@ static void free_ioctx_reqs(struct percpu_ref *ref)
+  * and ctx->users has dropped to 0, so we know no more kiocbs can be submitted -
+  * now it's safe to cancel any that need to be.
+  */
+-static void free_ioctx_users(struct percpu_ref *ref)
++static void free_ioctx_users_work(struct swork_event *sev)
+ {
+-	struct kioctx *ctx = container_of(ref, struct kioctx, users);
++	struct kioctx *ctx = container_of(sev, struct kioctx, free_work);
+ 	struct kiocb *req;
+ 
+ 	spin_lock_irq(&ctx->ctx_lock);
+@@ -556,6 +558,14 @@ static void free_ioctx_users(struct percpu_ref *ref)
+ 	percpu_ref_put(&ctx->reqs);
+ }
+ 
++static void free_ioctx_users(struct percpu_ref *ref)
++{
++	struct kioctx *ctx = container_of(ref, struct kioctx, users);
++
++	INIT_SWORK(&ctx->free_work, free_ioctx_users_work);
++	swork_queue(&ctx->free_work);
++}
++
+ static int ioctx_add_table(struct kioctx *ctx, struct mm_struct *mm)
+ {
+ 	unsigned i, new_nr;
+diff --git a/fs/autofs4/autofs_i.h b/fs/autofs4/autofs_i.h
+index cecd252a3e01..cd334f184879 100644
+--- a/fs/autofs4/autofs_i.h
++++ b/fs/autofs4/autofs_i.h
+@@ -34,6 +34,7 @@
+ #include <linux/sched.h>
+ #include <linux/mount.h>
+ #include <linux/namei.h>
++#include <linux/delay.h>
+ #include <asm/current.h>
+ #include <asm/uaccess.h>
+ 
+diff --git a/fs/autofs4/expire.c b/fs/autofs4/expire.c
+index 513b8e5d151c..3bd55ae47957 100644
+--- a/fs/autofs4/expire.c
++++ b/fs/autofs4/expire.c
+@@ -151,7 +151,7 @@ again:
+ 			parent = p->d_parent;
+ 			if (!spin_trylock(&parent->d_lock)) {
+ 				spin_unlock(&p->d_lock);
+-				cpu_relax();
++				cpu_chill();
+ 				goto relock;
+ 			}
+ 			spin_unlock(&p->d_lock);
+diff --git a/fs/buffer.c b/fs/buffer.c
+index 20805db2c987..8b9768093844 100644
+--- a/fs/buffer.c
++++ b/fs/buffer.c
+@@ -301,8 +301,7 @@ static void end_buffer_async_read(struct buffer_head *bh, int uptodate)
+ 	 * decide that the page is now completely done.
+ 	 */
+ 	first = page_buffers(page);
+-	local_irq_save(flags);
+-	bit_spin_lock(BH_Uptodate_Lock, &first->b_state);
++	flags = bh_uptodate_lock_irqsave(first);
+ 	clear_buffer_async_read(bh);
+ 	unlock_buffer(bh);
+ 	tmp = bh;
+@@ -315,8 +314,7 @@ static void end_buffer_async_read(struct buffer_head *bh, int uptodate)
+ 		}
+ 		tmp = tmp->b_this_page;
+ 	} while (tmp != bh);
+-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
+-	local_irq_restore(flags);
++	bh_uptodate_unlock_irqrestore(first, flags);
+ 
+ 	/*
+ 	 * If none of the buffers had errors and they are all
+@@ -328,9 +326,7 @@ static void end_buffer_async_read(struct buffer_head *bh, int uptodate)
+ 	return;
+ 
+ still_busy:
+-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
+-	local_irq_restore(flags);
+-	return;
++	bh_uptodate_unlock_irqrestore(first, flags);
+ }
+ 
+ /*
+@@ -358,8 +354,7 @@ void end_buffer_async_write(struct buffer_head *bh, int uptodate)
+ 	}
+ 
+ 	first = page_buffers(page);
+-	local_irq_save(flags);
+-	bit_spin_lock(BH_Uptodate_Lock, &first->b_state);
++	flags = bh_uptodate_lock_irqsave(first);
+ 
+ 	clear_buffer_async_write(bh);
+ 	unlock_buffer(bh);
+@@ -371,15 +366,12 @@ void end_buffer_async_write(struct buffer_head *bh, int uptodate)
+ 		}
+ 		tmp = tmp->b_this_page;
+ 	}
+-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
+-	local_irq_restore(flags);
++	bh_uptodate_unlock_irqrestore(first, flags);
+ 	end_page_writeback(page);
+ 	return;
+ 
+ still_busy:
+-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
+-	local_irq_restore(flags);
+-	return;
++	bh_uptodate_unlock_irqrestore(first, flags);
+ }
+ EXPORT_SYMBOL(end_buffer_async_write);
+ 
+@@ -3325,6 +3317,7 @@ struct buffer_head *alloc_buffer_head(gfp_t gfp_flags)
+ 	struct buffer_head *ret = kmem_cache_zalloc(bh_cachep, gfp_flags);
+ 	if (ret) {
+ 		INIT_LIST_HEAD(&ret->b_assoc_buffers);
++		buffer_head_init_locks(ret);
+ 		preempt_disable();
+ 		__this_cpu_inc(bh_accounting.nr);
+ 		recalc_bh_state();
+diff --git a/fs/dcache.c b/fs/dcache.c
+index ae7106123047..fa390c6077f1 100644
+--- a/fs/dcache.c
++++ b/fs/dcache.c
+@@ -19,6 +19,7 @@
+ #include <linux/mm.h>
+ #include <linux/fs.h>
+ #include <linux/fsnotify.h>
++#include <linux/delay.h>
+ #include <linux/slab.h>
+ #include <linux/init.h>
+ #include <linux/hash.h>
+@@ -721,6 +722,8 @@ static inline bool fast_dput(struct dentry *dentry)
+  */
+ void dput(struct dentry *dentry)
+ {
++	struct dentry *parent;
++
+ 	if (unlikely(!dentry))
+ 		return;
+ 
+@@ -757,9 +760,18 @@ repeat:
+ 	return;
+ 
+ kill_it:
+-	dentry = dentry_kill(dentry);
+-	if (dentry) {
+-		cond_resched();
++	parent = dentry_kill(dentry);
++	if (parent) {
++		int r;
++
++		if (parent == dentry) {
++			/* the task with the highest priority won't schedule */
++			r = cond_resched();
++			if (!r)
++				cpu_chill();
++		} else {
++			dentry = parent;
++		}
+ 		goto repeat;
+ 	}
+ }
+@@ -2405,7 +2417,7 @@ again:
+ 	if (dentry->d_lockref.count == 1) {
+ 		if (!spin_trylock(&inode->i_lock)) {
+ 			spin_unlock(&dentry->d_lock);
+-			cpu_relax();
++			cpu_chill();
+ 			goto again;
+ 		}
+ 		dentry->d_flags &= ~DCACHE_CANT_MOUNT;
+diff --git a/fs/eventpoll.c b/fs/eventpoll.c
+index 7bcfff900f05..3a1e81b80080 100644
+--- a/fs/eventpoll.c
++++ b/fs/eventpoll.c
+@@ -505,12 +505,12 @@ static int ep_poll_wakeup_proc(void *priv, void *cookie, int call_nests)
+  */
+ static void ep_poll_safewake(wait_queue_head_t *wq)
+ {
+-	int this_cpu = get_cpu();
++	int this_cpu = get_cpu_light();
+ 
+ 	ep_call_nested(&poll_safewake_ncalls, EP_MAX_NESTS,
+ 		       ep_poll_wakeup_proc, NULL, wq, (void *) (long) this_cpu);
+ 
+-	put_cpu();
++	put_cpu_light();
+ }
+ 
+ static void ep_remove_wait_queue(struct eppoll_entry *pwq)
+diff --git a/fs/exec.c b/fs/exec.c
+index 515081d66c60..de87a2890bed 100644
+--- a/fs/exec.c
++++ b/fs/exec.c
+@@ -865,12 +865,14 @@ static int exec_mmap(struct mm_struct *mm)
+ 		}
+ 	}
+ 	task_lock(tsk);
++	preempt_disable_rt();
+ 	active_mm = tsk->active_mm;
+ 	tsk->mm = mm;
+ 	tsk->active_mm = mm;
+ 	activate_mm(active_mm, mm);
+ 	tsk->mm->vmacache_seqnum = 0;
+ 	vmacache_flush(tsk);
++	preempt_enable_rt();
+ 	task_unlock(tsk);
+ 	if (old_mm) {
+ 		up_read(&old_mm->mmap_sem);
+diff --git a/fs/f2fs/f2fs.h b/fs/f2fs/f2fs.h
+index 8171e80b2ee9..5bb02f8e1f06 100644
+--- a/fs/f2fs/f2fs.h
++++ b/fs/f2fs/f2fs.h
+@@ -22,7 +22,6 @@
+ 
+ #ifdef CONFIG_F2FS_CHECK_FS
+ #define f2fs_bug_on(sbi, condition)	BUG_ON(condition)
+-#define f2fs_down_write(x, y)	down_write_nest_lock(x, y)
+ #else
+ #define f2fs_bug_on(sbi, condition)					\
+ 	do {								\
+@@ -31,7 +30,6 @@
+ 			sbi->need_fsck = true;				\
+ 		}							\
+ 	} while (0)
+-#define f2fs_down_write(x, y)	down_write(x)
+ #endif
+ 
+ /*
+@@ -699,7 +697,7 @@ static inline void f2fs_unlock_op(struct f2fs_sb_info *sbi)
+ 
+ static inline void f2fs_lock_all(struct f2fs_sb_info *sbi)
+ {
+-	f2fs_down_write(&sbi->cp_rwsem, &sbi->cp_mutex);
++	down_write(&sbi->cp_rwsem);
+ }
+ 
+ static inline void f2fs_unlock_all(struct f2fs_sb_info *sbi)
+diff --git a/fs/jbd/checkpoint.c b/fs/jbd/checkpoint.c
+index 08c03044abdd..95debd71e5fa 100644
+--- a/fs/jbd/checkpoint.c
++++ b/fs/jbd/checkpoint.c
+@@ -129,6 +129,8 @@ void __log_wait_for_space(journal_t *journal)
+ 		if (journal->j_flags & JFS_ABORT)
+ 			return;
+ 		spin_unlock(&journal->j_state_lock);
++		if (current->plug)
++			io_schedule();
+ 		mutex_lock(&journal->j_checkpoint_mutex);
+ 
+ 		/*
+diff --git a/fs/jbd2/checkpoint.c b/fs/jbd2/checkpoint.c
+index 8c44654ce274..78c1545a3fab 100644
+--- a/fs/jbd2/checkpoint.c
++++ b/fs/jbd2/checkpoint.c
+@@ -116,6 +116,8 @@ void __jbd2_log_wait_for_space(journal_t *journal)
+ 	nblocks = jbd2_space_needed(journal);
+ 	while (jbd2_log_space_left(journal) < nblocks) {
+ 		write_unlock(&journal->j_state_lock);
++		if (current->plug)
++			io_schedule();
+ 		mutex_lock(&journal->j_checkpoint_mutex);
+ 
+ 		/*
+diff --git a/fs/namespace.c b/fs/namespace.c
+index d0cd3f4012ec..5a27e2edbb75 100644
+--- a/fs/namespace.c
++++ b/fs/namespace.c
+@@ -14,6 +14,7 @@
+ #include <linux/mnt_namespace.h>
+ #include <linux/user_namespace.h>
+ #include <linux/namei.h>
++#include <linux/delay.h>
+ #include <linux/security.h>
+ #include <linux/idr.h>
+ #include <linux/init.h>		/* init_rootfs */
+@@ -344,8 +345,11 @@ int __mnt_want_write(struct vfsmount *m)
+ 	 * incremented count after it has set MNT_WRITE_HOLD.
+ 	 */
+ 	smp_mb();
+-	while (ACCESS_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)
+-		cpu_relax();
++	while (ACCESS_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD) {
++		preempt_enable();
++		cpu_chill();
++		preempt_disable();
++	}
+ 	/*
+ 	 * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will
+ 	 * be set to match its requirements. So we must not load that until
+diff --git a/fs/ntfs/aops.c b/fs/ntfs/aops.c
+index 7521e11db728..f0de4b6b8bf3 100644
+--- a/fs/ntfs/aops.c
++++ b/fs/ntfs/aops.c
+@@ -107,8 +107,7 @@ static void ntfs_end_buffer_async_read(struct buffer_head *bh, int uptodate)
+ 				"0x%llx.", (unsigned long long)bh->b_blocknr);
+ 	}
+ 	first = page_buffers(page);
+-	local_irq_save(flags);
+-	bit_spin_lock(BH_Uptodate_Lock, &first->b_state);
++	flags = bh_uptodate_lock_irqsave(first);
+ 	clear_buffer_async_read(bh);
+ 	unlock_buffer(bh);
+ 	tmp = bh;
+@@ -123,8 +122,7 @@ static void ntfs_end_buffer_async_read(struct buffer_head *bh, int uptodate)
+ 		}
+ 		tmp = tmp->b_this_page;
+ 	} while (tmp != bh);
+-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
+-	local_irq_restore(flags);
++	bh_uptodate_unlock_irqrestore(first, flags);
+ 	/*
+ 	 * If none of the buffers had errors then we can set the page uptodate,
+ 	 * but we first have to perform the post read mst fixups, if the
+@@ -145,13 +143,13 @@ static void ntfs_end_buffer_async_read(struct buffer_head *bh, int uptodate)
+ 		recs = PAGE_CACHE_SIZE / rec_size;
+ 		/* Should have been verified before we got here... */
+ 		BUG_ON(!recs);
+-		local_irq_save(flags);
++		local_irq_save_nort(flags);
+ 		kaddr = kmap_atomic(page);
+ 		for (i = 0; i < recs; i++)
+ 			post_read_mst_fixup((NTFS_RECORD*)(kaddr +
+ 					i * rec_size), rec_size);
+ 		kunmap_atomic(kaddr);
+-		local_irq_restore(flags);
++		local_irq_restore_nort(flags);
+ 		flush_dcache_page(page);
+ 		if (likely(page_uptodate && !PageError(page)))
+ 			SetPageUptodate(page);
+@@ -159,9 +157,7 @@ static void ntfs_end_buffer_async_read(struct buffer_head *bh, int uptodate)
+ 	unlock_page(page);
+ 	return;
+ still_busy:
+-	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
+-	local_irq_restore(flags);
+-	return;
++	bh_uptodate_unlock_irqrestore(first, flags);
+ }
+ 
+ /**
+diff --git a/fs/timerfd.c b/fs/timerfd.c
+index 94de69ec6af6..fe62e612c11f 100644
+--- a/fs/timerfd.c
++++ b/fs/timerfd.c
+@@ -460,7 +460,10 @@ static int do_timerfd_settime(int ufd, int flags,
+ 				break;
+ 		}
+ 		spin_unlock_irq(&ctx->wqh.lock);
+-		cpu_relax();
++		if (isalarm(ctx))
++			hrtimer_wait_for_timer(&ctx->t.alarm.timer);
++		else
++			hrtimer_wait_for_timer(&ctx->t.tmr);
+ 	}
+ 
+ 	/*
+diff --git a/fs/xfs/xfs_linux.h b/fs/xfs/xfs_linux.h
+index 6a51619d8690..430e7987d6ad 100644
+--- a/fs/xfs/xfs_linux.h
++++ b/fs/xfs/xfs_linux.h
+@@ -119,7 +119,7 @@ typedef __uint64_t __psunsigned_t;
+ /*
+  * Feature macros (disable/enable)
+  */
+-#ifdef CONFIG_SMP
++#if defined(CONFIG_SMP) && !defined(CONFIG_PREEMPT_RT_FULL)
+ #define HAVE_PERCPU_SB	/* per cpu superblock counters are a 2.6 feature */
+ #else
+ #undef  HAVE_PERCPU_SB	/* per cpu superblock counters are a 2.6 feature */
+diff --git a/include/acpi/platform/aclinux.h b/include/acpi/platform/aclinux.h
+index 1ba7c190c2cc..21653ac47b4e 100644
+--- a/include/acpi/platform/aclinux.h
++++ b/include/acpi/platform/aclinux.h
+@@ -123,6 +123,7 @@
+ 
+ #define acpi_cache_t                        struct kmem_cache
+ #define acpi_spinlock                       spinlock_t *
++#define acpi_raw_spinlock		raw_spinlock_t *
+ #define acpi_cpu_flags                      unsigned long
+ 
+ /* Use native linux version of acpi_os_allocate_zeroed */
+@@ -141,6 +142,20 @@
+ #define ACPI_USE_ALTERNATE_PROTOTYPE_acpi_os_get_thread_id
+ #define ACPI_USE_ALTERNATE_PROTOTYPE_acpi_os_create_lock
+ 
++#define acpi_os_create_raw_lock(__handle)			\
++({								\
++	 raw_spinlock_t *lock = ACPI_ALLOCATE(sizeof(*lock));	\
++								\
++	 if (lock) {						\
++		*(__handle) = lock;				\
++		raw_spin_lock_init(*(__handle));		\
++	 }							\
++	 lock ? AE_OK : AE_NO_MEMORY;				\
++ })
++
++#define acpi_os_delete_raw_lock(__handle)	kfree(__handle)
++
++
+ /*
+  * OSL interfaces used by debugger/disassembler
+  */
+diff --git a/include/asm-generic/bug.h b/include/asm-generic/bug.h
+index 630dd2372238..850e4d993a88 100644
+--- a/include/asm-generic/bug.h
++++ b/include/asm-generic/bug.h
+@@ -206,6 +206,20 @@ extern void warn_slowpath_null(const char *file, const int line);
+ # define WARN_ON_SMP(x)			({0;})
+ #endif
+ 
++#ifdef CONFIG_PREEMPT_RT_BASE
++# define BUG_ON_RT(c)			BUG_ON(c)
++# define BUG_ON_NONRT(c)		do { } while (0)
++# define WARN_ON_RT(condition)		WARN_ON(condition)
++# define WARN_ON_NONRT(condition)	do { } while (0)
++# define WARN_ON_ONCE_NONRT(condition)	do { } while (0)
++#else
++# define BUG_ON_RT(c)			do { } while (0)
++# define BUG_ON_NONRT(c)		BUG_ON(c)
++# define WARN_ON_RT(condition)		do { } while (0)
++# define WARN_ON_NONRT(condition)	WARN_ON(condition)
++# define WARN_ON_ONCE_NONRT(condition)	WARN_ON_ONCE(condition)
++#endif
++
+ #endif /* __ASSEMBLY__ */
+ 
+ #endif
+diff --git a/include/asm-generic/preempt.h b/include/asm-generic/preempt.h
+index 1cd3f5d767a8..ed1881dd9b36 100644
+--- a/include/asm-generic/preempt.h
++++ b/include/asm-generic/preempt.h
+@@ -7,10 +7,10 @@
+ 
+ static __always_inline int preempt_count(void)
+ {
+-	return current_thread_info()->preempt_count;
++	return READ_ONCE(current_thread_info()->preempt_count);
+ }
+ 
+-static __always_inline int *preempt_count_ptr(void)
++static __always_inline volatile int *preempt_count_ptr(void)
+ {
+ 	return &current_thread_info()->preempt_count;
+ }
+diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
+index c9be1589415a..81a5ea3a7823 100644
+--- a/include/linux/blk-mq.h
++++ b/include/linux/blk-mq.h
+@@ -169,6 +169,7 @@ struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag);
+ 
+ struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *, const int ctx_index);
+ struct blk_mq_hw_ctx *blk_mq_alloc_single_hw_queue(struct blk_mq_tag_set *, unsigned int, int);
++void __blk_mq_complete_request_remote_work(struct work_struct *work);
+ 
+ void blk_mq_start_request(struct request *rq);
+ void blk_mq_end_request(struct request *rq, int error);
+diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
+index d94f4d0145a4..48c8a326e3f0 100644
+--- a/include/linux/blkdev.h
++++ b/include/linux/blkdev.h
+@@ -101,6 +101,7 @@ struct request {
+ 	struct list_head queuelist;
+ 	union {
+ 		struct call_single_data csd;
++		struct work_struct work;
+ 		unsigned long fifo_time;
+ 	};
+ 
+@@ -478,7 +479,7 @@ struct request_queue {
+ 	struct throtl_data *td;
+ #endif
+ 	struct rcu_head		rcu_head;
+-	wait_queue_head_t	mq_freeze_wq;
++	struct swait_head	mq_freeze_wq;
+ 	struct percpu_ref	mq_usage_counter;
+ 	struct list_head	all_q_node;
+ 
+diff --git a/include/linux/bottom_half.h b/include/linux/bottom_half.h
+index 86c12c93e3cf..8ca9389352f2 100644
+--- a/include/linux/bottom_half.h
++++ b/include/linux/bottom_half.h
+@@ -4,6 +4,17 @@
+ #include <linux/preempt.h>
+ #include <linux/preempt_mask.h>
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++
++extern void local_bh_disable(void);
++extern void _local_bh_enable(void);
++extern void local_bh_enable(void);
++extern void local_bh_enable_ip(unsigned long ip);
++extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
++extern void __local_bh_enable_ip(unsigned long ip, unsigned int cnt);
++
++#else
++
+ #ifdef CONFIG_TRACE_IRQFLAGS
+ extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
+ #else
+@@ -31,5 +42,6 @@ static inline void local_bh_enable(void)
+ {
+ 	__local_bh_enable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET);
+ }
++#endif
+ 
+ #endif /* _LINUX_BH_H */
+diff --git a/include/linux/buffer_head.h b/include/linux/buffer_head.h
+index 73b45225a7ca..ea72f52032d7 100644
+--- a/include/linux/buffer_head.h
++++ b/include/linux/buffer_head.h
+@@ -75,8 +75,52 @@ struct buffer_head {
+ 	struct address_space *b_assoc_map;	/* mapping this buffer is
+ 						   associated with */
+ 	atomic_t b_count;		/* users using this buffer_head */
++#ifdef CONFIG_PREEMPT_RT_BASE
++	spinlock_t b_uptodate_lock;
++#if defined(CONFIG_JBD) || defined(CONFIG_JBD_MODULE) || \
++	defined(CONFIG_JBD2) || defined(CONFIG_JBD2_MODULE)
++	spinlock_t b_state_lock;
++	spinlock_t b_journal_head_lock;
++#endif
++#endif
+ };
+ 
++static inline unsigned long bh_uptodate_lock_irqsave(struct buffer_head *bh)
++{
++	unsigned long flags;
++
++#ifndef CONFIG_PREEMPT_RT_BASE
++	local_irq_save(flags);
++	bit_spin_lock(BH_Uptodate_Lock, &bh->b_state);
++#else
++	spin_lock_irqsave(&bh->b_uptodate_lock, flags);
++#endif
++	return flags;
++}
++
++static inline void
++bh_uptodate_unlock_irqrestore(struct buffer_head *bh, unsigned long flags)
++{
++#ifndef CONFIG_PREEMPT_RT_BASE
++	bit_spin_unlock(BH_Uptodate_Lock, &bh->b_state);
++	local_irq_restore(flags);
++#else
++	spin_unlock_irqrestore(&bh->b_uptodate_lock, flags);
++#endif
++}
++
++static inline void buffer_head_init_locks(struct buffer_head *bh)
++{
++#ifdef CONFIG_PREEMPT_RT_BASE
++	spin_lock_init(&bh->b_uptodate_lock);
++#if defined(CONFIG_JBD) || defined(CONFIG_JBD_MODULE) || \
++	defined(CONFIG_JBD2) || defined(CONFIG_JBD2_MODULE)
++	spin_lock_init(&bh->b_state_lock);
++	spin_lock_init(&bh->b_journal_head_lock);
++#endif
++#endif
++}
++
+ /*
+  * macro tricks to expand the set_buffer_foo(), clear_buffer_foo()
+  * and buffer_foo() functions.
+diff --git a/include/linux/cgroup.h b/include/linux/cgroup.h
+index 1d5196889048..650dd9f04047 100644
+--- a/include/linux/cgroup.h
++++ b/include/linux/cgroup.h
+@@ -22,6 +22,7 @@
+ #include <linux/seq_file.h>
+ #include <linux/kernfs.h>
+ #include <linux/wait.h>
++#include <linux/work-simple.h>
+ 
+ #ifdef CONFIG_CGROUPS
+ 
+@@ -91,6 +92,7 @@ struct cgroup_subsys_state {
+ 	/* percpu_ref killing and RCU release */
+ 	struct rcu_head rcu_head;
+ 	struct work_struct destroy_work;
++	struct swork_event destroy_swork;
+ };
+ 
+ /* bits in struct cgroup_subsys_state flags field */
+diff --git a/include/linux/completion.h b/include/linux/completion.h
+index 5d5aaae3af43..3fe8d14c98c0 100644
+--- a/include/linux/completion.h
++++ b/include/linux/completion.h
+@@ -7,8 +7,7 @@
+  * Atomic wait-for-completion handler data structures.
+  * See kernel/sched/completion.c for details.
+  */
+-
+-#include <linux/wait.h>
++#include <linux/wait-simple.h>
+ 
+ /*
+  * struct completion - structure used to maintain state for a "completion"
+@@ -24,11 +23,11 @@
+  */
+ struct completion {
+ 	unsigned int done;
+-	wait_queue_head_t wait;
++	struct swait_head wait;
+ };
+ 
+ #define COMPLETION_INITIALIZER(work) \
+-	{ 0, __WAIT_QUEUE_HEAD_INITIALIZER((work).wait) }
++	{ 0, SWAIT_HEAD_INITIALIZER((work).wait) }
+ 
+ #define COMPLETION_INITIALIZER_ONSTACK(work) \
+ 	({ init_completion(&work); work; })
+@@ -73,7 +72,7 @@ struct completion {
+ static inline void init_completion(struct completion *x)
+ {
+ 	x->done = 0;
+-	init_waitqueue_head(&x->wait);
++	init_swait_head(&x->wait);
+ }
+ 
+ /**
+diff --git a/include/linux/cpu.h b/include/linux/cpu.h
+index b2d9a43012b2..da87af1b3e33 100644
+--- a/include/linux/cpu.h
++++ b/include/linux/cpu.h
+@@ -217,6 +217,8 @@ extern bool try_get_online_cpus(void);
+ extern void put_online_cpus(void);
+ extern void cpu_hotplug_disable(void);
+ extern void cpu_hotplug_enable(void);
++extern void pin_current_cpu(void);
++extern void unpin_current_cpu(void);
+ #define hotcpu_notifier(fn, pri)	cpu_notifier(fn, pri)
+ #define __hotcpu_notifier(fn, pri)	__cpu_notifier(fn, pri)
+ #define register_hotcpu_notifier(nb)	register_cpu_notifier(nb)
+@@ -235,6 +237,8 @@ static inline void cpu_hotplug_done(void) {}
+ #define put_online_cpus()	do { } while (0)
+ #define cpu_hotplug_disable()	do { } while (0)
+ #define cpu_hotplug_enable()	do { } while (0)
++static inline void pin_current_cpu(void) { }
++static inline void unpin_current_cpu(void) { }
+ #define hotcpu_notifier(fn, pri)	do { (void)(fn); } while (0)
+ #define __hotcpu_notifier(fn, pri)	do { (void)(fn); } while (0)
+ /* These aren't inline functions due to a GCC bug. */
+diff --git a/include/linux/delay.h b/include/linux/delay.h
+index a6ecb34cf547..37caab306336 100644
+--- a/include/linux/delay.h
++++ b/include/linux/delay.h
+@@ -52,4 +52,10 @@ static inline void ssleep(unsigned int seconds)
+ 	msleep(seconds * 1000);
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++extern void cpu_chill(void);
++#else
++# define cpu_chill()	cpu_relax()
++#endif
++
+ #endif /* defined(_LINUX_DELAY_H) */
+diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h
+index 662697babd48..7bb6024e935e 100644
+--- a/include/linux/ftrace.h
++++ b/include/linux/ftrace.h
+@@ -643,6 +643,18 @@ static inline void __ftrace_enabled_restore(int enabled)
+ #define CALLER_ADDR5 ((unsigned long)ftrace_return_address(5))
+ #define CALLER_ADDR6 ((unsigned long)ftrace_return_address(6))
+ 
++static inline unsigned long get_lock_parent_ip(void)
++{
++	unsigned long addr = CALLER_ADDR0;
++
++	if (!in_lock_functions(addr))
++		return addr;
++	addr = CALLER_ADDR1;
++	if (!in_lock_functions(addr))
++		return addr;
++	return CALLER_ADDR2;
++}
++
+ #ifdef CONFIG_IRQSOFF_TRACER
+   extern void time_hardirqs_on(unsigned long a0, unsigned long a1);
+   extern void time_hardirqs_off(unsigned long a0, unsigned long a1);
+diff --git a/include/linux/ftrace_event.h b/include/linux/ftrace_event.h
+index 3d0320675e12..52aad942c682 100644
+--- a/include/linux/ftrace_event.h
++++ b/include/linux/ftrace_event.h
+@@ -61,6 +61,9 @@ struct trace_entry {
+ 	unsigned char		flags;
+ 	unsigned char		preempt_count;
+ 	int			pid;
++	unsigned short		migrate_disable;
++	unsigned short		padding;
++	unsigned char		preempt_lazy_count;
+ };
+ 
+ #define FTRACE_MAX_EVENT						\
+diff --git a/include/linux/highmem.h b/include/linux/highmem.h
+index 9286a46b7d69..f443283294f8 100644
+--- a/include/linux/highmem.h
++++ b/include/linux/highmem.h
+@@ -7,6 +7,7 @@
+ #include <linux/mm.h>
+ #include <linux/uaccess.h>
+ #include <linux/hardirq.h>
++#include <linux/sched.h>
+ 
+ #include <asm/cacheflush.h>
+ 
+@@ -85,32 +86,51 @@ static inline void __kunmap_atomic(void *addr)
+ 
+ #if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ DECLARE_PER_CPU(int, __kmap_atomic_idx);
++#endif
+ 
+ static inline int kmap_atomic_idx_push(void)
+ {
++#ifndef CONFIG_PREEMPT_RT_FULL
+ 	int idx = __this_cpu_inc_return(__kmap_atomic_idx) - 1;
+ 
+-#ifdef CONFIG_DEBUG_HIGHMEM
++# ifdef CONFIG_DEBUG_HIGHMEM
+ 	WARN_ON_ONCE(in_irq() && !irqs_disabled());
+ 	BUG_ON(idx >= KM_TYPE_NR);
+-#endif
++# endif
+ 	return idx;
++#else
++	current->kmap_idx++;
++	BUG_ON(current->kmap_idx > KM_TYPE_NR);
++	return current->kmap_idx - 1;
++#endif
+ }
+ 
+ static inline int kmap_atomic_idx(void)
+ {
++#ifndef CONFIG_PREEMPT_RT_FULL
+ 	return __this_cpu_read(__kmap_atomic_idx) - 1;
++#else
++	return current->kmap_idx - 1;
++#endif
+ }
+ 
+ static inline void kmap_atomic_idx_pop(void)
+ {
+-#ifdef CONFIG_DEBUG_HIGHMEM
++#ifndef CONFIG_PREEMPT_RT_FULL
++# ifdef CONFIG_DEBUG_HIGHMEM
+ 	int idx = __this_cpu_dec_return(__kmap_atomic_idx);
+ 
+ 	BUG_ON(idx < 0);
+-#else
++# else
+ 	__this_cpu_dec(__kmap_atomic_idx);
++# endif
++#else
++	current->kmap_idx--;
++# ifdef CONFIG_DEBUG_HIGHMEM
++	BUG_ON(current->kmap_idx < 0);
++# endif
+ #endif
+ }
+ 
+diff --git a/include/linux/hrtimer.h b/include/linux/hrtimer.h
+index a036d058a249..32a0af13f438 100644
+--- a/include/linux/hrtimer.h
++++ b/include/linux/hrtimer.h
+@@ -111,6 +111,11 @@ struct hrtimer {
+ 	enum hrtimer_restart		(*function)(struct hrtimer *);
+ 	struct hrtimer_clock_base	*base;
+ 	unsigned long			state;
++	struct list_head		cb_entry;
++	int				irqsafe;
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++	ktime_t				praecox;
++#endif
+ #ifdef CONFIG_TIMER_STATS
+ 	int				start_pid;
+ 	void				*start_site;
+@@ -147,6 +152,7 @@ struct hrtimer_clock_base {
+ 	int			index;
+ 	clockid_t		clockid;
+ 	struct timerqueue_head	active;
++	struct list_head	expired;
+ 	ktime_t			resolution;
+ 	ktime_t			(*get_time)(void);
+ 	ktime_t			softirq_time;
+@@ -192,6 +198,9 @@ struct hrtimer_cpu_base {
+ 	unsigned long			nr_hangs;
+ 	ktime_t				max_hang_time;
+ #endif
++#ifdef CONFIG_PREEMPT_RT_BASE
++	wait_queue_head_t		wait;
++#endif
+ 	struct hrtimer_clock_base	clock_base[HRTIMER_MAX_CLOCK_BASES];
+ };
+ 
+@@ -379,6 +388,13 @@ static inline int hrtimer_restart(struct hrtimer *timer)
+ 	return hrtimer_start_expires(timer, HRTIMER_MODE_ABS);
+ }
+ 
++/* Softirq preemption could deadlock timer removal */
++#ifdef CONFIG_PREEMPT_RT_BASE
++  extern void hrtimer_wait_for_timer(const struct hrtimer *timer);
++#else
++# define hrtimer_wait_for_timer(timer)	do { cpu_relax(); } while (0)
++#endif
++
+ /* Query timers: */
+ extern ktime_t hrtimer_get_remaining(const struct hrtimer *timer);
+ extern int hrtimer_get_res(const clockid_t which_clock, struct timespec *tp);
+diff --git a/include/linux/idr.h b/include/linux/idr.h
+index 013fd9bc4cb6..f62be0aec911 100644
+--- a/include/linux/idr.h
++++ b/include/linux/idr.h
+@@ -95,10 +95,14 @@ bool idr_is_empty(struct idr *idp);
+  * Each idr_preload() should be matched with an invocation of this
+  * function.  See idr_preload() for details.
+  */
++#ifdef CONFIG_PREEMPT_RT_FULL
++void idr_preload_end(void);
++#else
+ static inline void idr_preload_end(void)
+ {
+ 	preempt_enable();
+ }
++#endif
+ 
+ /**
+  * idr_find - return pointer for given id
+diff --git a/include/linux/init_task.h b/include/linux/init_task.h
+index 77fc43f8fb72..dc778344d298 100644
+--- a/include/linux/init_task.h
++++ b/include/linux/init_task.h
+@@ -147,9 +147,16 @@ extern struct task_group root_task_group;
+ # define INIT_PERF_EVENTS(tsk)
+ #endif
+ 
++#ifdef CONFIG_PREEMPT_RT_BASE
++# define INIT_TIMER_LIST		.posix_timer_list = NULL,
++#else
++# define INIT_TIMER_LIST
++#endif
++
+ #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
+ # define INIT_VTIME(tsk)						\
+-	.vtime_seqlock = __SEQLOCK_UNLOCKED(tsk.vtime_seqlock),	\
++	.vtime_lock = __RAW_SPIN_LOCK_UNLOCKED(tsk.vtime_lock),	\
++	.vtime_seq = SEQCNT_ZERO(tsk.vtime_seq),			\
+ 	.vtime_snap = 0,				\
+ 	.vtime_snap_whence = VTIME_SYS,
+ #else
+@@ -219,6 +226,7 @@ extern struct task_group root_task_group;
+ 	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
+ 	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(tsk.pi_lock),	\
+ 	.timer_slack_ns = 50000, /* 50 usec default slack */		\
++	INIT_TIMER_LIST							\
+ 	.pids = {							\
+ 		[PIDTYPE_PID]  = INIT_PID_LINK(PIDTYPE_PID),		\
+ 		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),		\
+diff --git a/include/linux/interrupt.h b/include/linux/interrupt.h
+index 69517a24bc50..ce50b6ebd65d 100644
+--- a/include/linux/interrupt.h
++++ b/include/linux/interrupt.h
+@@ -57,6 +57,7 @@
+  * IRQF_NO_THREAD - Interrupt cannot be threaded
+  * IRQF_EARLY_RESUME - Resume IRQ early during syscore instead of at device
+  *                resume time.
++ * IRQF_NO_SOFTIRQ_CALL - Do not process softirqs in the irq thread context (RT)
+  */
+ #define IRQF_DISABLED		0x00000020
+ #define IRQF_SHARED		0x00000080
+@@ -70,6 +71,7 @@
+ #define IRQF_FORCE_RESUME	0x00008000
+ #define IRQF_NO_THREAD		0x00010000
+ #define IRQF_EARLY_RESUME	0x00020000
++#define IRQF_NO_SOFTIRQ_CALL	0x00080000
+ 
+ #define IRQF_TIMER		(__IRQF_TIMER | IRQF_NO_SUSPEND | IRQF_NO_THREAD)
+ 
+@@ -98,6 +100,7 @@ typedef irqreturn_t (*irq_handler_t)(int, void *);
+  * @flags:	flags (see IRQF_* above)
+  * @thread_fn:	interrupt handler function for threaded interrupts
+  * @thread:	thread pointer for threaded interrupts
++ * @secondary:	pointer to secondary irqaction (force threading)
+  * @thread_flags:	flags related to @thread
+  * @thread_mask:	bitmask for keeping track of @thread activity
+  * @dir:	pointer to the proc/irq/NN/name entry
+@@ -109,6 +112,7 @@ struct irqaction {
+ 	struct irqaction	*next;
+ 	irq_handler_t		thread_fn;
+ 	struct task_struct	*thread;
++	struct irqaction	*secondary;
+ 	unsigned int		irq;
+ 	unsigned int		flags;
+ 	unsigned long		thread_flags;
+@@ -180,7 +184,7 @@ extern void devm_free_irq(struct device *dev, unsigned int irq, void *dev_id);
+ #ifdef CONFIG_LOCKDEP
+ # define local_irq_enable_in_hardirq()	do { } while (0)
+ #else
+-# define local_irq_enable_in_hardirq()	local_irq_enable()
++# define local_irq_enable_in_hardirq()	local_irq_enable_nort()
+ #endif
+ 
+ extern void disable_irq_nosync(unsigned int irq);
+@@ -210,6 +214,7 @@ struct irq_affinity_notify {
+ 	unsigned int irq;
+ 	struct kref kref;
+ 	struct work_struct work;
++	struct list_head list;
+ 	void (*notify)(struct irq_affinity_notify *, const cpumask_t *mask);
+ 	void (*release)(struct kref *ref);
+ };
+@@ -358,9 +363,13 @@ static inline int disable_irq_wake(unsigned int irq)
+ 
+ 
+ #ifdef CONFIG_IRQ_FORCED_THREADING
++# ifndef CONFIG_PREEMPT_RT_BASE
+ extern bool force_irqthreads;
++# else
++#  define force_irqthreads	(true)
++# endif
+ #else
+-#define force_irqthreads	(0)
++#define force_irqthreads	(false)
+ #endif
+ 
+ #ifndef __ARCH_SET_SOFTIRQ_PENDING
+@@ -416,9 +425,10 @@ struct softirq_action
+ 	void	(*action)(struct softirq_action *);
+ };
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ asmlinkage void do_softirq(void);
+ asmlinkage void __do_softirq(void);
+-
++static inline void thread_do_softirq(void) { do_softirq(); }
+ #ifdef __ARCH_HAS_DO_SOFTIRQ
+ void do_softirq_own_stack(void);
+ #else
+@@ -427,13 +437,25 @@ static inline void do_softirq_own_stack(void)
+ 	__do_softirq();
+ }
+ #endif
++#else
++extern void thread_do_softirq(void);
++#endif
+ 
+ extern void open_softirq(int nr, void (*action)(struct softirq_action *));
+ extern void softirq_init(void);
+ extern void __raise_softirq_irqoff(unsigned int nr);
++#ifdef CONFIG_PREEMPT_RT_FULL
++extern void __raise_softirq_irqoff_ksoft(unsigned int nr);
++#else
++static inline void __raise_softirq_irqoff_ksoft(unsigned int nr)
++{
++	__raise_softirq_irqoff(nr);
++}
++#endif
+ 
+ extern void raise_softirq_irqoff(unsigned int nr);
+ extern void raise_softirq(unsigned int nr);
++extern void softirq_check_pending_idle(void);
+ 
+ DECLARE_PER_CPU(struct task_struct *, ksoftirqd);
+ 
+@@ -455,8 +477,9 @@ static inline struct task_struct *this_cpu_ksoftirqd(void)
+      to be executed on some cpu at least once after this.
+    * If the tasklet is already scheduled, but its execution is still not
+      started, it will be executed only once.
+-   * If this tasklet is already running on another CPU (or schedule is called
+-     from tasklet itself), it is rescheduled for later.
++   * If this tasklet is already running on another CPU, it is rescheduled
++     for later.
++   * Schedule must not be called from the tasklet itself (a lockup occurs)
+    * Tasklet is strictly serialized wrt itself, but not
+      wrt another tasklets. If client needs some intertask synchronization,
+      he makes it with spinlocks.
+@@ -481,27 +504,36 @@ struct tasklet_struct name = { NULL, 0, ATOMIC_INIT(1), func, data }
+ enum
+ {
+ 	TASKLET_STATE_SCHED,	/* Tasklet is scheduled for execution */
+-	TASKLET_STATE_RUN	/* Tasklet is running (SMP only) */
++	TASKLET_STATE_RUN,	/* Tasklet is running (SMP only) */
++	TASKLET_STATE_PENDING	/* Tasklet is pending */
+ };
+ 
+-#ifdef CONFIG_SMP
++#define TASKLET_STATEF_SCHED	(1 << TASKLET_STATE_SCHED)
++#define TASKLET_STATEF_RUN	(1 << TASKLET_STATE_RUN)
++#define TASKLET_STATEF_PENDING	(1 << TASKLET_STATE_PENDING)
++
++#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT_FULL)
+ static inline int tasklet_trylock(struct tasklet_struct *t)
+ {
+ 	return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);
+ }
+ 
++static inline int tasklet_tryunlock(struct tasklet_struct *t)
++{
++	return cmpxchg(&t->state, TASKLET_STATEF_RUN, 0) == TASKLET_STATEF_RUN;
++}
++
+ static inline void tasklet_unlock(struct tasklet_struct *t)
+ {
+ 	smp_mb__before_atomic();
+ 	clear_bit(TASKLET_STATE_RUN, &(t)->state);
+ }
+ 
+-static inline void tasklet_unlock_wait(struct tasklet_struct *t)
+-{
+-	while (test_bit(TASKLET_STATE_RUN, &(t)->state)) { barrier(); }
+-}
++extern void tasklet_unlock_wait(struct tasklet_struct *t);
++
+ #else
+ #define tasklet_trylock(t) 1
++#define tasklet_tryunlock(t)	1
+ #define tasklet_unlock_wait(t) do { } while (0)
+ #define tasklet_unlock(t) do { } while (0)
+ #endif
+@@ -550,17 +582,8 @@ static inline void tasklet_disable(struct tasklet_struct *t)
+ 	smp_mb();
+ }
+ 
+-static inline void tasklet_enable(struct tasklet_struct *t)
+-{
+-	smp_mb__before_atomic();
+-	atomic_dec(&t->count);
+-}
+-
+-static inline void tasklet_hi_enable(struct tasklet_struct *t)
+-{
+-	smp_mb__before_atomic();
+-	atomic_dec(&t->count);
+-}
++extern void tasklet_enable(struct tasklet_struct *t);
++extern void tasklet_hi_enable(struct tasklet_struct *t);
+ 
+ extern void tasklet_kill(struct tasklet_struct *t);
+ extern void tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu);
+@@ -592,6 +615,12 @@ void tasklet_hrtimer_cancel(struct tasklet_hrtimer *ttimer)
+ 	tasklet_kill(&ttimer->tasklet);
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++extern void softirq_early_init(void);
++#else
++static inline void softirq_early_init(void) { }
++#endif
++
+ /*
+  * Autoprobing for irqs:
+  *
+diff --git a/include/linux/irq.h b/include/linux/irq.h
+index c91ce60746f7..4b7e52f8e8bb 100644
+--- a/include/linux/irq.h
++++ b/include/linux/irq.h
+@@ -74,6 +74,7 @@ typedef	void (*irq_preflow_handler_t)(struct irq_data *data);
+  * IRQ_IS_POLLED		- Always polled by another interrupt. Exclude
+  *				  it from the spurious interrupt detection
+  *				  mechanism and from core side polling.
++ * IRQ_NO_SOFTIRQ_CALL		- No softirq processing in the irq thread context (RT)
+  */
+ enum {
+ 	IRQ_TYPE_NONE		= 0x00000000,
+@@ -99,13 +100,14 @@ enum {
+ 	IRQ_NOTHREAD		= (1 << 16),
+ 	IRQ_PER_CPU_DEVID	= (1 << 17),
+ 	IRQ_IS_POLLED		= (1 << 18),
++	IRQ_NO_SOFTIRQ_CALL     = (1 << 19),
+ };
+ 
+ #define IRQF_MODIFY_MASK	\
+ 	(IRQ_TYPE_SENSE_MASK | IRQ_NOPROBE | IRQ_NOREQUEST | \
+ 	 IRQ_NOAUTOEN | IRQ_MOVE_PCNTXT | IRQ_LEVEL | IRQ_NO_BALANCING | \
+ 	 IRQ_PER_CPU | IRQ_NESTED_THREAD | IRQ_NOTHREAD | IRQ_PER_CPU_DEVID | \
+-	 IRQ_IS_POLLED)
++	 IRQ_IS_POLLED | IRQ_NO_SOFTIRQ_CALL)
+ 
+ #define IRQ_NO_BALANCING_MASK	(IRQ_PER_CPU | IRQ_NO_BALANCING)
+ 
+diff --git a/include/linux/irq_work.h b/include/linux/irq_work.h
+index bf3fe719c7ce..af7ed9ad52c3 100644
+--- a/include/linux/irq_work.h
++++ b/include/linux/irq_work.h
+@@ -16,6 +16,7 @@
+ #define IRQ_WORK_BUSY		2UL
+ #define IRQ_WORK_FLAGS		3UL
+ #define IRQ_WORK_LAZY		4UL /* Doesn't want IPI, wait for tick */
++#define IRQ_WORK_HARD_IRQ	8UL /* Run hard IRQ context, even on RT */
+ 
+ struct irq_work {
+ 	unsigned long flags;
+@@ -50,4 +51,10 @@ bool irq_work_needs_cpu(void);
+ static inline bool irq_work_needs_cpu(void) { return false; }
+ #endif
+ 
++#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_PREEMPT_RT_FULL)
++void irq_work_tick_soft(void);
++#else
++static inline void irq_work_tick_soft(void) { }
++#endif
++
+ #endif /* _LINUX_IRQ_WORK_H */
+diff --git a/include/linux/irqdesc.h b/include/linux/irqdesc.h
+index faf433af425e..c51a2e5ec9ac 100644
+--- a/include/linux/irqdesc.h
++++ b/include/linux/irqdesc.h
+@@ -63,6 +63,7 @@ struct irq_desc {
+ 	unsigned int		irqs_unhandled;
+ 	atomic_t		threads_handled;
+ 	int			threads_handled_last;
++	u64			random_ip;
+ 	raw_spinlock_t		lock;
+ 	struct cpumask		*percpu_enabled;
+ #ifdef CONFIG_SMP
+diff --git a/include/linux/irqflags.h b/include/linux/irqflags.h
+index d176d658fe25..097782947852 100644
+--- a/include/linux/irqflags.h
++++ b/include/linux/irqflags.h
+@@ -25,8 +25,6 @@
+ # define trace_softirqs_enabled(p)	((p)->softirqs_enabled)
+ # define trace_hardirq_enter()	do { current->hardirq_context++; } while (0)
+ # define trace_hardirq_exit()	do { current->hardirq_context--; } while (0)
+-# define lockdep_softirq_enter()	do { current->softirq_context++; } while (0)
+-# define lockdep_softirq_exit()	do { current->softirq_context--; } while (0)
+ # define INIT_TRACE_IRQFLAGS	.softirqs_enabled = 1,
+ #else
+ # define trace_hardirqs_on()		do { } while (0)
+@@ -39,9 +37,15 @@
+ # define trace_softirqs_enabled(p)	0
+ # define trace_hardirq_enter()		do { } while (0)
+ # define trace_hardirq_exit()		do { } while (0)
++# define INIT_TRACE_IRQFLAGS
++#endif
++
++#if defined(CONFIG_TRACE_IRQFLAGS) && !defined(CONFIG_PREEMPT_RT_FULL)
++# define lockdep_softirq_enter() do { current->softirq_context++; } while (0)
++# define lockdep_softirq_exit()	 do { current->softirq_context--; } while (0)
++#else
+ # define lockdep_softirq_enter()	do { } while (0)
+ # define lockdep_softirq_exit()		do { } while (0)
+-# define INIT_TRACE_IRQFLAGS
+ #endif
+ 
+ #if defined(CONFIG_IRQSOFF_TRACER) || \
+@@ -147,4 +151,23 @@
+ 
+ #endif /* CONFIG_TRACE_IRQFLAGS_SUPPORT */
+ 
++/*
++ * local_irq* variants depending on RT/!RT
++ */
++#ifdef CONFIG_PREEMPT_RT_FULL
++# define local_irq_disable_nort()	do { } while (0)
++# define local_irq_enable_nort()	do { } while (0)
++# define local_irq_save_nort(flags)	local_save_flags(flags)
++# define local_irq_restore_nort(flags)	(void)(flags)
++# define local_irq_disable_rt()		local_irq_disable()
++# define local_irq_enable_rt()		local_irq_enable()
++#else
++# define local_irq_disable_nort()	local_irq_disable()
++# define local_irq_enable_nort()	local_irq_enable()
++# define local_irq_save_nort(flags)	local_irq_save(flags)
++# define local_irq_restore_nort(flags)	local_irq_restore(flags)
++# define local_irq_disable_rt()		do { } while (0)
++# define local_irq_enable_rt()		do { } while (0)
++#endif
++
+ #endif
+diff --git a/include/linux/jbd_common.h b/include/linux/jbd_common.h
+index 3dc53432355f..a90a6f5ca899 100644
+--- a/include/linux/jbd_common.h
++++ b/include/linux/jbd_common.h
+@@ -15,32 +15,56 @@ static inline struct journal_head *bh2jh(struct buffer_head *bh)
+ 
+ static inline void jbd_lock_bh_state(struct buffer_head *bh)
+ {
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	bit_spin_lock(BH_State, &bh->b_state);
++#else
++	spin_lock(&bh->b_state_lock);
++#endif
+ }
+ 
+ static inline int jbd_trylock_bh_state(struct buffer_head *bh)
+ {
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	return bit_spin_trylock(BH_State, &bh->b_state);
++#else
++	return spin_trylock(&bh->b_state_lock);
++#endif
+ }
+ 
+ static inline int jbd_is_locked_bh_state(struct buffer_head *bh)
+ {
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	return bit_spin_is_locked(BH_State, &bh->b_state);
++#else
++	return spin_is_locked(&bh->b_state_lock);
++#endif
+ }
+ 
+ static inline void jbd_unlock_bh_state(struct buffer_head *bh)
+ {
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	bit_spin_unlock(BH_State, &bh->b_state);
++#else
++	spin_unlock(&bh->b_state_lock);
++#endif
+ }
+ 
+ static inline void jbd_lock_bh_journal_head(struct buffer_head *bh)
+ {
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	bit_spin_lock(BH_JournalHead, &bh->b_state);
++#else
++	spin_lock(&bh->b_journal_head_lock);
++#endif
+ }
+ 
+ static inline void jbd_unlock_bh_journal_head(struct buffer_head *bh)
+ {
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	bit_spin_unlock(BH_JournalHead, &bh->b_state);
++#else
++	spin_unlock(&bh->b_journal_head_lock);
++#endif
+ }
+ 
+ #endif
+diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
+index 98f923b6a0ea..e17a47eae339 100644
+--- a/include/linux/jump_label.h
++++ b/include/linux/jump_label.h
+@@ -55,7 +55,8 @@ extern bool static_key_initialized;
+ 				    "%s used before call to jump_label_init", \
+ 				    __func__)
+ 
+-#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
++#if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL) && \
++	!defined(CONFIG_PREEMPT_BASE)
+ 
+ struct static_key {
+ 	atomic_t enabled;
+diff --git a/include/linux/kdb.h b/include/linux/kdb.h
+index 290db1269c4c..d9ae75ee77cd 100644
+--- a/include/linux/kdb.h
++++ b/include/linux/kdb.h
+@@ -116,7 +116,7 @@ extern int kdb_trap_printk;
+ extern __printf(1, 0) int vkdb_printf(const char *fmt, va_list args);
+ extern __printf(1, 2) int kdb_printf(const char *, ...);
+ typedef __printf(1, 2) int (*kdb_printf_t)(const char *, ...);
+-
++#define in_kdb_printk() (kdb_trap_printk)
+ extern void kdb_init(int level);
+ 
+ /* Access to kdb specific polling devices */
+@@ -151,6 +151,7 @@ extern int kdb_register_repeat(char *, kdb_func_t, char *, char *,
+ extern int kdb_unregister(char *);
+ #else /* ! CONFIG_KGDB_KDB */
+ static inline __printf(1, 2) int kdb_printf(const char *fmt, ...) { return 0; }
++#define in_kdb_printk() (0)
+ static inline void kdb_init(int level) {}
+ static inline int kdb_register(char *cmd, kdb_func_t func, char *usage,
+ 			       char *help, short minlen) { return 0; }
+diff --git a/include/linux/kernel.h b/include/linux/kernel.h
+index 0fe0cb8a5862..26d4654d5de0 100644
+--- a/include/linux/kernel.h
++++ b/include/linux/kernel.h
+@@ -451,6 +451,7 @@ extern enum system_states {
+ 	SYSTEM_HALT,
+ 	SYSTEM_POWER_OFF,
+ 	SYSTEM_RESTART,
++	SYSTEM_SUSPEND,
+ } system_state;
+ 
+ #define TAINT_PROPRIETARY_MODULE	0
+diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
+index aa75be9cbc3d..caccd77581ca 100644
+--- a/include/linux/kvm_host.h
++++ b/include/linux/kvm_host.h
+@@ -245,7 +245,7 @@ struct kvm_vcpu {
+ 
+ 	int fpu_active;
+ 	int guest_fpu_loaded, guest_xcr0_loaded;
+-	wait_queue_head_t wq;
++	struct swait_head wq;
+ 	struct pid *pid;
+ 	int sigset_active;
+ 	sigset_t sigset;
+@@ -688,7 +688,7 @@ static inline bool kvm_arch_has_noncoherent_dma(struct kvm *kvm)
+ }
+ #endif
+ 
+-static inline wait_queue_head_t *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
++static inline struct swait_head *kvm_arch_vcpu_wq(struct kvm_vcpu *vcpu)
+ {
+ #ifdef __KVM_HAVE_ARCH_WQP
+ 	return vcpu->arch.wqp;
+diff --git a/include/linux/lglock.h b/include/linux/lglock.h
+index 0081f000e34b..9603a1500267 100644
+--- a/include/linux/lglock.h
++++ b/include/linux/lglock.h
+@@ -34,22 +34,39 @@
+ #endif
+ 
+ struct lglock {
++#ifndef CONFIG_PREEMPT_RT_FULL
+ 	arch_spinlock_t __percpu *lock;
++#else
++	struct rt_mutex __percpu *lock;
++#endif
+ #ifdef CONFIG_DEBUG_LOCK_ALLOC
+ 	struct lock_class_key lock_key;
+ 	struct lockdep_map    lock_dep_map;
+ #endif
+ };
+ 
+-#define DEFINE_LGLOCK(name)						\
++#ifndef CONFIG_PREEMPT_RT_FULL
++# define DEFINE_LGLOCK(name)						\
+ 	static DEFINE_PER_CPU(arch_spinlock_t, name ## _lock)		\
+ 	= __ARCH_SPIN_LOCK_UNLOCKED;					\
+ 	struct lglock name = { .lock = &name ## _lock }
+ 
+-#define DEFINE_STATIC_LGLOCK(name)					\
++# define DEFINE_STATIC_LGLOCK(name)					\
+ 	static DEFINE_PER_CPU(arch_spinlock_t, name ## _lock)		\
+ 	= __ARCH_SPIN_LOCK_UNLOCKED;					\
+ 	static struct lglock name = { .lock = &name ## _lock }
++#else
++
++# define DEFINE_LGLOCK(name)						\
++	static DEFINE_PER_CPU(struct rt_mutex, name ## _lock)		\
++	= __RT_MUTEX_INITIALIZER( name ## _lock);			\
++	struct lglock name = { .lock = &name ## _lock }
++
++# define DEFINE_STATIC_LGLOCK(name)					\
++	static DEFINE_PER_CPU(struct rt_mutex, name ## _lock)		\
++	= __RT_MUTEX_INITIALIZER( name ## _lock);			\
++	static struct lglock name = { .lock = &name ## _lock }
++#endif
+ 
+ void lg_lock_init(struct lglock *lg, char *name);
+ void lg_local_lock(struct lglock *lg);
+@@ -59,6 +76,12 @@ void lg_local_unlock_cpu(struct lglock *lg, int cpu);
+ void lg_global_lock(struct lglock *lg);
+ void lg_global_unlock(struct lglock *lg);
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++#define lg_global_trylock_relax(name)	lg_global_lock(name)
++#else
++void lg_global_trylock_relax(struct lglock *lg);
++#endif
++
+ #else
+ /* When !CONFIG_SMP, map lglock to spinlock */
+ #define lglock spinlock
+diff --git a/include/linux/list_bl.h b/include/linux/list_bl.h
+index 2eb88556c5c5..017d0f1c1eb4 100644
+--- a/include/linux/list_bl.h
++++ b/include/linux/list_bl.h
+@@ -2,6 +2,7 @@
+ #define _LINUX_LIST_BL_H
+ 
+ #include <linux/list.h>
++#include <linux/spinlock.h>
+ #include <linux/bit_spinlock.h>
+ 
+ /*
+@@ -32,13 +33,24 @@
+ 
+ struct hlist_bl_head {
+ 	struct hlist_bl_node *first;
++#ifdef CONFIG_PREEMPT_RT_BASE
++	raw_spinlock_t lock;
++#endif
+ };
+ 
+ struct hlist_bl_node {
+ 	struct hlist_bl_node *next, **pprev;
+ };
+-#define INIT_HLIST_BL_HEAD(ptr) \
+-	((ptr)->first = NULL)
++
++#ifdef CONFIG_PREEMPT_RT_BASE
++#define INIT_HLIST_BL_HEAD(h)		\
++do {					\
++	(h)->first = NULL;		\
++	raw_spin_lock_init(&(h)->lock);	\
++} while (0)
++#else
++#define INIT_HLIST_BL_HEAD(h) (h)->first = NULL
++#endif
+ 
+ static inline void INIT_HLIST_BL_NODE(struct hlist_bl_node *h)
+ {
+@@ -117,12 +129,26 @@ static inline void hlist_bl_del_init(struct hlist_bl_node *n)
+ 
+ static inline void hlist_bl_lock(struct hlist_bl_head *b)
+ {
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	bit_spin_lock(0, (unsigned long *)b);
++#else
++	raw_spin_lock(&b->lock);
++#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
++	__set_bit(0, (unsigned long *)b);
++#endif
++#endif
+ }
+ 
+ static inline void hlist_bl_unlock(struct hlist_bl_head *b)
+ {
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	__bit_spin_unlock(0, (unsigned long *)b);
++#else
++#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
++	__clear_bit(0, (unsigned long *)b);
++#endif
++	raw_spin_unlock(&b->lock);
++#endif
+ }
+ 
+ static inline bool hlist_bl_is_locked(struct hlist_bl_head *b)
+diff --git a/include/linux/locallock.h b/include/linux/locallock.h
+new file mode 100644
+index 000000000000..015271ff8ec8
+--- /dev/null
++++ b/include/linux/locallock.h
+@@ -0,0 +1,276 @@
++#ifndef _LINUX_LOCALLOCK_H
++#define _LINUX_LOCALLOCK_H
++
++#include <linux/percpu.h>
++#include <linux/spinlock.h>
++
++#ifdef CONFIG_PREEMPT_RT_BASE
++
++#ifdef CONFIG_DEBUG_SPINLOCK
++# define LL_WARN(cond)	WARN_ON(cond)
++#else
++# define LL_WARN(cond)	do { } while (0)
++#endif
++
++/*
++ * per cpu lock based substitute for local_irq_*()
++ */
++struct local_irq_lock {
++	spinlock_t		lock;
++	struct task_struct	*owner;
++	int			nestcnt;
++	unsigned long		flags;
++};
++
++#define DEFINE_LOCAL_IRQ_LOCK(lvar)					\
++	DEFINE_PER_CPU(struct local_irq_lock, lvar) = {			\
++		.lock = __SPIN_LOCK_UNLOCKED((lvar).lock) }
++
++#define DECLARE_LOCAL_IRQ_LOCK(lvar)					\
++	DECLARE_PER_CPU(struct local_irq_lock, lvar)
++
++#define local_irq_lock_init(lvar)					\
++	do {								\
++		int __cpu;						\
++		for_each_possible_cpu(__cpu)				\
++			spin_lock_init(&per_cpu(lvar, __cpu).lock);	\
++	} while (0)
++
++/*
++ * spin_lock|trylock|unlock_local flavour that does not migrate disable
++ * used for __local_lock|trylock|unlock where get_local_var/put_local_var
++ * already takes care of the migrate_disable/enable
++ * for CONFIG_PREEMPT_BASE map to the normal spin_* calls.
++ */
++#ifdef CONFIG_PREEMPT_RT_FULL
++# define spin_lock_local(lock)			rt_spin_lock(lock)
++# define spin_trylock_local(lock)		rt_spin_trylock(lock)
++# define spin_unlock_local(lock)		rt_spin_unlock(lock)
++#else
++# define spin_lock_local(lock)			spin_lock(lock)
++# define spin_trylock_local(lock)		spin_trylock(lock)
++# define spin_unlock_local(lock)		spin_unlock(lock)
++#endif
++
++static inline void __local_lock(struct local_irq_lock *lv)
++{
++	if (lv->owner != current) {
++		spin_lock_local(&lv->lock);
++		LL_WARN(lv->owner);
++		LL_WARN(lv->nestcnt);
++		lv->owner = current;
++	}
++	lv->nestcnt++;
++}
++
++#define local_lock(lvar)					\
++	do { __local_lock(&get_local_var(lvar)); } while (0)
++
++#define local_lock_on(lvar, cpu)				\
++	do { __local_lock(&per_cpu(lvar, cpu)); } while (0)
++
++static inline int __local_trylock(struct local_irq_lock *lv)
++{
++	if (lv->owner != current && spin_trylock_local(&lv->lock)) {
++		LL_WARN(lv->owner);
++		LL_WARN(lv->nestcnt);
++		lv->owner = current;
++		lv->nestcnt = 1;
++		return 1;
++	}
++	return 0;
++}
++
++#define local_trylock(lvar)						\
++	({								\
++		int __locked;						\
++		__locked = __local_trylock(&get_local_var(lvar));	\
++		if (!__locked)						\
++			put_local_var(lvar);				\
++		__locked;						\
++	})
++
++static inline void __local_unlock(struct local_irq_lock *lv)
++{
++	LL_WARN(lv->nestcnt == 0);
++	LL_WARN(lv->owner != current);
++	if (--lv->nestcnt)
++		return;
++
++	lv->owner = NULL;
++	spin_unlock_local(&lv->lock);
++}
++
++#define local_unlock(lvar)					\
++	do {							\
++		__local_unlock(&__get_cpu_var(lvar));		\
++		put_local_var(lvar);				\
++	} while (0)
++
++#define local_unlock_on(lvar, cpu)                       \
++	do { __local_unlock(&per_cpu(lvar, cpu)); } while (0)
++
++static inline void __local_lock_irq(struct local_irq_lock *lv)
++{
++	spin_lock_irqsave(&lv->lock, lv->flags);
++	LL_WARN(lv->owner);
++	LL_WARN(lv->nestcnt);
++	lv->owner = current;
++	lv->nestcnt = 1;
++}
++
++#define local_lock_irq(lvar)						\
++	do { __local_lock_irq(&get_local_var(lvar)); } while (0)
++
++#define local_lock_irq_on(lvar, cpu)					\
++	do { __local_lock_irq(&per_cpu(lvar, cpu)); } while (0)
++
++static inline void __local_unlock_irq(struct local_irq_lock *lv)
++{
++	LL_WARN(!lv->nestcnt);
++	LL_WARN(lv->owner != current);
++	lv->owner = NULL;
++	lv->nestcnt = 0;
++	spin_unlock_irq(&lv->lock);
++}
++
++#define local_unlock_irq(lvar)						\
++	do {								\
++		__local_unlock_irq(&__get_cpu_var(lvar));		\
++		put_local_var(lvar);					\
++	} while (0)
++
++#define local_unlock_irq_on(lvar, cpu)					\
++	do {								\
++		__local_unlock_irq(&per_cpu(lvar, cpu));		\
++	} while (0)
++
++static inline int __local_lock_irqsave(struct local_irq_lock *lv)
++{
++	if (lv->owner != current) {
++		__local_lock_irq(lv);
++		return 0;
++	} else {
++		lv->nestcnt++;
++		return 1;
++	}
++}
++
++#define local_lock_irqsave(lvar, _flags)				\
++	do {								\
++		if (__local_lock_irqsave(&get_local_var(lvar)))		\
++			put_local_var(lvar);				\
++		_flags = __get_cpu_var(lvar).flags;			\
++	} while (0)
++
++#define local_lock_irqsave_on(lvar, _flags, cpu)			\
++	do {								\
++		__local_lock_irqsave(&per_cpu(lvar, cpu));		\
++		_flags = per_cpu(lvar, cpu).flags;			\
++	} while (0)
++
++static inline int __local_unlock_irqrestore(struct local_irq_lock *lv,
++					    unsigned long flags)
++{
++	LL_WARN(!lv->nestcnt);
++	LL_WARN(lv->owner != current);
++	if (--lv->nestcnt)
++		return 0;
++
++	lv->owner = NULL;
++	spin_unlock_irqrestore(&lv->lock, lv->flags);
++	return 1;
++}
++
++#define local_unlock_irqrestore(lvar, flags)				\
++	do {								\
++		if (__local_unlock_irqrestore(&__get_cpu_var(lvar), flags)) \
++			put_local_var(lvar);				\
++	} while (0)
++
++#define local_unlock_irqrestore_on(lvar, flags, cpu)			\
++	do {								\
++		__local_unlock_irqrestore(&per_cpu(lvar, cpu), flags);	\
++	} while (0)
++
++#define local_spin_trylock_irq(lvar, lock)				\
++	({								\
++		int __locked;						\
++		local_lock_irq(lvar);					\
++		__locked = spin_trylock(lock);				\
++		if (!__locked)						\
++			local_unlock_irq(lvar);				\
++		__locked;						\
++	})
++
++#define local_spin_lock_irq(lvar, lock)					\
++	do {								\
++		local_lock_irq(lvar);					\
++		spin_lock(lock);					\
++	} while (0)
++
++#define local_spin_unlock_irq(lvar, lock)				\
++	do {								\
++		spin_unlock(lock);					\
++		local_unlock_irq(lvar);					\
++	} while (0)
++
++#define local_spin_lock_irqsave(lvar, lock, flags)			\
++	do {								\
++		local_lock_irqsave(lvar, flags);			\
++		spin_lock(lock);					\
++	} while (0)
++
++#define local_spin_unlock_irqrestore(lvar, lock, flags)			\
++	do {								\
++		spin_unlock(lock);					\
++		local_unlock_irqrestore(lvar, flags);			\
++	} while (0)
++
++#define get_locked_var(lvar, var)					\
++	(*({								\
++		local_lock(lvar);					\
++		&__get_cpu_var(var);					\
++	}))
++
++#define put_locked_var(lvar, var)	local_unlock(lvar);
++
++#define local_lock_cpu(lvar)						\
++	({								\
++		local_lock(lvar);					\
++		smp_processor_id();					\
++	})
++
++#define local_unlock_cpu(lvar)			local_unlock(lvar)
++
++#else /* PREEMPT_RT_BASE */
++
++#define DEFINE_LOCAL_IRQ_LOCK(lvar)		__typeof__(const int) lvar
++#define DECLARE_LOCAL_IRQ_LOCK(lvar)		extern __typeof__(const int) lvar
++
++static inline void local_irq_lock_init(int lvar) { }
++
++#define local_lock(lvar)			preempt_disable()
++#define local_unlock(lvar)			preempt_enable()
++#define local_lock_irq(lvar)			local_irq_disable()
++#define local_unlock_irq(lvar)			local_irq_enable()
++#define local_lock_irqsave(lvar, flags)		local_irq_save(flags)
++#define local_unlock_irqrestore(lvar, flags)	local_irq_restore(flags)
++
++#define local_spin_trylock_irq(lvar, lock)	spin_trylock_irq(lock)
++#define local_spin_lock_irq(lvar, lock)		spin_lock_irq(lock)
++#define local_spin_unlock_irq(lvar, lock)	spin_unlock_irq(lock)
++#define local_spin_lock_irqsave(lvar, lock, flags)	\
++	spin_lock_irqsave(lock, flags)
++#define local_spin_unlock_irqrestore(lvar, lock, flags)	\
++	spin_unlock_irqrestore(lock, flags)
++
++#define get_locked_var(lvar, var)		get_cpu_var(var)
++#define put_locked_var(lvar, var)		put_cpu_var(var)
++
++#define local_lock_cpu(lvar)			get_cpu()
++#define local_unlock_cpu(lvar)			put_cpu()
++
++#endif
++
++#endif
+diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
+index 6e0b286649f1..5e4b03bcf1a0 100644
+--- a/include/linux/mm_types.h
++++ b/include/linux/mm_types.h
+@@ -11,6 +11,7 @@
+ #include <linux/completion.h>
+ #include <linux/cpumask.h>
+ #include <linux/page-debug-flags.h>
++#include <linux/rcupdate.h>
+ #include <linux/uprobes.h>
+ #include <linux/page-flags-layout.h>
+ #include <asm/page.h>
+@@ -454,6 +455,9 @@ struct mm_struct {
+ 	bool tlb_flush_pending;
+ #endif
+ 	struct uprobes_state uprobes_state;
++#ifdef CONFIG_PREEMPT_RT_BASE
++	struct rcu_head delayed_drop;
++#endif
+ };
+ 
+ static inline void mm_init_cpumask(struct mm_struct *mm)
+diff --git a/include/linux/module.h b/include/linux/module.h
+index 6fc269ce701c..208518a81281 100644
+--- a/include/linux/module.h
++++ b/include/linux/module.h
+@@ -394,6 +394,7 @@ static inline int module_is_live(struct module *mod)
+ struct module *__module_text_address(unsigned long addr);
+ struct module *__module_address(unsigned long addr);
+ bool is_module_address(unsigned long addr);
++bool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr);
+ bool is_module_percpu_address(unsigned long addr);
+ bool is_module_text_address(unsigned long addr);
+ 
+@@ -546,6 +547,11 @@ static inline bool is_module_percpu_address(unsigned long addr)
+ 	return false;
+ }
+ 
++static inline bool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr)
++{
++	return false;
++}
++
+ static inline bool is_module_text_address(unsigned long addr)
+ {
+ 	return false;
+diff --git a/include/linux/mutex.h b/include/linux/mutex.h
+index cc31498fc526..a9143091a468 100644
+--- a/include/linux/mutex.h
++++ b/include/linux/mutex.h
+@@ -19,6 +19,17 @@
+ #include <asm/processor.h>
+ #include <linux/osq_lock.h>
+ 
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++# define __DEP_MAP_MUTEX_INITIALIZER(lockname) \
++	, .dep_map = { .name = #lockname }
++#else
++# define __DEP_MAP_MUTEX_INITIALIZER(lockname)
++#endif
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++# include <linux/mutex_rt.h>
++#else
++
+ /*
+  * Simple, straightforward mutexes with strict semantics:
+  *
+@@ -100,13 +111,6 @@ do {							\
+ static inline void mutex_destroy(struct mutex *lock) {}
+ #endif
+ 
+-#ifdef CONFIG_DEBUG_LOCK_ALLOC
+-# define __DEP_MAP_MUTEX_INITIALIZER(lockname) \
+-		, .dep_map = { .name = #lockname }
+-#else
+-# define __DEP_MAP_MUTEX_INITIALIZER(lockname)
+-#endif
+-
+ #define __MUTEX_INITIALIZER(lockname) \
+ 		{ .count = ATOMIC_INIT(1) \
+ 		, .wait_lock = __SPIN_LOCK_UNLOCKED(lockname.wait_lock) \
+@@ -174,6 +178,8 @@ extern int __must_check mutex_lock_killable(struct mutex *lock);
+ extern int mutex_trylock(struct mutex *lock);
+ extern void mutex_unlock(struct mutex *lock);
+ 
++#endif /* !PREEMPT_RT_FULL */
++
+ extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);
+ 
+ #endif /* __LINUX_MUTEX_H */
+diff --git a/include/linux/mutex_rt.h b/include/linux/mutex_rt.h
+new file mode 100644
+index 000000000000..e0284edec655
+--- /dev/null
++++ b/include/linux/mutex_rt.h
+@@ -0,0 +1,89 @@
++#ifndef __LINUX_MUTEX_RT_H
++#define __LINUX_MUTEX_RT_H
++
++#ifndef __LINUX_MUTEX_H
++#error "Please include mutex.h"
++#endif
++
++#include <linux/rtmutex.h>
++
++/* FIXME: Just for __lockfunc */
++#include <linux/spinlock.h>
++
++struct mutex {
++	struct rt_mutex		lock;
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++	struct lockdep_map	dep_map;
++#endif
++};
++
++#define __MUTEX_INITIALIZER(mutexname)					\
++	{								\
++		.lock = __RT_MUTEX_INITIALIZER(mutexname.lock)		\
++		__DEP_MAP_MUTEX_INITIALIZER(mutexname)			\
++	}
++
++#define DEFINE_MUTEX(mutexname)						\
++	struct mutex mutexname = __MUTEX_INITIALIZER(mutexname)
++
++extern void __mutex_do_init(struct mutex *lock, const char *name, struct lock_class_key *key);
++extern void __lockfunc _mutex_lock(struct mutex *lock);
++extern int __lockfunc _mutex_lock_interruptible(struct mutex *lock);
++extern int __lockfunc _mutex_lock_killable(struct mutex *lock);
++extern void __lockfunc _mutex_lock_nested(struct mutex *lock, int subclass);
++extern void __lockfunc _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest_lock);
++extern int __lockfunc _mutex_lock_interruptible_nested(struct mutex *lock, int subclass);
++extern int __lockfunc _mutex_lock_killable_nested(struct mutex *lock, int subclass);
++extern int __lockfunc _mutex_trylock(struct mutex *lock);
++extern void __lockfunc _mutex_unlock(struct mutex *lock);
++
++#define mutex_is_locked(l)		rt_mutex_is_locked(&(l)->lock)
++#define mutex_lock(l)			_mutex_lock(l)
++#define mutex_lock_interruptible(l)	_mutex_lock_interruptible(l)
++#define mutex_lock_killable(l)		_mutex_lock_killable(l)
++#define mutex_trylock(l)		_mutex_trylock(l)
++#define mutex_unlock(l)			_mutex_unlock(l)
++
++#ifdef CONFIG_DEBUG_MUTEXES
++#define mutex_destroy(l)		rt_mutex_destroy(&(l)->lock)
++#else
++static inline void mutex_destroy(struct mutex *lock) {}
++#endif
++
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++# define mutex_lock_nested(l, s)	_mutex_lock_nested(l, s)
++# define mutex_lock_interruptible_nested(l, s) \
++					_mutex_lock_interruptible_nested(l, s)
++# define mutex_lock_killable_nested(l, s) \
++					_mutex_lock_killable_nested(l, s)
++
++# define mutex_lock_nest_lock(lock, nest_lock)				\
++do {									\
++	typecheck(struct lockdep_map *, &(nest_lock)->dep_map);		\
++	_mutex_lock_nest_lock(lock, &(nest_lock)->dep_map);		\
++} while (0)
++
++#else
++# define mutex_lock_nested(l, s)	_mutex_lock(l)
++# define mutex_lock_interruptible_nested(l, s) \
++					_mutex_lock_interruptible(l)
++# define mutex_lock_killable_nested(l, s) \
++					_mutex_lock_killable(l)
++# define mutex_lock_nest_lock(lock, nest_lock) mutex_lock(lock)
++#endif
++
++# define mutex_init(mutex)				\
++do {							\
++	static struct lock_class_key __key;		\
++							\
++	rt_mutex_init(&(mutex)->lock);			\
++	__mutex_do_init((mutex), #mutex, &__key);	\
++} while (0)
++
++# define __mutex_init(mutex, name, key)			\
++do {							\
++	rt_mutex_init(&(mutex)->lock);			\
++	__mutex_do_init((mutex), name, key);		\
++} while (0)
++
++#endif
+diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
+index 49ac10f99da0..685291def58f 100644
+--- a/include/linux/netdevice.h
++++ b/include/linux/netdevice.h
+@@ -2132,11 +2132,20 @@ void netdev_freemem(struct net_device *dev);
+ void synchronize_net(void);
+ int init_dummy_netdev(struct net_device *dev);
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++static inline int dev_recursion_level(void)
++{
++	return current->xmit_recursion;
++}
++
++#else
++
+ DECLARE_PER_CPU(int, xmit_recursion);
+ static inline int dev_recursion_level(void)
+ {
+ 	return this_cpu_read(xmit_recursion);
+ }
++#endif
+ 
+ struct net_device *dev_get_by_index(struct net *net, int ifindex);
+ struct net_device *__dev_get_by_index(struct net *net, int ifindex);
+@@ -2361,6 +2370,7 @@ struct softnet_data {
+ 	unsigned int		dropped;
+ 	struct sk_buff_head	input_pkt_queue;
+ 	struct napi_struct	backlog;
++	struct sk_buff_head	tofree_queue;
+ 
+ #ifdef CONFIG_NET_FLOW_LIMIT
+ 	struct sd_flow_limit __rcu *flow_limit;
+diff --git a/include/linux/netfilter/x_tables.h b/include/linux/netfilter/x_tables.h
+index cc615e273f80..1a6ba6d7ff8b 100644
+--- a/include/linux/netfilter/x_tables.h
++++ b/include/linux/netfilter/x_tables.h
+@@ -3,6 +3,7 @@
+ 
+ 
+ #include <linux/netdevice.h>
++#include <linux/locallock.h>
+ #include <uapi/linux/netfilter/x_tables.h>
+ 
+ /**
+@@ -293,6 +294,8 @@ void xt_free_table_info(struct xt_table_info *info);
+  */
+ DECLARE_PER_CPU(seqcount_t, xt_recseq);
+ 
++DECLARE_LOCAL_IRQ_LOCK(xt_write_lock);
++
+ /**
+  * xt_write_recseq_begin - start of a write section
+  *
+@@ -307,6 +310,9 @@ static inline unsigned int xt_write_recseq_begin(void)
+ {
+ 	unsigned int addend;
+ 
++	/* RT protection */
++	local_lock(xt_write_lock);
++
+ 	/*
+ 	 * Low order bit of sequence is set if we already
+ 	 * called xt_write_recseq_begin().
+@@ -337,6 +343,7 @@ static inline void xt_write_recseq_end(unsigned int addend)
+ 	/* this is kind of a write_seqcount_end(), but addend is 0 or 1 */
+ 	smp_wmb();
+ 	__this_cpu_add(xt_recseq.sequence, addend);
++	local_unlock(xt_write_lock);
+ }
+ 
+ /*
+diff --git a/include/linux/notifier.h b/include/linux/notifier.h
+index d14a4c362465..2e4414a0c1c4 100644
+--- a/include/linux/notifier.h
++++ b/include/linux/notifier.h
+@@ -6,7 +6,7 @@
+  *
+  *				Alan Cox <Alan.Cox@linux.org>
+  */
+- 
++
+ #ifndef _LINUX_NOTIFIER_H
+ #define _LINUX_NOTIFIER_H
+ #include <linux/errno.h>
+@@ -42,9 +42,7 @@
+  * in srcu_notifier_call_chain(): no cache bounces and no memory barriers.
+  * As compensation, srcu_notifier_chain_unregister() is rather expensive.
+  * SRCU notifier chains should be used when the chain will be called very
+- * often but notifier_blocks will seldom be removed.  Also, SRCU notifier
+- * chains are slightly more difficult to use because they require special
+- * runtime initialization.
++ * often but notifier_blocks will seldom be removed.
+  */
+ 
+ typedef	int (*notifier_fn_t)(struct notifier_block *nb,
+@@ -88,7 +86,7 @@ struct srcu_notifier_head {
+ 		(name)->head = NULL;		\
+ 	} while (0)
+ 
+-/* srcu_notifier_heads must be initialized and cleaned up dynamically */
++/* srcu_notifier_heads must be cleaned up dynamically */
+ extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
+ #define srcu_cleanup_notifier_head(name)	\
+ 		cleanup_srcu_struct(&(name)->srcu);
+@@ -101,7 +99,13 @@ extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
+ 		.head = NULL }
+ #define RAW_NOTIFIER_INIT(name)	{				\
+ 		.head = NULL }
+-/* srcu_notifier_heads cannot be initialized statically */
++
++#define SRCU_NOTIFIER_INIT(name, pcpu)				\
++	{							\
++		.mutex = __MUTEX_INITIALIZER(name.mutex),	\
++		.head = NULL,					\
++		.srcu = __SRCU_STRUCT_INIT(name.srcu, pcpu),	\
++	}
+ 
+ #define ATOMIC_NOTIFIER_HEAD(name)				\
+ 	struct atomic_notifier_head name =			\
+@@ -113,6 +117,18 @@ extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
+ 	struct raw_notifier_head name =				\
+ 		RAW_NOTIFIER_INIT(name)
+ 
++#define _SRCU_NOTIFIER_HEAD(name, mod)				\
++	static DEFINE_PER_CPU(struct srcu_struct_array,		\
++			name##_head_srcu_array);		\
++	mod struct srcu_notifier_head name =			\
++			SRCU_NOTIFIER_INIT(name, name##_head_srcu_array)
++
++#define SRCU_NOTIFIER_HEAD(name)				\
++	_SRCU_NOTIFIER_HEAD(name, )
++
++#define SRCU_NOTIFIER_HEAD_STATIC(name)				\
++	_SRCU_NOTIFIER_HEAD(name, static)
++
+ #ifdef __KERNEL__
+ 
+ extern int atomic_notifier_chain_register(struct atomic_notifier_head *nh,
+@@ -182,12 +198,12 @@ static inline int notifier_to_errno(int ret)
+ 
+ /*
+  *	Declared notifiers so far. I can imagine quite a few more chains
+- *	over time (eg laptop power reset chains, reboot chain (to clean 
++ *	over time (eg laptop power reset chains, reboot chain (to clean
+  *	device units up), device [un]mount chain, module load/unload chain,
+- *	low memory chain, screenblank chain (for plug in modular screenblankers) 
++ *	low memory chain, screenblank chain (for plug in modular screenblankers)
+  *	VC switch chains (for loadable kernel svgalib VC switch helpers) etc...
+  */
+- 
++
+ /* CPU notfiers are defined in include/linux/cpu.h. */
+ 
+ /* netdevice notifiers are defined in include/linux/netdevice.h */
+diff --git a/include/linux/percpu.h b/include/linux/percpu.h
+index a3aa63e47637..a3a446c2901b 100644
+--- a/include/linux/percpu.h
++++ b/include/linux/percpu.h
+@@ -23,6 +23,35 @@
+ 	 PERCPU_MODULE_RESERVE)
+ #endif
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++
++#define get_local_var(var) (*({		\
++	       migrate_disable();	\
++	       &__get_cpu_var(var);	}))
++
++#define put_local_var(var) do {	\
++	(void)&(var);		\
++	migrate_enable();	\
++} while (0)
++
++# define get_local_ptr(var) ({		\
++		migrate_disable();	\
++		this_cpu_ptr(var);	})
++
++# define put_local_ptr(var) do {	\
++	(void)(var);			\
++	migrate_enable();		\
++} while (0)
++
++#else
++
++#define get_local_var(var)	get_cpu_var(var)
++#define put_local_var(var)	put_cpu_var(var)
++#define get_local_ptr(var)	get_cpu_ptr(var)
++#define put_local_ptr(var)	put_cpu_ptr(var)
++
++#endif
++
+ /* minimum unit size, also is the maximum supported allocation size */
+ #define PCPU_MIN_UNIT_SIZE		PFN_ALIGN(32 << 10)
+ 
+@@ -115,6 +144,7 @@ extern int __init pcpu_page_first_chunk(size_t reserved_size,
+ #endif
+ 
+ extern void __percpu *__alloc_reserved_percpu(size_t size, size_t align);
++extern bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr);
+ extern bool is_kernel_percpu_address(unsigned long addr);
+ 
+ #if !defined(CONFIG_SMP) || !defined(CONFIG_HAVE_SETUP_PER_CPU_AREA)
+diff --git a/include/linux/pid.h b/include/linux/pid.h
+index 97b745ddece5..01a5460a0c85 100644
+--- a/include/linux/pid.h
++++ b/include/linux/pid.h
+@@ -2,6 +2,7 @@
+ #define _LINUX_PID_H
+ 
+ #include <linux/rcupdate.h>
++#include <linux/atomic.h>
+ 
+ enum pid_type
+ {
+diff --git a/include/linux/preempt.h b/include/linux/preempt.h
+index de83b4eb1642..66587bf4563a 100644
+--- a/include/linux/preempt.h
++++ b/include/linux/preempt.h
+@@ -33,6 +33,20 @@ extern void preempt_count_sub(int val);
+ #define preempt_count_inc() preempt_count_add(1)
+ #define preempt_count_dec() preempt_count_sub(1)
+ 
++#ifdef CONFIG_PREEMPT_LAZY
++#define add_preempt_lazy_count(val)	do { preempt_lazy_count() += (val); } while (0)
++#define sub_preempt_lazy_count(val)	do { preempt_lazy_count() -= (val); } while (0)
++#define inc_preempt_lazy_count()	add_preempt_lazy_count(1)
++#define dec_preempt_lazy_count()	sub_preempt_lazy_count(1)
++#define preempt_lazy_count()		(current_thread_info()->preempt_lazy_count)
++#else
++#define add_preempt_lazy_count(val)	do { } while (0)
++#define sub_preempt_lazy_count(val)	do { } while (0)
++#define inc_preempt_lazy_count()	do { } while (0)
++#define dec_preempt_lazy_count()	do { } while (0)
++#define preempt_lazy_count()		(0)
++#endif
++
+ #ifdef CONFIG_PREEMPT_COUNT
+ 
+ #define preempt_disable() \
+@@ -41,13 +55,25 @@ do { \
+ 	barrier(); \
+ } while (0)
+ 
++#define preempt_lazy_disable() \
++do { \
++	inc_preempt_lazy_count(); \
++	barrier(); \
++} while (0)
++
+ #define sched_preempt_enable_no_resched() \
+ do { \
+ 	barrier(); \
+ 	preempt_count_dec(); \
+ } while (0)
+ 
+-#define preempt_enable_no_resched() sched_preempt_enable_no_resched()
++#ifdef CONFIG_PREEMPT_RT_BASE
++# define preempt_enable_no_resched() sched_preempt_enable_no_resched()
++# define preempt_check_resched_rt() preempt_check_resched()
++#else
++# define preempt_enable_no_resched() preempt_enable()
++# define preempt_check_resched_rt() barrier();
++#endif
+ 
+ #ifdef CONFIG_PREEMPT
+ #define preempt_enable() \
+@@ -63,6 +89,13 @@ do { \
+ 		__preempt_schedule(); \
+ } while (0)
+ 
++#define preempt_lazy_enable() \
++do { \
++	dec_preempt_lazy_count(); \
++	barrier(); \
++	preempt_check_resched(); \
++} while (0)
++
+ #else
+ #define preempt_enable() \
+ do { \
+@@ -121,6 +154,7 @@ do { \
+ #define preempt_disable_notrace()		barrier()
+ #define preempt_enable_no_resched_notrace()	barrier()
+ #define preempt_enable_notrace()		barrier()
++#define preempt_check_resched_rt()		barrier()
+ 
+ #endif /* CONFIG_PREEMPT_COUNT */
+ 
+@@ -140,10 +174,31 @@ do { \
+ } while (0)
+ #define preempt_fold_need_resched() \
+ do { \
+-	if (tif_need_resched()) \
++	if (tif_need_resched_now()) \
+ 		set_preempt_need_resched(); \
+ } while (0)
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++# define preempt_disable_rt()		preempt_disable()
++# define preempt_enable_rt()		preempt_enable()
++# define preempt_disable_nort()		barrier()
++# define preempt_enable_nort()		barrier()
++# ifdef CONFIG_SMP
++   extern void migrate_disable(void);
++   extern void migrate_enable(void);
++# else /* CONFIG_SMP */
++#  define migrate_disable()		barrier()
++#  define migrate_enable()		barrier()
++# endif /* CONFIG_SMP */
++#else
++# define preempt_disable_rt()		barrier()
++# define preempt_enable_rt()		barrier()
++# define preempt_disable_nort()		preempt_disable()
++# define preempt_enable_nort()		preempt_enable()
++# define migrate_disable()		preempt_disable()
++# define migrate_enable()		preempt_enable()
++#endif
++
+ #ifdef CONFIG_PREEMPT_NOTIFIERS
+ 
+ struct preempt_notifier;
+diff --git a/include/linux/preempt_mask.h b/include/linux/preempt_mask.h
+index dbeec4d4a3be..c7e373dc7314 100644
+--- a/include/linux/preempt_mask.h
++++ b/include/linux/preempt_mask.h
+@@ -44,16 +44,26 @@
+ #define HARDIRQ_OFFSET	(1UL << HARDIRQ_SHIFT)
+ #define NMI_OFFSET	(1UL << NMI_SHIFT)
+ 
+-#define SOFTIRQ_DISABLE_OFFSET	(2 * SOFTIRQ_OFFSET)
++#ifndef CONFIG_PREEMPT_RT_FULL
++# define SOFTIRQ_DISABLE_OFFSET	(2 * SOFTIRQ_OFFSET)
++#else
++# define SOFTIRQ_DISABLE_OFFSET	(0)
++#endif
+ 
+ #define PREEMPT_ACTIVE_BITS	1
+ #define PREEMPT_ACTIVE_SHIFT	(NMI_SHIFT + NMI_BITS)
+ #define PREEMPT_ACTIVE	(__IRQ_MASK(PREEMPT_ACTIVE_BITS) << PREEMPT_ACTIVE_SHIFT)
+ 
+ #define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
+-#define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
+ #define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK \
+ 				 | NMI_MASK))
++#ifndef CONFIG_PREEMPT_RT_FULL
++# define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
++# define in_serving_softirq()	(softirq_count() & SOFTIRQ_OFFSET)
++#else
++# define softirq_count()	(0UL)
++extern int in_serving_softirq(void);
++#endif
+ 
+ /*
+  * Are we doing bottom half or hardware interrupt processing?
+@@ -64,7 +74,6 @@
+ #define in_irq()		(hardirq_count())
+ #define in_softirq()		(softirq_count())
+ #define in_interrupt()		(irq_count())
+-#define in_serving_softirq()	(softirq_count() & SOFTIRQ_OFFSET)
+ 
+ /*
+  * Are we in NMI context?
+diff --git a/include/linux/printk.h b/include/linux/printk.h
+index d78125f73ac4..9fd2d6fa4489 100644
+--- a/include/linux/printk.h
++++ b/include/linux/printk.h
+@@ -119,9 +119,11 @@ int no_printk(const char *fmt, ...)
+ extern asmlinkage __printf(1, 2)
+ void early_printk(const char *fmt, ...);
+ void early_vprintk(const char *fmt, va_list ap);
++extern void printk_kill(void);
+ #else
+ static inline __printf(1, 2) __cold
+ void early_printk(const char *s, ...) { }
++static inline void printk_kill(void) { }
+ #endif
+ 
+ #ifdef CONFIG_PRINTK
+@@ -155,7 +157,6 @@ extern int __printk_ratelimit(const char *func);
+ #define printk_ratelimit() __printk_ratelimit(__func__)
+ extern bool printk_timed_ratelimit(unsigned long *caller_jiffies,
+ 				   unsigned int interval_msec);
+-
+ extern int printk_delay_msec;
+ extern int dmesg_restrict;
+ extern int kptr_restrict;
+diff --git a/include/linux/radix-tree.h b/include/linux/radix-tree.h
+index 673dee29a9b9..e46b414e9e39 100644
+--- a/include/linux/radix-tree.h
++++ b/include/linux/radix-tree.h
+@@ -279,6 +279,8 @@ unsigned int radix_tree_gang_lookup_slot(struct radix_tree_root *root,
+ 			unsigned long first_index, unsigned int max_items);
+ int radix_tree_preload(gfp_t gfp_mask);
+ int radix_tree_maybe_preload(gfp_t gfp_mask);
++void radix_tree_preload_end(void);
++
+ void radix_tree_init(void);
+ void *radix_tree_tag_set(struct radix_tree_root *root,
+ 			unsigned long index, unsigned int tag);
+@@ -301,11 +303,6 @@ unsigned long radix_tree_range_tag_if_tagged(struct radix_tree_root *root,
+ int radix_tree_tagged(struct radix_tree_root *root, unsigned int tag);
+ unsigned long radix_tree_locate_item(struct radix_tree_root *root, void *item);
+ 
+-static inline void radix_tree_preload_end(void)
+-{
+-	preempt_enable();
+-}
+-
+ /**
+  * struct radix_tree_iter - radix tree iterator state
+  *
+diff --git a/include/linux/random.h b/include/linux/random.h
+index 0fe49a14daa5..c71b98cd667f 100644
+--- a/include/linux/random.h
++++ b/include/linux/random.h
+@@ -11,7 +11,7 @@
+ extern void add_device_randomness(const void *, unsigned int);
+ extern void add_input_randomness(unsigned int type, unsigned int code,
+ 				 unsigned int value);
+-extern void add_interrupt_randomness(int irq, int irq_flags);
++extern void add_interrupt_randomness(int irq, int irq_flags, __u64 ip);
+ 
+ extern void get_random_bytes(void *buf, int nbytes);
+ extern void get_random_bytes_arch(void *buf, int nbytes);
+diff --git a/include/linux/rcupdate.h b/include/linux/rcupdate.h
+index a4a819ffb2d1..74dbf3df737f 100644
+--- a/include/linux/rcupdate.h
++++ b/include/linux/rcupdate.h
+@@ -147,6 +147,9 @@ void call_rcu(struct rcu_head *head,
+ 
+ #endif /* #else #ifdef CONFIG_PREEMPT_RCU */
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++#define call_rcu_bh	call_rcu
++#else
+ /**
+  * call_rcu_bh() - Queue an RCU for invocation after a quicker grace period.
+  * @head: structure to be used for queueing the RCU updates.
+@@ -170,6 +173,7 @@ void call_rcu(struct rcu_head *head,
+  */
+ void call_rcu_bh(struct rcu_head *head,
+ 		 void (*func)(struct rcu_head *head));
++#endif
+ 
+ /**
+  * call_rcu_sched() - Queue an RCU for invocation after sched grace period.
+@@ -231,6 +235,11 @@ void synchronize_rcu(void);
+  * types of kernel builds, the rcu_read_lock() nesting depth is unknowable.
+  */
+ #define rcu_preempt_depth() (current->rcu_read_lock_nesting)
++#ifndef CONFIG_PREEMPT_RT_FULL
++#define sched_rcu_preempt_depth()	rcu_preempt_depth()
++#else
++static inline int sched_rcu_preempt_depth(void) { return 0; }
++#endif
+ 
+ #else /* #ifdef CONFIG_PREEMPT_RCU */
+ 
+@@ -254,6 +263,8 @@ static inline int rcu_preempt_depth(void)
+ 	return 0;
+ }
+ 
++#define sched_rcu_preempt_depth()	rcu_preempt_depth()
++
+ #endif /* #else #ifdef CONFIG_PREEMPT_RCU */
+ 
+ /* Internal to kernel */
+@@ -430,7 +441,14 @@ extern struct lockdep_map rcu_callback_map;
+ int debug_lockdep_rcu_enabled(void);
+ 
+ int rcu_read_lock_held(void);
++#ifdef CONFIG_PREEMPT_RT_FULL
++static inline int rcu_read_lock_bh_held(void)
++{
++	return rcu_read_lock_held();
++}
++#else
+ int rcu_read_lock_bh_held(void);
++#endif
+ 
+ /**
+  * rcu_read_lock_sched_held() - might we be in RCU-sched read-side critical section?
+@@ -940,10 +958,14 @@ static inline void rcu_read_unlock(void)
+ static inline void rcu_read_lock_bh(void)
+ {
+ 	local_bh_disable();
++#ifdef CONFIG_PREEMPT_RT_FULL
++	rcu_read_lock();
++#else
+ 	__acquire(RCU_BH);
+ 	rcu_lock_acquire(&rcu_bh_lock_map);
+ 	rcu_lockdep_assert(rcu_is_watching(),
+ 			   "rcu_read_lock_bh() used illegally while idle");
++#endif
+ }
+ 
+ /*
+@@ -953,10 +975,14 @@ static inline void rcu_read_lock_bh(void)
+  */
+ static inline void rcu_read_unlock_bh(void)
+ {
++#ifdef CONFIG_PREEMPT_RT_FULL
++	rcu_read_unlock();
++#else
+ 	rcu_lockdep_assert(rcu_is_watching(),
+ 			   "rcu_read_unlock_bh() used illegally while idle");
+ 	rcu_lock_release(&rcu_bh_lock_map);
+ 	__release(RCU_BH);
++#endif
+ 	local_bh_enable();
+ }
+ 
+diff --git a/include/linux/rcutree.h b/include/linux/rcutree.h
+index 3e2f5d432743..b9899eff12f9 100644
+--- a/include/linux/rcutree.h
++++ b/include/linux/rcutree.h
+@@ -46,7 +46,11 @@ static inline void rcu_virt_note_context_switch(int cpu)
+ 	rcu_note_context_switch(cpu);
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++# define synchronize_rcu_bh	synchronize_rcu
++#else
+ void synchronize_rcu_bh(void);
++#endif
+ void synchronize_sched_expedited(void);
+ void synchronize_rcu_expedited(void);
+ 
+@@ -74,7 +78,11 @@ static inline void synchronize_rcu_bh_expedited(void)
+ }
+ 
+ void rcu_barrier(void);
++#ifdef CONFIG_PREEMPT_RT_FULL
++# define rcu_barrier_bh                rcu_barrier
++#else
+ void rcu_barrier_bh(void);
++#endif
+ void rcu_barrier_sched(void);
+ unsigned long get_state_synchronize_rcu(void);
+ void cond_synchronize_rcu(unsigned long oldstate);
+@@ -82,12 +90,10 @@ void cond_synchronize_rcu(unsigned long oldstate);
+ extern unsigned long rcutorture_testseq;
+ extern unsigned long rcutorture_vernum;
+ long rcu_batches_completed(void);
+-long rcu_batches_completed_bh(void);
+ long rcu_batches_completed_sched(void);
+ void show_rcu_gp_kthreads(void);
+ 
+ void rcu_force_quiescent_state(void);
+-void rcu_bh_force_quiescent_state(void);
+ void rcu_sched_force_quiescent_state(void);
+ 
+ void exit_rcu(void);
+@@ -97,4 +103,12 @@ extern int rcu_scheduler_active __read_mostly;
+ 
+ bool rcu_is_watching(void);
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++void rcu_bh_force_quiescent_state(void);
++long rcu_batches_completed_bh(void);
++#else
++# define rcu_bh_force_quiescent_state	rcu_force_quiescent_state
++# define rcu_batches_completed_bh	rcu_batches_completed
++#endif
++
+ #endif /* __LINUX_RCUTREE_H */
+diff --git a/include/linux/rtmutex.h b/include/linux/rtmutex.h
+index 1abba5ce2a2f..d5a04ea47a13 100644
+--- a/include/linux/rtmutex.h
++++ b/include/linux/rtmutex.h
+@@ -14,10 +14,14 @@
+ 
+ #include <linux/linkage.h>
+ #include <linux/rbtree.h>
+-#include <linux/spinlock_types.h>
++#include <linux/spinlock_types_raw.h>
+ 
+ extern int max_lock_depth; /* for sysctl */
+ 
++#ifdef CONFIG_DEBUG_MUTEXES
++#include <linux/debug_locks.h>
++#endif
++
+ /**
+  * The rt_mutex structure
+  *
+@@ -31,8 +35,8 @@ struct rt_mutex {
+ 	struct rb_root          waiters;
+ 	struct rb_node          *waiters_leftmost;
+ 	struct task_struct	*owner;
+-#ifdef CONFIG_DEBUG_RT_MUTEXES
+ 	int			save_state;
++#ifdef CONFIG_DEBUG_RT_MUTEXES
+ 	const char 		*name, *file;
+ 	int			line;
+ 	void			*magic;
+@@ -55,22 +59,33 @@ struct hrtimer_sleeper;
+ # define rt_mutex_debug_check_no_locks_held(task)	do { } while (0)
+ #endif
+ 
++# define rt_mutex_init(mutex)					\
++	do {							\
++		raw_spin_lock_init(&(mutex)->wait_lock);	\
++		__rt_mutex_init(mutex, #mutex);			\
++	} while (0)
++
+ #ifdef CONFIG_DEBUG_RT_MUTEXES
+ # define __DEBUG_RT_MUTEX_INITIALIZER(mutexname) \
+ 	, .name = #mutexname, .file = __FILE__, .line = __LINE__
+-# define rt_mutex_init(mutex)			__rt_mutex_init(mutex, __func__)
+  extern void rt_mutex_debug_task_free(struct task_struct *tsk);
+ #else
+ # define __DEBUG_RT_MUTEX_INITIALIZER(mutexname)
+-# define rt_mutex_init(mutex)			__rt_mutex_init(mutex, NULL)
+ # define rt_mutex_debug_task_free(t)			do { } while (0)
+ #endif
+ 
+-#define __RT_MUTEX_INITIALIZER(mutexname) \
+-	{ .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
++#define __RT_MUTEX_INITIALIZER_PLAIN(mutexname) \
++	 .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(mutexname.wait_lock) \
+ 	, .waiters = RB_ROOT \
+ 	, .owner = NULL \
+-	__DEBUG_RT_MUTEX_INITIALIZER(mutexname)}
++	__DEBUG_RT_MUTEX_INITIALIZER(mutexname)
++
++#define __RT_MUTEX_INITIALIZER(mutexname) \
++	{ __RT_MUTEX_INITIALIZER_PLAIN(mutexname) }
++
++#define __RT_MUTEX_INITIALIZER_SAVE_STATE(mutexname) \
++	{ __RT_MUTEX_INITIALIZER_PLAIN(mutexname)    \
++	, .save_state = 1 }
+ 
+ #define DEFINE_RT_MUTEX(mutexname) \
+ 	struct rt_mutex mutexname = __RT_MUTEX_INITIALIZER(mutexname)
+@@ -91,6 +106,7 @@ extern void rt_mutex_destroy(struct rt_mutex *lock);
+ 
+ extern void rt_mutex_lock(struct rt_mutex *lock);
+ extern int rt_mutex_lock_interruptible(struct rt_mutex *lock);
++extern int rt_mutex_lock_killable(struct rt_mutex *lock);
+ extern int rt_mutex_timed_lock(struct rt_mutex *lock,
+ 			       struct hrtimer_sleeper *timeout);
+ 
+diff --git a/include/linux/rwlock_rt.h b/include/linux/rwlock_rt.h
+new file mode 100644
+index 000000000000..49ed2d45d3be
+--- /dev/null
++++ b/include/linux/rwlock_rt.h
+@@ -0,0 +1,99 @@
++#ifndef __LINUX_RWLOCK_RT_H
++#define __LINUX_RWLOCK_RT_H
++
++#ifndef __LINUX_SPINLOCK_H
++#error Do not include directly. Use spinlock.h
++#endif
++
++#define rwlock_init(rwl)				\
++do {							\
++	static struct lock_class_key __key;		\
++							\
++	rt_mutex_init(&(rwl)->lock);			\
++	__rt_rwlock_init(rwl, #rwl, &__key);		\
++} while (0)
++
++extern void __lockfunc rt_write_lock(rwlock_t *rwlock);
++extern void __lockfunc rt_read_lock(rwlock_t *rwlock);
++extern int __lockfunc rt_write_trylock(rwlock_t *rwlock);
++extern int __lockfunc rt_write_trylock_irqsave(rwlock_t *trylock, unsigned long *flags);
++extern int __lockfunc rt_read_trylock(rwlock_t *rwlock);
++extern void __lockfunc rt_write_unlock(rwlock_t *rwlock);
++extern void __lockfunc rt_read_unlock(rwlock_t *rwlock);
++extern unsigned long __lockfunc rt_write_lock_irqsave(rwlock_t *rwlock);
++extern unsigned long __lockfunc rt_read_lock_irqsave(rwlock_t *rwlock);
++extern void __rt_rwlock_init(rwlock_t *rwlock, char *name, struct lock_class_key *key);
++
++#define read_trylock(lock)	__cond_lock(lock, rt_read_trylock(lock))
++#define write_trylock(lock)	__cond_lock(lock, rt_write_trylock(lock))
++
++#define write_trylock_irqsave(lock, flags)	\
++	__cond_lock(lock, rt_write_trylock_irqsave(lock, &flags))
++
++#define read_lock_irqsave(lock, flags)			\
++	do {						\
++		typecheck(unsigned long, flags);	\
++		flags = rt_read_lock_irqsave(lock);	\
++	} while (0)
++
++#define write_lock_irqsave(lock, flags)			\
++	do {						\
++		typecheck(unsigned long, flags);	\
++		flags = rt_write_lock_irqsave(lock);	\
++	} while (0)
++
++#define read_lock(lock)		rt_read_lock(lock)
++
++#define read_lock_bh(lock)				\
++	do {						\
++		local_bh_disable();			\
++		rt_read_lock(lock);			\
++	} while (0)
++
++#define read_lock_irq(lock)	read_lock(lock)
++
++#define write_lock(lock)	rt_write_lock(lock)
++
++#define write_lock_bh(lock)				\
++	do {						\
++		local_bh_disable();			\
++		rt_write_lock(lock);			\
++	} while (0)
++
++#define write_lock_irq(lock)	write_lock(lock)
++
++#define read_unlock(lock)	rt_read_unlock(lock)
++
++#define read_unlock_bh(lock)				\
++	do {						\
++		rt_read_unlock(lock);			\
++		local_bh_enable();			\
++	} while (0)
++
++#define read_unlock_irq(lock)	read_unlock(lock)
++
++#define write_unlock(lock)	rt_write_unlock(lock)
++
++#define write_unlock_bh(lock)				\
++	do {						\
++		rt_write_unlock(lock);			\
++		local_bh_enable();			\
++	} while (0)
++
++#define write_unlock_irq(lock)	write_unlock(lock)
++
++#define read_unlock_irqrestore(lock, flags)		\
++	do {						\
++		typecheck(unsigned long, flags);	\
++		(void) flags;				\
++		rt_read_unlock(lock);			\
++	} while (0)
++
++#define write_unlock_irqrestore(lock, flags) \
++	do {						\
++		typecheck(unsigned long, flags);	\
++		(void) flags;				\
++		rt_write_unlock(lock);			\
++	} while (0)
++
++#endif
+diff --git a/include/linux/rwlock_types.h b/include/linux/rwlock_types.h
+index cc0072e93e36..d0da966ad7a0 100644
+--- a/include/linux/rwlock_types.h
++++ b/include/linux/rwlock_types.h
+@@ -1,6 +1,10 @@
+ #ifndef __LINUX_RWLOCK_TYPES_H
+ #define __LINUX_RWLOCK_TYPES_H
+ 
++#if !defined(__LINUX_SPINLOCK_TYPES_H)
++# error "Do not include directly, include spinlock_types.h"
++#endif
++
+ /*
+  * include/linux/rwlock_types.h - generic rwlock type definitions
+  *				  and initializers
+@@ -43,6 +47,7 @@ typedef struct {
+ 				RW_DEP_MAP_INIT(lockname) }
+ #endif
+ 
+-#define DEFINE_RWLOCK(x)	rwlock_t x = __RW_LOCK_UNLOCKED(x)
++#define DEFINE_RWLOCK(name) \
++	rwlock_t name __cacheline_aligned_in_smp = __RW_LOCK_UNLOCKED(name)
+ 
+ #endif /* __LINUX_RWLOCK_TYPES_H */
+diff --git a/include/linux/rwlock_types_rt.h b/include/linux/rwlock_types_rt.h
+new file mode 100644
+index 000000000000..b13832119591
+--- /dev/null
++++ b/include/linux/rwlock_types_rt.h
+@@ -0,0 +1,33 @@
++#ifndef __LINUX_RWLOCK_TYPES_RT_H
++#define __LINUX_RWLOCK_TYPES_RT_H
++
++#ifndef __LINUX_SPINLOCK_TYPES_H
++#error "Do not include directly. Include spinlock_types.h instead"
++#endif
++
++/*
++ * rwlocks - rtmutex which allows single reader recursion
++ */
++typedef struct {
++	struct rt_mutex		lock;
++	int			read_depth;
++	unsigned int		break_lock;
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++	struct lockdep_map	dep_map;
++#endif
++} rwlock_t;
++
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++# define RW_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
++#else
++# define RW_DEP_MAP_INIT(lockname)
++#endif
++
++#define __RW_LOCK_UNLOCKED(name) \
++	{ .lock = __RT_MUTEX_INITIALIZER_SAVE_STATE(name.lock),	\
++	  RW_DEP_MAP_INIT(name) }
++
++#define DEFINE_RWLOCK(name) \
++	rwlock_t name __cacheline_aligned_in_smp = __RW_LOCK_UNLOCKED(name)
++
++#endif
+diff --git a/include/linux/rwsem.h b/include/linux/rwsem.h
+index 8f498cdde280..2b2148431f14 100644
+--- a/include/linux/rwsem.h
++++ b/include/linux/rwsem.h
+@@ -18,6 +18,10 @@
+ #include <linux/osq_lock.h>
+ #endif
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++#include <linux/rwsem_rt.h>
++#else /* PREEMPT_RT_FULL */
++
+ struct rw_semaphore;
+ 
+ #ifdef CONFIG_RWSEM_GENERIC_SPINLOCK
+@@ -177,4 +181,6 @@ extern void up_read_non_owner(struct rw_semaphore *sem);
+ # define up_read_non_owner(sem)			up_read(sem)
+ #endif
+ 
++#endif /* !PREEMPT_RT_FULL */
++
+ #endif /* _LINUX_RWSEM_H */
+diff --git a/include/linux/rwsem_rt.h b/include/linux/rwsem_rt.h
+new file mode 100644
+index 000000000000..924c2d274ab5
+--- /dev/null
++++ b/include/linux/rwsem_rt.h
+@@ -0,0 +1,134 @@
++#ifndef _LINUX_RWSEM_RT_H
++#define _LINUX_RWSEM_RT_H
++
++#ifndef _LINUX_RWSEM_H
++#error "Include rwsem.h"
++#endif
++
++/*
++ * RW-semaphores are a spinlock plus a reader-depth count.
++ *
++ * Note that the semantics are different from the usual
++ * Linux rw-sems, in PREEMPT_RT mode we do not allow
++ * multiple readers to hold the lock at once, we only allow
++ * a read-lock owner to read-lock recursively. This is
++ * better for latency, makes the implementation inherently
++ * fair and makes it simpler as well.
++ */
++
++#include <linux/rtmutex.h>
++
++struct rw_semaphore {
++	struct rt_mutex		lock;
++	int			read_depth;
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++	struct lockdep_map	dep_map;
++#endif
++};
++
++#define __RWSEM_INITIALIZER(name) \
++	{ .lock = __RT_MUTEX_INITIALIZER(name.lock), \
++	  RW_DEP_MAP_INIT(name) }
++
++#define DECLARE_RWSEM(lockname) \
++	struct rw_semaphore lockname = __RWSEM_INITIALIZER(lockname)
++
++extern void  __rt_rwsem_init(struct rw_semaphore *rwsem, const char *name,
++				     struct lock_class_key *key);
++
++#define __rt_init_rwsem(sem, name, key)			\
++	do {						\
++		rt_mutex_init(&(sem)->lock);		\
++		__rt_rwsem_init((sem), (name), (key));\
++	} while (0)
++
++#define __init_rwsem(sem, name, key) __rt_init_rwsem(sem, name, key)
++
++# define rt_init_rwsem(sem)				\
++do {							\
++	static struct lock_class_key __key;		\
++							\
++	__rt_init_rwsem((sem), #sem, &__key);		\
++} while (0)
++
++extern void  rt_down_write(struct rw_semaphore *rwsem);
++extern void rt_down_read_nested(struct rw_semaphore *rwsem, int subclass);
++extern void rt_down_write_nested(struct rw_semaphore *rwsem, int subclass);
++extern void rt_down_write_nested_lock(struct rw_semaphore *rwsem,
++		struct lockdep_map *nest);
++extern void  rt_down_read(struct rw_semaphore *rwsem);
++extern int  rt_down_write_trylock(struct rw_semaphore *rwsem);
++extern int  rt_down_read_trylock(struct rw_semaphore *rwsem);
++extern void  rt_up_read(struct rw_semaphore *rwsem);
++extern void  rt_up_write(struct rw_semaphore *rwsem);
++extern void  rt_downgrade_write(struct rw_semaphore *rwsem);
++
++#define init_rwsem(sem)		rt_init_rwsem(sem)
++#define rwsem_is_locked(s)	rt_mutex_is_locked(&(s)->lock)
++
++static inline int rwsem_is_contended(struct rw_semaphore *sem)
++{
++	/* rt_mutex_has_waiters() */
++	return !RB_EMPTY_ROOT(&sem->lock.waiters);
++}
++
++static inline void down_read(struct rw_semaphore *sem)
++{
++	rt_down_read(sem);
++}
++
++static inline int down_read_trylock(struct rw_semaphore *sem)
++{
++	return rt_down_read_trylock(sem);
++}
++
++static inline void down_write(struct rw_semaphore *sem)
++{
++	rt_down_write(sem);
++}
++
++static inline int down_write_trylock(struct rw_semaphore *sem)
++{
++	return rt_down_write_trylock(sem);
++}
++
++static inline void up_read(struct rw_semaphore *sem)
++{
++	rt_up_read(sem);
++}
++
++static inline void up_write(struct rw_semaphore *sem)
++{
++	rt_up_write(sem);
++}
++
++static inline void downgrade_write(struct rw_semaphore *sem)
++{
++	rt_downgrade_write(sem);
++}
++
++static inline void down_read_nested(struct rw_semaphore *sem, int subclass)
++{
++	return rt_down_read_nested(sem, subclass);
++}
++
++static inline void down_write_nested(struct rw_semaphore *sem, int subclass)
++{
++	rt_down_write_nested(sem, subclass);
++}
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++static inline void down_write_nest_lock(struct rw_semaphore *sem,
++		struct rw_semaphore *nest_lock)
++{
++	rt_down_write_nested_lock(sem, &nest_lock->dep_map);
++}
++
++#else
++
++static inline void down_write_nest_lock(struct rw_semaphore *sem,
++		struct rw_semaphore *nest_lock)
++{
++	rt_down_write_nested_lock(sem, NULL);
++}
++#endif
++#endif
+diff --git a/include/linux/sched.h b/include/linux/sched.h
+index ab10455e5b02..cc7349a2c0cf 100644
+--- a/include/linux/sched.h
++++ b/include/linux/sched.h
+@@ -26,6 +26,7 @@ struct sched_param {
+ #include <linux/nodemask.h>
+ #include <linux/mm_types.h>
+ #include <linux/preempt_mask.h>
++#include <asm/kmap_types.h>
+ 
+ #include <asm/page.h>
+ #include <asm/ptrace.h>
+@@ -56,6 +57,7 @@ struct sched_param {
+ #include <linux/cred.h>
+ #include <linux/llist.h>
+ #include <linux/uidgid.h>
++#include <linux/hardirq.h>
+ #include <linux/gfp.h>
+ #include <linux/magic.h>
+ 
+@@ -176,8 +178,6 @@ extern void get_iowait_load(unsigned long *nr_waiters, unsigned long *load);
+ extern void calc_global_load(unsigned long ticks);
+ extern void update_cpu_load_nohz(void);
+ 
+-extern unsigned long get_parent_ip(unsigned long addr);
+-
+ extern void dump_cpu_task(int cpu);
+ 
+ struct seq_file;
+@@ -235,10 +235,7 @@ extern char ___assert_task_state[1 - 2*!!(
+ 				 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
+ 				 __TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD)
+ 
+-#define task_is_traced(task)	((task->state & __TASK_TRACED) != 0)
+ #define task_is_stopped(task)	((task->state & __TASK_STOPPED) != 0)
+-#define task_is_stopped_or_traced(task)	\
+-			((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
+ #define task_contributes_to_load(task)	\
+ 				((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
+ 				 (task->flags & PF_FROZEN) == 0)
+@@ -868,6 +865,50 @@ enum cpu_idle_type {
+ #define SCHED_CAPACITY_SCALE	(1L << SCHED_CAPACITY_SHIFT)
+ 
+ /*
++ * Wake-queues are lists of tasks with a pending wakeup, whose
++ * callers have already marked the task as woken internally,
++ * and can thus carry on. A common use case is being able to
++ * do the wakeups once the corresponding user lock as been
++ * released.
++ *
++ * We hold reference to each task in the list across the wakeup,
++ * thus guaranteeing that the memory is still valid by the time
++ * the actual wakeups are performed in wake_up_q().
++ *
++ * One per task suffices, because there's never a need for a task to be
++ * in two wake queues simultaneously; it is forbidden to abandon a task
++ * in a wake queue (a call to wake_up_q() _must_ follow), so if a task is
++ * already in a wake queue, the wakeup will happen soon and the second
++ * waker can just skip it.
++ *
++ * The WAKE_Q macro declares and initializes the list head.
++ * wake_up_q() does NOT reinitialize the list; it's expected to be
++ * called near the end of a function, where the fact that the queue is
++ * not used again will be easy to see by inspection.
++ *
++ * Note that this can cause spurious wakeups. schedule() callers
++ * must ensure the call is done inside a loop, confirming that the
++ * wakeup condition has in fact occurred.
++ */
++struct wake_q_node {
++	struct wake_q_node *next;
++};
++
++struct wake_q_head {
++	struct wake_q_node *first;
++	struct wake_q_node **lastp;
++};
++
++#define WAKE_Q_TAIL ((struct wake_q_node *) 0x01)
++
++#define WAKE_Q(name)					\
++	struct wake_q_head name = { WAKE_Q_TAIL, &name.first }
++
++extern void wake_q_add(struct wake_q_head *head,
++		       struct task_struct *task);
++extern void wake_up_q(struct wake_q_head *head);
++
++/*
+  * sched-domains (multiprocessor balancing) declarations:
+  */
+ #ifdef CONFIG_SMP
+@@ -1246,6 +1287,7 @@ enum perf_event_task_context {
+ 
+ struct task_struct {
+ 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
++	volatile long saved_state;	/* saved state for "spinlock sleepers" */
+ 	void *stack;
+ 	atomic_t usage;
+ 	unsigned int flags;	/* per process flags, defined below */
+@@ -1282,6 +1324,12 @@ struct task_struct {
+ #endif
+ 
+ 	unsigned int policy;
++#ifdef CONFIG_PREEMPT_RT_FULL
++	int migrate_disable;
++# ifdef CONFIG_SCHED_DEBUG
++	int migrate_disable_atomic;
++# endif
++#endif
+ 	int nr_cpus_allowed;
+ 	cpumask_t cpus_allowed;
+ 
+@@ -1383,7 +1431,8 @@ struct task_struct {
+ 	struct cputime prev_cputime;
+ #endif
+ #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
+-	seqlock_t vtime_seqlock;
++	raw_spinlock_t vtime_lock;
++	seqcount_t vtime_seq;
+ 	unsigned long long vtime_snap;
+ 	enum {
+ 		VTIME_SLEEPING = 0,
+@@ -1399,6 +1448,9 @@ struct task_struct {
+ 
+ 	struct task_cputime cputime_expires;
+ 	struct list_head cpu_timers[3];
++#ifdef CONFIG_PREEMPT_RT_BASE
++	struct task_struct *posix_timer_list;
++#endif
+ 
+ /* process credentials */
+ 	const struct cred __rcu *real_cred; /* objective and real subjective task
+@@ -1431,10 +1483,15 @@ struct task_struct {
+ /* signal handlers */
+ 	struct signal_struct *signal;
+ 	struct sighand_struct *sighand;
++	struct sigqueue *sigqueue_cache;
+ 
+ 	sigset_t blocked, real_blocked;
+ 	sigset_t saved_sigmask;	/* restored if set_restore_sigmask() was used */
+ 	struct sigpending pending;
++#ifdef CONFIG_PREEMPT_RT_FULL
++	/* TODO: move me into ->restart_block ? */
++	struct siginfo forced_info;
++#endif
+ 
+ 	unsigned long sas_ss_sp;
+ 	size_t sas_ss_size;
+@@ -1460,6 +1517,8 @@ struct task_struct {
+ 	/* Protection of the PI data structures: */
+ 	raw_spinlock_t pi_lock;
+ 
++	struct wake_q_node wake_q;
++
+ #ifdef CONFIG_RT_MUTEXES
+ 	/* PI waiters blocked on a rt_mutex held by this task */
+ 	struct rb_root pi_waiters;
+@@ -1472,6 +1531,10 @@ struct task_struct {
+ 	/* mutex deadlock detection */
+ 	struct mutex_waiter *blocked_on;
+ #endif
++#ifdef CONFIG_PREEMPT_RT_FULL
++	int xmit_recursion;
++	int pagefault_disabled;
++#endif
+ #ifdef CONFIG_TRACE_IRQFLAGS
+ 	unsigned int irq_events;
+ 	unsigned long hardirq_enable_ip;
+@@ -1656,6 +1719,12 @@ struct task_struct {
+ 	unsigned long trace;
+ 	/* bitmask and counter of trace recursion */
+ 	unsigned long trace_recursion;
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++	u64 preempt_timestamp_hist;
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++	long timer_offset;
++#endif
++#endif
+ #endif /* CONFIG_TRACING */
+ #ifdef CONFIG_MEMCG /* memcg uses this to do batch job */
+ 	unsigned int memcg_kmem_skip_account;
+@@ -1673,11 +1742,19 @@ struct task_struct {
+ 	unsigned int	sequential_io;
+ 	unsigned int	sequential_io_avg;
+ #endif
++#ifdef CONFIG_PREEMPT_RT_BASE
++	struct rcu_head put_rcu;
++	int softirq_nestcnt;
++	unsigned int softirqs_raised;
++#endif
++#ifdef CONFIG_PREEMPT_RT_FULL
++# if defined CONFIG_HIGHMEM || defined CONFIG_X86_32
++	int kmap_idx;
++	pte_t kmap_pte[KM_TYPE_NR];
++# endif
++#endif
+ };
+ 
+-/* Future-safe accessor for struct task_struct's cpus_allowed. */
+-#define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
+-
+ #define TNF_MIGRATED	0x01
+ #define TNF_NO_GROUP	0x02
+ #define TNF_SHARED	0x04
+@@ -1712,6 +1789,17 @@ static inline bool should_numa_migrate_memory(struct task_struct *p,
+ }
+ #endif
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++static inline bool cur_pf_disabled(void) { return current->pagefault_disabled; }
++#else
++static inline bool cur_pf_disabled(void) { return false; }
++#endif
++
++static inline bool pagefault_disabled(void)
++{
++	return in_atomic() || cur_pf_disabled();
++}
++
+ static inline struct pid *task_pid(struct task_struct *task)
+ {
+ 	return task->pids[PIDTYPE_PID].pid;
+@@ -1869,6 +1957,15 @@ extern struct pid *cad_pid;
+ extern void free_task(struct task_struct *tsk);
+ #define get_task_struct(tsk) do { atomic_inc(&(tsk)->usage); } while(0)
+ 
++#ifdef CONFIG_PREEMPT_RT_BASE
++extern void __put_task_struct_cb(struct rcu_head *rhp);
++
++static inline void put_task_struct(struct task_struct *t)
++{
++	if (atomic_dec_and_test(&t->usage))
++		call_rcu(&t->put_rcu, __put_task_struct_cb);
++}
++#else
+ extern void __put_task_struct(struct task_struct *t);
+ 
+ static inline void put_task_struct(struct task_struct *t)
+@@ -1876,6 +1973,7 @@ static inline void put_task_struct(struct task_struct *t)
+ 	if (atomic_dec_and_test(&t->usage))
+ 		__put_task_struct(t);
+ }
++#endif
+ 
+ #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
+ extern void task_cputime(struct task_struct *t,
+@@ -1914,6 +2012,7 @@ extern void thread_group_cputime_adjusted(struct task_struct *p, cputime_t *ut,
+ /*
+  * Per process flags
+  */
++#define PF_IN_SOFTIRQ	0x00000001	/* Task is serving softirq */
+ #define PF_EXITING	0x00000004	/* getting shut down */
+ #define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */
+ #define PF_VCPU		0x00000010	/* I'm a virtual CPU */
+@@ -2074,6 +2173,10 @@ extern void do_set_cpus_allowed(struct task_struct *p,
+ 
+ extern int set_cpus_allowed_ptr(struct task_struct *p,
+ 				const struct cpumask *new_mask);
++int migrate_me(void);
++void tell_sched_cpu_down_begin(int cpu);
++void tell_sched_cpu_down_done(int cpu);
++
+ #else
+ static inline void do_set_cpus_allowed(struct task_struct *p,
+ 				      const struct cpumask *new_mask)
+@@ -2086,6 +2189,9 @@ static inline int set_cpus_allowed_ptr(struct task_struct *p,
+ 		return -EINVAL;
+ 	return 0;
+ }
++static inline int migrate_me(void) { return 0; }
++static inline void tell_sched_cpu_down_begin(int cpu) { }
++static inline void tell_sched_cpu_down_done(int cpu) { }
+ #endif
+ 
+ #ifdef CONFIG_NO_HZ_COMMON
+@@ -2306,6 +2412,7 @@ extern void xtime_update(unsigned long ticks);
+ 
+ extern int wake_up_state(struct task_struct *tsk, unsigned int state);
+ extern int wake_up_process(struct task_struct *tsk);
++extern int wake_up_lock_sleeper(struct task_struct * tsk);
+ extern void wake_up_new_task(struct task_struct *tsk);
+ #ifdef CONFIG_SMP
+  extern void kick_process(struct task_struct *tsk);
+@@ -2422,12 +2529,24 @@ extern struct mm_struct * mm_alloc(void);
+ 
+ /* mmdrop drops the mm and the page tables */
+ extern void __mmdrop(struct mm_struct *);
++
+ static inline void mmdrop(struct mm_struct * mm)
+ {
+ 	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
+ 		__mmdrop(mm);
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_BASE
++extern void __mmdrop_delayed(struct rcu_head *rhp);
++static inline void mmdrop_delayed(struct mm_struct *mm)
++{
++	if (atomic_dec_and_test(&mm->mm_count))
++		call_rcu(&mm->delayed_drop, __mmdrop_delayed);
++}
++#else
++# define mmdrop_delayed(mm)	mmdrop(mm)
++#endif
++
+ /* mmput gets rid of the mappings and all user-space */
+ extern void mmput(struct mm_struct *);
+ /* Grab a reference to a task's mm, if it is not already going away */
+@@ -2735,6 +2854,43 @@ static inline int test_tsk_need_resched(struct task_struct *tsk)
+ 	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));
+ }
+ 
++#ifdef CONFIG_PREEMPT_LAZY
++static inline void set_tsk_need_resched_lazy(struct task_struct *tsk)
++{
++	set_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY);
++}
++
++static inline void clear_tsk_need_resched_lazy(struct task_struct *tsk)
++{
++	clear_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY);
++}
++
++static inline int test_tsk_need_resched_lazy(struct task_struct *tsk)
++{
++	return unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED_LAZY));
++}
++
++static inline int need_resched_lazy(void)
++{
++	return test_thread_flag(TIF_NEED_RESCHED_LAZY);
++}
++
++static inline int need_resched_now(void)
++{
++	return test_thread_flag(TIF_NEED_RESCHED);
++}
++
++#else
++static inline void clear_tsk_need_resched_lazy(struct task_struct *tsk) { }
++static inline int need_resched_lazy(void) { return 0; }
++
++static inline int need_resched_now(void)
++{
++	return test_thread_flag(TIF_NEED_RESCHED);
++}
++
++#endif
++
+ static inline int restart_syscall(void)
+ {
+ 	set_tsk_thread_flag(current, TIF_SIGPENDING);
+@@ -2766,6 +2922,51 @@ static inline int signal_pending_state(long state, struct task_struct *p)
+ 	return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);
+ }
+ 
++static inline bool __task_is_stopped_or_traced(struct task_struct *task)
++{
++	if (task->state & (__TASK_STOPPED | __TASK_TRACED))
++		return true;
++#ifdef CONFIG_PREEMPT_RT_FULL
++	if (task->saved_state & (__TASK_STOPPED | __TASK_TRACED))
++		return true;
++#endif
++	return false;
++}
++
++static inline bool task_is_stopped_or_traced(struct task_struct *task)
++{
++	bool traced_stopped;
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++	unsigned long flags;
++
++	raw_spin_lock_irqsave(&task->pi_lock, flags);
++	traced_stopped = __task_is_stopped_or_traced(task);
++	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
++#else
++	traced_stopped = __task_is_stopped_or_traced(task);
++#endif
++	return traced_stopped;
++}
++
++static inline bool task_is_traced(struct task_struct *task)
++{
++	bool traced = false;
++
++	if (task->state & __TASK_TRACED)
++		return true;
++#ifdef CONFIG_PREEMPT_RT_FULL
++	/* in case the task is sleeping on tasklist_lock */
++	raw_spin_lock_irq(&task->pi_lock);
++	if (task->state & __TASK_TRACED)
++		traced = true;
++	else if (task->saved_state & __TASK_TRACED)
++		traced = true;
++	raw_spin_unlock_irq(&task->pi_lock);
++#endif
++	return traced;
++}
++
+ /*
+  * cond_resched() and cond_resched_lock(): latency reduction via
+  * explicit rescheduling in places that are safe. The return
+@@ -2782,7 +2983,7 @@ extern int _cond_resched(void);
+ 
+ extern int __cond_resched_lock(spinlock_t *lock);
+ 
+-#ifdef CONFIG_PREEMPT_COUNT
++#if defined(CONFIG_PREEMPT_COUNT) && !defined(CONFIG_PREEMPT_RT_FULL)
+ #define PREEMPT_LOCK_OFFSET	PREEMPT_OFFSET
+ #else
+ #define PREEMPT_LOCK_OFFSET	0
+@@ -2793,12 +2994,16 @@ extern int __cond_resched_lock(spinlock_t *lock);
+ 	__cond_resched_lock(lock);				\
+ })
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ extern int __cond_resched_softirq(void);
+ 
+ #define cond_resched_softirq() ({					\
+ 	__might_sleep(__FILE__, __LINE__, SOFTIRQ_DISABLE_OFFSET);	\
+ 	__cond_resched_softirq();					\
+ })
++#else
++# define cond_resched_softirq()		cond_resched()
++#endif
+ 
+ static inline void cond_resched_rcu(void)
+ {
+@@ -2965,6 +3170,26 @@ static inline void set_task_cpu(struct task_struct *p, unsigned int cpu)
+ 
+ #endif /* CONFIG_SMP */
+ 
++static inline int __migrate_disabled(struct task_struct *p)
++{
++#ifdef CONFIG_PREEMPT_RT_FULL
++	return p->migrate_disable;
++#else
++	return 0;
++#endif
++}
++
++/* Future-safe accessor for struct task_struct's cpus_allowed. */
++static inline const struct cpumask *tsk_cpus_allowed(struct task_struct *p)
++{
++#ifdef CONFIG_PREEMPT_RT_FULL
++	if (p->migrate_disable)
++		return cpumask_of(task_cpu(p));
++#endif
++
++	return &p->cpus_allowed;
++}
++
+ extern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);
+ extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
+ 
+diff --git a/include/linux/seqlock.h b/include/linux/seqlock.h
+index f5df8f687b4d..4acd0e2fb5cb 100644
+--- a/include/linux/seqlock.h
++++ b/include/linux/seqlock.h
+@@ -219,20 +219,30 @@ static inline int read_seqcount_retry(const seqcount_t *s, unsigned start)
+ 	return __read_seqcount_retry(s, start);
+ }
+ 
+-
+-
+-static inline void raw_write_seqcount_begin(seqcount_t *s)
++static inline void __raw_write_seqcount_begin(seqcount_t *s)
+ {
+ 	s->sequence++;
+ 	smp_wmb();
+ }
+ 
+-static inline void raw_write_seqcount_end(seqcount_t *s)
++static inline void raw_write_seqcount_begin(seqcount_t *s)
++{
++	preempt_disable_rt();
++	__raw_write_seqcount_begin(s);
++}
++
++static inline void __raw_write_seqcount_end(seqcount_t *s)
+ {
+ 	smp_wmb();
+ 	s->sequence++;
+ }
+ 
++static inline void raw_write_seqcount_end(seqcount_t *s)
++{
++	__raw_write_seqcount_end(s);
++	preempt_enable_rt();
++}
++
+ /*
+  * raw_write_seqcount_latch - redirect readers to even/odd copy
+  * @s: pointer to seqcount_t
+@@ -305,10 +315,32 @@ typedef struct {
+ /*
+  * Read side functions for starting and finalizing a read side section.
+  */
++#ifndef CONFIG_PREEMPT_RT_FULL
+ static inline unsigned read_seqbegin(const seqlock_t *sl)
+ {
+ 	return read_seqcount_begin(&sl->seqcount);
+ }
++#else
++/*
++ * Starvation safe read side for RT
++ */
++static inline unsigned read_seqbegin(seqlock_t *sl)
++{
++	unsigned ret;
++
++repeat:
++	ret = ACCESS_ONCE(sl->seqcount.sequence);
++	if (unlikely(ret & 1)) {
++		/*
++		 * Take the lock and let the writer proceed (i.e. evtl
++		 * boost it), otherwise we could loop here forever.
++		 */
++		spin_unlock_wait(&sl->lock);
++		goto repeat;
++	}
++	return ret;
++}
++#endif
+ 
+ static inline unsigned read_seqretry(const seqlock_t *sl, unsigned start)
+ {
+@@ -323,36 +355,36 @@ static inline unsigned read_seqretry(const seqlock_t *sl, unsigned start)
+ static inline void write_seqlock(seqlock_t *sl)
+ {
+ 	spin_lock(&sl->lock);
+-	write_seqcount_begin(&sl->seqcount);
++	__raw_write_seqcount_begin(&sl->seqcount);
+ }
+ 
+ static inline void write_sequnlock(seqlock_t *sl)
+ {
+-	write_seqcount_end(&sl->seqcount);
++	__raw_write_seqcount_end(&sl->seqcount);
+ 	spin_unlock(&sl->lock);
+ }
+ 
+ static inline void write_seqlock_bh(seqlock_t *sl)
+ {
+ 	spin_lock_bh(&sl->lock);
+-	write_seqcount_begin(&sl->seqcount);
++	__raw_write_seqcount_begin(&sl->seqcount);
+ }
+ 
+ static inline void write_sequnlock_bh(seqlock_t *sl)
+ {
+-	write_seqcount_end(&sl->seqcount);
++	__raw_write_seqcount_end(&sl->seqcount);
+ 	spin_unlock_bh(&sl->lock);
+ }
+ 
+ static inline void write_seqlock_irq(seqlock_t *sl)
+ {
+ 	spin_lock_irq(&sl->lock);
+-	write_seqcount_begin(&sl->seqcount);
++	__raw_write_seqcount_begin(&sl->seqcount);
+ }
+ 
+ static inline void write_sequnlock_irq(seqlock_t *sl)
+ {
+-	write_seqcount_end(&sl->seqcount);
++	__raw_write_seqcount_end(&sl->seqcount);
+ 	spin_unlock_irq(&sl->lock);
+ }
+ 
+@@ -361,7 +393,7 @@ static inline unsigned long __write_seqlock_irqsave(seqlock_t *sl)
+ 	unsigned long flags;
+ 
+ 	spin_lock_irqsave(&sl->lock, flags);
+-	write_seqcount_begin(&sl->seqcount);
++	__raw_write_seqcount_begin(&sl->seqcount);
+ 	return flags;
+ }
+ 
+@@ -371,7 +403,7 @@ static inline unsigned long __write_seqlock_irqsave(seqlock_t *sl)
+ static inline void
+ write_sequnlock_irqrestore(seqlock_t *sl, unsigned long flags)
+ {
+-	write_seqcount_end(&sl->seqcount);
++	__raw_write_seqcount_end(&sl->seqcount);
+ 	spin_unlock_irqrestore(&sl->lock, flags);
+ }
+ 
+diff --git a/include/linux/signal.h b/include/linux/signal.h
+index ab1e0392b5ac..66215b20aa8d 100644
+--- a/include/linux/signal.h
++++ b/include/linux/signal.h
+@@ -218,6 +218,7 @@ static inline void init_sigpending(struct sigpending *sig)
+ }
+ 
+ extern void flush_sigqueue(struct sigpending *queue);
++extern void flush_task_sigqueue(struct task_struct *tsk);
+ 
+ /* Test if 'sig' is valid signal. Use this instead of testing _NSIG directly */
+ static inline int valid_signal(unsigned long sig)
+diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
+index 2ff757f2d3a3..62d8f329a7de 100644
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -172,6 +172,7 @@ struct sk_buff_head {
+ 
+ 	__u32		qlen;
+ 	spinlock_t	lock;
++	raw_spinlock_t	raw_lock;
+ };
+ 
+ struct sk_buff;
+@@ -1329,6 +1330,12 @@ static inline void skb_queue_head_init(struct sk_buff_head *list)
+ 	__skb_queue_head_init(list);
+ }
+ 
++static inline void skb_queue_head_init_raw(struct sk_buff_head *list)
++{
++	raw_spin_lock_init(&list->raw_lock);
++	__skb_queue_head_init(list);
++}
++
+ static inline void skb_queue_head_init_class(struct sk_buff_head *list,
+ 		struct lock_class_key *class)
+ {
+diff --git a/include/linux/smp.h b/include/linux/smp.h
+index 93dff5fff524..42c3d2ca1d75 100644
+--- a/include/linux/smp.h
++++ b/include/linux/smp.h
+@@ -178,6 +178,9 @@ static inline void wake_up_all_idle_cpus(void) {  }
+ #define get_cpu()		({ preempt_disable(); smp_processor_id(); })
+ #define put_cpu()		preempt_enable()
+ 
++#define get_cpu_light()		({ migrate_disable(); smp_processor_id(); })
++#define put_cpu_light()		migrate_enable()
++
+ /*
+  * Callback to arch code if there's nosmp or maxcpus=0 on the
+  * boot command line:
+diff --git a/include/linux/spinlock.h b/include/linux/spinlock.h
+index 262ba4ef9a8e..98e8026598f5 100644
+--- a/include/linux/spinlock.h
++++ b/include/linux/spinlock.h
+@@ -278,7 +278,11 @@ static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
+ #define raw_spin_can_lock(lock)	(!raw_spin_is_locked(lock))
+ 
+ /* Include rwlock functions */
+-#include <linux/rwlock.h>
++#ifdef CONFIG_PREEMPT_RT_FULL
++# include <linux/rwlock_rt.h>
++#else
++# include <linux/rwlock.h>
++#endif
+ 
+ /*
+  * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:
+@@ -289,6 +293,10 @@ static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
+ # include <linux/spinlock_api_up.h>
+ #endif
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++# include <linux/spinlock_rt.h>
++#else /* PREEMPT_RT_FULL */
++
+ /*
+  * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
+  */
+@@ -418,4 +426,6 @@ extern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);
+ #define atomic_dec_and_lock(atomic, lock) \
+ 		__cond_lock(lock, _atomic_dec_and_lock(atomic, lock))
+ 
++#endif /* !PREEMPT_RT_FULL */
++
+ #endif /* __LINUX_SPINLOCK_H */
+diff --git a/include/linux/spinlock_api_smp.h b/include/linux/spinlock_api_smp.h
+index 42dfab89e740..29d99ae5a8ab 100644
+--- a/include/linux/spinlock_api_smp.h
++++ b/include/linux/spinlock_api_smp.h
+@@ -187,6 +187,8 @@ static inline int __raw_spin_trylock_bh(raw_spinlock_t *lock)
+ 	return 0;
+ }
+ 
+-#include <linux/rwlock_api_smp.h>
++#ifndef CONFIG_PREEMPT_RT_FULL
++# include <linux/rwlock_api_smp.h>
++#endif
+ 
+ #endif /* __LINUX_SPINLOCK_API_SMP_H */
+diff --git a/include/linux/spinlock_rt.h b/include/linux/spinlock_rt.h
+new file mode 100644
+index 000000000000..c0d1367d3e10
+--- /dev/null
++++ b/include/linux/spinlock_rt.h
+@@ -0,0 +1,167 @@
++#ifndef __LINUX_SPINLOCK_RT_H
++#define __LINUX_SPINLOCK_RT_H
++
++#ifndef __LINUX_SPINLOCK_H
++#error Do not include directly. Use spinlock.h
++#endif
++
++#include <linux/bug.h>
++
++extern void
++__rt_spin_lock_init(spinlock_t *lock, char *name, struct lock_class_key *key);
++
++#define spin_lock_init(slock)				\
++do {							\
++	static struct lock_class_key __key;		\
++							\
++	rt_mutex_init(&(slock)->lock);			\
++	__rt_spin_lock_init(slock, #slock, &__key);	\
++} while (0)
++
++extern void __lockfunc rt_spin_lock(spinlock_t *lock);
++extern unsigned long __lockfunc rt_spin_lock_trace_flags(spinlock_t *lock);
++extern void __lockfunc rt_spin_lock_nested(spinlock_t *lock, int subclass);
++extern void __lockfunc rt_spin_unlock(spinlock_t *lock);
++extern void __lockfunc rt_spin_unlock_after_trylock_in_irq(spinlock_t *lock);
++extern void __lockfunc rt_spin_unlock_wait(spinlock_t *lock);
++extern int __lockfunc rt_spin_trylock_irqsave(spinlock_t *lock, unsigned long *flags);
++extern int __lockfunc rt_spin_trylock_bh(spinlock_t *lock);
++extern int __lockfunc rt_spin_trylock(spinlock_t *lock);
++extern int atomic_dec_and_spin_lock(atomic_t *atomic, spinlock_t *lock);
++
++/*
++ * lockdep-less calls, for derived types like rwlock:
++ * (for trylock they can use rt_mutex_trylock() directly.
++ */
++extern void __lockfunc __rt_spin_lock(struct rt_mutex *lock);
++extern void __lockfunc __rt_spin_unlock(struct rt_mutex *lock);
++extern int __lockfunc __rt_spin_trylock(struct rt_mutex *lock);
++
++#define spin_lock(lock)				\
++	do {					\
++		migrate_disable();		\
++		rt_spin_lock(lock);		\
++	} while (0)
++
++#define spin_lock_bh(lock)			\
++	do {					\
++		local_bh_disable();		\
++		migrate_disable();		\
++		rt_spin_lock(lock);		\
++	} while (0)
++
++#define spin_lock_irq(lock)		spin_lock(lock)
++
++#define spin_do_trylock(lock)		__cond_lock(lock, rt_spin_trylock(lock))
++
++#define spin_trylock(lock)			\
++({						\
++	int __locked;				\
++	migrate_disable();			\
++	__locked = spin_do_trylock(lock);	\
++	if (!__locked)				\
++		migrate_enable();		\
++	__locked;				\
++})
++
++#ifdef CONFIG_LOCKDEP
++# define spin_lock_nested(lock, subclass)		\
++	do {						\
++		migrate_disable();			\
++		rt_spin_lock_nested(lock, subclass);	\
++	} while (0)
++
++# define spin_lock_irqsave_nested(lock, flags, subclass) \
++	do {						 \
++		typecheck(unsigned long, flags);	 \
++		flags = 0;				 \
++		migrate_disable();			 \
++		rt_spin_lock_nested(lock, subclass);	 \
++	} while (0)
++#else
++# define spin_lock_nested(lock, subclass)	spin_lock(lock)
++
++# define spin_lock_irqsave_nested(lock, flags, subclass) \
++	do {						 \
++		typecheck(unsigned long, flags);	 \
++		flags = 0;				 \
++		spin_lock(lock);			 \
++	} while (0)
++#endif
++
++#define spin_lock_irqsave(lock, flags)			 \
++	do {						 \
++		typecheck(unsigned long, flags);	 \
++		flags = 0;				 \
++		spin_lock(lock);			 \
++	} while (0)
++
++static inline unsigned long spin_lock_trace_flags(spinlock_t *lock)
++{
++	unsigned long flags = 0;
++#ifdef CONFIG_TRACE_IRQFLAGS
++	flags = rt_spin_lock_trace_flags(lock);
++#else
++	spin_lock(lock); /* lock_local */
++#endif
++	return flags;
++}
++
++/* FIXME: we need rt_spin_lock_nest_lock */
++#define spin_lock_nest_lock(lock, nest_lock) spin_lock_nested(lock, 0)
++
++#define spin_unlock(lock)				\
++	do {						\
++		rt_spin_unlock(lock);			\
++		migrate_enable();			\
++	} while (0)
++
++#define spin_unlock_bh(lock)				\
++	do {						\
++		rt_spin_unlock(lock);			\
++		migrate_enable();			\
++		local_bh_enable();			\
++	} while (0)
++
++#define spin_unlock_irq(lock)		spin_unlock(lock)
++
++#define spin_unlock_irqrestore(lock, flags)		\
++	do {						\
++		typecheck(unsigned long, flags);	\
++		(void) flags;				\
++		spin_unlock(lock);			\
++	} while (0)
++
++#define spin_trylock_bh(lock)	__cond_lock(lock, rt_spin_trylock_bh(lock))
++#define spin_trylock_irq(lock)	spin_trylock(lock)
++
++#define spin_trylock_irqsave(lock, flags)	\
++	rt_spin_trylock_irqsave(lock, &(flags))
++
++#define spin_unlock_wait(lock)		rt_spin_unlock_wait(lock)
++
++#ifdef CONFIG_GENERIC_LOCKBREAK
++# define spin_is_contended(lock)	((lock)->break_lock)
++#else
++# define spin_is_contended(lock)	(((void)(lock), 0))
++#endif
++
++static inline int spin_can_lock(spinlock_t *lock)
++{
++	return !rt_mutex_is_locked(&lock->lock);
++}
++
++static inline int spin_is_locked(spinlock_t *lock)
++{
++	return rt_mutex_is_locked(&lock->lock);
++}
++
++static inline void assert_spin_locked(spinlock_t *lock)
++{
++	BUG_ON(!spin_is_locked(lock));
++}
++
++#define atomic_dec_and_lock(atomic, lock) \
++	atomic_dec_and_spin_lock(atomic, lock)
++
++#endif
+diff --git a/include/linux/spinlock_types.h b/include/linux/spinlock_types.h
+index 73548eb13a5d..10bac715ea96 100644
+--- a/include/linux/spinlock_types.h
++++ b/include/linux/spinlock_types.h
+@@ -9,80 +9,15 @@
+  * Released under the General Public License (GPL).
+  */
+ 
+-#if defined(CONFIG_SMP)
+-# include <asm/spinlock_types.h>
+-#else
+-# include <linux/spinlock_types_up.h>
+-#endif
+-
+-#include <linux/lockdep.h>
+-
+-typedef struct raw_spinlock {
+-	arch_spinlock_t raw_lock;
+-#ifdef CONFIG_GENERIC_LOCKBREAK
+-	unsigned int break_lock;
+-#endif
+-#ifdef CONFIG_DEBUG_SPINLOCK
+-	unsigned int magic, owner_cpu;
+-	void *owner;
+-#endif
+-#ifdef CONFIG_DEBUG_LOCK_ALLOC
+-	struct lockdep_map dep_map;
+-#endif
+-} raw_spinlock_t;
+-
+-#define SPINLOCK_MAGIC		0xdead4ead
+-
+-#define SPINLOCK_OWNER_INIT	((void *)-1L)
+-
+-#ifdef CONFIG_DEBUG_LOCK_ALLOC
+-# define SPIN_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
+-#else
+-# define SPIN_DEP_MAP_INIT(lockname)
+-#endif
++#include <linux/spinlock_types_raw.h>
+ 
+-#ifdef CONFIG_DEBUG_SPINLOCK
+-# define SPIN_DEBUG_INIT(lockname)		\
+-	.magic = SPINLOCK_MAGIC,		\
+-	.owner_cpu = -1,			\
+-	.owner = SPINLOCK_OWNER_INIT,
++#ifndef CONFIG_PREEMPT_RT_FULL
++# include <linux/spinlock_types_nort.h>
++# include <linux/rwlock_types.h>
+ #else
+-# define SPIN_DEBUG_INIT(lockname)
++# include <linux/rtmutex.h>
++# include <linux/spinlock_types_rt.h>
++# include <linux/rwlock_types_rt.h>
+ #endif
+ 
+-#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
+-	{					\
+-	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
+-	SPIN_DEBUG_INIT(lockname)		\
+-	SPIN_DEP_MAP_INIT(lockname) }
+-
+-#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
+-	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)
+-
+-#define DEFINE_RAW_SPINLOCK(x)	raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)
+-
+-typedef struct spinlock {
+-	union {
+-		struct raw_spinlock rlock;
+-
+-#ifdef CONFIG_DEBUG_LOCK_ALLOC
+-# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
+-		struct {
+-			u8 __padding[LOCK_PADSIZE];
+-			struct lockdep_map dep_map;
+-		};
+-#endif
+-	};
+-} spinlock_t;
+-
+-#define __SPIN_LOCK_INITIALIZER(lockname) \
+-	{ { .rlock = __RAW_SPIN_LOCK_INITIALIZER(lockname) } }
+-
+-#define __SPIN_LOCK_UNLOCKED(lockname) \
+-	(spinlock_t ) __SPIN_LOCK_INITIALIZER(lockname)
+-
+-#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
+-
+-#include <linux/rwlock_types.h>
+-
+ #endif /* __LINUX_SPINLOCK_TYPES_H */
+diff --git a/include/linux/spinlock_types_nort.h b/include/linux/spinlock_types_nort.h
+new file mode 100644
+index 000000000000..f1dac1fb1d6a
+--- /dev/null
++++ b/include/linux/spinlock_types_nort.h
+@@ -0,0 +1,33 @@
++#ifndef __LINUX_SPINLOCK_TYPES_NORT_H
++#define __LINUX_SPINLOCK_TYPES_NORT_H
++
++#ifndef __LINUX_SPINLOCK_TYPES_H
++#error "Do not include directly. Include spinlock_types.h instead"
++#endif
++
++/*
++ * The non RT version maps spinlocks to raw_spinlocks
++ */
++typedef struct spinlock {
++	union {
++		struct raw_spinlock rlock;
++
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
++		struct {
++			u8 __padding[LOCK_PADSIZE];
++			struct lockdep_map dep_map;
++		};
++#endif
++	};
++} spinlock_t;
++
++#define __SPIN_LOCK_INITIALIZER(lockname) \
++	{ { .rlock = __RAW_SPIN_LOCK_INITIALIZER(lockname) } }
++
++#define __SPIN_LOCK_UNLOCKED(lockname) \
++	(spinlock_t ) __SPIN_LOCK_INITIALIZER(lockname)
++
++#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
++
++#endif
+diff --git a/include/linux/spinlock_types_raw.h b/include/linux/spinlock_types_raw.h
+new file mode 100644
+index 000000000000..edffc4d53fc9
+--- /dev/null
++++ b/include/linux/spinlock_types_raw.h
+@@ -0,0 +1,56 @@
++#ifndef __LINUX_SPINLOCK_TYPES_RAW_H
++#define __LINUX_SPINLOCK_TYPES_RAW_H
++
++#if defined(CONFIG_SMP)
++# include <asm/spinlock_types.h>
++#else
++# include <linux/spinlock_types_up.h>
++#endif
++
++#include <linux/lockdep.h>
++
++typedef struct raw_spinlock {
++	arch_spinlock_t raw_lock;
++#ifdef CONFIG_GENERIC_LOCKBREAK
++	unsigned int break_lock;
++#endif
++#ifdef CONFIG_DEBUG_SPINLOCK
++	unsigned int magic, owner_cpu;
++	void *owner;
++#endif
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++	struct lockdep_map dep_map;
++#endif
++} raw_spinlock_t;
++
++#define SPINLOCK_MAGIC		0xdead4ead
++
++#define SPINLOCK_OWNER_INIT	((void *)-1L)
++
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++# define SPIN_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
++#else
++# define SPIN_DEP_MAP_INIT(lockname)
++#endif
++
++#ifdef CONFIG_DEBUG_SPINLOCK
++# define SPIN_DEBUG_INIT(lockname)		\
++	.magic = SPINLOCK_MAGIC,		\
++	.owner_cpu = -1,			\
++	.owner = SPINLOCK_OWNER_INIT,
++#else
++# define SPIN_DEBUG_INIT(lockname)
++#endif
++
++#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
++	{					\
++	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
++	SPIN_DEBUG_INIT(lockname)		\
++	SPIN_DEP_MAP_INIT(lockname) }
++
++#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
++	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)
++
++#define DEFINE_RAW_SPINLOCK(x)	raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)
++
++#endif
+diff --git a/include/linux/spinlock_types_rt.h b/include/linux/spinlock_types_rt.h
+new file mode 100644
+index 000000000000..9fd431967abc
+--- /dev/null
++++ b/include/linux/spinlock_types_rt.h
+@@ -0,0 +1,51 @@
++#ifndef __LINUX_SPINLOCK_TYPES_RT_H
++#define __LINUX_SPINLOCK_TYPES_RT_H
++
++#ifndef __LINUX_SPINLOCK_TYPES_H
++#error "Do not include directly. Include spinlock_types.h instead"
++#endif
++
++#include <linux/cache.h>
++
++/*
++ * PREEMPT_RT: spinlocks - an RT mutex plus lock-break field:
++ */
++typedef struct spinlock {
++	struct rt_mutex		lock;
++	unsigned int		break_lock;
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++	struct lockdep_map	dep_map;
++#endif
++} spinlock_t;
++
++#ifdef CONFIG_DEBUG_RT_MUTEXES
++# define __RT_SPIN_INITIALIZER(name) \
++	{ \
++	.wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock), \
++	.save_state = 1, \
++	.file = __FILE__, \
++	.line = __LINE__ , \
++	}
++#else
++# define __RT_SPIN_INITIALIZER(name) \
++	{								\
++	.wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock),		\
++	.save_state = 1, \
++	}
++#endif
++
++/*
++.wait_list = PLIST_HEAD_INIT_RAW((name).lock.wait_list, (name).lock.wait_lock)
++*/
++
++#define __SPIN_LOCK_UNLOCKED(name)			\
++	{ .lock = __RT_SPIN_INITIALIZER(name.lock),		\
++	  SPIN_DEP_MAP_INIT(name) }
++
++#define __DEFINE_SPINLOCK(name) \
++	spinlock_t name = __SPIN_LOCK_UNLOCKED(name)
++
++#define DEFINE_SPINLOCK(name) \
++	spinlock_t name __cacheline_aligned_in_smp = __SPIN_LOCK_UNLOCKED(name)
++
++#endif
+diff --git a/include/linux/srcu.h b/include/linux/srcu.h
+index a2783cb5d275..f12ffc644adb 100644
+--- a/include/linux/srcu.h
++++ b/include/linux/srcu.h
+@@ -84,10 +84,10 @@ int init_srcu_struct(struct srcu_struct *sp);
+ 
+ void process_srcu(struct work_struct *work);
+ 
+-#define __SRCU_STRUCT_INIT(name)					\
++#define __SRCU_STRUCT_INIT(name, pcpu_name)				\
+ 	{								\
+ 		.completed = -300,					\
+-		.per_cpu_ref = &name##_srcu_array,			\
++		.per_cpu_ref = &pcpu_name,				\
+ 		.queue_lock = __SPIN_LOCK_UNLOCKED(name.queue_lock),	\
+ 		.running = false,					\
+ 		.batch_queue = RCU_BATCH_INIT(name.batch_queue),	\
+@@ -104,11 +104,12 @@ void process_srcu(struct work_struct *work);
+  */
+ #define DEFINE_SRCU(name)						\
+ 	static DEFINE_PER_CPU(struct srcu_struct_array, name##_srcu_array);\
+-	struct srcu_struct name = __SRCU_STRUCT_INIT(name);
++	struct srcu_struct name = __SRCU_STRUCT_INIT(name, name##_srcu_array);
+ 
+ #define DEFINE_STATIC_SRCU(name)					\
+ 	static DEFINE_PER_CPU(struct srcu_struct_array, name##_srcu_array);\
+-	static struct srcu_struct name = __SRCU_STRUCT_INIT(name);
++	static struct srcu_struct name = __SRCU_STRUCT_INIT(\
++		name, name##_srcu_array);
+ 
+ /**
+  * call_srcu() - Queue a callback for invocation after an SRCU grace period
+diff --git a/include/linux/swap.h b/include/linux/swap.h
+index 1dc0e886227d..e3652ddc384f 100644
+--- a/include/linux/swap.h
++++ b/include/linux/swap.h
+@@ -11,6 +11,7 @@
+ #include <linux/fs.h>
+ #include <linux/atomic.h>
+ #include <linux/page-flags.h>
++#include <linux/locallock.h>
+ #include <asm/page.h>
+ 
+ struct notifier_block;
+@@ -260,7 +261,8 @@ struct swap_info_struct {
+ void *workingset_eviction(struct address_space *mapping, struct page *page);
+ bool workingset_refault(void *shadow);
+ void workingset_activation(struct page *page);
+-extern struct list_lru workingset_shadow_nodes;
++extern struct list_lru __workingset_shadow_nodes;
++DECLARE_LOCAL_IRQ_LOCK(workingset_shadow_lock);
+ 
+ static inline unsigned int workingset_node_pages(struct radix_tree_node *node)
+ {
+diff --git a/include/linux/sysctl.h b/include/linux/sysctl.h
+index b7361f831226..22050201d20e 100644
+--- a/include/linux/sysctl.h
++++ b/include/linux/sysctl.h
+@@ -25,6 +25,7 @@
+ #include <linux/rcupdate.h>
+ #include <linux/wait.h>
+ #include <linux/rbtree.h>
++#include <linux/atomic.h>
+ #include <uapi/linux/sysctl.h>
+ 
+ /* For the /proc/sys support */
+diff --git a/include/linux/thread_info.h b/include/linux/thread_info.h
+index ff307b548ed3..be9f9dc6a4e1 100644
+--- a/include/linux/thread_info.h
++++ b/include/linux/thread_info.h
+@@ -102,7 +102,17 @@ static inline int test_ti_thread_flag(struct thread_info *ti, int flag)
+ #define test_thread_flag(flag) \
+ 	test_ti_thread_flag(current_thread_info(), flag)
+ 
+-#define tif_need_resched() test_thread_flag(TIF_NEED_RESCHED)
++#ifdef CONFIG_PREEMPT_LAZY
++#define tif_need_resched()	(test_thread_flag(TIF_NEED_RESCHED) || \
++				 test_thread_flag(TIF_NEED_RESCHED_LAZY))
++#define tif_need_resched_now()	(test_thread_flag(TIF_NEED_RESCHED))
++#define tif_need_resched_lazy()	test_thread_flag(TIF_NEED_RESCHED_LAZY))
++
++#else
++#define tif_need_resched()	test_thread_flag(TIF_NEED_RESCHED)
++#define tif_need_resched_now()	test_thread_flag(TIF_NEED_RESCHED)
++#define tif_need_resched_lazy()	0
++#endif
+ 
+ #if defined TIF_RESTORE_SIGMASK && !defined HAVE_SET_RESTORE_SIGMASK
+ /*
+diff --git a/include/linux/timer.h b/include/linux/timer.h
+index 8c5a197e1587..5fcd72c57ebe 100644
+--- a/include/linux/timer.h
++++ b/include/linux/timer.h
+@@ -241,7 +241,7 @@ extern void add_timer(struct timer_list *timer);
+ 
+ extern int try_to_del_timer_sync(struct timer_list *timer);
+ 
+-#ifdef CONFIG_SMP
++#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT_FULL)
+   extern int del_timer_sync(struct timer_list *timer);
+ #else
+ # define del_timer_sync(t)		del_timer(t)
+diff --git a/include/linux/uaccess.h b/include/linux/uaccess.h
+index ecd3319dac33..303a282920e5 100644
+--- a/include/linux/uaccess.h
++++ b/include/linux/uaccess.h
+@@ -6,14 +6,9 @@
+ 
+ /*
+  * These routines enable/disable the pagefault handler in that
+- * it will not take any locks and go straight to the fixup table.
+- *
+- * They have great resemblance to the preempt_disable/enable calls
+- * and in fact they are identical; this is because currently there is
+- * no other way to make the pagefault handlers do this. So we do
+- * disable preemption but we don't necessarily care about that.
++ * it will not take any MM locks and go straight to the fixup table.
+  */
+-static inline void pagefault_disable(void)
++static inline void raw_pagefault_disable(void)
+ {
+ 	preempt_count_inc();
+ 	/*
+@@ -23,7 +18,7 @@ static inline void pagefault_disable(void)
+ 	barrier();
+ }
+ 
+-static inline void pagefault_enable(void)
++static inline void raw_pagefault_enable(void)
+ {
+ #ifndef CONFIG_PREEMPT
+ 	/*
+@@ -37,6 +32,21 @@ static inline void pagefault_enable(void)
+ #endif
+ }
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++static inline void pagefault_disable(void)
++{
++	raw_pagefault_disable();
++}
++
++static inline void pagefault_enable(void)
++{
++	raw_pagefault_enable();
++}
++#else
++extern void pagefault_disable(void);
++extern void pagefault_enable(void);
++#endif
++
+ #ifndef ARCH_HAS_NOCACHE_UACCESS
+ 
+ static inline unsigned long __copy_from_user_inatomic_nocache(void *to,
+@@ -76,9 +86,9 @@ static inline unsigned long __copy_from_user_nocache(void *to,
+ 		mm_segment_t old_fs = get_fs();		\
+ 							\
+ 		set_fs(KERNEL_DS);			\
+-		pagefault_disable();			\
++		raw_pagefault_disable();		\
+ 		ret = __copy_from_user_inatomic(&(retval), (__force typeof(retval) __user *)(addr), sizeof(retval));		\
+-		pagefault_enable();			\
++		raw_pagefault_enable();			\
+ 		set_fs(old_fs);				\
+ 		ret;					\
+ 	})
+diff --git a/include/linux/uprobes.h b/include/linux/uprobes.h
+index 60beb5dc7977..f5a644c649b4 100644
+--- a/include/linux/uprobes.h
++++ b/include/linux/uprobes.h
+@@ -27,6 +27,7 @@
+ #include <linux/errno.h>
+ #include <linux/rbtree.h>
+ #include <linux/types.h>
++#include <linux/wait.h>
+ 
+ struct vm_area_struct;
+ struct mm_struct;
+diff --git a/include/linux/vmstat.h b/include/linux/vmstat.h
+index 82e7db7f7100..3feaf770a8bd 100644
+--- a/include/linux/vmstat.h
++++ b/include/linux/vmstat.h
+@@ -33,7 +33,9 @@ DECLARE_PER_CPU(struct vm_event_state, vm_event_states);
+  */
+ static inline void __count_vm_event(enum vm_event_item item)
+ {
++	preempt_disable_rt();
+ 	raw_cpu_inc(vm_event_states.event[item]);
++	preempt_enable_rt();
+ }
+ 
+ static inline void count_vm_event(enum vm_event_item item)
+@@ -43,7 +45,9 @@ static inline void count_vm_event(enum vm_event_item item)
+ 
+ static inline void __count_vm_events(enum vm_event_item item, long delta)
+ {
++	preempt_disable_rt();
+ 	raw_cpu_add(vm_event_states.event[item], delta);
++	preempt_enable_rt();
+ }
+ 
+ static inline void count_vm_events(enum vm_event_item item, long delta)
+diff --git a/include/linux/wait-simple.h b/include/linux/wait-simple.h
+new file mode 100644
+index 000000000000..f86bca2c41d5
+--- /dev/null
++++ b/include/linux/wait-simple.h
+@@ -0,0 +1,207 @@
++#ifndef _LINUX_WAIT_SIMPLE_H
++#define _LINUX_WAIT_SIMPLE_H
++
++#include <linux/spinlock.h>
++#include <linux/list.h>
++
++#include <asm/current.h>
++
++struct swaiter {
++	struct task_struct	*task;
++	struct list_head	node;
++};
++
++#define DEFINE_SWAITER(name)					\
++	struct swaiter name = {					\
++		.task	= current,				\
++		.node	= LIST_HEAD_INIT((name).node),		\
++	}
++
++struct swait_head {
++	raw_spinlock_t		lock;
++	struct list_head	list;
++};
++
++#define SWAIT_HEAD_INITIALIZER(name) {				\
++		.lock	= __RAW_SPIN_LOCK_UNLOCKED(name.lock),	\
++		.list	= LIST_HEAD_INIT((name).list),		\
++	}
++
++#define DEFINE_SWAIT_HEAD(name)					\
++	struct swait_head name = SWAIT_HEAD_INITIALIZER(name)
++
++extern void __init_swait_head(struct swait_head *h, struct lock_class_key *key);
++
++#define init_swait_head(swh)					\
++	do {							\
++		static struct lock_class_key __key;		\
++								\
++		__init_swait_head((swh), &__key);		\
++	} while (0)
++
++/*
++ * Waiter functions
++ */
++extern void swait_prepare_locked(struct swait_head *head, struct swaiter *w);
++extern void swait_prepare(struct swait_head *head, struct swaiter *w, int state);
++extern void swait_finish_locked(struct swait_head *head, struct swaiter *w);
++extern void swait_finish(struct swait_head *head, struct swaiter *w);
++
++/* Check whether a head has waiters enqueued */
++static inline bool swaitqueue_active(struct swait_head *h)
++{
++	/* Make sure the condition is visible before checking list_empty() */
++	smp_mb();
++	return !list_empty(&h->list);
++}
++
++/*
++ * Wakeup functions
++ */
++extern unsigned int __swait_wake(struct swait_head *head, unsigned int state, unsigned int num);
++extern unsigned int __swait_wake_locked(struct swait_head *head, unsigned int state, unsigned int num);
++
++#define swait_wake(head)			__swait_wake(head, TASK_NORMAL, 1)
++#define swait_wake_interruptible(head)		__swait_wake(head, TASK_INTERRUPTIBLE, 1)
++#define swait_wake_all(head)			__swait_wake(head, TASK_NORMAL, 0)
++#define swait_wake_all_interruptible(head)	__swait_wake(head, TASK_INTERRUPTIBLE, 0)
++
++/*
++ * Event API
++ */
++#define __swait_event(wq, condition)					\
++do {									\
++	DEFINE_SWAITER(__wait);						\
++									\
++	for (;;) {							\
++		swait_prepare(&wq, &__wait, TASK_UNINTERRUPTIBLE);	\
++		if (condition)						\
++			break;						\
++		schedule();						\
++	}								\
++	swait_finish(&wq, &__wait);					\
++} while (0)
++
++/**
++ * swait_event - sleep until a condition gets true
++ * @wq: the waitqueue to wait on
++ * @condition: a C expression for the event to wait for
++ *
++ * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
++ * @condition evaluates to true. The @condition is checked each time
++ * the waitqueue @wq is woken up.
++ *
++ * wake_up() has to be called after changing any variable that could
++ * change the result of the wait condition.
++ */
++#define swait_event(wq, condition)					\
++do {									\
++	if (condition)							\
++		break;							\
++	__swait_event(wq, condition);					\
++} while (0)
++
++#define __swait_event_interruptible(wq, condition, ret)			\
++do {									\
++	DEFINE_SWAITER(__wait);						\
++									\
++	for (;;) {							\
++		swait_prepare(&wq, &__wait, TASK_INTERRUPTIBLE);	\
++		if (condition)						\
++			break;						\
++		if (signal_pending(current)) {				\
++			ret = -ERESTARTSYS;				\
++			break;						\
++		}							\
++		schedule();						\
++	}								\
++	swait_finish(&wq, &__wait);					\
++} while (0)
++
++#define __swait_event_interruptible_timeout(wq, condition, ret)		\
++do {									\
++	DEFINE_SWAITER(__wait);						\
++									\
++	for (;;) {							\
++		swait_prepare(&wq, &__wait, TASK_INTERRUPTIBLE);	\
++		if (condition)						\
++			break;						\
++		if (signal_pending(current)) {				\
++			ret = -ERESTARTSYS;				\
++			break;						\
++		}							\
++		ret = schedule_timeout(ret);				\
++		if (!ret)						\
++			break;						\
++	}								\
++	swait_finish(&wq, &__wait);					\
++} while (0)
++
++/**
++ * swait_event_interruptible - sleep until a condition gets true
++ * @wq: the waitqueue to wait on
++ * @condition: a C expression for the event to wait for
++ *
++ * The process is put to sleep (TASK_INTERRUPTIBLE) until the
++ * @condition evaluates to true. The @condition is checked each time
++ * the waitqueue @wq is woken up.
++ *
++ * wake_up() has to be called after changing any variable that could
++ * change the result of the wait condition.
++ */
++#define swait_event_interruptible(wq, condition)			\
++({									\
++	int __ret = 0;							\
++	if (!(condition))						\
++		__swait_event_interruptible(wq, condition, __ret);	\
++	__ret;								\
++})
++
++#define swait_event_interruptible_timeout(wq, condition, timeout)	\
++({									\
++	int __ret = timeout;						\
++	if (!(condition))						\
++		__swait_event_interruptible_timeout(wq, condition, __ret);	\
++	__ret;								\
++})
++
++#define __swait_event_timeout(wq, condition, ret)			\
++do {									\
++	DEFINE_SWAITER(__wait);						\
++									\
++	for (;;) {							\
++		swait_prepare(&wq, &__wait, TASK_UNINTERRUPTIBLE);	\
++		if (condition)						\
++			break;						\
++		ret = schedule_timeout(ret);				\
++		if (!ret)						\
++			break;						\
++	}								\
++	swait_finish(&wq, &__wait);					\
++} while (0)
++
++/**
++ * swait_event_timeout - sleep until a condition gets true or a timeout elapses
++ * @wq: the waitqueue to wait on
++ * @condition: a C expression for the event to wait for
++ * @timeout: timeout, in jiffies
++ *
++ * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
++ * @condition evaluates to true. The @condition is checked each time
++ * the waitqueue @wq is woken up.
++ *
++ * wake_up() has to be called after changing any variable that could
++ * change the result of the wait condition.
++ *
++ * The function returns 0 if the @timeout elapsed, and the remaining
++ * jiffies if the condition evaluated to true before the timeout elapsed.
++ */
++#define swait_event_timeout(wq, condition, timeout)			\
++({									\
++	long __ret = timeout;						\
++	if (!(condition))						\
++		__swait_event_timeout(wq, condition, __ret);		\
++	__ret;								\
++})
++
++#endif
+diff --git a/include/linux/wait.h b/include/linux/wait.h
+index fc0e99395fbb..5e04bf44762d 100644
+--- a/include/linux/wait.h
++++ b/include/linux/wait.h
+@@ -8,6 +8,7 @@
+ #include <linux/spinlock.h>
+ #include <asm/current.h>
+ #include <uapi/linux/wait.h>
++#include <linux/atomic.h>
+ 
+ typedef struct __wait_queue wait_queue_t;
+ typedef int (*wait_queue_func_t)(wait_queue_t *wait, unsigned mode, int flags, void *key);
+diff --git a/include/linux/work-simple.h b/include/linux/work-simple.h
+new file mode 100644
+index 000000000000..f175fa9a6016
+--- /dev/null
++++ b/include/linux/work-simple.h
+@@ -0,0 +1,24 @@
++#ifndef _LINUX_SWORK_H
++#define _LINUX_SWORK_H
++
++#include <linux/list.h>
++
++struct swork_event {
++	struct list_head item;
++	unsigned long flags;
++	void (*func)(struct swork_event *);
++};
++
++static inline void INIT_SWORK(struct swork_event *event,
++			      void (*func)(struct swork_event *))
++{
++	event->flags = 0;
++	event->func = func;
++}
++
++bool swork_queue(struct swork_event *sev);
++
++int swork_get(void);
++void swork_put(void);
++
++#endif /* _LINUX_SWORK_H */
+diff --git a/include/net/dst.h b/include/net/dst.h
+index 182b812d45e1..74baade721d6 100644
+--- a/include/net/dst.h
++++ b/include/net/dst.h
+@@ -436,7 +436,7 @@ static inline void dst_confirm(struct dst_entry *dst)
+ static inline int dst_neigh_output(struct dst_entry *dst, struct neighbour *n,
+ 				   struct sk_buff *skb)
+ {
+-	const struct hh_cache *hh;
++	struct hh_cache *hh;
+ 
+ 	if (dst->pending_confirm) {
+ 		unsigned long now = jiffies;
+diff --git a/include/net/neighbour.h b/include/net/neighbour.h
+index f60558d0254c..58f0c09c9a05 100644
+--- a/include/net/neighbour.h
++++ b/include/net/neighbour.h
+@@ -387,7 +387,7 @@ static inline int neigh_hh_bridge(struct hh_cache *hh, struct sk_buff *skb)
+ }
+ #endif
+ 
+-static inline int neigh_hh_output(const struct hh_cache *hh, struct sk_buff *skb)
++static inline int neigh_hh_output(struct hh_cache *hh, struct sk_buff *skb)
+ {
+ 	unsigned int seq;
+ 	int hh_len;
+@@ -442,7 +442,7 @@ struct neighbour_cb {
+ 
+ #define NEIGH_CB(skb)	((struct neighbour_cb *)(skb)->cb)
+ 
+-static inline void neigh_ha_snapshot(char *dst, const struct neighbour *n,
++static inline void neigh_ha_snapshot(char *dst, struct neighbour *n,
+ 				     const struct net_device *dev)
+ {
+ 	unsigned int seq;
+diff --git a/include/net/netns/ipv4.h b/include/net/netns/ipv4.h
+index 0ffef1a38efc..dddc3a717bcb 100644
+--- a/include/net/netns/ipv4.h
++++ b/include/net/netns/ipv4.h
+@@ -67,6 +67,7 @@ struct netns_ipv4 {
+ 
+ 	int sysctl_icmp_echo_ignore_all;
+ 	int sysctl_icmp_echo_ignore_broadcasts;
++	int sysctl_icmp_echo_sysrq;
+ 	int sysctl_icmp_ignore_bogus_error_responses;
+ 	int sysctl_icmp_ratelimit;
+ 	int sysctl_icmp_ratemask;
+diff --git a/include/trace/events/hist.h b/include/trace/events/hist.h
+new file mode 100644
+index 000000000000..f7710de1b1f3
+--- /dev/null
++++ b/include/trace/events/hist.h
+@@ -0,0 +1,73 @@
++#undef TRACE_SYSTEM
++#define TRACE_SYSTEM hist
++
++#if !defined(_TRACE_HIST_H) || defined(TRACE_HEADER_MULTI_READ)
++#define _TRACE_HIST_H
++
++#include "latency_hist.h"
++#include <linux/tracepoint.h>
++
++#if !defined(CONFIG_PREEMPT_OFF_HIST) && !defined(CONFIG_INTERRUPT_OFF_HIST)
++#define trace_preemptirqsoff_hist(a, b)
++#define trace_preemptirqsoff_hist_rcuidle(a, b)
++#else
++TRACE_EVENT(preemptirqsoff_hist,
++
++	TP_PROTO(int reason, int starthist),
++
++	TP_ARGS(reason, starthist),
++
++	TP_STRUCT__entry(
++		__field(int,	reason)
++		__field(int,	starthist)
++	),
++
++	TP_fast_assign(
++		__entry->reason		= reason;
++		__entry->starthist	= starthist;
++	),
++
++	TP_printk("reason=%s starthist=%s", getaction(__entry->reason),
++		  __entry->starthist ? "start" : "stop")
++);
++#endif
++
++#ifndef CONFIG_MISSED_TIMER_OFFSETS_HIST
++#define trace_hrtimer_interrupt(a, b, c, d)
++#else
++TRACE_EVENT(hrtimer_interrupt,
++
++	TP_PROTO(int cpu, long long offset, struct task_struct *curr,
++		struct task_struct *task),
++
++	TP_ARGS(cpu, offset, curr, task),
++
++	TP_STRUCT__entry(
++		__field(int,		cpu)
++		__field(long long,	offset)
++		__array(char,		ccomm,	TASK_COMM_LEN)
++		__field(int,		cprio)
++		__array(char,		tcomm,	TASK_COMM_LEN)
++		__field(int,		tprio)
++	),
++
++	TP_fast_assign(
++		__entry->cpu	= cpu;
++		__entry->offset	= offset;
++		memcpy(__entry->ccomm, curr->comm, TASK_COMM_LEN);
++		__entry->cprio  = curr->prio;
++		memcpy(__entry->tcomm, task != NULL ? task->comm : "<none>",
++			task != NULL ? TASK_COMM_LEN : 7);
++		__entry->tprio  = task != NULL ? task->prio : -1;
++	),
++
++	TP_printk("cpu=%d offset=%lld curr=%s[%d] thread=%s[%d]",
++		__entry->cpu, __entry->offset, __entry->ccomm,
++		__entry->cprio, __entry->tcomm, __entry->tprio)
++);
++#endif
++
++#endif /* _TRACE_HIST_H */
++
++/* This part must be outside protection */
++#include <trace/define_trace.h>
+diff --git a/include/trace/events/latency_hist.h b/include/trace/events/latency_hist.h
+new file mode 100644
+index 000000000000..d3f2fbd560b1
+--- /dev/null
++++ b/include/trace/events/latency_hist.h
+@@ -0,0 +1,29 @@
++#ifndef _LATENCY_HIST_H
++#define _LATENCY_HIST_H
++
++enum hist_action {
++	IRQS_ON,
++	PREEMPT_ON,
++	TRACE_STOP,
++	IRQS_OFF,
++	PREEMPT_OFF,
++	TRACE_START,
++};
++
++static char *actions[] = {
++	"IRQS_ON",
++	"PREEMPT_ON",
++	"TRACE_STOP",
++	"IRQS_OFF",
++	"PREEMPT_OFF",
++	"TRACE_START",
++};
++
++static inline char *getaction(int action)
++{
++	if (action >= 0 && action <= sizeof(actions)/sizeof(actions[0]))
++		return actions[action];
++	return "unknown";
++}
++
++#endif /* _LATENCY_HIST_H */
+diff --git a/include/trace/events/sched.h b/include/trace/events/sched.h
+index a7d67bc14906..09f27eb85ef8 100644
+--- a/include/trace/events/sched.h
++++ b/include/trace/events/sched.h
+@@ -55,9 +55,9 @@ TRACE_EVENT(sched_kthread_stop_ret,
+  */
+ DECLARE_EVENT_CLASS(sched_wakeup_template,
+ 
+-	TP_PROTO(struct task_struct *p, int success),
++	TP_PROTO(struct task_struct *p),
+ 
+-	TP_ARGS(__perf_task(p), success),
++	TP_ARGS(__perf_task(p)),
+ 
+ 	TP_STRUCT__entry(
+ 		__array(	char,	comm,	TASK_COMM_LEN	)
+@@ -71,25 +71,37 @@ DECLARE_EVENT_CLASS(sched_wakeup_template,
+ 		memcpy(__entry->comm, p->comm, TASK_COMM_LEN);
+ 		__entry->pid		= p->pid;
+ 		__entry->prio		= p->prio;
+-		__entry->success	= success;
++		__entry->success	= 1; /* rudiment, kill when possible */
+ 		__entry->target_cpu	= task_cpu(p);
+ 	),
+ 
+-	TP_printk("comm=%s pid=%d prio=%d success=%d target_cpu=%03d",
++	TP_printk("comm=%s pid=%d prio=%d target_cpu=%03d",
+ 		  __entry->comm, __entry->pid, __entry->prio,
+-		  __entry->success, __entry->target_cpu)
++		  __entry->target_cpu)
+ );
+ 
++/*
++ * Tracepoint called when waking a task; this tracepoint is guaranteed to be
++ * called from the waking context.
++ */
++DEFINE_EVENT(sched_wakeup_template, sched_waking,
++	     TP_PROTO(struct task_struct *p),
++	     TP_ARGS(p));
++
++/*
++ * Tracepoint called when the task is actually woken; p->state == TASK_RUNNNG.
++ * It it not always called from the waking context.
++ */
+ DEFINE_EVENT(sched_wakeup_template, sched_wakeup,
+-	     TP_PROTO(struct task_struct *p, int success),
+-	     TP_ARGS(p, success));
++	     TP_PROTO(struct task_struct *p),
++	     TP_ARGS(p));
+ 
+ /*
+  * Tracepoint for waking up a new task:
+  */
+ DEFINE_EVENT(sched_wakeup_template, sched_wakeup_new,
+-	     TP_PROTO(struct task_struct *p, int success),
+-	     TP_ARGS(p, success));
++	     TP_PROTO(struct task_struct *p),
++	     TP_ARGS(p));
+ 
+ #ifdef CREATE_TRACE_POINTS
+ static inline long __trace_sched_switch_state(struct task_struct *p)
+diff --git a/init/Kconfig b/init/Kconfig
+index 2081a4d3d917..c164babf84b0 100644
+--- a/init/Kconfig
++++ b/init/Kconfig
+@@ -635,7 +635,7 @@ config RCU_FANOUT_EXACT
+ 
+ config RCU_FAST_NO_HZ
+ 	bool "Accelerate last non-dyntick-idle CPU's grace periods"
+-	depends on NO_HZ_COMMON && SMP
++	depends on NO_HZ_COMMON && SMP && !PREEMPT_RT_FULL
+ 	default n
+ 	help
+ 	  This option permits CPUs to enter dynticks-idle state even if
+@@ -662,7 +662,7 @@ config TREE_RCU_TRACE
+ config RCU_BOOST
+ 	bool "Enable RCU priority boosting"
+ 	depends on RT_MUTEXES && PREEMPT_RCU
+-	default n
++	default y if PREEMPT_RT_FULL
+ 	help
+ 	  This option boosts the priority of preempted RCU readers that
+ 	  block the current preemptible RCU grace period for too long.
+@@ -1106,6 +1106,7 @@ config CFS_BANDWIDTH
+ config RT_GROUP_SCHED
+ 	bool "Group scheduling for SCHED_RR/FIFO"
+ 	depends on CGROUP_SCHED
++	depends on !PREEMPT_RT_FULL
+ 	default n
+ 	help
+ 	  This feature lets you explicitly allocate real CPU bandwidth
+@@ -1677,6 +1678,7 @@ choice
+ 
+ config SLAB
+ 	bool "SLAB"
++	depends on !PREEMPT_RT_FULL
+ 	help
+ 	  The regular slab allocator that is established and known to work
+ 	  well in all environments. It organizes cache hot objects in
+@@ -1695,6 +1697,7 @@ config SLUB
+ config SLOB
+ 	depends on EXPERT
+ 	bool "SLOB (Simple Allocator)"
++	depends on !PREEMPT_RT_FULL
+ 	help
+ 	   SLOB replaces the stock allocator with a drastically simpler
+ 	   allocator. SLOB is generally more space efficient but
+diff --git a/init/Makefile b/init/Makefile
+index 7bc47ee31c36..88cf473554e0 100644
+--- a/init/Makefile
++++ b/init/Makefile
+@@ -33,4 +33,4 @@ silent_chk_compile.h = :
+ include/generated/compile.h: FORCE
+ 	@$($(quiet)chk_compile.h)
+ 	$(Q)$(CONFIG_SHELL) $(srctree)/scripts/mkcompile_h $@ \
+-	"$(UTS_MACHINE)" "$(CONFIG_SMP)" "$(CONFIG_PREEMPT)" "$(CC) $(KBUILD_CFLAGS)"
++	"$(UTS_MACHINE)" "$(CONFIG_SMP)" "$(CONFIG_PREEMPT)" "$(CONFIG_PREEMPT_RT_FULL)" "$(CC) $(KBUILD_CFLAGS)"
+diff --git a/init/main.c b/init/main.c
+index 32940a68ea48..0f0444b6e0b1 100644
+--- a/init/main.c
++++ b/init/main.c
+@@ -533,6 +533,7 @@ asmlinkage __visible void __init start_kernel(void)
+ 	setup_command_line(command_line);
+ 	setup_nr_cpu_ids();
+ 	setup_per_cpu_areas();
++	softirq_early_init();
+ 	smp_prepare_boot_cpu();	/* arch-specific boot-cpu hooks */
+ 
+ 	build_all_zonelists(NULL, NULL);
+diff --git a/ipc/mqueue.c b/ipc/mqueue.c
+index ebe44a53fe5a..8f6b9068875f 100644
+--- a/ipc/mqueue.c
++++ b/ipc/mqueue.c
+@@ -47,8 +47,7 @@
+ #define RECV		1
+ 
+ #define STATE_NONE	0
+-#define STATE_PENDING	1
+-#define STATE_READY	2
++#define STATE_READY	1
+ 
+ struct posix_msg_tree_node {
+ 	struct rb_node		rb_node;
+@@ -568,15 +567,12 @@ static int wq_sleep(struct mqueue_inode_info *info, int sr,
+ 	wq_add(info, sr, ewp);
+ 
+ 	for (;;) {
+-		set_current_state(TASK_INTERRUPTIBLE);
++		__set_current_state(TASK_INTERRUPTIBLE);
+ 
+ 		spin_unlock(&info->lock);
+ 		time = schedule_hrtimeout_range_clock(timeout, 0,
+ 			HRTIMER_MODE_ABS, CLOCK_REALTIME);
+ 
+-		while (ewp->state == STATE_PENDING)
+-			cpu_relax();
+-
+ 		if (ewp->state == STATE_READY) {
+ 			retval = 0;
+ 			goto out;
+@@ -904,11 +900,15 @@ out_name:
+  * list of waiting receivers. A sender checks that list before adding the new
+  * message into the message array. If there is a waiting receiver, then it
+  * bypasses the message array and directly hands the message over to the
+- * receiver.
+- * The receiver accepts the message and returns without grabbing the queue
+- * spinlock. Therefore an intermediate STATE_PENDING state and memory barriers
+- * are necessary. The same algorithm is used for sysv semaphores, see
+- * ipc/sem.c for more details.
++ * receiver. The receiver accepts the message and returns without grabbing the
++ * queue spinlock:
++ *
++ * - Set pointer to message.
++ * - Queue the receiver task for later wakeup (without the info->lock).
++ * - Update its state to STATE_READY. Now the receiver can continue.
++ * - Wake up the process after the lock is dropped. Should the process wake up
++ *   before this wakeup (due to a timeout or a signal) it will either see
++ *   STATE_READY and continue or acquire the lock to check the state again.
+  *
+  * The same algorithm is used for senders.
+  */
+@@ -916,21 +916,34 @@ out_name:
+ /* pipelined_send() - send a message directly to the task waiting in
+  * sys_mq_timedreceive() (without inserting message into a queue).
+  */
+-static inline void pipelined_send(struct mqueue_inode_info *info,
++static inline void pipelined_send(struct wake_q_head *wake_q,
++				  struct mqueue_inode_info *info,
+ 				  struct msg_msg *message,
+ 				  struct ext_wait_queue *receiver)
+ {
++	/*
++	 * Keep them in one critical section for PREEMPT_RT:
++	 */
++	preempt_disable_rt();
+ 	receiver->msg = message;
+ 	list_del(&receiver->list);
+-	receiver->state = STATE_PENDING;
+-	wake_up_process(receiver->task);
+-	smp_wmb();
++	wake_q_add(wake_q, receiver->task);
++	/*
++	 * Rely on the implicit cmpxchg barrier from wake_q_add such
++	 * that we can ensure that updating receiver->state is the last
++	 * write operation: As once set, the receiver can continue,
++	 * and if we don't have the reference count from the wake_q,
++	 * yet, at that point we can later have a use-after-free
++	 * condition and bogus wakeup.
++	 */
+ 	receiver->state = STATE_READY;
++	preempt_enable_rt();
+ }
+ 
+ /* pipelined_receive() - if there is task waiting in sys_mq_timedsend()
+  * gets its message and put to the queue (we have one free place for sure). */
+-static inline void pipelined_receive(struct mqueue_inode_info *info)
++static inline void pipelined_receive(struct wake_q_head *wake_q,
++				     struct mqueue_inode_info *info)
+ {
+ 	struct ext_wait_queue *sender = wq_get_first_waiter(info, SEND);
+ 
+@@ -939,13 +952,16 @@ static inline void pipelined_receive(struct mqueue_inode_info *info)
+ 		wake_up_interruptible(&info->wait_q);
+ 		return;
+ 	}
+-	if (msg_insert(sender->msg, info))
+-		return;
+-	list_del(&sender->list);
+-	sender->state = STATE_PENDING;
+-	wake_up_process(sender->task);
+-	smp_wmb();
+-	sender->state = STATE_READY;
++	/*
++	 * Keep them in one critical section for PREEMPT_RT:
++	 */
++	preempt_disable_rt();
++	if (!msg_insert(sender->msg, info)) {
++		list_del(&sender->list);
++		wake_q_add(wake_q, sender->task);
++		sender->state = STATE_READY;
++	}
++	preempt_enable_rt();
+ }
+ 
+ SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,
+@@ -962,6 +978,7 @@ SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,
+ 	struct timespec ts;
+ 	struct posix_msg_tree_node *new_leaf = NULL;
+ 	int ret = 0;
++	WAKE_Q(wake_q);
+ 
+ 	if (u_abs_timeout) {
+ 		int res = prepare_timeout(u_abs_timeout, &expires, &ts);
+@@ -1045,7 +1062,7 @@ SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,
+ 	} else {
+ 		receiver = wq_get_first_waiter(info, RECV);
+ 		if (receiver) {
+-			pipelined_send(info, msg_ptr, receiver);
++			pipelined_send(&wake_q, info, msg_ptr, receiver);
+ 		} else {
+ 			/* adds message to the queue */
+ 			ret = msg_insert(msg_ptr, info);
+@@ -1058,6 +1075,7 @@ SYSCALL_DEFINE5(mq_timedsend, mqd_t, mqdes, const char __user *, u_msg_ptr,
+ 	}
+ out_unlock:
+ 	spin_unlock(&info->lock);
++	wake_up_q(&wake_q);
+ out_free:
+ 	if (ret)
+ 		free_msg(msg_ptr);
+@@ -1144,14 +1162,17 @@ SYSCALL_DEFINE5(mq_timedreceive, mqd_t, mqdes, char __user *, u_msg_ptr,
+ 			msg_ptr = wait.msg;
+ 		}
+ 	} else {
++		WAKE_Q(wake_q);
++
+ 		msg_ptr = msg_get(info);
+ 
+ 		inode->i_atime = inode->i_mtime = inode->i_ctime =
+ 				CURRENT_TIME;
+ 
+ 		/* There is now free space in queue. */
+-		pipelined_receive(info);
++		pipelined_receive(&wake_q, info);
+ 		spin_unlock(&info->lock);
++		wake_up_q(&wake_q);
+ 		ret = 0;
+ 	}
+ 	if (ret == 0) {
+diff --git a/ipc/msg.c b/ipc/msg.c
+index 02e72d3db498..2381f5f20a7f 100644
+--- a/ipc/msg.c
++++ b/ipc/msg.c
+@@ -188,6 +188,12 @@ static void expunge_all(struct msg_queue *msq, int res)
+ 	struct msg_receiver *msr, *t;
+ 
+ 	list_for_each_entry_safe(msr, t, &msq->q_receivers, r_list) {
++		/*
++		 * Make sure that the wakeup doesnt preempt
++		 * this CPU prematurely. (on PREEMPT_RT)
++		 */
++		preempt_disable_rt();
++
+ 		msr->r_msg = NULL; /* initialize expunge ordering */
+ 		wake_up_process(msr->r_tsk);
+ 		/*
+@@ -198,6 +204,8 @@ static void expunge_all(struct msg_queue *msq, int res)
+ 		 */
+ 		smp_mb();
+ 		msr->r_msg = ERR_PTR(res);
++
++		preempt_enable_rt();
+ 	}
+ }
+ 
+@@ -574,6 +582,11 @@ static inline int pipelined_send(struct msg_queue *msq, struct msg_msg *msg)
+ 		if (testmsg(msg, msr->r_msgtype, msr->r_mode) &&
+ 		    !security_msg_queue_msgrcv(msq, msg, msr->r_tsk,
+ 					       msr->r_msgtype, msr->r_mode)) {
++			/*
++			 * Make sure that the wakeup doesnt preempt
++			 * this CPU prematurely. (on PREEMPT_RT)
++			 */
++			preempt_disable_rt();
+ 
+ 			list_del(&msr->r_list);
+ 			if (msr->r_maxsize < msg->m_ts) {
+@@ -595,12 +608,13 @@ static inline int pipelined_send(struct msg_queue *msq, struct msg_msg *msg)
+ 				 */
+ 				smp_mb();
+ 				msr->r_msg = msg;
++				preempt_enable_rt();
+ 
+ 				return 1;
+ 			}
++			preempt_enable_rt();
+ 		}
+ 	}
+-
+ 	return 0;
+ }
+ 
+diff --git a/ipc/sem.c b/ipc/sem.c
+index e59935d0dfcb..1be83c33b511 100644
+--- a/ipc/sem.c
++++ b/ipc/sem.c
+@@ -689,6 +689,13 @@ undo:
+ static void wake_up_sem_queue_prepare(struct list_head *pt,
+ 				struct sem_queue *q, int error)
+ {
++#ifdef CONFIG_PREEMPT_RT_BASE
++	struct task_struct *p = q->sleeper;
++	get_task_struct(p);
++	q->status = error;
++	wake_up_process(p);
++	put_task_struct(p);
++#else
+ 	if (list_empty(pt)) {
+ 		/*
+ 		 * Hold preempt off so that we don't get preempted and have the
+@@ -700,6 +707,7 @@ static void wake_up_sem_queue_prepare(struct list_head *pt,
+ 	q->pid = error;
+ 
+ 	list_add_tail(&q->list, pt);
++#endif
+ }
+ 
+ /**
+@@ -713,6 +721,7 @@ static void wake_up_sem_queue_prepare(struct list_head *pt,
+  */
+ static void wake_up_sem_queue_do(struct list_head *pt)
+ {
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	struct sem_queue *q, *t;
+ 	int did_something;
+ 
+@@ -725,6 +734,7 @@ static void wake_up_sem_queue_do(struct list_head *pt)
+ 	}
+ 	if (did_something)
+ 		preempt_enable();
++#endif
+ }
+ 
+ static void unlink_queue(struct sem_array *sma, struct sem_queue *q)
+diff --git a/kernel/Kconfig.locks b/kernel/Kconfig.locks
+index 76768ee812b2..68212ae72f3f 100644
+--- a/kernel/Kconfig.locks
++++ b/kernel/Kconfig.locks
+@@ -225,11 +225,11 @@ config ARCH_SUPPORTS_ATOMIC_RMW
+ 
+ config MUTEX_SPIN_ON_OWNER
+ 	def_bool y
+-	depends on SMP && !DEBUG_MUTEXES && ARCH_SUPPORTS_ATOMIC_RMW
++	depends on SMP && !DEBUG_MUTEXES && ARCH_SUPPORTS_ATOMIC_RMW && !PREEMPT_RT_FULL
+ 
+ config RWSEM_SPIN_ON_OWNER
+        def_bool y
+-       depends on SMP && RWSEM_XCHGADD_ALGORITHM && ARCH_SUPPORTS_ATOMIC_RMW
++       depends on SMP && RWSEM_XCHGADD_ALGORITHM && ARCH_SUPPORTS_ATOMIC_RMW && !PREEMPT_RT_FULL
+ 
+ config ARCH_USE_QUEUE_RWLOCK
+ 	bool
+diff --git a/kernel/Kconfig.preempt b/kernel/Kconfig.preempt
+index 3f9c97419f02..11dbe26a8279 100644
+--- a/kernel/Kconfig.preempt
++++ b/kernel/Kconfig.preempt
+@@ -1,3 +1,16 @@
++config PREEMPT
++	bool
++	select PREEMPT_COUNT
++
++config PREEMPT_RT_BASE
++	bool
++	select PREEMPT
++
++config HAVE_PREEMPT_LAZY
++	bool
++
++config PREEMPT_LAZY
++	def_bool y if HAVE_PREEMPT_LAZY && PREEMPT_RT_FULL
+ 
+ choice
+ 	prompt "Preemption Model"
+@@ -33,9 +46,9 @@ config PREEMPT_VOLUNTARY
+ 
+ 	  Select this if you are building a kernel for a desktop system.
+ 
+-config PREEMPT
++config PREEMPT__LL
+ 	bool "Preemptible Kernel (Low-Latency Desktop)"
+-	select PREEMPT_COUNT
++	select PREEMPT
+ 	select UNINLINE_SPIN_UNLOCK if !ARCH_INLINE_SPIN_UNLOCK
+ 	help
+ 	  This option reduces the latency of the kernel by making
+@@ -52,6 +65,22 @@ config PREEMPT
+ 	  embedded system with latency requirements in the milliseconds
+ 	  range.
+ 
++config PREEMPT_RTB
++	bool "Preemptible Kernel (Basic RT)"
++	select PREEMPT_RT_BASE
++	help
++	  This option is basically the same as (Low-Latency Desktop) but
++	  enables changes which are preliminary for the full preemptible
++	  RT kernel.
++
++config PREEMPT_RT_FULL
++	bool "Fully Preemptible Kernel (RT)"
++	depends on IRQ_FORCED_THREADING
++	select PREEMPT_RT_BASE
++	select PREEMPT_RCU
++	help
++	  All and everything
++
+ endchoice
+ 
+ config PREEMPT_COUNT
+diff --git a/kernel/cgroup.c b/kernel/cgroup.c
+index f033fbd94ac3..0979e0a5dd11 100644
+--- a/kernel/cgroup.c
++++ b/kernel/cgroup.c
+@@ -4355,10 +4355,10 @@ static void css_free_rcu_fn(struct rcu_head *rcu_head)
+ 	queue_work(cgroup_destroy_wq, &css->destroy_work);
+ }
+ 
+-static void css_release_work_fn(struct work_struct *work)
++static void css_release_work_fn(struct swork_event *sev)
+ {
+ 	struct cgroup_subsys_state *css =
+-		container_of(work, struct cgroup_subsys_state, destroy_work);
++		container_of(sev, struct cgroup_subsys_state, destroy_swork);
+ 	struct cgroup_subsys *ss = css->ss;
+ 	struct cgroup *cgrp = css->cgroup;
+ 
+@@ -4395,8 +4395,8 @@ static void css_release(struct percpu_ref *ref)
+ 	struct cgroup_subsys_state *css =
+ 		container_of(ref, struct cgroup_subsys_state, refcnt);
+ 
+-	INIT_WORK(&css->destroy_work, css_release_work_fn);
+-	queue_work(cgroup_destroy_wq, &css->destroy_work);
++	INIT_SWORK(&css->destroy_swork, css_release_work_fn);
++	swork_queue(&css->destroy_swork);
+ }
+ 
+ static void init_and_link_css(struct cgroup_subsys_state *css,
+@@ -4994,6 +4994,7 @@ static int __init cgroup_wq_init(void)
+ 	 */
+ 	cgroup_destroy_wq = alloc_workqueue("cgroup_destroy", 0, 1);
+ 	BUG_ON(!cgroup_destroy_wq);
++	BUG_ON(swork_get());
+ 
+ 	/*
+ 	 * Used to destroy pidlists and separate to serve as flush domain.
+diff --git a/kernel/cpu.c b/kernel/cpu.c
+index 90a3d017b90c..27f44cd27d9d 100644
+--- a/kernel/cpu.c
++++ b/kernel/cpu.c
+@@ -86,6 +86,290 @@ static struct {
+ #define cpuhp_lock_acquire()      lock_map_acquire(&cpu_hotplug.dep_map)
+ #define cpuhp_lock_release()      lock_map_release(&cpu_hotplug.dep_map)
+ 
++/**
++ * hotplug_pcp	- per cpu hotplug descriptor
++ * @unplug:	set when pin_current_cpu() needs to sync tasks
++ * @sync_tsk:	the task that waits for tasks to finish pinned sections
++ * @refcount:	counter of tasks in pinned sections
++ * @grab_lock:	set when the tasks entering pinned sections should wait
++ * @synced:	notifier for @sync_tsk to tell cpu_down it's finished
++ * @mutex:	the mutex to make tasks wait (used when @grab_lock is true)
++ * @mutex_init:	zero if the mutex hasn't been initialized yet.
++ *
++ * Although @unplug and @sync_tsk may point to the same task, the @unplug
++ * is used as a flag and still exists after @sync_tsk has exited and
++ * @sync_tsk set to NULL.
++ */
++struct hotplug_pcp {
++	struct task_struct *unplug;
++	struct task_struct *sync_tsk;
++	int refcount;
++	int grab_lock;
++	struct completion synced;
++	struct completion unplug_wait;
++#ifdef CONFIG_PREEMPT_RT_FULL
++	/*
++	 * Note, on PREEMPT_RT, the hotplug lock must save the state of
++	 * the task, otherwise the mutex will cause the task to fail
++	 * to sleep when required. (Because it's called from migrate_disable())
++	 *
++	 * The spinlock_t on PREEMPT_RT is a mutex that saves the task's
++	 * state.
++	 */
++	spinlock_t lock;
++#else
++	struct mutex mutex;
++#endif
++	int mutex_init;
++};
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++# define hotplug_lock(hp) rt_spin_lock(&(hp)->lock)
++# define hotplug_unlock(hp) rt_spin_unlock(&(hp)->lock)
++#else
++# define hotplug_lock(hp) mutex_lock(&(hp)->mutex)
++# define hotplug_unlock(hp) mutex_unlock(&(hp)->mutex)
++#endif
++
++static DEFINE_PER_CPU(struct hotplug_pcp, hotplug_pcp);
++
++/**
++ * pin_current_cpu - Prevent the current cpu from being unplugged
++ *
++ * Lightweight version of get_online_cpus() to prevent cpu from being
++ * unplugged when code runs in a migration disabled region.
++ *
++ * Must be called with preemption disabled (preempt_count = 1)!
++ */
++void pin_current_cpu(void)
++{
++	struct hotplug_pcp *hp;
++	int force = 0;
++
++retry:
++	hp = &__get_cpu_var(hotplug_pcp);
++
++	if (!hp->unplug || hp->refcount || force || preempt_count() > 1 ||
++	    hp->unplug == current) {
++		hp->refcount++;
++		return;
++	}
++	if (hp->grab_lock) {
++		preempt_enable();
++		hotplug_lock(hp);
++		hotplug_unlock(hp);
++	} else {
++		preempt_enable();
++		/*
++		 * Try to push this task off of this CPU.
++		 */
++		if (!migrate_me()) {
++			preempt_disable();
++			hp = &__get_cpu_var(hotplug_pcp);
++			if (!hp->grab_lock) {
++				/*
++				 * Just let it continue it's already pinned
++				 * or about to sleep.
++				 */
++				force = 1;
++				goto retry;
++			}
++			preempt_enable();
++		}
++	}
++	preempt_disable();
++	goto retry;
++}
++
++/**
++ * unpin_current_cpu - Allow unplug of current cpu
++ *
++ * Must be called with preemption or interrupts disabled!
++ */
++void unpin_current_cpu(void)
++{
++	struct hotplug_pcp *hp = &__get_cpu_var(hotplug_pcp);
++
++	WARN_ON(hp->refcount <= 0);
++
++	/* This is safe. sync_unplug_thread is pinned to this cpu */
++	if (!--hp->refcount && hp->unplug && hp->unplug != current)
++		wake_up_process(hp->unplug);
++}
++
++static void wait_for_pinned_cpus(struct hotplug_pcp *hp)
++{
++	set_current_state(TASK_UNINTERRUPTIBLE);
++	while (hp->refcount) {
++		schedule_preempt_disabled();
++		set_current_state(TASK_UNINTERRUPTIBLE);
++	}
++}
++
++static int sync_unplug_thread(void *data)
++{
++	struct hotplug_pcp *hp = data;
++
++	wait_for_completion(&hp->unplug_wait);
++	preempt_disable();
++	hp->unplug = current;
++	wait_for_pinned_cpus(hp);
++
++	/*
++	 * This thread will synchronize the cpu_down() with threads
++	 * that have pinned the CPU. When the pinned CPU count reaches
++	 * zero, we inform the cpu_down code to continue to the next step.
++	 */
++	set_current_state(TASK_UNINTERRUPTIBLE);
++	preempt_enable();
++	complete(&hp->synced);
++
++	/*
++	 * If all succeeds, the next step will need tasks to wait till
++	 * the CPU is offline before continuing. To do this, the grab_lock
++	 * is set and tasks going into pin_current_cpu() will block on the
++	 * mutex. But we still need to wait for those that are already in
++	 * pinned CPU sections. If the cpu_down() failed, the kthread_should_stop()
++	 * will kick this thread out.
++	 */
++	while (!hp->grab_lock && !kthread_should_stop()) {
++		schedule();
++		set_current_state(TASK_UNINTERRUPTIBLE);
++	}
++
++	/* Make sure grab_lock is seen before we see a stale completion */
++	smp_mb();
++
++	/*
++	 * Now just before cpu_down() enters stop machine, we need to make
++	 * sure all tasks that are in pinned CPU sections are out, and new
++	 * tasks will now grab the lock, keeping them from entering pinned
++	 * CPU sections.
++	 */
++	if (!kthread_should_stop()) {
++		preempt_disable();
++		wait_for_pinned_cpus(hp);
++		preempt_enable();
++		complete(&hp->synced);
++	}
++
++	set_current_state(TASK_UNINTERRUPTIBLE);
++	while (!kthread_should_stop()) {
++		schedule();
++		set_current_state(TASK_UNINTERRUPTIBLE);
++	}
++	set_current_state(TASK_RUNNING);
++
++	/*
++	 * Force this thread off this CPU as it's going down and
++	 * we don't want any more work on this CPU.
++	 */
++	current->flags &= ~PF_NO_SETAFFINITY;
++	set_cpus_allowed_ptr(current, cpu_present_mask);
++	migrate_me();
++	return 0;
++}
++
++static void __cpu_unplug_sync(struct hotplug_pcp *hp)
++{
++	wake_up_process(hp->sync_tsk);
++	wait_for_completion(&hp->synced);
++}
++
++static void __cpu_unplug_wait(unsigned int cpu)
++{
++	struct hotplug_pcp *hp = &per_cpu(hotplug_pcp, cpu);
++
++	complete(&hp->unplug_wait);
++	wait_for_completion(&hp->synced);
++}
++
++/*
++ * Start the sync_unplug_thread on the target cpu and wait for it to
++ * complete.
++ */
++static int cpu_unplug_begin(unsigned int cpu)
++{
++	struct hotplug_pcp *hp = &per_cpu(hotplug_pcp, cpu);
++	int err;
++
++	/* Protected by cpu_hotplug.lock */
++	if (!hp->mutex_init) {
++#ifdef CONFIG_PREEMPT_RT_FULL
++		spin_lock_init(&hp->lock);
++#else
++		mutex_init(&hp->mutex);
++#endif
++		hp->mutex_init = 1;
++	}
++
++	/* Inform the scheduler to migrate tasks off this CPU */
++	tell_sched_cpu_down_begin(cpu);
++
++	init_completion(&hp->synced);
++	init_completion(&hp->unplug_wait);
++
++	hp->sync_tsk = kthread_create(sync_unplug_thread, hp, "sync_unplug/%d", cpu);
++	if (IS_ERR(hp->sync_tsk)) {
++		err = PTR_ERR(hp->sync_tsk);
++		hp->sync_tsk = NULL;
++		return err;
++	}
++	kthread_bind(hp->sync_tsk, cpu);
++
++	/*
++	 * Wait for tasks to get out of the pinned sections,
++	 * it's still OK if new tasks enter. Some CPU notifiers will
++	 * wait for tasks that are going to enter these sections and
++	 * we must not have them block.
++	 */
++	wake_up_process(hp->sync_tsk);
++	return 0;
++}
++
++static void cpu_unplug_sync(unsigned int cpu)
++{
++	struct hotplug_pcp *hp = &per_cpu(hotplug_pcp, cpu);
++
++	init_completion(&hp->synced);
++	/* The completion needs to be initialzied before setting grab_lock */
++	smp_wmb();
++
++	/* Grab the mutex before setting grab_lock */
++	hotplug_lock(hp);
++	hp->grab_lock = 1;
++
++	/*
++	 * The CPU notifiers have been completed.
++	 * Wait for tasks to get out of pinned CPU sections and have new
++	 * tasks block until the CPU is completely down.
++	 */
++	__cpu_unplug_sync(hp);
++
++	/* All done with the sync thread */
++	kthread_stop(hp->sync_tsk);
++	hp->sync_tsk = NULL;
++}
++
++static void cpu_unplug_done(unsigned int cpu)
++{
++	struct hotplug_pcp *hp = &per_cpu(hotplug_pcp, cpu);
++
++	hp->unplug = NULL;
++	/* Let all tasks know cpu unplug is finished before cleaning up */
++	smp_wmb();
++
++	if (hp->sync_tsk)
++		kthread_stop(hp->sync_tsk);
++
++	if (hp->grab_lock) {
++		hotplug_unlock(hp);
++		/* protected by cpu_hotplug.lock */
++		hp->grab_lock = 0;
++	}
++	tell_sched_cpu_down_done(cpu);
++}
++
+ void get_online_cpus(void)
+ {
+ 	might_sleep();
+@@ -102,6 +386,7 @@ bool try_get_online_cpus(void)
+ {
+ 	if (cpu_hotplug.active_writer == current)
+ 		return true;
++
+ 	if (!mutex_trylock(&cpu_hotplug.lock))
+ 		return false;
+ 	cpuhp_lock_acquire_tryread();
+@@ -349,13 +634,15 @@ static int __ref take_cpu_down(void *_param)
+ /* Requires cpu_add_remove_lock to be held */
+ static int __ref _cpu_down(unsigned int cpu, int tasks_frozen)
+ {
+-	int err, nr_calls = 0;
++	int mycpu, err, nr_calls = 0;
+ 	void *hcpu = (void *)(long)cpu;
+ 	unsigned long mod = tasks_frozen ? CPU_TASKS_FROZEN : 0;
+ 	struct take_cpu_down_param tcd_param = {
+ 		.mod = mod,
+ 		.hcpu = hcpu,
+ 	};
++	cpumask_var_t cpumask;
++	cpumask_var_t cpumask_org;
+ 
+ 	if (num_online_cpus() == 1)
+ 		return -EBUSY;
+@@ -363,7 +650,34 @@ static int __ref _cpu_down(unsigned int cpu, int tasks_frozen)
+ 	if (!cpu_online(cpu))
+ 		return -EINVAL;
+ 
++	/* Move the downtaker off the unplug cpu */
++	if (!alloc_cpumask_var(&cpumask, GFP_KERNEL))
++		return -ENOMEM;
++	if (!alloc_cpumask_var(&cpumask_org, GFP_KERNEL))  {
++		free_cpumask_var(cpumask);
++		return -ENOMEM;
++	}
++
++	cpumask_copy(cpumask_org, tsk_cpus_allowed(current));
++	cpumask_andnot(cpumask, cpu_online_mask, cpumask_of(cpu));
++	set_cpus_allowed_ptr(current, cpumask);
++	free_cpumask_var(cpumask);
++	migrate_disable();
++	mycpu = smp_processor_id();
++	if (mycpu == cpu) {
++		printk(KERN_ERR "Yuck! Still on unplug CPU\n!");
++		migrate_enable();
++		err = -EBUSY;
++		goto restore_cpus;
++	}
++	migrate_enable();
++
+ 	cpu_hotplug_begin();
++	err = cpu_unplug_begin(cpu);
++	if (err) {
++		printk("cpu_unplug_begin(%d) failed\n", cpu);
++		goto out_cancel;
++	}
+ 
+ 	err = __cpu_notify(CPU_DOWN_PREPARE | mod, hcpu, -1, &nr_calls);
+ 	if (err) {
+@@ -389,8 +703,12 @@ static int __ref _cpu_down(unsigned int cpu, int tasks_frozen)
+ #endif
+ 	synchronize_rcu();
+ 
++	__cpu_unplug_wait(cpu);
+ 	smpboot_park_threads(cpu);
+ 
++	/* Notifiers are done. Don't let any more tasks pin this CPU. */
++	cpu_unplug_sync(cpu);
++
+ 	/*
+ 	 * So now all preempt/rcu users must observe !cpu_active().
+ 	 */
+@@ -423,9 +741,14 @@ static int __ref _cpu_down(unsigned int cpu, int tasks_frozen)
+ 	check_for_tasks(cpu);
+ 
+ out_release:
++	cpu_unplug_done(cpu);
++out_cancel:
+ 	cpu_hotplug_done();
+ 	if (!err)
+ 		cpu_notify_nofail(CPU_POST_DEAD | mod, hcpu);
++restore_cpus:
++	set_cpus_allowed_ptr(current, cpumask_org);
++	free_cpumask_var(cpumask_org);
+ 	return err;
+ }
+ 
+diff --git a/kernel/debug/kdb/kdb_io.c b/kernel/debug/kdb/kdb_io.c
+index 7c70812caea5..3f8e05ff4cfd 100644
+--- a/kernel/debug/kdb/kdb_io.c
++++ b/kernel/debug/kdb/kdb_io.c
+@@ -554,7 +554,6 @@ int vkdb_printf(const char *fmt, va_list ap)
+ 	int linecount;
+ 	int colcount;
+ 	int logging, saved_loglevel = 0;
+-	int saved_trap_printk;
+ 	int got_printf_lock = 0;
+ 	int retlen = 0;
+ 	int fnd, len;
+@@ -565,8 +564,6 @@ int vkdb_printf(const char *fmt, va_list ap)
+ 	unsigned long uninitialized_var(flags);
+ 
+ 	preempt_disable();
+-	saved_trap_printk = kdb_trap_printk;
+-	kdb_trap_printk = 0;
+ 
+ 	/* Serialize kdb_printf if multiple cpus try to write at once.
+ 	 * But if any cpu goes recursive in kdb, just print the output,
+@@ -833,7 +830,6 @@ kdb_print_out:
+ 	} else {
+ 		__release(kdb_printf_lock);
+ 	}
+-	kdb_trap_printk = saved_trap_printk;
+ 	preempt_enable();
+ 	return retlen;
+ }
+@@ -843,9 +839,11 @@ int kdb_printf(const char *fmt, ...)
+ 	va_list ap;
+ 	int r;
+ 
++	kdb_trap_printk++;
+ 	va_start(ap, fmt);
+ 	r = vkdb_printf(fmt, ap);
+ 	va_end(ap);
++	kdb_trap_printk--;
+ 
+ 	return r;
+ }
+diff --git a/kernel/events/core.c b/kernel/events/core.c
+index 9b12efcefdf7..3d3702a309cc 100644
+--- a/kernel/events/core.c
++++ b/kernel/events/core.c
+@@ -6534,6 +6534,7 @@ static void perf_swevent_init_hrtimer(struct perf_event *event)
+ 
+ 	hrtimer_init(&hwc->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+ 	hwc->hrtimer.function = perf_swevent_hrtimer;
++	hwc->hrtimer.irqsafe = 1;
+ 
+ 	/*
+ 	 * Since hrtimers have a fixed rate, we can do a static freq->period
+diff --git a/kernel/exit.c b/kernel/exit.c
+index 654021d9f1b3..a9bda5938e7f 100644
+--- a/kernel/exit.c
++++ b/kernel/exit.c
+@@ -147,7 +147,7 @@ static void __exit_signal(struct task_struct *tsk)
+ 	 * Do this under ->siglock, we can race with another thread
+ 	 * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.
+ 	 */
+-	flush_sigqueue(&tsk->pending);
++	flush_task_sigqueue(tsk);
+ 	tsk->sighand = NULL;
+ 	spin_unlock(&sighand->siglock);
+ 
+diff --git a/kernel/fork.c b/kernel/fork.c
+index f45e647f8e55..b24572489168 100644
+--- a/kernel/fork.c
++++ b/kernel/fork.c
+@@ -97,7 +97,7 @@ int max_threads;		/* tunable limit on nr_threads */
+ 
+ DEFINE_PER_CPU(unsigned long, process_counts) = 0;
+ 
+-__cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
++DEFINE_RWLOCK(tasklist_lock);  /* outer */
+ 
+ #ifdef CONFIG_PROVE_RCU
+ int lockdep_tasklist_lock_is_held(void)
+@@ -233,7 +233,9 @@ static inline void put_signal_struct(struct signal_struct *sig)
+ 	if (atomic_dec_and_test(&sig->sigcnt))
+ 		free_signal_struct(sig);
+ }
+-
++#ifdef CONFIG_PREEMPT_RT_BASE
++static
++#endif
+ void __put_task_struct(struct task_struct *tsk)
+ {
+ 	WARN_ON(!tsk->exit_state);
+@@ -249,7 +251,18 @@ void __put_task_struct(struct task_struct *tsk)
+ 	if (!profile_handoff_task(tsk))
+ 		free_task(tsk);
+ }
++#ifndef CONFIG_PREEMPT_RT_BASE
+ EXPORT_SYMBOL_GPL(__put_task_struct);
++#else
++void __put_task_struct_cb(struct rcu_head *rhp)
++{
++	struct task_struct *tsk = container_of(rhp, struct task_struct, put_rcu);
++
++	__put_task_struct(tsk);
++
++}
++EXPORT_SYMBOL_GPL(__put_task_struct_cb);
++#endif
+ 
+ void __init __weak arch_task_cache_init(void) { }
+ 
+@@ -351,6 +364,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
+ #endif
+ 	tsk->splice_pipe = NULL;
+ 	tsk->task_frag.page = NULL;
++	tsk->wake_q.next = NULL;
+ 
+ 	account_kernel_stack(ti, 1);
+ 
+@@ -643,6 +657,19 @@ void __mmdrop(struct mm_struct *mm)
+ }
+ EXPORT_SYMBOL_GPL(__mmdrop);
+ 
++#ifdef CONFIG_PREEMPT_RT_BASE
++/*
++ * RCU callback for delayed mm drop. Not strictly rcu, but we don't
++ * want another facility to make this work.
++ */
++void __mmdrop_delayed(struct rcu_head *rhp)
++{
++	struct mm_struct *mm = container_of(rhp, struct mm_struct, delayed_drop);
++
++	__mmdrop(mm);
++}
++#endif
++
+ /*
+  * Decrement the use count and release all resources for an mm.
+  */
+@@ -1157,6 +1184,9 @@ static void rt_mutex_init_task(struct task_struct *p)
+  */
+ static void posix_cpu_timers_init(struct task_struct *tsk)
+ {
++#ifdef CONFIG_PREEMPT_RT_BASE
++	tsk->posix_timer_list = NULL;
++#endif
+ 	tsk->cputime_expires.prof_exp = 0;
+ 	tsk->cputime_expires.virt_exp = 0;
+ 	tsk->cputime_expires.sched_exp = 0;
+@@ -1284,6 +1314,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
+ 	spin_lock_init(&p->alloc_lock);
+ 
+ 	init_sigpending(&p->pending);
++	p->sigqueue_cache = NULL;
+ 
+ 	p->utime = p->stime = p->gtime = 0;
+ 	p->utimescaled = p->stimescaled = 0;
+@@ -1291,7 +1322,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
+ 	p->prev_cputime.utime = p->prev_cputime.stime = 0;
+ #endif
+ #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
+-	seqlock_init(&p->vtime_seqlock);
++	raw_spin_lock_init(&p->vtime_lock);
++	seqcount_init(&p->vtime_seq);
+ 	p->vtime_snap = 0;
+ 	p->vtime_snap_whence = VTIME_SLEEPING;
+ #endif
+@@ -1342,6 +1374,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
+ 	p->hardirq_context = 0;
+ 	p->softirq_context = 0;
+ #endif
++#ifdef CONFIG_PREEMPT_RT_FULL
++	p->pagefault_disabled = 0;
++#endif
+ #ifdef CONFIG_LOCKDEP
+ 	p->lockdep_depth = 0; /* no locks held yet */
+ 	p->curr_chain_key = 0;
+diff --git a/kernel/futex.c b/kernel/futex.c
+index 54ebb63711f4..55b191359f93 100644
+--- a/kernel/futex.c
++++ b/kernel/futex.c
+@@ -738,7 +738,9 @@ void exit_pi_state_list(struct task_struct *curr)
+ 		 * task still owns the PI-state:
+ 		 */
+ 		if (head->next != next) {
++			raw_spin_unlock_irq(&curr->pi_lock);
+ 			spin_unlock(&hb->lock);
++			raw_spin_lock_irq(&curr->pi_lock);
+ 			continue;
+ 		}
+ 
+@@ -1090,9 +1092,11 @@ static void __unqueue_futex(struct futex_q *q)
+ 
+ /*
+  * The hash bucket lock must be held when this is called.
+- * Afterwards, the futex_q must not be accessed.
++ * Afterwards, the futex_q must not be accessed. Callers
++ * must ensure to later call wake_up_q() for the actual
++ * wakeups to occur.
+  */
+-static void wake_futex(struct futex_q *q)
++static void mark_wake_futex(struct wake_q_head *wake_q, struct futex_q *q)
+ {
+ 	struct task_struct *p = q->task;
+ 
+@@ -1100,14 +1104,10 @@ static void wake_futex(struct futex_q *q)
+ 		return;
+ 
+ 	/*
+-	 * We set q->lock_ptr = NULL _before_ we wake up the task. If
+-	 * a non-futex wake up happens on another CPU then the task
+-	 * might exit and p would dereference a non-existing task
+-	 * struct. Prevent this by holding a reference on p across the
+-	 * wake up.
++	 * Queue the task for later wakeup for after we've released
++	 * the hb->lock. wake_q_add() grabs reference to p.
+ 	 */
+-	get_task_struct(p);
+-
++	wake_q_add(wake_q, p);
+ 	__unqueue_futex(q);
+ 	/*
+ 	 * The waiting task can free the futex_q as soon as
+@@ -1117,9 +1117,6 @@ static void wake_futex(struct futex_q *q)
+ 	 */
+ 	smp_wmb();
+ 	q->lock_ptr = NULL;
+-
+-	wake_up_state(p, TASK_NORMAL);
+-	put_task_struct(p);
+ }
+ 
+ static int wake_futex_pi(u32 __user *uaddr, u32 uval, struct futex_q *this)
+@@ -1227,6 +1224,7 @@ futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)
+ 	struct futex_q *this, *next;
+ 	union futex_key key = FUTEX_KEY_INIT;
+ 	int ret;
++	WAKE_Q(wake_q);
+ 
+ 	if (!bitset)
+ 		return -EINVAL;
+@@ -1254,13 +1252,14 @@ futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)
+ 			if (!(this->bitset & bitset))
+ 				continue;
+ 
+-			wake_futex(this);
++			mark_wake_futex(&wake_q, this);
+ 			if (++ret >= nr_wake)
+ 				break;
+ 		}
+ 	}
+ 
+ 	spin_unlock(&hb->lock);
++	wake_up_q(&wake_q);
+ out_put_key:
+ 	put_futex_key(&key);
+ out:
+@@ -1279,6 +1278,7 @@ futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,
+ 	struct futex_hash_bucket *hb1, *hb2;
+ 	struct futex_q *this, *next;
+ 	int ret, op_ret;
++	WAKE_Q(wake_q);
+ 
+ retry:
+ 	ret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);
+@@ -1330,7 +1330,7 @@ retry_private:
+ 				ret = -EINVAL;
+ 				goto out_unlock;
+ 			}
+-			wake_futex(this);
++			mark_wake_futex(&wake_q, this);
+ 			if (++ret >= nr_wake)
+ 				break;
+ 		}
+@@ -1344,7 +1344,7 @@ retry_private:
+ 					ret = -EINVAL;
+ 					goto out_unlock;
+ 				}
+-				wake_futex(this);
++				mark_wake_futex(&wake_q, this);
+ 				if (++op_ret >= nr_wake2)
+ 					break;
+ 			}
+@@ -1354,6 +1354,7 @@ retry_private:
+ 
+ out_unlock:
+ 	double_unlock_hb(hb1, hb2);
++	wake_up_q(&wake_q);
+ out_put_keys:
+ 	put_futex_key(&key2);
+ out_put_key1:
+@@ -1513,6 +1514,7 @@ static int futex_requeue(u32 __user *uaddr1, unsigned int flags,
+ 	struct futex_pi_state *pi_state = NULL;
+ 	struct futex_hash_bucket *hb1, *hb2;
+ 	struct futex_q *this, *next;
++	WAKE_Q(wake_q);
+ 
+ 	if (requeue_pi) {
+ 		/*
+@@ -1689,7 +1691,7 @@ retry_private:
+ 		 * woken by futex_unlock_pi().
+ 		 */
+ 		if (++task_count <= nr_wake && !requeue_pi) {
+-			wake_futex(this);
++			mark_wake_futex(&wake_q, this);
+ 			continue;
+ 		}
+ 
+@@ -1715,6 +1717,16 @@ retry_private:
+ 				requeue_pi_wake_futex(this, &key2, hb2);
+ 				drop_count++;
+ 				continue;
++			} else if (ret == -EAGAIN) {
++				/*
++				 * Waiter was woken by timeout or
++				 * signal and has set pi_blocked_on to
++				 * PI_WAKEUP_INPROGRESS before we
++				 * tried to enqueue it on the rtmutex.
++				 */
++				this->pi_state = NULL;
++				free_pi_state(pi_state);
++				continue;
+ 			} else if (ret) {
+ 				/* -EDEADLK */
+ 				this->pi_state = NULL;
+@@ -1729,6 +1741,7 @@ retry_private:
+ out_unlock:
+ 	free_pi_state(pi_state);
+ 	double_unlock_hb(hb1, hb2);
++	wake_up_q(&wake_q);
+ 	hb_waiters_dec(hb2);
+ 
+ 	/*
+@@ -2567,7 +2580,7 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
+ {
+ 	struct hrtimer_sleeper timeout, *to = NULL;
+ 	struct rt_mutex_waiter rt_waiter;
+-	struct futex_hash_bucket *hb;
++	struct futex_hash_bucket *hb, *hb2;
+ 	union futex_key key2 = FUTEX_KEY_INIT;
+ 	struct futex_q q = futex_q_init;
+ 	int res, ret;
+@@ -2592,10 +2605,7 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
+ 	 * The waiter is allocated on our stack, manipulated by the requeue
+ 	 * code while we sleep on uaddr.
+ 	 */
+-	debug_rt_mutex_init_waiter(&rt_waiter);
+-	RB_CLEAR_NODE(&rt_waiter.pi_tree_entry);
+-	RB_CLEAR_NODE(&rt_waiter.tree_entry);
+-	rt_waiter.task = NULL;
++	rt_mutex_init_waiter(&rt_waiter, false);
+ 
+ 	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);
+ 	if (unlikely(ret != 0))
+@@ -2626,20 +2636,55 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
+ 	/* Queue the futex_q, drop the hb lock, wait for wakeup. */
+ 	futex_wait_queue_me(hb, &q, to);
+ 
+-	spin_lock(&hb->lock);
+-	ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
+-	spin_unlock(&hb->lock);
+-	if (ret)
+-		goto out_put_keys;
++	/*
++	 * On RT we must avoid races with requeue and trying to block
++	 * on two mutexes (hb->lock and uaddr2's rtmutex) by
++	 * serializing access to pi_blocked_on with pi_lock.
++	 */
++	raw_spin_lock_irq(&current->pi_lock);
++	if (current->pi_blocked_on) {
++		/*
++		 * We have been requeued or are in the process of
++		 * being requeued.
++		 */
++		raw_spin_unlock_irq(&current->pi_lock);
++	} else {
++		/*
++		 * Setting pi_blocked_on to PI_WAKEUP_INPROGRESS
++		 * prevents a concurrent requeue from moving us to the
++		 * uaddr2 rtmutex. After that we can safely acquire
++		 * (and possibly block on) hb->lock.
++		 */
++		current->pi_blocked_on = PI_WAKEUP_INPROGRESS;
++		raw_spin_unlock_irq(&current->pi_lock);
++
++		spin_lock(&hb->lock);
++
++		/*
++		 * Clean up pi_blocked_on. We might leak it otherwise
++		 * when we succeeded with the hb->lock in the fast
++		 * path.
++		 */
++		raw_spin_lock_irq(&current->pi_lock);
++		current->pi_blocked_on = NULL;
++		raw_spin_unlock_irq(&current->pi_lock);
++
++		ret = handle_early_requeue_pi_wakeup(hb, &q, &key2, to);
++		spin_unlock(&hb->lock);
++		if (ret)
++			goto out_put_keys;
++	}
+ 
+ 	/*
+-	 * In order for us to be here, we know our q.key == key2, and since
+-	 * we took the hb->lock above, we also know that futex_requeue() has
+-	 * completed and we no longer have to concern ourselves with a wakeup
+-	 * race with the atomic proxy lock acquisition by the requeue code. The
+-	 * futex_requeue dropped our key1 reference and incremented our key2
+-	 * reference count.
++	 * In order to be here, we have either been requeued, are in
++	 * the process of being requeued, or requeue successfully
++	 * acquired uaddr2 on our behalf.  If pi_blocked_on was
++	 * non-null above, we may be racing with a requeue.  Do not
++	 * rely on q->lock_ptr to be hb2->lock until after blocking on
++	 * hb->lock or hb2->lock. The futex_requeue dropped our key1
++	 * reference and incremented our key2 reference count.
+ 	 */
++	hb2 = hash_futex(&key2);
+ 
+ 	/* Check if the requeue code acquired the second futex for us. */
+ 	if (!q.rt_waiter) {
+@@ -2648,7 +2693,8 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
+ 		 * did a lock-steal - fix up the PI-state in that case.
+ 		 */
+ 		if (q.pi_state && (q.pi_state->owner != current)) {
+-			spin_lock(q.lock_ptr);
++			spin_lock(&hb2->lock);
++			BUG_ON(&hb2->lock != q.lock_ptr);
+ 			ret = fixup_pi_state_owner(uaddr2, &q, current);
+ 			if (ret && rt_mutex_owner(&q.pi_state->pi_mutex) == current)
+ 				rt_mutex_unlock(&q.pi_state->pi_mutex);
+@@ -2657,7 +2703,7 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
+ 			 * the requeue_pi() code acquired for us.
+ 			 */
+ 			free_pi_state(q.pi_state);
+-			spin_unlock(q.lock_ptr);
++			spin_unlock(&hb2->lock);
+ 		}
+ 	} else {
+ 		struct rt_mutex *pi_mutex;
+@@ -2672,7 +2718,8 @@ static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,
+ 		ret = rt_mutex_finish_proxy_lock(pi_mutex, to, &rt_waiter);
+ 		debug_rt_mutex_free_waiter(&rt_waiter);
+ 
+-		spin_lock(q.lock_ptr);
++		spin_lock(&hb2->lock);
++		BUG_ON(&hb2->lock != q.lock_ptr);
+ 		/*
+ 		 * Fixup the pi_state owner and possibly acquire the lock if we
+ 		 * haven't already.
+diff --git a/kernel/irq/handle.c b/kernel/irq/handle.c
+index 635480270858..26a63672c263 100644
+--- a/kernel/irq/handle.c
++++ b/kernel/irq/handle.c
+@@ -133,6 +133,8 @@ void __irq_wake_thread(struct irq_desc *desc, struct irqaction *action)
+ irqreturn_t
+ handle_irq_event_percpu(struct irq_desc *desc, struct irqaction *action)
+ {
++	struct pt_regs *regs = get_irq_regs();
++	u64 ip = regs ? instruction_pointer(regs) : 0;
+ 	irqreturn_t retval = IRQ_NONE;
+ 	unsigned int flags = 0, irq = desc->irq_data.irq;
+ 
+@@ -173,7 +175,11 @@ handle_irq_event_percpu(struct irq_desc *desc, struct irqaction *action)
+ 		action = action->next;
+ 	} while (action);
+ 
+-	add_interrupt_randomness(irq, flags);
++#ifndef CONFIG_PREEMPT_RT_FULL
++	add_interrupt_randomness(irq, flags, ip);
++#else
++	desc->random_ip = ip;
++#endif
+ 
+ 	if (!noirqdebug)
+ 		note_interrupt(irq, desc, retval);
+diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
+index e7ef539c56d9..4bd36ebe5015 100644
+--- a/kernel/irq/manage.c
++++ b/kernel/irq/manage.c
+@@ -22,6 +22,7 @@
+ #include "internals.h"
+ 
+ #ifdef CONFIG_IRQ_FORCED_THREADING
++# ifndef CONFIG_PREEMPT_RT_BASE
+ __read_mostly bool force_irqthreads;
+ 
+ static int __init setup_forced_irqthreads(char *arg)
+@@ -30,6 +31,7 @@ static int __init setup_forced_irqthreads(char *arg)
+ 	return 0;
+ }
+ early_param("threadirqs", setup_forced_irqthreads);
++# endif
+ #endif
+ 
+ static void __synchronize_hardirq(struct irq_desc *desc)
+@@ -173,6 +175,62 @@ static inline void
+ irq_get_pending(struct cpumask *mask, struct irq_desc *desc) { }
+ #endif
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++static void _irq_affinity_notify(struct irq_affinity_notify *notify);
++static struct task_struct *set_affinity_helper;
++static LIST_HEAD(affinity_list);
++static DEFINE_RAW_SPINLOCK(affinity_list_lock);
++
++static int set_affinity_thread(void *unused)
++{
++	while (1) {
++		struct irq_affinity_notify *notify;
++		int empty;
++
++		set_current_state(TASK_INTERRUPTIBLE);
++
++		raw_spin_lock_irq(&affinity_list_lock);
++		empty = list_empty(&affinity_list);
++		raw_spin_unlock_irq(&affinity_list_lock);
++
++		if (empty)
++			schedule();
++		if (kthread_should_stop())
++			break;
++		set_current_state(TASK_RUNNING);
++try_next:
++		notify = NULL;
++
++		raw_spin_lock_irq(&affinity_list_lock);
++		if (!list_empty(&affinity_list)) {
++			notify = list_first_entry(&affinity_list,
++					struct irq_affinity_notify, list);
++			list_del_init(&notify->list);
++		}
++		raw_spin_unlock_irq(&affinity_list_lock);
++
++		if (!notify)
++			continue;
++		_irq_affinity_notify(notify);
++		goto try_next;
++	}
++	return 0;
++}
++
++static void init_helper_thread(void)
++{
++	if (set_affinity_helper)
++		return;
++	set_affinity_helper = kthread_run(set_affinity_thread, NULL,
++			"affinity-cb");
++	WARN_ON(IS_ERR(set_affinity_helper));
++}
++#else
++
++static inline void init_helper_thread(void) { }
++
++#endif
++
+ int irq_do_set_affinity(struct irq_data *data, const struct cpumask *mask,
+ 			bool force)
+ {
+@@ -211,7 +269,17 @@ int irq_set_affinity_locked(struct irq_data *data, const struct cpumask *mask,
+ 
+ 	if (desc->affinity_notify) {
+ 		kref_get(&desc->affinity_notify->kref);
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++		raw_spin_lock(&affinity_list_lock);
++		if (list_empty(&desc->affinity_notify->list))
++			list_add_tail(&affinity_list,
++					&desc->affinity_notify->list);
++		raw_spin_unlock(&affinity_list_lock);
++		wake_up_process(set_affinity_helper);
++#else
+ 		schedule_work(&desc->affinity_notify->work);
++#endif
+ 	}
+ 	irqd_set(data, IRQD_AFFINITY_SET);
+ 
+@@ -246,10 +314,8 @@ int irq_set_affinity_hint(unsigned int irq, const struct cpumask *m)
+ }
+ EXPORT_SYMBOL_GPL(irq_set_affinity_hint);
+ 
+-static void irq_affinity_notify(struct work_struct *work)
++static void _irq_affinity_notify(struct irq_affinity_notify *notify)
+ {
+-	struct irq_affinity_notify *notify =
+-		container_of(work, struct irq_affinity_notify, work);
+ 	struct irq_desc *desc = irq_to_desc(notify->irq);
+ 	cpumask_var_t cpumask;
+ 	unsigned long flags;
+@@ -271,6 +337,13 @@ out:
+ 	kref_put(&notify->kref, notify->release);
+ }
+ 
++static void irq_affinity_notify(struct work_struct *work)
++{
++	struct irq_affinity_notify *notify =
++		container_of(work, struct irq_affinity_notify, work);
++	_irq_affinity_notify(notify);
++}
++
+ /**
+  *	irq_set_affinity_notifier - control notification of IRQ affinity changes
+  *	@irq:		Interrupt for which to enable/disable notification
+@@ -300,6 +373,8 @@ irq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify)
+ 		notify->irq = irq;
+ 		kref_init(&notify->kref);
+ 		INIT_WORK(&notify->work, irq_affinity_notify);
++		INIT_LIST_HEAD(&notify->list);
++		init_helper_thread();
+ 	}
+ 
+ 	raw_spin_lock_irqsave(&desc->lock, flags);
+@@ -660,6 +735,12 @@ static irqreturn_t irq_nested_primary_handler(int irq, void *dev_id)
+ 	return IRQ_NONE;
+ }
+ 
++static irqreturn_t irq_forced_secondary_handler(int irq, void *dev_id)
++{
++	WARN(1, "Secondary action handler called for irq %d\n", irq);
++	return IRQ_NONE;
++}
++
+ static int irq_wait_for_interrupt(struct irqaction *action)
+ {
+ 	set_current_state(TASK_INTERRUPTIBLE);
+@@ -686,7 +767,8 @@ static int irq_wait_for_interrupt(struct irqaction *action)
+ static void irq_finalize_oneshot(struct irq_desc *desc,
+ 				 struct irqaction *action)
+ {
+-	if (!(desc->istate & IRQS_ONESHOT))
++	if (!(desc->istate & IRQS_ONESHOT) ||
++	    action->handler == irq_forced_secondary_handler)
+ 		return;
+ again:
+ 	chip_bus_lock(desc);
+@@ -788,7 +870,15 @@ irq_forced_thread_fn(struct irq_desc *desc, struct irqaction *action)
+ 	local_bh_disable();
+ 	ret = action->thread_fn(action->irq, action->dev_id);
+ 	irq_finalize_oneshot(desc, action);
+-	local_bh_enable();
++	/*
++	 * Interrupts which have real time requirements can be set up
++	 * to avoid softirq processing in the thread handler. This is
++	 * safe as these interrupts do not raise soft interrupts.
++	 */
++	if (irq_settings_no_softirq_call(desc))
++		_local_bh_enable();
++	else
++		local_bh_enable();
+ 	return ret;
+ }
+ 
+@@ -840,6 +930,18 @@ static void irq_thread_dtor(struct callback_head *unused)
+ 	irq_finalize_oneshot(desc, action);
+ }
+ 
++static void irq_wake_secondary(struct irq_desc *desc, struct irqaction *action)
++{
++	struct irqaction *secondary = action->secondary;
++
++	if (WARN_ON_ONCE(!secondary))
++		return;
++
++	raw_spin_lock_irq(&desc->lock);
++	__irq_wake_thread(desc, secondary);
++	raw_spin_unlock_irq(&desc->lock);
++}
++
+ /*
+  * Interrupt handler thread
+  */
+@@ -870,7 +972,15 @@ static int irq_thread(void *data)
+ 		action_ret = handler_fn(desc, action);
+ 		if (action_ret == IRQ_HANDLED)
+ 			atomic_inc(&desc->threads_handled);
+-
++		if (action_ret == IRQ_WAKE_THREAD)
++			irq_wake_secondary(desc, action);
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++		migrate_disable();
++		add_interrupt_randomness(action->irq, 0,
++				 desc->random_ip ^ (unsigned long) action);
++		migrate_enable();
++#endif
+ 		wake_threads_waitq(desc);
+ 	}
+ 
+@@ -914,20 +1024,36 @@ void irq_wake_thread(unsigned int irq, void *dev_id)
+ }
+ EXPORT_SYMBOL_GPL(irq_wake_thread);
+ 
+-static void irq_setup_forced_threading(struct irqaction *new)
++static int irq_setup_forced_threading(struct irqaction *new)
+ {
+ 	if (!force_irqthreads)
+-		return;
++		return 0;
+ 	if (new->flags & (IRQF_NO_THREAD | IRQF_PERCPU | IRQF_ONESHOT))
+-		return;
++		return 0;
+ 
+ 	new->flags |= IRQF_ONESHOT;
+ 
+-	if (!new->thread_fn) {
+-		set_bit(IRQTF_FORCED_THREAD, &new->thread_flags);
+-		new->thread_fn = new->handler;
+-		new->handler = irq_default_primary_handler;
++	/*
++	 * Handle the case where we have a real primary handler and a
++	 * thread handler. We force thread them as well by creating a
++	 * secondary action.
++	 */
++	if (new->handler != irq_default_primary_handler && new->thread_fn) {
++		/* Allocate the secondary action */
++		new->secondary = kzalloc(sizeof(struct irqaction), GFP_KERNEL);
++		if (!new->secondary)
++			return -ENOMEM;
++		new->secondary->handler = irq_forced_secondary_handler;
++		new->secondary->thread_fn = new->thread_fn;
++		new->secondary->dev_id = new->dev_id;
++		new->secondary->irq = new->irq;
++		new->secondary->name = new->name;
+ 	}
++	/* Deal with the primary handler */
++	set_bit(IRQTF_FORCED_THREAD, &new->thread_flags);
++	new->thread_fn = new->handler;
++	new->handler = irq_default_primary_handler;
++	return 0;
+ }
+ 
+ static int irq_request_resources(struct irq_desc *desc)
+@@ -947,6 +1073,48 @@ static void irq_release_resources(struct irq_desc *desc)
+ 		c->irq_release_resources(d);
+ }
+ 
++static int
++setup_irq_thread(struct irqaction *new, unsigned int irq, bool secondary)
++{
++	struct task_struct *t;
++	struct sched_param param = {
++		.sched_priority = MAX_USER_RT_PRIO/2,
++	};
++
++	if (!secondary) {
++		t = kthread_create(irq_thread, new, "irq/%d-%s", irq,
++				   new->name);
++	} else {
++		t = kthread_create(irq_thread, new, "irq/%d-s-%s", irq,
++				   new->name);
++		param.sched_priority += 1;
++	}
++
++	if (IS_ERR(t))
++		return PTR_ERR(t);
++
++	sched_setscheduler_nocheck(t, SCHED_FIFO, &param);
++
++	/*
++	 * We keep the reference to the task struct even if
++	 * the thread dies to avoid that the interrupt code
++	 * references an already freed task_struct.
++	 */
++	get_task_struct(t);
++	new->thread = t;
++	/*
++	 * Tell the thread to set its affinity. This is
++	 * important for shared interrupt handlers as we do
++	 * not invoke setup_affinity() for the secondary
++	 * handlers as everything is already set up. Even for
++	 * interrupts marked with IRQF_NO_BALANCE this is
++	 * correct as we want the thread to move to the cpu(s)
++	 * on which the requesting code placed the interrupt.
++	 */
++	set_bit(IRQTF_AFFINITY, &new->thread_flags);
++	return 0;
++}
++
+ /*
+  * Internal function to register an irqaction - typically used to
+  * allocate special interrupts that are part of the architecture.
+@@ -967,6 +1135,8 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
+ 	if (!try_module_get(desc->owner))
+ 		return -ENODEV;
+ 
++	new->irq = irq;
++
+ 	/*
+ 	 * Check whether the interrupt nests into another interrupt
+ 	 * thread.
+@@ -984,8 +1154,11 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
+ 		 */
+ 		new->handler = irq_nested_primary_handler;
+ 	} else {
+-		if (irq_settings_can_thread(desc))
+-			irq_setup_forced_threading(new);
++		if (irq_settings_can_thread(desc)) {
++			ret = irq_setup_forced_threading(new);
++			if (ret)
++				goto out_mput;
++		}
+ 	}
+ 
+ 	/*
+@@ -994,37 +1167,14 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
+ 	 * thread.
+ 	 */
+ 	if (new->thread_fn && !nested) {
+-		struct task_struct *t;
+-		static const struct sched_param param = {
+-			.sched_priority = MAX_USER_RT_PRIO/2,
+-		};
+-
+-		t = kthread_create(irq_thread, new, "irq/%d-%s", irq,
+-				   new->name);
+-		if (IS_ERR(t)) {
+-			ret = PTR_ERR(t);
++		ret = setup_irq_thread(new, irq, false);
++		if (ret)
+ 			goto out_mput;
++		if (new->secondary) {
++			ret = setup_irq_thread(new->secondary, irq, true);
++			if (ret)
++				goto out_thread;
+ 		}
+-
+-		sched_setscheduler_nocheck(t, SCHED_FIFO, &param);
+-
+-		/*
+-		 * We keep the reference to the task struct even if
+-		 * the thread dies to avoid that the interrupt code
+-		 * references an already freed task_struct.
+-		 */
+-		get_task_struct(t);
+-		new->thread = t;
+-		/*
+-		 * Tell the thread to set its affinity. This is
+-		 * important for shared interrupt handlers as we do
+-		 * not invoke setup_affinity() for the secondary
+-		 * handlers as everything is already set up. Even for
+-		 * interrupts marked with IRQF_NO_BALANCE this is
+-		 * correct as we want the thread to move to the cpu(s)
+-		 * on which the requesting code placed the interrupt.
+-		 */
+-		set_bit(IRQTF_AFFINITY, &new->thread_flags);
+ 	}
+ 
+ 	if (!alloc_cpumask_var(&mask, GFP_KERNEL)) {
+@@ -1186,6 +1336,9 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
+ 			irqd_set(&desc->irq_data, IRQD_NO_BALANCING);
+ 		}
+ 
++		if (new->flags & IRQF_NO_SOFTIRQ_CALL)
++			irq_settings_set_no_softirq_call(desc);
++
+ 		/* Set default affinity mask once everything is setup */
+ 		setup_affinity(irq, desc, mask);
+ 
+@@ -1199,7 +1352,6 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
+ 				   irq, nmsk, omsk);
+ 	}
+ 
+-	new->irq = irq;
+ 	*old_ptr = new;
+ 
+ 	irq_pm_install_action(desc, new);
+@@ -1225,6 +1377,8 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
+ 	 */
+ 	if (new->thread)
+ 		wake_up_process(new->thread);
++	if (new->secondary)
++		wake_up_process(new->secondary->thread);
+ 
+ 	register_irq_proc(irq, desc);
+ 	new->dir = NULL;
+@@ -1255,6 +1409,13 @@ out_thread:
+ 		kthread_stop(t);
+ 		put_task_struct(t);
+ 	}
++	if (new->secondary && new->secondary->thread) {
++		struct task_struct *t = new->secondary->thread;
++
++		new->secondary->thread = NULL;
++		kthread_stop(t);
++		put_task_struct(t);
++	}
+ out_mput:
+ 	module_put(desc->owner);
+ 	return ret;
+@@ -1362,9 +1523,14 @@ static struct irqaction *__free_irq(unsigned int irq, void *dev_id)
+ 	if (action->thread) {
+ 		kthread_stop(action->thread);
+ 		put_task_struct(action->thread);
++		if (action->secondary && action->secondary->thread) {
++			kthread_stop(action->secondary->thread);
++			put_task_struct(action->secondary->thread);
++		}
+ 	}
+ 
+ 	module_put(desc->owner);
++	kfree(action->secondary);
+ 	return action;
+ }
+ 
+@@ -1503,8 +1669,10 @@ int request_threaded_irq(unsigned int irq, irq_handler_t handler,
+ 	retval = __setup_irq(irq, desc, action);
+ 	chip_bus_sync_unlock(desc);
+ 
+-	if (retval)
++	if (retval) {
++		kfree(action->secondary);
+ 		kfree(action);
++	}
+ 
+ #ifdef CONFIG_DEBUG_SHIRQ_FIXME
+ 	if (!retval && (irqflags & IRQF_SHARED)) {
+diff --git a/kernel/irq/settings.h b/kernel/irq/settings.h
+index 3320b84cc60f..34b803b89d41 100644
+--- a/kernel/irq/settings.h
++++ b/kernel/irq/settings.h
+@@ -15,6 +15,7 @@ enum {
+ 	_IRQ_NESTED_THREAD	= IRQ_NESTED_THREAD,
+ 	_IRQ_PER_CPU_DEVID	= IRQ_PER_CPU_DEVID,
+ 	_IRQ_IS_POLLED		= IRQ_IS_POLLED,
++	_IRQ_NO_SOFTIRQ_CALL	= IRQ_NO_SOFTIRQ_CALL,
+ 	_IRQF_MODIFY_MASK	= IRQF_MODIFY_MASK,
+ };
+ 
+@@ -28,6 +29,7 @@ enum {
+ #define IRQ_NESTED_THREAD	GOT_YOU_MORON
+ #define IRQ_PER_CPU_DEVID	GOT_YOU_MORON
+ #define IRQ_IS_POLLED		GOT_YOU_MORON
++#define IRQ_NO_SOFTIRQ_CALL	GOT_YOU_MORON
+ #undef IRQF_MODIFY_MASK
+ #define IRQF_MODIFY_MASK	GOT_YOU_MORON
+ 
+@@ -38,6 +40,16 @@ irq_settings_clr_and_set(struct irq_desc *desc, u32 clr, u32 set)
+ 	desc->status_use_accessors |= (set & _IRQF_MODIFY_MASK);
+ }
+ 
++static inline bool irq_settings_no_softirq_call(struct irq_desc *desc)
++{
++	return desc->status_use_accessors & _IRQ_NO_SOFTIRQ_CALL;
++}
++
++static inline void irq_settings_set_no_softirq_call(struct irq_desc *desc)
++{
++	desc->status_use_accessors |= _IRQ_NO_SOFTIRQ_CALL;
++}
++
+ static inline bool irq_settings_is_per_cpu(struct irq_desc *desc)
+ {
+ 	return desc->status_use_accessors & _IRQ_PER_CPU;
+diff --git a/kernel/irq/spurious.c b/kernel/irq/spurious.c
+index e2514b0e439e..903a69c45689 100644
+--- a/kernel/irq/spurious.c
++++ b/kernel/irq/spurious.c
+@@ -444,6 +444,10 @@ MODULE_PARM_DESC(noirqdebug, "Disable irq lockup detection when true");
+ 
+ static int __init irqfixup_setup(char *str)
+ {
++#ifdef CONFIG_PREEMPT_RT_BASE
++	pr_warn("irqfixup boot option not supported w/ CONFIG_PREEMPT_RT_BASE\n");
++	return 1;
++#endif
+ 	irqfixup = 1;
+ 	printk(KERN_WARNING "Misrouted IRQ fixup support enabled.\n");
+ 	printk(KERN_WARNING "This may impact system performance.\n");
+@@ -456,6 +460,10 @@ module_param(irqfixup, int, 0644);
+ 
+ static int __init irqpoll_setup(char *str)
+ {
++#ifdef CONFIG_PREEMPT_RT_BASE
++	pr_warn("irqpoll boot option not supported w/ CONFIG_PREEMPT_RT_BASE\n");
++	return 1;
++#endif
+ 	irqfixup = 2;
+ 	printk(KERN_WARNING "Misrouted IRQ fixup and polling support "
+ 				"enabled\n");
+diff --git a/kernel/irq_work.c b/kernel/irq_work.c
+index 3ab9048483fa..3d5a476b58b9 100644
+--- a/kernel/irq_work.c
++++ b/kernel/irq_work.c
+@@ -17,6 +17,7 @@
+ #include <linux/cpu.h>
+ #include <linux/notifier.h>
+ #include <linux/smp.h>
++#include <linux/interrupt.h>
+ #include <asm/processor.h>
+ 
+ 
+@@ -65,6 +66,8 @@ void __weak arch_irq_work_raise(void)
+  */
+ bool irq_work_queue_on(struct irq_work *work, int cpu)
+ {
++	struct llist_head *list;
++
+ 	/* All work should have been flushed before going offline */
+ 	WARN_ON_ONCE(cpu_is_offline(cpu));
+ 
+@@ -75,7 +78,12 @@ bool irq_work_queue_on(struct irq_work *work, int cpu)
+ 	if (!irq_work_claim(work))
+ 		return false;
+ 
+-	if (llist_add(&work->llnode, &per_cpu(raised_list, cpu)))
++	if (IS_ENABLED(CONFIG_PREEMPT_RT_FULL) && !(work->flags & IRQ_WORK_HARD_IRQ))
++		list = &per_cpu(lazy_list, cpu);
++	else
++		list = &per_cpu(raised_list, cpu);
++
++	if (llist_add(&work->llnode, list))
+ 		arch_send_call_function_single_ipi(cpu);
+ 
+ 	return true;
+@@ -86,6 +94,9 @@ EXPORT_SYMBOL_GPL(irq_work_queue_on);
+ /* Enqueue the irq work @work on the current CPU */
+ bool irq_work_queue(struct irq_work *work)
+ {
++	struct llist_head *list;
++	bool lazy_work, realtime = IS_ENABLED(CONFIG_PREEMPT_RT_FULL);
++
+ 	/* Only queue if not already pending */
+ 	if (!irq_work_claim(work))
+ 		return false;
+@@ -93,13 +104,15 @@ bool irq_work_queue(struct irq_work *work)
+ 	/* Queue the entry and raise the IPI if needed. */
+ 	preempt_disable();
+ 
+-	/* If the work is "lazy", handle it from next tick if any */
+-	if (work->flags & IRQ_WORK_LAZY) {
+-		if (llist_add(&work->llnode, this_cpu_ptr(&lazy_list)) &&
+-		    tick_nohz_tick_stopped())
+-			arch_irq_work_raise();
+-	} else {
+-		if (llist_add(&work->llnode, this_cpu_ptr(&raised_list)))
++	lazy_work = work->flags & IRQ_WORK_LAZY;
++
++	if (lazy_work || (realtime && !(work->flags & IRQ_WORK_HARD_IRQ)))
++		list = this_cpu_ptr(&lazy_list);
++	else
++		list = this_cpu_ptr(&raised_list);
++
++	if (llist_add(&work->llnode, list)) {
++		if (!lazy_work || tick_nohz_tick_stopped())
+ 			arch_irq_work_raise();
+ 	}
+ 
+@@ -116,9 +129,8 @@ bool irq_work_needs_cpu(void)
+ 	raised = this_cpu_ptr(&raised_list);
+ 	lazy = this_cpu_ptr(&lazy_list);
+ 
+-	if (llist_empty(raised) || arch_irq_work_has_interrupt())
+-		if (llist_empty(lazy))
+-			return false;
++	if (llist_empty(raised) && llist_empty(lazy))
++		return false;
+ 
+ 	/* All work should have been flushed before going offline */
+ 	WARN_ON_ONCE(cpu_is_offline(smp_processor_id()));
+@@ -132,7 +144,7 @@ static void irq_work_run_list(struct llist_head *list)
+ 	struct irq_work *work;
+ 	struct llist_node *llnode;
+ 
+-	BUG_ON(!irqs_disabled());
++	BUG_ON(!IS_ENABLED(CONFIG_PREEMPT_RT_FULL) && !irqs_disabled());
+ 
+ 	if (llist_empty(list))
+ 		return;
+@@ -169,18 +181,36 @@ static void irq_work_run_list(struct llist_head *list)
+ void irq_work_run(void)
+ {
+ 	irq_work_run_list(this_cpu_ptr(&raised_list));
+-	irq_work_run_list(this_cpu_ptr(&lazy_list));
++	if (IS_ENABLED(CONFIG_PREEMPT_RT_FULL)) {
++		/*
++		 * NOTE: we raise softirq via IPI for safety,
++		 * and execute in irq_work_tick() to move the
++		 * overhead from hard to soft irq context.
++		 */
++		if (!llist_empty(this_cpu_ptr(&lazy_list)))
++			raise_softirq(TIMER_SOFTIRQ);
++	} else
++		irq_work_run_list(this_cpu_ptr(&lazy_list));
+ }
+ EXPORT_SYMBOL_GPL(irq_work_run);
+ 
+ void irq_work_tick(void)
+ {
+-	struct llist_head *raised = &__get_cpu_var(raised_list);
++	struct llist_head *raised = this_cpu_ptr(&raised_list);
+ 
+ 	if (!llist_empty(raised) && !arch_irq_work_has_interrupt())
+ 		irq_work_run_list(raised);
+-	irq_work_run_list(&__get_cpu_var(lazy_list));
++
++	if (!IS_ENABLED(CONFIG_PREEMPT_RT_FULL))
++		irq_work_run_list(this_cpu_ptr(&lazy_list));
++}
++
++#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_PREEMPT_RT_FULL)
++void irq_work_tick_soft(void)
++{
++	irq_work_run_list(this_cpu_ptr(&lazy_list));
+ }
++#endif
+ 
+ /*
+  * Synchronize against the irq_work @entry, ensures the entry is not
+diff --git a/kernel/ksysfs.c b/kernel/ksysfs.c
+index 6683ccef9fff..d6fc8eeaab8f 100644
+--- a/kernel/ksysfs.c
++++ b/kernel/ksysfs.c
+@@ -136,6 +136,15 @@ KERNEL_ATTR_RO(vmcoreinfo);
+ 
+ #endif /* CONFIG_KEXEC */
+ 
++#if defined(CONFIG_PREEMPT_RT_FULL)
++static ssize_t  realtime_show(struct kobject *kobj,
++			      struct kobj_attribute *attr, char *buf)
++{
++	return sprintf(buf, "%d\n", 1);
++}
++KERNEL_ATTR_RO(realtime);
++#endif
++
+ /* whether file capabilities are enabled */
+ static ssize_t fscaps_show(struct kobject *kobj,
+ 				  struct kobj_attribute *attr, char *buf)
+@@ -203,6 +212,9 @@ static struct attribute * kernel_attrs[] = {
+ 	&vmcoreinfo_attr.attr,
+ #endif
+ 	&rcu_expedited_attr.attr,
++#ifdef CONFIG_PREEMPT_RT_FULL
++	&realtime_attr.attr,
++#endif
+ 	NULL
+ };
+ 
+diff --git a/kernel/locking/Makefile b/kernel/locking/Makefile
+index 8541bfdfd232..5cb3588be5c5 100644
+--- a/kernel/locking/Makefile
++++ b/kernel/locking/Makefile
+@@ -1,5 +1,5 @@
+ 
+-obj-y += mutex.o semaphore.o rwsem.o mcs_spinlock.o
++obj-y += semaphore.o mcs_spinlock.o
+ 
+ ifdef CONFIG_FUNCTION_TRACER
+ CFLAGS_REMOVE_lockdep.o = -pg
+@@ -8,7 +8,11 @@ CFLAGS_REMOVE_mutex-debug.o = -pg
+ CFLAGS_REMOVE_rtmutex-debug.o = -pg
+ endif
+ 
++ifneq ($(CONFIG_PREEMPT_RT_FULL),y)
++obj-y += mutex.o
+ obj-$(CONFIG_DEBUG_MUTEXES) += mutex-debug.o
++obj-y += rwsem.o
++endif
+ obj-$(CONFIG_LOCKDEP) += lockdep.o
+ ifeq ($(CONFIG_PROC_FS),y)
+ obj-$(CONFIG_LOCKDEP) += lockdep_proc.o
+@@ -21,8 +25,11 @@ obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
+ obj-$(CONFIG_RT_MUTEX_TESTER) += rtmutex-tester.o
+ obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock.o
+ obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock_debug.o
++ifneq ($(CONFIG_PREEMPT_RT_FULL),y)
+ obj-$(CONFIG_RWSEM_GENERIC_SPINLOCK) += rwsem-spinlock.o
+ obj-$(CONFIG_RWSEM_XCHGADD_ALGORITHM) += rwsem-xadd.o
++endif
+ obj-$(CONFIG_PERCPU_RWSEM) += percpu-rwsem.o
++obj-$(CONFIG_PREEMPT_RT_FULL) += rt.o
+ obj-$(CONFIG_QUEUE_RWLOCK) += qrwlock.o
+ obj-$(CONFIG_LOCK_TORTURE_TEST) += locktorture.o
+diff --git a/kernel/locking/lglock.c b/kernel/locking/lglock.c
+index 86ae2aebf004..9397974b142f 100644
+--- a/kernel/locking/lglock.c
++++ b/kernel/locking/lglock.c
+@@ -4,6 +4,15 @@
+ #include <linux/cpu.h>
+ #include <linux/string.h>
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++# define lg_lock_ptr		arch_spinlock_t
++# define lg_do_lock(l)		arch_spin_lock(l)
++# define lg_do_unlock(l)	arch_spin_unlock(l)
++#else
++# define lg_lock_ptr		struct rt_mutex
++# define lg_do_lock(l)		__rt_spin_lock(l)
++# define lg_do_unlock(l)	__rt_spin_unlock(l)
++#endif
+ /*
+  * Note there is no uninit, so lglocks cannot be defined in
+  * modules (but it's fine to use them from there)
+@@ -12,51 +21,60 @@
+ 
+ void lg_lock_init(struct lglock *lg, char *name)
+ {
++#ifdef CONFIG_PREEMPT_RT_FULL
++	int i;
++
++	for_each_possible_cpu(i) {
++		struct rt_mutex *lock = per_cpu_ptr(lg->lock, i);
++
++		rt_mutex_init(lock);
++	}
++#endif
+ 	LOCKDEP_INIT_MAP(&lg->lock_dep_map, name, &lg->lock_key, 0);
+ }
+ EXPORT_SYMBOL(lg_lock_init);
+ 
+ void lg_local_lock(struct lglock *lg)
+ {
+-	arch_spinlock_t *lock;
++	lg_lock_ptr *lock;
+ 
+-	preempt_disable();
++	migrate_disable();
+ 	lock_acquire_shared(&lg->lock_dep_map, 0, 0, NULL, _RET_IP_);
+ 	lock = this_cpu_ptr(lg->lock);
+-	arch_spin_lock(lock);
++	lg_do_lock(lock);
+ }
+ EXPORT_SYMBOL(lg_local_lock);
+ 
+ void lg_local_unlock(struct lglock *lg)
+ {
+-	arch_spinlock_t *lock;
++	lg_lock_ptr *lock;
+ 
+ 	lock_release(&lg->lock_dep_map, 1, _RET_IP_);
+ 	lock = this_cpu_ptr(lg->lock);
+-	arch_spin_unlock(lock);
+-	preempt_enable();
++	lg_do_unlock(lock);
++	migrate_enable();
+ }
+ EXPORT_SYMBOL(lg_local_unlock);
+ 
+ void lg_local_lock_cpu(struct lglock *lg, int cpu)
+ {
+-	arch_spinlock_t *lock;
++	lg_lock_ptr *lock;
+ 
+-	preempt_disable();
++	preempt_disable_nort();
+ 	lock_acquire_shared(&lg->lock_dep_map, 0, 0, NULL, _RET_IP_);
+ 	lock = per_cpu_ptr(lg->lock, cpu);
+-	arch_spin_lock(lock);
++	lg_do_lock(lock);
+ }
+ EXPORT_SYMBOL(lg_local_lock_cpu);
+ 
+ void lg_local_unlock_cpu(struct lglock *lg, int cpu)
+ {
+-	arch_spinlock_t *lock;
++	lg_lock_ptr *lock;
+ 
+ 	lock_release(&lg->lock_dep_map, 1, _RET_IP_);
+ 	lock = per_cpu_ptr(lg->lock, cpu);
+-	arch_spin_unlock(lock);
+-	preempt_enable();
++	lg_do_unlock(lock);
++	preempt_enable_nort();
+ }
+ EXPORT_SYMBOL(lg_local_unlock_cpu);
+ 
+@@ -64,12 +82,12 @@ void lg_global_lock(struct lglock *lg)
+ {
+ 	int i;
+ 
+-	preempt_disable();
++	preempt_disable_nort();
+ 	lock_acquire_exclusive(&lg->lock_dep_map, 0, 0, NULL, _RET_IP_);
+ 	for_each_possible_cpu(i) {
+-		arch_spinlock_t *lock;
++		lg_lock_ptr *lock;
+ 		lock = per_cpu_ptr(lg->lock, i);
+-		arch_spin_lock(lock);
++		lg_do_lock(lock);
+ 	}
+ }
+ EXPORT_SYMBOL(lg_global_lock);
+@@ -80,10 +98,35 @@ void lg_global_unlock(struct lglock *lg)
+ 
+ 	lock_release(&lg->lock_dep_map, 1, _RET_IP_);
+ 	for_each_possible_cpu(i) {
+-		arch_spinlock_t *lock;
++		lg_lock_ptr *lock;
+ 		lock = per_cpu_ptr(lg->lock, i);
+-		arch_spin_unlock(lock);
++		lg_do_unlock(lock);
+ 	}
+-	preempt_enable();
++	preempt_enable_nort();
+ }
+ EXPORT_SYMBOL(lg_global_unlock);
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++/*
++ * HACK: If you use this, you get to keep the pieces.
++ * Used in queue_stop_cpus_work() when stop machinery
++ * is called from inactive CPU, so we can't schedule.
++ */
++# define lg_do_trylock_relax(l)			\
++	do {					\
++		while (!__rt_spin_trylock(l))	\
++			cpu_relax();		\
++	} while (0)
++
++void lg_global_trylock_relax(struct lglock *lg)
++{
++	int i;
++
++	lock_acquire_exclusive(&lg->lock_dep_map, 0, 0, NULL, _RET_IP_);
++	for_each_possible_cpu(i) {
++		lg_lock_ptr *lock;
++		lock = per_cpu_ptr(lg->lock, i);
++		lg_do_trylock_relax(lock);
++	}
++}
++#endif
+diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c
+index 88d0d4420ad2..d4a4d8ea55d6 100644
+--- a/kernel/locking/lockdep.c
++++ b/kernel/locking/lockdep.c
+@@ -654,6 +654,7 @@ look_up_lock_class(struct lockdep_map *lock, unsigned int subclass)
+ 	struct lockdep_subclass_key *key;
+ 	struct list_head *hash_head;
+ 	struct lock_class *class;
++	bool is_static = false;
+ 
+ #ifdef CONFIG_DEBUG_LOCKDEP
+ 	/*
+@@ -681,10 +682,23 @@ look_up_lock_class(struct lockdep_map *lock, unsigned int subclass)
+ 
+ 	/*
+ 	 * Static locks do not have their class-keys yet - for them the key
+-	 * is the lock object itself:
++	 * is the lock object itself. If the lock is in the per cpu area,
++	 * the canonical address of the lock (per cpu offset removed) is
++	 * used.
+ 	 */
+-	if (unlikely(!lock->key))
+-		lock->key = (void *)lock;
++	if (unlikely(!lock->key)) {
++		unsigned long can_addr, addr = (unsigned long)lock;
++
++		if (__is_kernel_percpu_address(addr, &can_addr))
++			lock->key = (void *)can_addr;
++		else if (__is_module_percpu_address(addr, &can_addr))
++			lock->key = (void *)can_addr;
++		else if (static_obj(lock))
++			lock->key = (void *)lock;
++		else
++			return ERR_PTR(-EINVAL);
++		is_static = true;
++	}
+ 
+ 	/*
+ 	 * NOTE: the class-key must be unique. For dynamic locks, a static
+@@ -714,7 +728,7 @@ look_up_lock_class(struct lockdep_map *lock, unsigned int subclass)
+ 		}
+ 	}
+ 
+-	return NULL;
++	return is_static || static_obj(lock->key) ? NULL : ERR_PTR(-EINVAL);
+ }
+ 
+ /*
+@@ -731,13 +745,13 @@ register_lock_class(struct lockdep_map *lock, unsigned int subclass, int force)
+ 	unsigned long flags;
+ 
+ 	class = look_up_lock_class(lock, subclass);
+-	if (likely(class))
++	if (likely(!IS_ERR_OR_NULL(class)))
+ 		goto out_set_class_cache;
+ 
+ 	/*
+ 	 * Debug-check: all keys must be persistent!
+- 	 */
+-	if (!static_obj(lock->key)) {
++	 */
++	if (IS_ERR(class)) {
+ 		debug_locks_off();
+ 		printk("INFO: trying to register non-static key.\n");
+ 		printk("the code is fine but needs lockdep annotation.\n");
+@@ -3276,7 +3290,7 @@ static int match_held_lock(struct held_lock *hlock, struct lockdep_map *lock)
+ 		 * Clearly if the lock hasn't been acquired _ever_, we're not
+ 		 * holding it either, so report failure.
+ 		 */
+-		if (!class)
++		if (IS_ERR_OR_NULL(class))
+ 			return 0;
+ 
+ 		/*
+@@ -3542,6 +3556,7 @@ static void check_flags(unsigned long flags)
+ 		}
+ 	}
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ 	/*
+ 	 * We dont accurately track softirq state in e.g.
+ 	 * hardirq contexts (such as on 4KSTACKS), so only
+@@ -3556,6 +3571,7 @@ static void check_flags(unsigned long flags)
+ 			DEBUG_LOCKS_WARN_ON(!current->softirqs_enabled);
+ 		}
+ 	}
++#endif
+ 
+ 	if (!debug_locks)
+ 		print_irqtrace_events(current);
+@@ -3936,7 +3952,7 @@ void lockdep_reset_lock(struct lockdep_map *lock)
+ 		 * If the class exists we look it up and zap it:
+ 		 */
+ 		class = look_up_lock_class(lock, j);
+-		if (class)
++		if (!IS_ERR_OR_NULL(class))
+ 			zap_class(class);
+ 	}
+ 	/*
+diff --git a/kernel/locking/locktorture.c b/kernel/locking/locktorture.c
+index ec8cce259779..aa60d919e336 100644
+--- a/kernel/locking/locktorture.c
++++ b/kernel/locking/locktorture.c
+@@ -24,7 +24,6 @@
+ #include <linux/module.h>
+ #include <linux/kthread.h>
+ #include <linux/spinlock.h>
+-#include <linux/rwlock.h>
+ #include <linux/mutex.h>
+ #include <linux/rwsem.h>
+ #include <linux/smp.h>
+diff --git a/kernel/locking/percpu-rwsem.c b/kernel/locking/percpu-rwsem.c
+index 652a8ee8efe9..2db0f42d5c64 100644
+--- a/kernel/locking/percpu-rwsem.c
++++ b/kernel/locking/percpu-rwsem.c
+@@ -84,8 +84,12 @@ void percpu_down_read(struct percpu_rw_semaphore *brw)
+ 
+ 	down_read(&brw->rw_sem);
+ 	atomic_inc(&brw->slow_read_ctr);
++#ifdef CONFIG_PREEMPT_RT_FULL
++	up_read(&brw->rw_sem);
++#else
+ 	/* avoid up_read()->rwsem_release() */
+ 	__up_read(&brw->rw_sem);
++#endif
+ }
+ 
+ void percpu_up_read(struct percpu_rw_semaphore *brw)
+diff --git a/kernel/locking/rt.c b/kernel/locking/rt.c
+new file mode 100644
+index 000000000000..73c55089fb93
+--- /dev/null
++++ b/kernel/locking/rt.c
+@@ -0,0 +1,456 @@
++/*
++ * kernel/rt.c
++ *
++ * Real-Time Preemption Support
++ *
++ * started by Ingo Molnar:
++ *
++ *  Copyright (C) 2004-2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
++ *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
++ *
++ * historic credit for proving that Linux spinlocks can be implemented via
++ * RT-aware mutexes goes to many people: The Pmutex project (Dirk Grambow
++ * and others) who prototyped it on 2.4 and did lots of comparative
++ * research and analysis; TimeSys, for proving that you can implement a
++ * fully preemptible kernel via the use of IRQ threading and mutexes;
++ * Bill Huey for persuasively arguing on lkml that the mutex model is the
++ * right one; and to MontaVista, who ported pmutexes to 2.6.
++ *
++ * This code is a from-scratch implementation and is not based on pmutexes,
++ * but the idea of converting spinlocks to mutexes is used here too.
++ *
++ * lock debugging, locking tree, deadlock detection:
++ *
++ *  Copyright (C) 2004, LynuxWorks, Inc., Igor Manyilov, Bill Huey
++ *  Released under the General Public License (GPL).
++ *
++ * Includes portions of the generic R/W semaphore implementation from:
++ *
++ *  Copyright (c) 2001   David Howells (dhowells@redhat.com).
++ *  - Derived partially from idea by Andrea Arcangeli <andrea@suse.de>
++ *  - Derived also from comments by Linus
++ *
++ * Pending ownership of locks and ownership stealing:
++ *
++ *  Copyright (C) 2005, Kihon Technologies Inc., Steven Rostedt
++ *
++ *   (also by Steven Rostedt)
++ *    - Converted single pi_lock to individual task locks.
++ *
++ * By Esben Nielsen:
++ *    Doing priority inheritance with help of the scheduler.
++ *
++ *  Copyright (C) 2006, Timesys Corp., Thomas Gleixner <tglx@timesys.com>
++ *  - major rework based on Esben Nielsens initial patch
++ *  - replaced thread_info references by task_struct refs
++ *  - removed task->pending_owner dependency
++ *  - BKL drop/reacquire for semaphore style locks to avoid deadlocks
++ *    in the scheduler return path as discussed with Steven Rostedt
++ *
++ *  Copyright (C) 2006, Kihon Technologies Inc.
++ *    Steven Rostedt <rostedt@goodmis.org>
++ *  - debugged and patched Thomas Gleixner's rework.
++ *  - added back the cmpxchg to the rework.
++ *  - turned atomic require back on for SMP.
++ */
++
++#include <linux/spinlock.h>
++#include <linux/rtmutex.h>
++#include <linux/sched.h>
++#include <linux/delay.h>
++#include <linux/module.h>
++#include <linux/kallsyms.h>
++#include <linux/syscalls.h>
++#include <linux/interrupt.h>
++#include <linux/plist.h>
++#include <linux/fs.h>
++#include <linux/futex.h>
++#include <linux/hrtimer.h>
++
++#include "rtmutex_common.h"
++
++/*
++ * struct mutex functions
++ */
++void __mutex_do_init(struct mutex *mutex, const char *name,
++		     struct lock_class_key *key)
++{
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++	/*
++	 * Make sure we are not reinitializing a held lock:
++	 */
++	debug_check_no_locks_freed((void *)mutex, sizeof(*mutex));
++	lockdep_init_map(&mutex->dep_map, name, key, 0);
++#endif
++	mutex->lock.save_state = 0;
++}
++EXPORT_SYMBOL(__mutex_do_init);
++
++void __lockfunc _mutex_lock(struct mutex *lock)
++{
++	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
++	rt_mutex_lock(&lock->lock);
++}
++EXPORT_SYMBOL(_mutex_lock);
++
++int __lockfunc _mutex_lock_interruptible(struct mutex *lock)
++{
++	int ret;
++
++	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
++	ret = rt_mutex_lock_interruptible(&lock->lock);
++	if (ret)
++		mutex_release(&lock->dep_map, 1, _RET_IP_);
++	return ret;
++}
++EXPORT_SYMBOL(_mutex_lock_interruptible);
++
++int __lockfunc _mutex_lock_killable(struct mutex *lock)
++{
++	int ret;
++
++	mutex_acquire(&lock->dep_map, 0, 0, _RET_IP_);
++	ret = rt_mutex_lock_killable(&lock->lock);
++	if (ret)
++		mutex_release(&lock->dep_map, 1, _RET_IP_);
++	return ret;
++}
++EXPORT_SYMBOL(_mutex_lock_killable);
++
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++void __lockfunc _mutex_lock_nested(struct mutex *lock, int subclass)
++{
++	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
++	rt_mutex_lock(&lock->lock);
++}
++EXPORT_SYMBOL(_mutex_lock_nested);
++
++void __lockfunc _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest)
++{
++	mutex_acquire_nest(&lock->dep_map, 0, 0, nest, _RET_IP_);
++	rt_mutex_lock(&lock->lock);
++}
++EXPORT_SYMBOL(_mutex_lock_nest_lock);
++
++int __lockfunc _mutex_lock_interruptible_nested(struct mutex *lock, int subclass)
++{
++	int ret;
++
++	mutex_acquire_nest(&lock->dep_map, subclass, 0, NULL, _RET_IP_);
++	ret = rt_mutex_lock_interruptible(&lock->lock);
++	if (ret)
++		mutex_release(&lock->dep_map, 1, _RET_IP_);
++	return ret;
++}
++EXPORT_SYMBOL(_mutex_lock_interruptible_nested);
++
++int __lockfunc _mutex_lock_killable_nested(struct mutex *lock, int subclass)
++{
++	int ret;
++
++	mutex_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
++	ret = rt_mutex_lock_killable(&lock->lock);
++	if (ret)
++		mutex_release(&lock->dep_map, 1, _RET_IP_);
++	return ret;
++}
++EXPORT_SYMBOL(_mutex_lock_killable_nested);
++#endif
++
++int __lockfunc _mutex_trylock(struct mutex *lock)
++{
++	int ret = rt_mutex_trylock(&lock->lock);
++
++	if (ret)
++		mutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);
++
++	return ret;
++}
++EXPORT_SYMBOL(_mutex_trylock);
++
++void __lockfunc _mutex_unlock(struct mutex *lock)
++{
++	mutex_release(&lock->dep_map, 1, _RET_IP_);
++	rt_mutex_unlock(&lock->lock);
++}
++EXPORT_SYMBOL(_mutex_unlock);
++
++/*
++ * rwlock_t functions
++ */
++int __lockfunc rt_write_trylock(rwlock_t *rwlock)
++{
++	int ret;
++
++	migrate_disable();
++	ret = rt_mutex_trylock(&rwlock->lock);
++	if (ret)
++		rwlock_acquire(&rwlock->dep_map, 0, 1, _RET_IP_);
++	else
++		migrate_enable();
++
++	return ret;
++}
++EXPORT_SYMBOL(rt_write_trylock);
++
++int __lockfunc rt_write_trylock_irqsave(rwlock_t *rwlock, unsigned long *flags)
++{
++	int ret;
++
++	*flags = 0;
++	ret = rt_write_trylock(rwlock);
++	return ret;
++}
++EXPORT_SYMBOL(rt_write_trylock_irqsave);
++
++int __lockfunc rt_read_trylock(rwlock_t *rwlock)
++{
++	struct rt_mutex *lock = &rwlock->lock;
++	int ret = 1;
++
++	/*
++	 * recursive read locks succeed when current owns the lock,
++	 * but not when read_depth == 0 which means that the lock is
++	 * write locked.
++	 */
++	if (rt_mutex_owner(lock) != current) {
++		migrate_disable();
++		ret = rt_mutex_trylock(lock);
++		if (ret)
++			rwlock_acquire(&rwlock->dep_map, 0, 1, _RET_IP_);
++		else
++			migrate_enable();
++
++	} else if (!rwlock->read_depth) {
++		ret = 0;
++	}
++
++	if (ret)
++		rwlock->read_depth++;
++
++	return ret;
++}
++EXPORT_SYMBOL(rt_read_trylock);
++
++void __lockfunc rt_write_lock(rwlock_t *rwlock)
++{
++	rwlock_acquire(&rwlock->dep_map, 0, 0, _RET_IP_);
++	migrate_disable();
++	__rt_spin_lock(&rwlock->lock);
++}
++EXPORT_SYMBOL(rt_write_lock);
++
++void __lockfunc rt_read_lock(rwlock_t *rwlock)
++{
++	struct rt_mutex *lock = &rwlock->lock;
++
++
++	/*
++	 * recursive read locks succeed when current owns the lock
++	 */
++	if (rt_mutex_owner(lock) != current) {
++		migrate_disable();
++		rwlock_acquire(&rwlock->dep_map, 0, 0, _RET_IP_);
++		__rt_spin_lock(lock);
++	}
++	rwlock->read_depth++;
++}
++
++EXPORT_SYMBOL(rt_read_lock);
++
++void __lockfunc rt_write_unlock(rwlock_t *rwlock)
++{
++	/* NOTE: we always pass in '1' for nested, for simplicity */
++	rwlock_release(&rwlock->dep_map, 1, _RET_IP_);
++	__rt_spin_unlock(&rwlock->lock);
++	migrate_enable();
++}
++EXPORT_SYMBOL(rt_write_unlock);
++
++void __lockfunc rt_read_unlock(rwlock_t *rwlock)
++{
++	/* Release the lock only when read_depth is down to 0 */
++	if (--rwlock->read_depth == 0) {
++		rwlock_release(&rwlock->dep_map, 1, _RET_IP_);
++		__rt_spin_unlock(&rwlock->lock);
++		migrate_enable();
++	}
++}
++EXPORT_SYMBOL(rt_read_unlock);
++
++unsigned long __lockfunc rt_write_lock_irqsave(rwlock_t *rwlock)
++{
++	rt_write_lock(rwlock);
++
++	return 0;
++}
++EXPORT_SYMBOL(rt_write_lock_irqsave);
++
++unsigned long __lockfunc rt_read_lock_irqsave(rwlock_t *rwlock)
++{
++	rt_read_lock(rwlock);
++
++	return 0;
++}
++EXPORT_SYMBOL(rt_read_lock_irqsave);
++
++void __rt_rwlock_init(rwlock_t *rwlock, char *name, struct lock_class_key *key)
++{
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++	/*
++	 * Make sure we are not reinitializing a held lock:
++	 */
++	debug_check_no_locks_freed((void *)rwlock, sizeof(*rwlock));
++	lockdep_init_map(&rwlock->dep_map, name, key, 0);
++#endif
++	rwlock->lock.save_state = 1;
++	rwlock->read_depth = 0;
++}
++EXPORT_SYMBOL(__rt_rwlock_init);
++
++/*
++ * rw_semaphores
++ */
++
++void  rt_up_write(struct rw_semaphore *rwsem)
++{
++	rwsem_release(&rwsem->dep_map, 1, _RET_IP_);
++	rt_mutex_unlock(&rwsem->lock);
++}
++EXPORT_SYMBOL(rt_up_write);
++
++void  rt_up_read(struct rw_semaphore *rwsem)
++{
++	rwsem_release(&rwsem->dep_map, 1, _RET_IP_);
++	if (--rwsem->read_depth == 0)
++		rt_mutex_unlock(&rwsem->lock);
++}
++EXPORT_SYMBOL(rt_up_read);
++
++/*
++ * downgrade a write lock into a read lock
++ * - just wake up any readers at the front of the queue
++ */
++void  rt_downgrade_write(struct rw_semaphore *rwsem)
++{
++	BUG_ON(rt_mutex_owner(&rwsem->lock) != current);
++	rwsem->read_depth = 1;
++}
++EXPORT_SYMBOL(rt_downgrade_write);
++
++int  rt_down_write_trylock(struct rw_semaphore *rwsem)
++{
++	int ret = rt_mutex_trylock(&rwsem->lock);
++
++	if (ret)
++		rwsem_acquire(&rwsem->dep_map, 0, 1, _RET_IP_);
++	return ret;
++}
++EXPORT_SYMBOL(rt_down_write_trylock);
++
++void  rt_down_write(struct rw_semaphore *rwsem)
++{
++	rwsem_acquire(&rwsem->dep_map, 0, 0, _RET_IP_);
++	rt_mutex_lock(&rwsem->lock);
++}
++EXPORT_SYMBOL(rt_down_write);
++
++void  rt_down_write_nested(struct rw_semaphore *rwsem, int subclass)
++{
++	rwsem_acquire(&rwsem->dep_map, subclass, 0, _RET_IP_);
++	rt_mutex_lock(&rwsem->lock);
++}
++EXPORT_SYMBOL(rt_down_write_nested);
++
++void rt_down_write_nested_lock(struct rw_semaphore *rwsem,
++			       struct lockdep_map *nest)
++{
++	rwsem_acquire_nest(&rwsem->dep_map, 0, 0, nest, _RET_IP_);
++	rt_mutex_lock(&rwsem->lock);
++}
++EXPORT_SYMBOL(rt_down_write_nested_lock);
++
++int  rt_down_read_trylock(struct rw_semaphore *rwsem)
++{
++	struct rt_mutex *lock = &rwsem->lock;
++	int ret = 1;
++
++	/*
++	 * recursive read locks succeed when current owns the rwsem,
++	 * but not when read_depth == 0 which means that the rwsem is
++	 * write locked.
++	 */
++	if (rt_mutex_owner(lock) != current)
++		ret = rt_mutex_trylock(&rwsem->lock);
++	else if (!rwsem->read_depth)
++		ret = 0;
++
++	if (ret) {
++		rwsem->read_depth++;
++		rwsem_acquire(&rwsem->dep_map, 0, 1, _RET_IP_);
++	}
++	return ret;
++}
++EXPORT_SYMBOL(rt_down_read_trylock);
++
++static void __rt_down_read(struct rw_semaphore *rwsem, int subclass)
++{
++	struct rt_mutex *lock = &rwsem->lock;
++
++	rwsem_acquire_read(&rwsem->dep_map, subclass, 0, _RET_IP_);
++
++	if (rt_mutex_owner(lock) != current)
++		rt_mutex_lock(&rwsem->lock);
++	rwsem->read_depth++;
++}
++
++void  rt_down_read(struct rw_semaphore *rwsem)
++{
++	__rt_down_read(rwsem, 0);
++}
++EXPORT_SYMBOL(rt_down_read);
++
++void  rt_down_read_nested(struct rw_semaphore *rwsem, int subclass)
++{
++	__rt_down_read(rwsem, subclass);
++}
++EXPORT_SYMBOL(rt_down_read_nested);
++
++void  __rt_rwsem_init(struct rw_semaphore *rwsem, const char *name,
++			      struct lock_class_key *key)
++{
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++	/*
++	 * Make sure we are not reinitializing a held lock:
++	 */
++	debug_check_no_locks_freed((void *)rwsem, sizeof(*rwsem));
++	lockdep_init_map(&rwsem->dep_map, name, key, 0);
++#endif
++	rwsem->read_depth = 0;
++	rwsem->lock.save_state = 0;
++}
++EXPORT_SYMBOL(__rt_rwsem_init);
++
++/**
++ * atomic_dec_and_mutex_lock - return holding mutex if we dec to 0
++ * @cnt: the atomic which we are to dec
++ * @lock: the mutex to return holding if we dec to 0
++ *
++ * return true and hold lock if we dec to 0, return false otherwise
++ */
++int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)
++{
++	/* dec if we can't possibly hit 0 */
++	if (atomic_add_unless(cnt, -1, 1))
++		return 0;
++	/* we might hit 0, so take the lock */
++	mutex_lock(lock);
++	if (!atomic_dec_and_test(cnt)) {
++		/* when we actually did the dec, we didn't hit 0 */
++		mutex_unlock(lock);
++		return 0;
++	}
++	/* we hit 0, and we hold the lock */
++	return 1;
++}
++EXPORT_SYMBOL(atomic_dec_and_mutex_lock);
+diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c
+index 1929ee2291f2..8d950b4521fc 100644
+--- a/kernel/locking/rtmutex.c
++++ b/kernel/locking/rtmutex.c
+@@ -7,6 +7,11 @@
+  *  Copyright (C) 2005-2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>
+  *  Copyright (C) 2005 Kihon Technologies Inc., Steven Rostedt
+  *  Copyright (C) 2006 Esben Nielsen
++ *  Adaptive Spinlocks:
++ *  Copyright (C) 2008 Novell, Inc., Gregory Haskins, Sven Dietrich,
++ *				     and Peter Morreale,
++ * Adaptive Spinlocks simplification:
++ *  Copyright (C) 2008 Red Hat, Inc., Steven Rostedt <srostedt@redhat.com>
+  *
+  *  See Documentation/locking/rt-mutex-design.txt for details.
+  */
+@@ -16,6 +21,7 @@
+ #include <linux/sched/rt.h>
+ #include <linux/sched/deadline.h>
+ #include <linux/timer.h>
++#include <linux/ww_mutex.h>
+ 
+ #include "rtmutex_common.h"
+ 
+@@ -69,6 +75,12 @@ static void fixup_rt_mutex_waiters(struct rt_mutex *lock)
+ 		clear_rt_mutex_waiters(lock);
+ }
+ 
++static int rt_mutex_real_waiter(struct rt_mutex_waiter *waiter)
++{
++	return waiter && waiter != PI_WAKEUP_INPROGRESS &&
++		waiter != PI_REQUEUE_INPROGRESS;
++}
++
+ /*
+  * We can speed up the acquire/release, if the architecture
+  * supports cmpxchg and if there's no debugging state to be set up
+@@ -335,6 +347,14 @@ static bool rt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,
+ 	return debug_rt_mutex_detect_deadlock(waiter, chwalk);
+ }
+ 
++static void rt_mutex_wake_waiter(struct rt_mutex_waiter *waiter)
++{
++	if (waiter->savestate)
++		wake_up_lock_sleeper(waiter->task);
++	else
++		wake_up_process(waiter->task);
++}
++
+ /*
+  * Max number of times we'll walk the boosting chain:
+  */
+@@ -342,7 +362,8 @@ int max_lock_depth = 1024;
+ 
+ static inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)
+ {
+-	return p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;
++	return rt_mutex_real_waiter(p->pi_blocked_on) ?
++		p->pi_blocked_on->lock : NULL;
+ }
+ 
+ /*
+@@ -479,7 +500,7 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
+ 	 * reached or the state of the chain has changed while we
+ 	 * dropped the locks.
+ 	 */
+-	if (!waiter)
++	if (!rt_mutex_real_waiter(waiter))
+ 		goto out_unlock_pi;
+ 
+ 	/*
+@@ -641,13 +662,16 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
+ 	 * follow here. This is the end of the chain we are walking.
+ 	 */
+ 	if (!rt_mutex_owner(lock)) {
++		struct rt_mutex_waiter *lock_top_waiter;
++
+ 		/*
+ 		 * If the requeue [7] above changed the top waiter,
+ 		 * then we need to wake the new top waiter up to try
+ 		 * to get the lock.
+ 		 */
+-		if (prerequeue_top_waiter != rt_mutex_top_waiter(lock))
+-			wake_up_process(rt_mutex_top_waiter(lock)->task);
++		lock_top_waiter = rt_mutex_top_waiter(lock);
++		if (prerequeue_top_waiter != lock_top_waiter)
++			rt_mutex_wake_waiter(lock_top_waiter);
+ 		raw_spin_unlock(&lock->wait_lock);
+ 		return 0;
+ 	}
+@@ -740,6 +764,25 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
+ 	return ret;
+ }
+ 
++
++#define STEAL_NORMAL  0
++#define STEAL_LATERAL 1
++
++/*
++ * Note that RT tasks are excluded from lateral-steals to prevent the
++ * introduction of an unbounded latency
++ */
++static inline int lock_is_stealable(struct task_struct *task,
++				    struct task_struct *pendowner, int mode)
++{
++    if (mode == STEAL_NORMAL || rt_task(task)) {
++	    if (task->prio >= pendowner->prio)
++		    return 0;
++    } else if (task->prio > pendowner->prio)
++	    return 0;
++    return 1;
++}
++
+ /*
+  * Try to take an rt-mutex
+  *
+@@ -750,8 +793,9 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
+  * @waiter: The waiter that is queued to the lock's wait list if the
+  *	    callsite called task_blocked_on_lock(), otherwise NULL
+  */
+-static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
+-				struct rt_mutex_waiter *waiter)
++static int __try_to_take_rt_mutex(struct rt_mutex *lock,
++				  struct task_struct *task,
++				  struct rt_mutex_waiter *waiter, int mode)
+ {
+ 	unsigned long flags;
+ 
+@@ -790,8 +834,10 @@ static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
+ 		 * If waiter is not the highest priority waiter of
+ 		 * @lock, give up.
+ 		 */
+-		if (waiter != rt_mutex_top_waiter(lock))
++		if (waiter != rt_mutex_top_waiter(lock)) {
++			/* XXX lock_is_stealable() ? */
+ 			return 0;
++		}
+ 
+ 		/*
+ 		 * We can acquire the lock. Remove the waiter from the
+@@ -809,14 +855,10 @@ static int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
+ 		 * not need to be dequeued.
+ 		 */
+ 		if (rt_mutex_has_waiters(lock)) {
+-			/*
+-			 * If @task->prio is greater than or equal to
+-			 * the top waiter priority (kernel view),
+-			 * @task lost.
+-			 */
+-			if (task->prio >= rt_mutex_top_waiter(lock)->prio)
+-				return 0;
++			struct task_struct *pown = rt_mutex_top_waiter(lock)->task;
+ 
++			if (task != pown && !lock_is_stealable(task, pown, mode))
++				return 0;
+ 			/*
+ 			 * The current top waiter stays enqueued. We
+ 			 * don't have to change anything in the lock
+@@ -865,6 +907,369 @@ takeit:
+ 	return 1;
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++/*
++ * preemptible spin_lock functions:
++ */
++static inline void rt_spin_lock_fastlock(struct rt_mutex *lock,
++					 void  (*slowfn)(struct rt_mutex *lock))
++{
++	might_sleep();
++
++	if (likely(rt_mutex_cmpxchg(lock, NULL, current)))
++		rt_mutex_deadlock_account_lock(lock, current);
++	else
++		slowfn(lock);
++}
++
++static inline void rt_spin_lock_fastunlock(struct rt_mutex *lock,
++					   void  (*slowfn)(struct rt_mutex *lock))
++{
++	if (likely(rt_mutex_cmpxchg(lock, current, NULL)))
++		rt_mutex_deadlock_account_unlock(current);
++	else
++		slowfn(lock);
++}
++#ifdef CONFIG_SMP
++/*
++ * Note that owner is a speculative pointer and dereferencing relies
++ * on rcu_read_lock() and the check against the lock owner.
++ */
++static int adaptive_wait(struct rt_mutex *lock,
++			 struct task_struct *owner)
++{
++	int res = 0;
++
++	rcu_read_lock();
++	for (;;) {
++		if (owner != rt_mutex_owner(lock))
++			break;
++		/*
++		 * Ensure that owner->on_cpu is dereferenced _after_
++		 * checking the above to be valid.
++		 */
++		barrier();
++		if (!owner->on_cpu) {
++			res = 1;
++			break;
++		}
++		cpu_relax();
++	}
++	rcu_read_unlock();
++	return res;
++}
++#else
++static int adaptive_wait(struct rt_mutex *lock,
++			 struct task_struct *orig_owner)
++{
++	return 1;
++}
++#endif
++
++# define pi_lock(lock)		raw_spin_lock_irq(lock)
++# define pi_unlock(lock)	raw_spin_unlock_irq(lock)
++
++static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
++				   struct rt_mutex_waiter *waiter,
++				   struct task_struct *task,
++				   enum rtmutex_chainwalk chwalk);
++/*
++ * Slow path lock function spin_lock style: this variant is very
++ * careful not to miss any non-lock wakeups.
++ *
++ * We store the current state under p->pi_lock in p->saved_state and
++ * the try_to_wake_up() code handles this accordingly.
++ */
++static void  noinline __sched rt_spin_lock_slowlock(struct rt_mutex *lock)
++{
++	struct task_struct *lock_owner, *self = current;
++	struct rt_mutex_waiter waiter, *top_waiter;
++	int ret;
++
++	rt_mutex_init_waiter(&waiter, true);
++
++	raw_spin_lock(&lock->wait_lock);
++
++	if (__try_to_take_rt_mutex(lock, self, NULL, STEAL_LATERAL)) {
++		raw_spin_unlock(&lock->wait_lock);
++		return;
++	}
++
++	BUG_ON(rt_mutex_owner(lock) == self);
++
++	/*
++	 * We save whatever state the task is in and we'll restore it
++	 * after acquiring the lock taking real wakeups into account
++	 * as well. We are serialized via pi_lock against wakeups. See
++	 * try_to_wake_up().
++	 */
++	pi_lock(&self->pi_lock);
++	self->saved_state = self->state;
++	__set_current_state(TASK_UNINTERRUPTIBLE);
++	pi_unlock(&self->pi_lock);
++
++	ret = task_blocks_on_rt_mutex(lock, &waiter, self, RT_MUTEX_MIN_CHAINWALK);
++	BUG_ON(ret);
++
++	for (;;) {
++		/* Try to acquire the lock again. */
++		if (__try_to_take_rt_mutex(lock, self, &waiter, STEAL_LATERAL))
++			break;
++
++		top_waiter = rt_mutex_top_waiter(lock);
++		lock_owner = rt_mutex_owner(lock);
++
++		raw_spin_unlock(&lock->wait_lock);
++
++		debug_rt_mutex_print_deadlock(&waiter);
++
++		if (top_waiter != &waiter || adaptive_wait(lock, lock_owner))
++			schedule_rt_mutex(lock);
++
++		raw_spin_lock(&lock->wait_lock);
++
++		pi_lock(&self->pi_lock);
++		__set_current_state(TASK_UNINTERRUPTIBLE);
++		pi_unlock(&self->pi_lock);
++	}
++
++	/*
++	 * Restore the task state to current->saved_state. We set it
++	 * to the original state above and the try_to_wake_up() code
++	 * has possibly updated it when a real (non-rtmutex) wakeup
++	 * happened while we were blocked. Clear saved_state so
++	 * try_to_wakeup() does not get confused.
++	 */
++	pi_lock(&self->pi_lock);
++	__set_current_state(self->saved_state);
++	self->saved_state = TASK_RUNNING;
++	pi_unlock(&self->pi_lock);
++
++	/*
++	 * try_to_take_rt_mutex() sets the waiter bit
++	 * unconditionally. We might have to fix that up:
++	 */
++	fixup_rt_mutex_waiters(lock);
++
++	BUG_ON(rt_mutex_has_waiters(lock) && &waiter == rt_mutex_top_waiter(lock));
++	BUG_ON(!RB_EMPTY_NODE(&waiter.tree_entry));
++
++	raw_spin_unlock(&lock->wait_lock);
++
++	debug_rt_mutex_free_waiter(&waiter);
++}
++
++static void wakeup_next_waiter(struct rt_mutex *lock);
++/*
++ * Slow path to release a rt_mutex spin_lock style
++ */
++static void __sched __rt_spin_lock_slowunlock(struct rt_mutex *lock)
++{
++	debug_rt_mutex_unlock(lock);
++
++	rt_mutex_deadlock_account_unlock(current);
++
++	if (!rt_mutex_has_waiters(lock)) {
++		lock->owner = NULL;
++		raw_spin_unlock(&lock->wait_lock);
++		return;
++	}
++
++	wakeup_next_waiter(lock);
++
++	raw_spin_unlock(&lock->wait_lock);
++
++	/* Undo pi boosting.when necessary */
++	rt_mutex_adjust_prio(current);
++}
++
++static void  noinline __sched rt_spin_lock_slowunlock(struct rt_mutex *lock)
++{
++	raw_spin_lock(&lock->wait_lock);
++	__rt_spin_lock_slowunlock(lock);
++}
++
++static void  noinline __sched rt_spin_lock_slowunlock_hirq(struct rt_mutex *lock)
++{
++	int ret;
++
++	do {
++		ret = raw_spin_trylock(&lock->wait_lock);
++	} while (!ret);
++
++	__rt_spin_lock_slowunlock(lock);
++}
++
++void __lockfunc rt_spin_lock(spinlock_t *lock)
++{
++	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
++	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
++}
++EXPORT_SYMBOL(rt_spin_lock);
++
++void __lockfunc __rt_spin_lock(struct rt_mutex *lock)
++{
++	rt_spin_lock_fastlock(lock, rt_spin_lock_slowlock);
++}
++EXPORT_SYMBOL(__rt_spin_lock);
++
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++void __lockfunc rt_spin_lock_nested(spinlock_t *lock, int subclass)
++{
++	rt_spin_lock_fastlock(&lock->lock, rt_spin_lock_slowlock);
++	spin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);
++}
++EXPORT_SYMBOL(rt_spin_lock_nested);
++#endif
++
++void __lockfunc rt_spin_unlock(spinlock_t *lock)
++{
++	/* NOTE: we always pass in '1' for nested, for simplicity */
++	spin_release(&lock->dep_map, 1, _RET_IP_);
++	rt_spin_lock_fastunlock(&lock->lock, rt_spin_lock_slowunlock);
++}
++EXPORT_SYMBOL(rt_spin_unlock);
++
++void __lockfunc rt_spin_unlock_after_trylock_in_irq(spinlock_t *lock)
++{
++	/* NOTE: we always pass in '1' for nested, for simplicity */
++	spin_release(&lock->dep_map, 1, _RET_IP_);
++	rt_spin_lock_fastunlock(&lock->lock, rt_spin_lock_slowunlock_hirq);
++}
++
++void __lockfunc __rt_spin_unlock(struct rt_mutex *lock)
++{
++	rt_spin_lock_fastunlock(lock, rt_spin_lock_slowunlock);
++}
++EXPORT_SYMBOL(__rt_spin_unlock);
++
++/*
++ * Wait for the lock to get unlocked: instead of polling for an unlock
++ * (like raw spinlocks do), we lock and unlock, to force the kernel to
++ * schedule if there's contention:
++ */
++void __lockfunc rt_spin_unlock_wait(spinlock_t *lock)
++{
++	spin_lock(lock);
++	spin_unlock(lock);
++}
++EXPORT_SYMBOL(rt_spin_unlock_wait);
++
++int __lockfunc __rt_spin_trylock(struct rt_mutex *lock)
++{
++	return rt_mutex_trylock(lock);
++}
++
++int __lockfunc rt_spin_trylock(spinlock_t *lock)
++{
++	int ret = rt_mutex_trylock(&lock->lock);
++
++	if (ret)
++		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
++	return ret;
++}
++EXPORT_SYMBOL(rt_spin_trylock);
++
++int __lockfunc rt_spin_trylock_bh(spinlock_t *lock)
++{
++	int ret;
++
++	local_bh_disable();
++	ret = rt_mutex_trylock(&lock->lock);
++	if (ret) {
++		migrate_disable();
++		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
++	} else
++		local_bh_enable();
++	return ret;
++}
++EXPORT_SYMBOL(rt_spin_trylock_bh);
++
++int __lockfunc rt_spin_trylock_irqsave(spinlock_t *lock, unsigned long *flags)
++{
++	int ret;
++
++	*flags = 0;
++	ret = rt_mutex_trylock(&lock->lock);
++	if (ret) {
++		migrate_disable();
++		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
++	}
++	return ret;
++}
++EXPORT_SYMBOL(rt_spin_trylock_irqsave);
++
++int atomic_dec_and_spin_lock(atomic_t *atomic, spinlock_t *lock)
++{
++	/* Subtract 1 from counter unless that drops it to 0 (ie. it was 1) */
++	if (atomic_add_unless(atomic, -1, 1))
++		return 0;
++	migrate_disable();
++	rt_spin_lock(lock);
++	if (atomic_dec_and_test(atomic))
++		return 1;
++	rt_spin_unlock(lock);
++	migrate_enable();
++	return 0;
++}
++EXPORT_SYMBOL(atomic_dec_and_spin_lock);
++
++	void
++__rt_spin_lock_init(spinlock_t *lock, char *name, struct lock_class_key *key)
++{
++#ifdef CONFIG_DEBUG_LOCK_ALLOC
++	/*
++	 * Make sure we are not reinitializing a held lock:
++	 */
++	debug_check_no_locks_freed((void *)lock, sizeof(*lock));
++	lockdep_init_map(&lock->dep_map, name, key, 0);
++#endif
++}
++EXPORT_SYMBOL(__rt_spin_lock_init);
++
++#endif /* PREEMPT_RT_FULL */
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++	static inline int __sched
++__mutex_lock_check_stamp(struct rt_mutex *lock, struct ww_acquire_ctx *ctx)
++{
++	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base.lock);
++	struct ww_acquire_ctx *hold_ctx = ACCESS_ONCE(ww->ctx);
++
++	if (!hold_ctx)
++		return 0;
++
++	if (unlikely(ctx == hold_ctx))
++		return -EALREADY;
++
++	if (ctx->stamp - hold_ctx->stamp <= LONG_MAX &&
++	    (ctx->stamp != hold_ctx->stamp || ctx > hold_ctx)) {
++#ifdef CONFIG_DEBUG_MUTEXES
++		DEBUG_LOCKS_WARN_ON(ctx->contending_lock);
++		ctx->contending_lock = ww;
++#endif
++		return -EDEADLK;
++	}
++
++	return 0;
++}
++#else
++	static inline int __sched
++__mutex_lock_check_stamp(struct rt_mutex *lock, struct ww_acquire_ctx *ctx)
++{
++	BUG();
++	return 0;
++}
++
++#endif
++
++static inline int
++try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,
++		     struct rt_mutex_waiter *waiter)
++{
++	return __try_to_take_rt_mutex(lock, task, waiter, STEAL_NORMAL);
++}
++
+ /*
+  * Task blocks on lock.
+  *
+@@ -896,6 +1301,23 @@ static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
+ 		return -EDEADLK;
+ 
+ 	raw_spin_lock_irqsave(&task->pi_lock, flags);
++
++	/*
++	 * In the case of futex requeue PI, this will be a proxy
++	 * lock. The task will wake unaware that it is enqueueed on
++	 * this lock. Avoid blocking on two locks and corrupting
++	 * pi_blocked_on via the PI_WAKEUP_INPROGRESS
++	 * flag. futex_wait_requeue_pi() sets this when it wakes up
++	 * before requeue (due to a signal or timeout). Do not enqueue
++	 * the task if PI_WAKEUP_INPROGRESS is set.
++	 */
++	if (task != current && task->pi_blocked_on == PI_WAKEUP_INPROGRESS) {
++		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
++		return -EAGAIN;
++	}
++
++	BUG_ON(rt_mutex_real_waiter(task->pi_blocked_on));
++
+ 	__rt_mutex_adjust_prio(task);
+ 	waiter->task = task;
+ 	waiter->lock = lock;
+@@ -919,7 +1341,7 @@ static int task_blocks_on_rt_mutex(struct rt_mutex *lock,
+ 		rt_mutex_enqueue_pi(owner, waiter);
+ 
+ 		__rt_mutex_adjust_prio(owner);
+-		if (owner->pi_blocked_on)
++		if (rt_mutex_real_waiter(owner->pi_blocked_on))
+ 			chain_walk = 1;
+ 	} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {
+ 		chain_walk = 1;
+@@ -996,7 +1418,7 @@ static void wakeup_next_waiter(struct rt_mutex *lock)
+ 	 * long as we hold lock->wait_lock. The waiter task needs to
+ 	 * acquire it in order to dequeue the waiter.
+ 	 */
+-	wake_up_process(waiter->task);
++	rt_mutex_wake_waiter(waiter);
+ }
+ 
+ /*
+@@ -1010,7 +1432,7 @@ static void remove_waiter(struct rt_mutex *lock,
+ {
+ 	bool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));
+ 	struct task_struct *owner = rt_mutex_owner(lock);
+-	struct rt_mutex *next_lock;
++	struct rt_mutex *next_lock = NULL;
+ 	unsigned long flags;
+ 
+ 	raw_spin_lock_irqsave(&current->pi_lock, flags);
+@@ -1035,7 +1457,8 @@ static void remove_waiter(struct rt_mutex *lock,
+ 	__rt_mutex_adjust_prio(owner);
+ 
+ 	/* Store the lock on which owner is blocked or NULL */
+-	next_lock = task_blocked_on_lock(owner);
++	if (rt_mutex_real_waiter(owner->pi_blocked_on))
++		next_lock = task_blocked_on_lock(owner);
+ 
+ 	raw_spin_unlock_irqrestore(&owner->pi_lock, flags);
+ 
+@@ -1071,17 +1494,17 @@ void rt_mutex_adjust_pi(struct task_struct *task)
+ 	raw_spin_lock_irqsave(&task->pi_lock, flags);
+ 
+ 	waiter = task->pi_blocked_on;
+-	if (!waiter || (waiter->prio == task->prio &&
++	if (!rt_mutex_real_waiter(waiter) || (waiter->prio == task->prio &&
+ 			!dl_prio(task->prio))) {
+ 		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
+ 		return;
+ 	}
+ 	next_lock = waiter->lock;
+-	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
+ 
+ 	/* gets dropped in rt_mutex_adjust_prio_chain()! */
+ 	get_task_struct(task);
+ 
++	raw_spin_unlock_irqrestore(&task->pi_lock, flags);
+ 	rt_mutex_adjust_prio_chain(task, RT_MUTEX_MIN_CHAINWALK, NULL,
+ 				   next_lock, NULL, task);
+ }
+@@ -1099,7 +1522,8 @@ void rt_mutex_adjust_pi(struct task_struct *task)
+ static int __sched
+ __rt_mutex_slowlock(struct rt_mutex *lock, int state,
+ 		    struct hrtimer_sleeper *timeout,
+-		    struct rt_mutex_waiter *waiter)
++		    struct rt_mutex_waiter *waiter,
++		    struct ww_acquire_ctx *ww_ctx)
+ {
+ 	int ret = 0;
+ 
+@@ -1122,6 +1546,12 @@ __rt_mutex_slowlock(struct rt_mutex *lock, int state,
+ 				break;
+ 		}
+ 
++		if (ww_ctx && ww_ctx->acquired > 0) {
++			ret = __mutex_lock_check_stamp(lock, ww_ctx);
++			if (ret)
++				break;
++		}
++
+ 		raw_spin_unlock(&lock->wait_lock);
+ 
+ 		debug_rt_mutex_print_deadlock(waiter);
+@@ -1155,25 +1585,102 @@ static void rt_mutex_handle_deadlock(int res, int detect_deadlock,
+ 	}
+ }
+ 
++static __always_inline void ww_mutex_lock_acquired(struct ww_mutex *ww,
++						   struct ww_acquire_ctx *ww_ctx)
++{
++#ifdef CONFIG_DEBUG_MUTEXES
++	/*
++	 * If this WARN_ON triggers, you used ww_mutex_lock to acquire,
++	 * but released with a normal mutex_unlock in this call.
++	 *
++	 * This should never happen, always use ww_mutex_unlock.
++	 */
++	DEBUG_LOCKS_WARN_ON(ww->ctx);
++
++	/*
++	 * Not quite done after calling ww_acquire_done() ?
++	 */
++	DEBUG_LOCKS_WARN_ON(ww_ctx->done_acquire);
++
++	if (ww_ctx->contending_lock) {
++		/*
++		 * After -EDEADLK you tried to
++		 * acquire a different ww_mutex? Bad!
++		 */
++		DEBUG_LOCKS_WARN_ON(ww_ctx->contending_lock != ww);
++
++		/*
++		 * You called ww_mutex_lock after receiving -EDEADLK,
++		 * but 'forgot' to unlock everything else first?
++		 */
++		DEBUG_LOCKS_WARN_ON(ww_ctx->acquired > 0);
++		ww_ctx->contending_lock = NULL;
++	}
++
++	/*
++	 * Naughty, using a different class will lead to undefined behavior!
++	 */
++	DEBUG_LOCKS_WARN_ON(ww_ctx->ww_class != ww->ww_class);
++#endif
++	ww_ctx->acquired++;
++}
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++static void ww_mutex_account_lock(struct rt_mutex *lock,
++				  struct ww_acquire_ctx *ww_ctx)
++{
++	struct ww_mutex *ww = container_of(lock, struct ww_mutex, base.lock);
++	struct rt_mutex_waiter *waiter, *n;
++
++	/*
++	 * This branch gets optimized out for the common case,
++	 * and is only important for ww_mutex_lock.
++	 */
++	ww_mutex_lock_acquired(ww, ww_ctx);
++	ww->ctx = ww_ctx;
++
++	/*
++	 * Give any possible sleeping processes the chance to wake up,
++	 * so they can recheck if they have to back off.
++	 */
++	rbtree_postorder_for_each_entry_safe(waiter, n, &lock->waiters,
++					     tree_entry) {
++		/* XXX debug rt mutex waiter wakeup */
++
++		BUG_ON(waiter->lock != lock);
++		rt_mutex_wake_waiter(waiter);
++	}
++}
++
++#else
++
++static void ww_mutex_account_lock(struct rt_mutex *lock,
++				  struct ww_acquire_ctx *ww_ctx)
++{
++	BUG();
++}
++#endif
++
+ /*
+  * Slow path lock function:
+  */
+ static int __sched
+ rt_mutex_slowlock(struct rt_mutex *lock, int state,
+ 		  struct hrtimer_sleeper *timeout,
+-		  enum rtmutex_chainwalk chwalk)
++		  enum rtmutex_chainwalk chwalk,
++		  struct ww_acquire_ctx *ww_ctx)
+ {
+ 	struct rt_mutex_waiter waiter;
+ 	int ret = 0;
+ 
+-	debug_rt_mutex_init_waiter(&waiter);
+-	RB_CLEAR_NODE(&waiter.pi_tree_entry);
+-	RB_CLEAR_NODE(&waiter.tree_entry);
++	rt_mutex_init_waiter(&waiter, false);
+ 
+ 	raw_spin_lock(&lock->wait_lock);
+ 
+ 	/* Try to acquire the lock again: */
+ 	if (try_to_take_rt_mutex(lock, current, NULL)) {
++		if (ww_ctx)
++			ww_mutex_account_lock(lock, ww_ctx);
+ 		raw_spin_unlock(&lock->wait_lock);
+ 		return 0;
+ 	}
+@@ -1190,14 +1697,23 @@ rt_mutex_slowlock(struct rt_mutex *lock, int state,
+ 	ret = task_blocks_on_rt_mutex(lock, &waiter, current, chwalk);
+ 
+ 	if (likely(!ret))
+-		ret = __rt_mutex_slowlock(lock, state, timeout, &waiter);
++		ret = __rt_mutex_slowlock(lock, state, timeout, &waiter, ww_ctx);
++	else if (ww_ctx) {
++		/* ww_mutex received EDEADLK, let it become EALREADY */
++		ret = __mutex_lock_check_stamp(lock, ww_ctx);
++		BUG_ON(!ret);
++	}
+ 
+ 	set_current_state(TASK_RUNNING);
+ 
+ 	if (unlikely(ret)) {
+ 		if (rt_mutex_has_waiters(lock))
+ 			remove_waiter(lock, &waiter);
+-		rt_mutex_handle_deadlock(ret, chwalk, &waiter);
++		/* ww_mutex want to report EDEADLK/EALREADY, let them */
++		if (!ww_ctx)
++			rt_mutex_handle_deadlock(ret, chwalk, &waiter);
++	} else if (ww_ctx) {
++		ww_mutex_account_lock(lock, ww_ctx);
+ 	}
+ 
+ 	/*
+@@ -1236,7 +1752,8 @@ static inline int rt_mutex_slowtrylock(struct rt_mutex *lock)
+ 	 * The mutex has currently no owner. Lock the wait lock and
+ 	 * try to acquire the lock.
+ 	 */
+-	raw_spin_lock(&lock->wait_lock);
++	if (!raw_spin_trylock(&lock->wait_lock))
++		return 0;
+ 
+ 	ret = try_to_take_rt_mutex(lock, current, NULL);
+ 
+@@ -1322,31 +1839,36 @@ rt_mutex_slowunlock(struct rt_mutex *lock)
+  */
+ static inline int
+ rt_mutex_fastlock(struct rt_mutex *lock, int state,
++		  struct ww_acquire_ctx *ww_ctx,
+ 		  int (*slowfn)(struct rt_mutex *lock, int state,
+ 				struct hrtimer_sleeper *timeout,
+-				enum rtmutex_chainwalk chwalk))
++				enum rtmutex_chainwalk chwalk,
++				struct ww_acquire_ctx *ww_ctx))
+ {
+ 	if (likely(rt_mutex_cmpxchg(lock, NULL, current))) {
+ 		rt_mutex_deadlock_account_lock(lock, current);
+ 		return 0;
+ 	} else
+-		return slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);
++		return slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK,
++			      ww_ctx);
+ }
+ 
+ static inline int
+ rt_mutex_timed_fastlock(struct rt_mutex *lock, int state,
+ 			struct hrtimer_sleeper *timeout,
+ 			enum rtmutex_chainwalk chwalk,
++			struct ww_acquire_ctx *ww_ctx,
+ 			int (*slowfn)(struct rt_mutex *lock, int state,
+ 				      struct hrtimer_sleeper *timeout,
+-				      enum rtmutex_chainwalk chwalk))
++				      enum rtmutex_chainwalk chwalk,
++				      struct ww_acquire_ctx *ww_ctx))
+ {
+ 	if (chwalk == RT_MUTEX_MIN_CHAINWALK &&
+ 	    likely(rt_mutex_cmpxchg(lock, NULL, current))) {
+ 		rt_mutex_deadlock_account_lock(lock, current);
+ 		return 0;
+ 	} else
+-		return slowfn(lock, state, timeout, chwalk);
++		return slowfn(lock, state, timeout, chwalk, ww_ctx);
+ }
+ 
+ static inline int
+@@ -1379,7 +1901,7 @@ void __sched rt_mutex_lock(struct rt_mutex *lock)
+ {
+ 	might_sleep();
+ 
+-	rt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);
++	rt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, NULL, rt_mutex_slowlock);
+ }
+ EXPORT_SYMBOL_GPL(rt_mutex_lock);
+ 
+@@ -1396,7 +1918,7 @@ int __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)
+ {
+ 	might_sleep();
+ 
+-	return rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);
++	return rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, NULL, rt_mutex_slowlock);
+ }
+ EXPORT_SYMBOL_GPL(rt_mutex_lock_interruptible);
+ 
+@@ -1409,11 +1931,30 @@ int rt_mutex_timed_futex_lock(struct rt_mutex *lock,
+ 	might_sleep();
+ 
+ 	return rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,
+-				       RT_MUTEX_FULL_CHAINWALK,
++				       RT_MUTEX_FULL_CHAINWALK, NULL,
+ 				       rt_mutex_slowlock);
+ }
+ 
+ /**
++ * rt_mutex_lock_killable - lock a rt_mutex killable
++ *
++ * @lock:              the rt_mutex to be locked
++ * @detect_deadlock:   deadlock detection on/off
++ *
++ * Returns:
++ *  0          on success
++ * -EINTR      when interrupted by a signal
++ * -EDEADLK    when the lock would deadlock (when deadlock detection is on)
++ */
++int __sched rt_mutex_lock_killable(struct rt_mutex *lock)
++{
++	might_sleep();
++
++	return rt_mutex_fastlock(lock, TASK_KILLABLE, NULL, rt_mutex_slowlock);
++}
++EXPORT_SYMBOL_GPL(rt_mutex_lock_killable);
++
++/**
+  * rt_mutex_timed_lock - lock a rt_mutex interruptible
+  *			the timeout structure is provided
+  *			by the caller
+@@ -1433,6 +1974,7 @@ rt_mutex_timed_lock(struct rt_mutex *lock, struct hrtimer_sleeper *timeout)
+ 
+ 	return rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,
+ 				       RT_MUTEX_MIN_CHAINWALK,
++				       NULL,
+ 				       rt_mutex_slowlock);
+ }
+ EXPORT_SYMBOL_GPL(rt_mutex_timed_lock);
+@@ -1491,13 +2033,12 @@ EXPORT_SYMBOL_GPL(rt_mutex_destroy);
+ void __rt_mutex_init(struct rt_mutex *lock, const char *name)
+ {
+ 	lock->owner = NULL;
+-	raw_spin_lock_init(&lock->wait_lock);
+ 	lock->waiters = RB_ROOT;
+ 	lock->waiters_leftmost = NULL;
+ 
+ 	debug_rt_mutex_init(lock, name);
+ }
+-EXPORT_SYMBOL_GPL(__rt_mutex_init);
++EXPORT_SYMBOL(__rt_mutex_init);
+ 
+ /**
+  * rt_mutex_init_proxy_locked - initialize and lock a rt_mutex on behalf of a
+@@ -1512,7 +2053,7 @@ EXPORT_SYMBOL_GPL(__rt_mutex_init);
+ void rt_mutex_init_proxy_locked(struct rt_mutex *lock,
+ 				struct task_struct *proxy_owner)
+ {
+-	__rt_mutex_init(lock, NULL);
++	rt_mutex_init(lock);
+ 	debug_rt_mutex_proxy_lock(lock, proxy_owner);
+ 	rt_mutex_set_owner(lock, proxy_owner);
+ 	rt_mutex_deadlock_account_lock(lock, proxy_owner);
+@@ -1560,6 +2101,35 @@ int rt_mutex_start_proxy_lock(struct rt_mutex *lock,
+ 		return 1;
+ 	}
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++	/*
++	 * In PREEMPT_RT there's an added race.
++	 * If the task, that we are about to requeue, times out,
++	 * it can set the PI_WAKEUP_INPROGRESS. This tells the requeue
++	 * to skip this task. But right after the task sets
++	 * its pi_blocked_on to PI_WAKEUP_INPROGRESS it can then
++	 * block on the spin_lock(&hb->lock), which in RT is an rtmutex.
++	 * This will replace the PI_WAKEUP_INPROGRESS with the actual
++	 * lock that it blocks on. We *must not* place this task
++	 * on this proxy lock in that case.
++	 *
++	 * To prevent this race, we first take the task's pi_lock
++	 * and check if it has updated its pi_blocked_on. If it has,
++	 * we assume that it woke up and we return -EAGAIN.
++	 * Otherwise, we set the task's pi_blocked_on to
++	 * PI_REQUEUE_INPROGRESS, so that if the task is waking up
++	 * it will know that we are in the process of requeuing it.
++	 */
++	raw_spin_lock_irq(&task->pi_lock);
++	if (task->pi_blocked_on) {
++		raw_spin_unlock_irq(&task->pi_lock);
++		raw_spin_unlock(&lock->wait_lock);
++		return -EAGAIN;
++	}
++	task->pi_blocked_on = PI_REQUEUE_INPROGRESS;
++	raw_spin_unlock_irq(&task->pi_lock);
++#endif
++
+ 	/* We enforce deadlock detection for futexes */
+ 	ret = task_blocks_on_rt_mutex(lock, waiter, task,
+ 				      RT_MUTEX_FULL_CHAINWALK);
+@@ -1574,7 +2144,7 @@ int rt_mutex_start_proxy_lock(struct rt_mutex *lock,
+ 		ret = 0;
+ 	}
+ 
+-	if (unlikely(ret))
++	if (ret && rt_mutex_has_waiters(lock))
+ 		remove_waiter(lock, waiter);
+ 
+ 	raw_spin_unlock(&lock->wait_lock);
+@@ -1629,7 +2199,7 @@ int rt_mutex_finish_proxy_lock(struct rt_mutex *lock,
+ 
+ 	set_current_state(TASK_INTERRUPTIBLE);
+ 
+-	ret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter);
++	ret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter, NULL);
+ 
+ 	set_current_state(TASK_RUNNING);
+ 
+@@ -1646,3 +2216,89 @@ int rt_mutex_finish_proxy_lock(struct rt_mutex *lock,
+ 
+ 	return ret;
+ }
++
++static inline int
++ww_mutex_deadlock_injection(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)
++{
++#ifdef CONFIG_DEBUG_WW_MUTEX_SLOWPATH
++	unsigned tmp;
++
++	if (ctx->deadlock_inject_countdown-- == 0) {
++		tmp = ctx->deadlock_inject_interval;
++		if (tmp > UINT_MAX/4)
++			tmp = UINT_MAX;
++		else
++			tmp = tmp*2 + tmp + tmp/2;
++
++		ctx->deadlock_inject_interval = tmp;
++		ctx->deadlock_inject_countdown = tmp;
++		ctx->contending_lock = lock;
++
++		ww_mutex_unlock(lock);
++
++		return -EDEADLK;
++	}
++#endif
++
++	return 0;
++}
++
++#ifdef CONFIG_PREEMPT_RT_FULL
++int __sched
++__ww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx)
++{
++	int ret;
++
++	might_sleep();
++
++	mutex_acquire_nest(&lock->base.dep_map, 0, 0, &ww_ctx->dep_map, _RET_IP_);
++	ret = rt_mutex_slowlock(&lock->base.lock, TASK_INTERRUPTIBLE, NULL, 0, ww_ctx);
++	if (ret)
++		mutex_release(&lock->base.dep_map, 1, _RET_IP_);
++	else if (!ret && ww_ctx->acquired > 1)
++		return ww_mutex_deadlock_injection(lock, ww_ctx);
++
++	return ret;
++}
++EXPORT_SYMBOL_GPL(__ww_mutex_lock_interruptible);
++
++int __sched
++__ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ww_ctx)
++{
++	int ret;
++
++	might_sleep();
++
++	mutex_acquire_nest(&lock->base.dep_map, 0, 0, &ww_ctx->dep_map, _RET_IP_);
++	ret = rt_mutex_slowlock(&lock->base.lock, TASK_UNINTERRUPTIBLE, NULL, 0, ww_ctx);
++	if (ret)
++		mutex_release(&lock->base.dep_map, 1, _RET_IP_);
++	else if (!ret && ww_ctx->acquired > 1)
++		return ww_mutex_deadlock_injection(lock, ww_ctx);
++
++	return ret;
++}
++EXPORT_SYMBOL_GPL(__ww_mutex_lock);
++
++void __sched ww_mutex_unlock(struct ww_mutex *lock)
++{
++	int nest = !!lock->ctx;
++
++	/*
++	 * The unlocking fastpath is the 0->1 transition from 'locked'
++	 * into 'unlocked' state:
++	 */
++	if (nest) {
++#ifdef CONFIG_DEBUG_MUTEXES
++		DEBUG_LOCKS_WARN_ON(!lock->ctx->acquired);
++#endif
++		if (lock->ctx->acquired > 0)
++			lock->ctx->acquired--;
++		lock->ctx = NULL;
++	}
++
++	mutex_release(&lock->base.dep_map, nest, _RET_IP_);
++	rt_mutex_unlock(&lock->base.lock);
++}
++EXPORT_SYMBOL(ww_mutex_unlock);
++#endif
+diff --git a/kernel/locking/rtmutex_common.h b/kernel/locking/rtmutex_common.h
+index 855212501407..c6dcda5e53af 100644
+--- a/kernel/locking/rtmutex_common.h
++++ b/kernel/locking/rtmutex_common.h
+@@ -49,6 +49,7 @@ struct rt_mutex_waiter {
+ 	struct rb_node          pi_tree_entry;
+ 	struct task_struct	*task;
+ 	struct rt_mutex		*lock;
++	bool			savestate;
+ #ifdef CONFIG_DEBUG_RT_MUTEXES
+ 	unsigned long		ip;
+ 	struct pid		*deadlock_task_pid;
+@@ -119,6 +120,9 @@ enum rtmutex_chainwalk {
+ /*
+  * PI-futex support (proxy locking functions, etc.):
+  */
++#define PI_WAKEUP_INPROGRESS	((struct rt_mutex_waiter *) 1)
++#define PI_REQUEUE_INPROGRESS	((struct rt_mutex_waiter *) 2)
++
+ extern struct task_struct *rt_mutex_next_owner(struct rt_mutex *lock);
+ extern void rt_mutex_init_proxy_locked(struct rt_mutex *lock,
+ 				       struct task_struct *proxy_owner);
+@@ -138,4 +142,14 @@ extern int rt_mutex_timed_futex_lock(struct rt_mutex *l, struct hrtimer_sleeper
+ # include "rtmutex.h"
+ #endif
+ 
++static inline void
++rt_mutex_init_waiter(struct rt_mutex_waiter *waiter, bool savestate)
++{
++	debug_rt_mutex_init_waiter(waiter);
++	waiter->task = NULL;
++	waiter->savestate = savestate;
++	RB_CLEAR_NODE(&waiter->pi_tree_entry);
++	RB_CLEAR_NODE(&waiter->tree_entry);
++}
++
+ #endif
+diff --git a/kernel/locking/spinlock.c b/kernel/locking/spinlock.c
+index 4b082b5cac9e..5c76166f88e2 100644
+--- a/kernel/locking/spinlock.c
++++ b/kernel/locking/spinlock.c
+@@ -124,8 +124,11 @@ void __lockfunc __raw_##op##_lock_bh(locktype##_t *lock)		\
+  *         __[spin|read|write]_lock_bh()
+  */
+ BUILD_LOCK_OPS(spin, raw_spinlock);
++
++#ifndef CONFIG_PREEMPT_RT_FULL
+ BUILD_LOCK_OPS(read, rwlock);
+ BUILD_LOCK_OPS(write, rwlock);
++#endif
+ 
+ #endif
+ 
+@@ -209,6 +212,8 @@ void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)
+ EXPORT_SYMBOL(_raw_spin_unlock_bh);
+ #endif
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++
+ #ifndef CONFIG_INLINE_READ_TRYLOCK
+ int __lockfunc _raw_read_trylock(rwlock_t *lock)
+ {
+@@ -353,6 +358,8 @@ void __lockfunc _raw_write_unlock_bh(rwlock_t *lock)
+ EXPORT_SYMBOL(_raw_write_unlock_bh);
+ #endif
+ 
++#endif /* !PREEMPT_RT_FULL */
++
+ #ifdef CONFIG_DEBUG_LOCK_ALLOC
+ 
+ void __lockfunc _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)
+diff --git a/kernel/locking/spinlock_debug.c b/kernel/locking/spinlock_debug.c
+index 0374a596cffa..94970338d518 100644
+--- a/kernel/locking/spinlock_debug.c
++++ b/kernel/locking/spinlock_debug.c
+@@ -31,6 +31,7 @@ void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,
+ 
+ EXPORT_SYMBOL(__raw_spin_lock_init);
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ void __rwlock_init(rwlock_t *lock, const char *name,
+ 		   struct lock_class_key *key)
+ {
+@@ -48,6 +49,7 @@ void __rwlock_init(rwlock_t *lock, const char *name,
+ }
+ 
+ EXPORT_SYMBOL(__rwlock_init);
++#endif
+ 
+ static void spin_dump(raw_spinlock_t *lock, const char *msg)
+ {
+@@ -159,6 +161,7 @@ void do_raw_spin_unlock(raw_spinlock_t *lock)
+ 	arch_spin_unlock(&lock->raw_lock);
+ }
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ static void rwlock_bug(rwlock_t *lock, const char *msg)
+ {
+ 	if (!debug_locks_off())
+@@ -300,3 +303,5 @@ void do_raw_write_unlock(rwlock_t *lock)
+ 	debug_write_unlock(lock);
+ 	arch_write_unlock(&lock->raw_lock);
+ }
++
++#endif
+diff --git a/kernel/module.c b/kernel/module.c
+index 737d53099360..ab5169e72289 100644
+--- a/kernel/module.c
++++ b/kernel/module.c
+@@ -526,16 +526,7 @@ static void percpu_modcopy(struct module *mod,
+ 		memcpy(per_cpu_ptr(mod->percpu, cpu), from, size);
+ }
+ 
+-/**
+- * is_module_percpu_address - test whether address is from module static percpu
+- * @addr: address to test
+- *
+- * Test whether @addr belongs to module static percpu area.
+- *
+- * RETURNS:
+- * %true if @addr is from module static percpu area
+- */
+-bool is_module_percpu_address(unsigned long addr)
++bool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr)
+ {
+ 	struct module *mod;
+ 	unsigned int cpu;
+@@ -549,9 +540,11 @@ bool is_module_percpu_address(unsigned long addr)
+ 			continue;
+ 		for_each_possible_cpu(cpu) {
+ 			void *start = per_cpu_ptr(mod->percpu, cpu);
++			void *va = (void *)addr;
+ 
+-			if ((void *)addr >= start &&
+-			    (void *)addr < start + mod->percpu_size) {
++			if (va >= start && va < start + mod->percpu_size) {
++				if (can_addr)
++					*can_addr = (unsigned long) (va - start);
+ 				preempt_enable();
+ 				return true;
+ 			}
+@@ -562,6 +555,20 @@ bool is_module_percpu_address(unsigned long addr)
+ 	return false;
+ }
+ 
++/**
++ * is_module_percpu_address - test whether address is from module static percpu
++ * @addr: address to test
++ *
++ * Test whether @addr belongs to module static percpu area.
++ *
++ * RETURNS:
++ * %true if @addr is from module static percpu area
++ */
++bool is_module_percpu_address(unsigned long addr)
++{
++	return __is_module_percpu_address(addr, NULL);
++}
++
+ #else /* ... !CONFIG_SMP */
+ 
+ static inline void __percpu *mod_percpu(struct module *mod)
+@@ -593,6 +600,11 @@ bool is_module_percpu_address(unsigned long addr)
+ 	return false;
+ }
+ 
++bool __is_module_percpu_address(unsigned long addr, unsigned long *can_addr)
++{
++	return false;
++}
++
+ #endif /* CONFIG_SMP */
+ 
+ #define MODINFO_ATTR(field)	\
+diff --git a/kernel/panic.c b/kernel/panic.c
+index b81e85ba7de2..5179cc6872b9 100644
+--- a/kernel/panic.c
++++ b/kernel/panic.c
+@@ -396,9 +396,11 @@ static u64 oops_id;
+ 
+ static int init_oops_id(void)
+ {
++#ifndef CONFIG_PREEMPT_RT_FULL
+ 	if (!oops_id)
+ 		get_random_bytes(&oops_id, sizeof(oops_id));
+ 	else
++#endif
+ 		oops_id++;
+ 
+ 	return 0;
+diff --git a/kernel/power/hibernate.c b/kernel/power/hibernate.c
+index 1f35a3478f3c..f7170869157b 100644
+--- a/kernel/power/hibernate.c
++++ b/kernel/power/hibernate.c
+@@ -287,6 +287,8 @@ static int create_image(int platform_mode)
+ 
+ 	local_irq_disable();
+ 
++	system_state = SYSTEM_SUSPEND;
++
+ 	error = syscore_suspend();
+ 	if (error) {
+ 		printk(KERN_ERR "PM: Some system devices failed to power down, "
+@@ -316,6 +318,7 @@ static int create_image(int platform_mode)
+ 	syscore_resume();
+ 
+  Enable_irqs:
++	system_state = SYSTEM_RUNNING;
+ 	local_irq_enable();
+ 
+  Enable_cpus:
+@@ -439,6 +442,7 @@ static int resume_target_kernel(bool platform_mode)
+ 		goto Enable_cpus;
+ 
+ 	local_irq_disable();
++	system_state = SYSTEM_SUSPEND;
+ 
+ 	error = syscore_suspend();
+ 	if (error)
+@@ -472,6 +476,7 @@ static int resume_target_kernel(bool platform_mode)
+ 	syscore_resume();
+ 
+  Enable_irqs:
++	system_state = SYSTEM_RUNNING;
+ 	local_irq_enable();
+ 
+  Enable_cpus:
+@@ -557,6 +562,7 @@ int hibernation_platform_enter(void)
+ 		goto Platform_finish;
+ 
+ 	local_irq_disable();
++	system_state = SYSTEM_SUSPEND;
+ 	syscore_suspend();
+ 	if (pm_wakeup_pending()) {
+ 		error = -EAGAIN;
+@@ -569,6 +575,7 @@ int hibernation_platform_enter(void)
+ 
+  Power_up:
+ 	syscore_resume();
++	system_state = SYSTEM_RUNNING;
+ 	local_irq_enable();
+ 	enable_nonboot_cpus();
+ 
+diff --git a/kernel/power/suspend.c b/kernel/power/suspend.c
+index c347e3ce3a55..ae509484125c 100644
+--- a/kernel/power/suspend.c
++++ b/kernel/power/suspend.c
+@@ -318,6 +318,8 @@ static int suspend_enter(suspend_state_t state, bool *wakeup)
+ 	arch_suspend_disable_irqs();
+ 	BUG_ON(!irqs_disabled());
+ 
++	system_state = SYSTEM_SUSPEND;
++
+ 	error = syscore_suspend();
+ 	if (!error) {
+ 		*wakeup = pm_wakeup_pending();
+@@ -332,6 +334,8 @@ static int suspend_enter(suspend_state_t state, bool *wakeup)
+ 		syscore_resume();
+ 	}
+ 
++	system_state = SYSTEM_RUNNING;
++
+ 	arch_suspend_enable_irqs();
+ 	BUG_ON(irqs_disabled());
+ 
+diff --git a/kernel/printk/printk.c b/kernel/printk/printk.c
+index 3100e0dd560e..2799395a8f0a 100644
+--- a/kernel/printk/printk.c
++++ b/kernel/printk/printk.c
+@@ -1166,6 +1166,7 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
+ {
+ 	char *text;
+ 	int len = 0;
++	int attempts = 0;
+ 
+ 	text = kmalloc(LOG_LINE_MAX + PREFIX_MAX, GFP_KERNEL);
+ 	if (!text)
+@@ -1177,7 +1178,14 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
+ 		u64 seq;
+ 		u32 idx;
+ 		enum log_flags prev;
+-
++		int num_msg;
++try_again:
++		attempts++;
++		if (attempts > 10) {
++			len = -EBUSY;
++			goto out;
++		}
++		num_msg = 0;
+ 		if (clear_seq < log_first_seq) {
+ 			/* messages are gone, move to first available one */
+ 			clear_seq = log_first_seq;
+@@ -1198,6 +1206,14 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
+ 			prev = msg->flags;
+ 			idx = log_next(idx);
+ 			seq++;
++			num_msg++;
++			if (num_msg > 5) {
++				num_msg = 0;
++				raw_spin_unlock_irq(&logbuf_lock);
++				raw_spin_lock_irq(&logbuf_lock);
++				if (clear_seq < log_first_seq)
++					goto try_again;
++			}
+ 		}
+ 
+ 		/* move first record forward until length fits into the buffer */
+@@ -1211,6 +1227,14 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
+ 			prev = msg->flags;
+ 			idx = log_next(idx);
+ 			seq++;
++			num_msg++;
++			if (num_msg > 5) {
++				num_msg = 0;
++				raw_spin_unlock_irq(&logbuf_lock);
++				raw_spin_lock_irq(&logbuf_lock);
++				if (clear_seq < log_first_seq)
++					goto try_again;
++			}
+ 		}
+ 
+ 		/* last message fitting into this dump */
+@@ -1251,6 +1275,7 @@ static int syslog_print_all(char __user *buf, int size, bool clear)
+ 		clear_seq = log_next_seq;
+ 		clear_idx = log_next_idx;
+ 	}
++out:
+ 	raw_spin_unlock_irq(&logbuf_lock);
+ 
+ 	kfree(text);
+@@ -1404,6 +1429,12 @@ static void call_console_drivers(int level, const char *text, size_t len)
+ 	if (!console_drivers)
+ 		return;
+ 
++	if (IS_ENABLED(CONFIG_PREEMPT_RT_BASE)) {
++		if (in_irq() || in_nmi())
++			return;
++	}
++
++	migrate_disable();
+ 	for_each_console(con) {
+ 		if (exclusive_console && con != exclusive_console)
+ 			continue;
+@@ -1416,6 +1447,7 @@ static void call_console_drivers(int level, const char *text, size_t len)
+ 			continue;
+ 		con->write(con, text, len);
+ 	}
++	migrate_enable();
+ }
+ 
+ /*
+@@ -1476,6 +1508,15 @@ static inline int can_use_console(unsigned int cpu)
+ static int console_trylock_for_printk(void)
+ {
+ 	unsigned int cpu = smp_processor_id();
++#ifdef CONFIG_PREEMPT_RT_FULL
++	int lock = !early_boot_irqs_disabled && (preempt_count() == 0) &&
++		!irqs_disabled();
++#else
++	int lock = 1;
++#endif
++
++	if (!lock)
++		return 0;
+ 
+ 	if (!console_trylock())
+ 		return 0;
+@@ -1610,6 +1651,62 @@ static size_t cont_print_text(char *text, size_t size)
+ 	return textlen;
+ }
+ 
++#ifdef CONFIG_EARLY_PRINTK
++struct console *early_console;
++
++void early_vprintk(const char *fmt, va_list ap)
++{
++	if (early_console) {
++		char buf[512];
++		int n = vscnprintf(buf, sizeof(buf), fmt, ap);
++
++		early_console->write(early_console, buf, n);
++	}
++}
++
++asmlinkage void early_printk(const char *fmt, ...)
++{
++	va_list ap;
++
++	va_start(ap, fmt);
++	early_vprintk(fmt, ap);
++	va_end(ap);
++}
++
++/*
++ * This is independent of any log levels - a global
++ * kill switch that turns off all of printk.
++ *
++ * Used by the NMI watchdog if early-printk is enabled.
++ */
++static bool __read_mostly printk_killswitch;
++
++static int __init force_early_printk_setup(char *str)
++{
++	printk_killswitch = true;
++	return 0;
++}
++early_param("force_early_printk", force_early_printk_setup);
++
++void printk_kill(void)
++{
++	printk_killswitch = true;
++}
++
++static int forced_early_printk(const char *fmt, va_list ap)
++{
++	if (!printk_killswitch)
++		return 0;
++	early_vprintk(fmt, ap);
++	return 1;
++}
++#else
++static inline int forced_early_printk(const char *fmt, va_list ap)
++{
++	return 0;
++}
++#endif
++
+ asmlinkage int vprintk_emit(int facility, int level,
+ 			    const char *dict, size_t dictlen,
+ 			    const char *fmt, va_list args)
+@@ -1626,6 +1723,13 @@ asmlinkage int vprintk_emit(int facility, int level,
+ 	/* cpu currently holding logbuf_lock in this function */
+ 	static volatile unsigned int logbuf_cpu = UINT_MAX;
+ 
++	/*
++	 * Fall back to early_printk if a debugging subsystem has
++	 * killed printk output
++	 */
++	if (unlikely(forced_early_printk(fmt, args)))
++		return 1;
++
+ 	if (level == SCHED_MESSAGE_LOGLEVEL) {
+ 		level = -1;
+ 		in_sched = true;
+@@ -1766,8 +1870,7 @@ asmlinkage int vprintk_emit(int facility, int level,
+ 		 * console_sem which would prevent anyone from printing to
+ 		 * console
+ 		 */
+-		preempt_disable();
+-
++		migrate_disable();
+ 		/*
+ 		 * Try to acquire and then immediately release the console
+ 		 * semaphore.  The release will print out buffers and wake up
+@@ -1775,7 +1878,7 @@ asmlinkage int vprintk_emit(int facility, int level,
+ 		 */
+ 		if (console_trylock_for_printk())
+ 			console_unlock();
+-		preempt_enable();
++		migrate_enable();
+ 		lockdep_on();
+ 	}
+ 
+@@ -1875,29 +1978,6 @@ static size_t cont_print_text(char *text, size_t size) { return 0; }
+ 
+ #endif /* CONFIG_PRINTK */
+ 
+-#ifdef CONFIG_EARLY_PRINTK
+-struct console *early_console;
+-
+-void early_vprintk(const char *fmt, va_list ap)
+-{
+-	if (early_console) {
+-		char buf[512];
+-		int n = vscnprintf(buf, sizeof(buf), fmt, ap);
+-
+-		early_console->write(early_console, buf, n);
+-	}
+-}
+-
+-asmlinkage __visible void early_printk(const char *fmt, ...)
+-{
+-	va_list ap;
+-
+-	va_start(ap, fmt);
+-	early_vprintk(fmt, ap);
+-	va_end(ap);
+-}
+-#endif
+-
+ static int __add_preferred_console(char *name, int idx, char *options,
+ 				   char *brl_options)
+ {
+@@ -2137,11 +2217,16 @@ static void console_cont_flush(char *text, size_t size)
+ 		goto out;
+ 
+ 	len = cont_print_text(text, size);
++#ifndef CONFIG_PREEMPT_RT_FULL
+ 	raw_spin_unlock(&logbuf_lock);
+ 	stop_critical_timings();
+ 	call_console_drivers(cont.level, text, len);
+ 	start_critical_timings();
+ 	local_irq_restore(flags);
++#else
++	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
++	call_console_drivers(cont.level, text, len);
++#endif
+ 	return;
+ out:
+ 	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
+@@ -2240,13 +2325,17 @@ skip:
+ 		console_idx = log_next(console_idx);
+ 		console_seq++;
+ 		console_prev = msg->flags;
++#ifdef CONFIG_PREEMPT_RT_FULL
++		raw_spin_unlock_irqrestore(&logbuf_lock, flags);
++		call_console_drivers(level, text, len);
++#else
+ 		raw_spin_unlock(&logbuf_lock);
+ 
+ 		stop_critical_timings();	/* don't trace print latency */
+ 		call_console_drivers(level, text, len);
+ 		start_critical_timings();
+ 		local_irq_restore(flags);
+-
++#endif
+ 		if (do_cond_resched)
+ 			cond_resched();
+ 	}
+@@ -2298,6 +2387,11 @@ void console_unblank(void)
+ {
+ 	struct console *c;
+ 
++	if (IS_ENABLED(CONFIG_PREEMPT_RT_BASE)) {
++		if (in_irq() || in_nmi())
++			return;
++	}
++
+ 	/*
+ 	 * console_unblank can no longer be called in interrupt context unless
+ 	 * oops_in_progress is set to 1..
+diff --git a/kernel/ptrace.c b/kernel/ptrace.c
+index 8d2c10714530..622ca527950e 100644
+--- a/kernel/ptrace.c
++++ b/kernel/ptrace.c
+@@ -129,7 +129,14 @@ static bool ptrace_freeze_traced(struct task_struct *task)
+ 
+ 	spin_lock_irq(&task->sighand->siglock);
+ 	if (task_is_traced(task) && !__fatal_signal_pending(task)) {
+-		task->state = __TASK_TRACED;
++		unsigned long flags;
++
++		raw_spin_lock_irqsave(&task->pi_lock, flags);
++		if (task->state & __TASK_TRACED)
++			task->state = __TASK_TRACED;
++		else
++			task->saved_state = __TASK_TRACED;
++		raw_spin_unlock_irqrestore(&task->pi_lock, flags);
+ 		ret = true;
+ 	}
+ 	spin_unlock_irq(&task->sighand->siglock);
+diff --git a/kernel/rcu/rcutorture.c b/kernel/rcu/rcutorture.c
+index 240fa9094f83..ff0f2d5d6267 100644
+--- a/kernel/rcu/rcutorture.c
++++ b/kernel/rcu/rcutorture.c
+@@ -392,6 +392,7 @@ static struct rcu_torture_ops rcu_ops = {
+ 	.name		= "rcu"
+ };
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ /*
+  * Definitions for rcu_bh torture testing.
+  */
+@@ -435,6 +436,12 @@ static struct rcu_torture_ops rcu_bh_ops = {
+ 	.name		= "rcu_bh"
+ };
+ 
++#else
++static struct rcu_torture_ops rcu_bh_ops = {
++	.ttype		= INVALID_RCU_FLAVOR,
++};
++#endif
++
+ /*
+  * Don't even think about trying any of these in real life!!!
+  * The names includes "busted", and they really means it!
+diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
+index da80f2f73b50..afc35f0aea3e 100644
+--- a/kernel/rcu/tiny.c
++++ b/kernel/rcu/tiny.c
+@@ -375,6 +375,7 @@ void call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
+ }
+ EXPORT_SYMBOL_GPL(call_rcu_sched);
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ /*
+  * Post an RCU bottom-half callback to be invoked after any subsequent
+  * quiescent state.
+@@ -384,6 +385,7 @@ void call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
+ 	__call_rcu(head, func, &rcu_bh_ctrlblk);
+ }
+ EXPORT_SYMBOL_GPL(call_rcu_bh);
++#endif
+ 
+ void rcu_init(void)
+ {
+diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
+index 9815447d22e0..6aae258f9f62 100644
+--- a/kernel/rcu/tree.c
++++ b/kernel/rcu/tree.c
+@@ -56,6 +56,11 @@
+ #include <linux/random.h>
+ #include <linux/ftrace_event.h>
+ #include <linux/suspend.h>
++#include <linux/delay.h>
++#include <linux/gfp.h>
++#include <linux/oom.h>
++#include <linux/smpboot.h>
++#include "../time/tick-internal.h"
+ 
+ #include "tree.h"
+ #include "rcu.h"
+@@ -152,8 +157,6 @@ EXPORT_SYMBOL_GPL(rcu_scheduler_active);
+  */
+ static int rcu_scheduler_fully_active __read_mostly;
+ 
+-#ifdef CONFIG_RCU_BOOST
+-
+ /*
+  * Control variables for per-CPU and per-rcu_node kthreads.  These
+  * handle all flavors of RCU.
+@@ -163,8 +166,6 @@ DEFINE_PER_CPU(unsigned int, rcu_cpu_kthread_status);
+ DEFINE_PER_CPU(unsigned int, rcu_cpu_kthread_loops);
+ DEFINE_PER_CPU(char, rcu_cpu_has_work);
+ 
+-#endif /* #ifdef CONFIG_RCU_BOOST */
+-
+ static void rcu_boost_kthread_setaffinity(struct rcu_node *rnp, int outgoingcpu);
+ static void invoke_rcu_core(void);
+ static void invoke_rcu_callbacks(struct rcu_state *rsp, struct rcu_data *rdp);
+@@ -207,6 +208,19 @@ void rcu_sched_qs(void)
+ 	}
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++static void rcu_preempt_qs(void);
++
++void rcu_bh_qs(void)
++{
++	unsigned long flags;
++
++	/* Callers to this function, rcu_preempt_qs(), must disable irqs. */
++	local_irq_save(flags);
++	rcu_preempt_qs();
++	local_irq_restore(flags);
++}
++#else
+ void rcu_bh_qs(void)
+ {
+ 	if (!__this_cpu_read(rcu_bh_data.passed_quiesce)) {
+@@ -216,6 +230,7 @@ void rcu_bh_qs(void)
+ 		__this_cpu_write(rcu_bh_data.passed_quiesce, 1);
+ 	}
+ }
++#endif
+ 
+ static DEFINE_PER_CPU(int, rcu_sched_qs_mask);
+ 
+@@ -336,6 +351,7 @@ long rcu_batches_completed_sched(void)
+ }
+ EXPORT_SYMBOL_GPL(rcu_batches_completed_sched);
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ /*
+  * Return the number of RCU BH batches processed thus far for debug & stats.
+  */
+@@ -363,6 +379,13 @@ void rcu_bh_force_quiescent_state(void)
+ }
+ EXPORT_SYMBOL_GPL(rcu_bh_force_quiescent_state);
+ 
++#else
++void rcu_force_quiescent_state(void)
++{
++}
++EXPORT_SYMBOL_GPL(rcu_force_quiescent_state);
++#endif
++
+ /*
+  * Show the state of the grace-period kthreads.
+  */
+@@ -1411,7 +1434,7 @@ static void rcu_gp_kthread_wake(struct rcu_state *rsp)
+ 	    !ACCESS_ONCE(rsp->gp_flags) ||
+ 	    !rsp->gp_kthread)
+ 		return;
+-	wake_up(&rsp->gp_wq);
++	swait_wake(&rsp->gp_wq);
+ }
+ 
+ /*
+@@ -1793,7 +1816,7 @@ static int __noreturn rcu_gp_kthread(void *arg)
+ 					       ACCESS_ONCE(rsp->gpnum),
+ 					       TPS("reqwait"));
+ 			rsp->gp_state = RCU_GP_WAIT_GPS;
+-			wait_event_interruptible(rsp->gp_wq,
++			swait_event_interruptible(rsp->gp_wq,
+ 						 ACCESS_ONCE(rsp->gp_flags) &
+ 						 RCU_GP_FLAG_INIT);
+ 			/* Locking provides needed memory barrier. */
+@@ -1821,7 +1844,7 @@ static int __noreturn rcu_gp_kthread(void *arg)
+ 					       ACCESS_ONCE(rsp->gpnum),
+ 					       TPS("fqswait"));
+ 			rsp->gp_state = RCU_GP_WAIT_FQS;
+-			ret = wait_event_interruptible_timeout(rsp->gp_wq,
++			ret = swait_event_interruptible_timeout(rsp->gp_wq,
+ 					((gf = ACCESS_ONCE(rsp->gp_flags)) &
+ 					 RCU_GP_FLAG_FQS) ||
+ 					(!ACCESS_ONCE(rnp->qsmask) &&
+@@ -2565,16 +2588,14 @@ __rcu_process_callbacks(struct rcu_state *rsp)
+ /*
+  * Do RCU core processing for the current CPU.
+  */
+-static void rcu_process_callbacks(struct softirq_action *unused)
++static void rcu_process_callbacks(void)
+ {
+ 	struct rcu_state *rsp;
+ 
+ 	if (cpu_is_offline(smp_processor_id()))
+ 		return;
+-	trace_rcu_utilization(TPS("Start RCU core"));
+ 	for_each_rcu_flavor(rsp)
+ 		__rcu_process_callbacks(rsp);
+-	trace_rcu_utilization(TPS("End RCU core"));
+ }
+ 
+ /*
+@@ -2588,18 +2609,105 @@ static void invoke_rcu_callbacks(struct rcu_state *rsp, struct rcu_data *rdp)
+ {
+ 	if (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))
+ 		return;
+-	if (likely(!rsp->boost)) {
+-		rcu_do_batch(rsp, rdp);
++	rcu_do_batch(rsp, rdp);
++}
++
++static void rcu_wake_cond(struct task_struct *t, int status)
++{
++	/*
++	 * If the thread is yielding, only wake it when this
++	 * is invoked from idle
++	 */
++	if (t && (status != RCU_KTHREAD_YIELDING || is_idle_task(current)))
++		wake_up_process(t);
++}
++
++/*
++ * Wake up this CPU's rcuc kthread to do RCU core processing.
++ */
++static void invoke_rcu_core(void)
++{
++	unsigned long flags;
++	struct task_struct *t;
++
++	if (!cpu_online(smp_processor_id()))
+ 		return;
++	local_irq_save(flags);
++	__this_cpu_write(rcu_cpu_has_work, 1);
++	t = __this_cpu_read(rcu_cpu_kthread_task);
++	if (t != NULL && current != t)
++		rcu_wake_cond(t, __this_cpu_read(rcu_cpu_kthread_status));
++	local_irq_restore(flags);
++}
++
++static void rcu_cpu_kthread_park(unsigned int cpu)
++{
++	per_cpu(rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;
++}
++
++static int rcu_cpu_kthread_should_run(unsigned int cpu)
++{
++	return __this_cpu_read(rcu_cpu_has_work);
++}
++
++/*
++ * Per-CPU kernel thread that invokes RCU callbacks.  This replaces the
++ * RCU softirq used in flavors and configurations of RCU that do not
++ * support RCU priority boosting.
++ */
++static void rcu_cpu_kthread(unsigned int cpu)
++{
++	unsigned int *statusp = &__get_cpu_var(rcu_cpu_kthread_status);
++	char work, *workp = &__get_cpu_var(rcu_cpu_has_work);
++	int spincnt;
++
++	for (spincnt = 0; spincnt < 10; spincnt++) {
++		trace_rcu_utilization(TPS("Start CPU kthread@rcu_wait"));
++		local_bh_disable();
++		*statusp = RCU_KTHREAD_RUNNING;
++		this_cpu_inc(rcu_cpu_kthread_loops);
++		local_irq_disable();
++		work = *workp;
++		*workp = 0;
++		local_irq_enable();
++		if (work)
++			rcu_process_callbacks();
++		local_bh_enable();
++		if (*workp == 0) {
++			trace_rcu_utilization(TPS("End CPU kthread@rcu_wait"));
++			*statusp = RCU_KTHREAD_WAITING;
++			return;
++		}
+ 	}
+-	invoke_rcu_callbacks_kthread();
++	*statusp = RCU_KTHREAD_YIELDING;
++	trace_rcu_utilization(TPS("Start CPU kthread@rcu_yield"));
++	schedule_timeout_interruptible(2);
++	trace_rcu_utilization(TPS("End CPU kthread@rcu_yield"));
++	*statusp = RCU_KTHREAD_WAITING;
+ }
+ 
+-static void invoke_rcu_core(void)
++static struct smp_hotplug_thread rcu_cpu_thread_spec = {
++	.store			= &rcu_cpu_kthread_task,
++	.thread_should_run	= rcu_cpu_kthread_should_run,
++	.thread_fn		= rcu_cpu_kthread,
++	.thread_comm		= "rcuc/%u",
++	.setup			= rcu_cpu_kthread_setup,
++	.park			= rcu_cpu_kthread_park,
++};
++
++/*
++ * Spawn per-CPU RCU core processing kthreads.
++ */
++static int __init rcu_spawn_core_kthreads(void)
+ {
+-	if (cpu_online(smp_processor_id()))
+-		raise_softirq(RCU_SOFTIRQ);
++	int cpu;
++
++	for_each_possible_cpu(cpu)
++		per_cpu(rcu_cpu_has_work, cpu) = 0;
++	BUG_ON(smpboot_register_percpu_thread(&rcu_cpu_thread_spec));
++	return 0;
+ }
++early_initcall(rcu_spawn_core_kthreads);
+ 
+ /*
+  * Handle any core-RCU processing required by a call_rcu() invocation.
+@@ -2734,6 +2842,7 @@ void call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
+ }
+ EXPORT_SYMBOL_GPL(call_rcu_sched);
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ /*
+  * Queue an RCU callback for invocation after a quicker grace period.
+  */
+@@ -2742,6 +2851,7 @@ void call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
+ 	__call_rcu(head, func, &rcu_bh_state, -1, 0);
+ }
+ EXPORT_SYMBOL_GPL(call_rcu_bh);
++#endif
+ 
+ /*
+  * Queue an RCU callback for lazy invocation after a grace period.
+@@ -2833,6 +2943,7 @@ void synchronize_sched(void)
+ }
+ EXPORT_SYMBOL_GPL(synchronize_sched);
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ /**
+  * synchronize_rcu_bh - wait until an rcu_bh grace period has elapsed.
+  *
+@@ -2859,6 +2970,7 @@ void synchronize_rcu_bh(void)
+ 		wait_rcu_gp(call_rcu_bh);
+ }
+ EXPORT_SYMBOL_GPL(synchronize_rcu_bh);
++#endif
+ 
+ /**
+  * get_state_synchronize_rcu - Snapshot current RCU state
+@@ -3341,6 +3453,7 @@ static void _rcu_barrier(struct rcu_state *rsp)
+ 	mutex_unlock(&rsp->barrier_mutex);
+ }
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ /**
+  * rcu_barrier_bh - Wait until all in-flight call_rcu_bh() callbacks complete.
+  */
+@@ -3349,6 +3462,7 @@ void rcu_barrier_bh(void)
+ 	_rcu_barrier(&rcu_bh_state);
+ }
+ EXPORT_SYMBOL_GPL(rcu_barrier_bh);
++#endif
+ 
+ /**
+  * rcu_barrier_sched - Wait for in-flight call_rcu_sched() callbacks.
+@@ -3658,7 +3772,7 @@ static void __init rcu_init_one(struct rcu_state *rsp,
+ 	}
+ 
+ 	rsp->rda = rda;
+-	init_waitqueue_head(&rsp->gp_wq);
++	init_swait_head(&rsp->gp_wq);
+ 	rnp = rsp->level[rcu_num_lvls - 1];
+ 	for_each_possible_cpu(i) {
+ 		while (i > rnp->grphi)
+@@ -3755,7 +3869,6 @@ void __init rcu_init(void)
+ 	rcu_init_one(&rcu_bh_state, &rcu_bh_data);
+ 	rcu_init_one(&rcu_sched_state, &rcu_sched_data);
+ 	__rcu_init_preempt();
+-	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);
+ 
+ 	/*
+ 	 * We don't need protection against CPU-hotplug here because
+diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
+index bbdc45d8d74f..e8508b4febc5 100644
+--- a/kernel/rcu/tree.h
++++ b/kernel/rcu/tree.h
+@@ -28,6 +28,7 @@
+ #include <linux/cpumask.h>
+ #include <linux/seqlock.h>
+ #include <linux/irq_work.h>
++#include <linux/wait-simple.h>
+ 
+ /*
+  * Define shape of hierarchy based on NR_CPUS, CONFIG_RCU_FANOUT, and
+@@ -172,11 +173,6 @@ struct rcu_node {
+ 				/*  queued on this rcu_node structure that */
+ 				/*  are blocking the current grace period, */
+ 				/*  there can be no such task. */
+-	struct completion boost_completion;
+-				/* Used to ensure that the rt_mutex used */
+-				/*  to carry out the boosting is fully */
+-				/*  released with no future boostee accesses */
+-				/*  before that rt_mutex is re-initialized. */
+ 	struct rt_mutex boost_mtx;
+ 				/* Used only for the priority-boosting */
+ 				/*  side effect, not as a lock. */
+@@ -208,7 +204,7 @@ struct rcu_node {
+ 				/*  This can happen due to race conditions. */
+ #endif /* #ifdef CONFIG_RCU_BOOST */
+ #ifdef CONFIG_RCU_NOCB_CPU
+-	wait_queue_head_t nocb_gp_wq[2];
++	struct swait_head nocb_gp_wq[2];
+ 				/* Place for rcu_nocb_kthread() to wait GP. */
+ #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
+ 	int need_future_gp[2];
+@@ -348,7 +344,7 @@ struct rcu_data {
+ 	atomic_long_t nocb_follower_count_lazy; /*  (approximate). */
+ 	int nocb_p_count;		/* # CBs being invoked by kthread */
+ 	int nocb_p_count_lazy;		/*  (approximate). */
+-	wait_queue_head_t nocb_wq;	/* For nocb kthreads to sleep on. */
++	struct swait_head nocb_wq;	/* For nocb kthreads to sleep on. */
+ 	struct task_struct *nocb_kthread;
+ 	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
+ 
+@@ -439,7 +435,7 @@ struct rcu_state {
+ 	unsigned long gpnum;			/* Current gp number. */
+ 	unsigned long completed;		/* # of last completed gp. */
+ 	struct task_struct *gp_kthread;		/* Task for grace periods. */
+-	wait_queue_head_t gp_wq;		/* Where GP task waits. */
++	struct swait_head gp_wq;		/* Where GP task waits. */
+ 	short gp_flags;				/* Commands for GP task. */
+ 	short gp_state;				/* GP kthread sleep state. */
+ 
+@@ -570,10 +566,9 @@ static void rcu_report_exp_rnp(struct rcu_state *rsp, struct rcu_node *rnp,
+ static void __init __rcu_init_preempt(void);
+ static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
+ static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);
+-static void invoke_rcu_callbacks_kthread(void);
+ static bool rcu_is_callbacks_kthread(void);
++static void rcu_cpu_kthread_setup(unsigned int cpu);
+ #ifdef CONFIG_RCU_BOOST
+-static void rcu_preempt_do_callbacks(void);
+ static int rcu_spawn_one_boost_kthread(struct rcu_state *rsp,
+ 						 struct rcu_node *rnp);
+ #endif /* #ifdef CONFIG_RCU_BOOST */
+diff --git a/kernel/rcu/tree_plugin.h b/kernel/rcu/tree_plugin.h
+index c1d7f27bd38f..ecd967ecc7ce 100644
+--- a/kernel/rcu/tree_plugin.h
++++ b/kernel/rcu/tree_plugin.h
+@@ -24,12 +24,6 @@
+  *	   Paul E. McKenney <paulmck@linux.vnet.ibm.com>
+  */
+ 
+-#include <linux/delay.h>
+-#include <linux/gfp.h>
+-#include <linux/oom.h>
+-#include <linux/smpboot.h>
+-#include "../time/tick-internal.h"
+-
+ #define RCU_KTHREAD_PRIO 1
+ 
+ #ifdef CONFIG_RCU_BOOST
+@@ -335,7 +329,7 @@ void rcu_read_unlock_special(struct task_struct *t)
+ 	}
+ 
+ 	/* Hardware IRQ handlers cannot block, complain if they get here. */
+-	if (WARN_ON_ONCE(in_irq() || in_serving_softirq())) {
++	if (WARN_ON_ONCE(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_OFFSET))) {
+ 		local_irq_restore(flags);
+ 		return;
+ 	}
+@@ -398,10 +392,8 @@ void rcu_read_unlock_special(struct task_struct *t)
+ 
+ #ifdef CONFIG_RCU_BOOST
+ 		/* Unboost if we were boosted. */
+-		if (drop_boost_mutex) {
++		if (drop_boost_mutex)
+ 			rt_mutex_unlock(&rnp->boost_mtx);
+-			complete(&rnp->boost_completion);
+-		}
+ #endif /* #ifdef CONFIG_RCU_BOOST */
+ 
+ 		/*
+@@ -635,15 +627,6 @@ static void rcu_preempt_check_callbacks(int cpu)
+ 		t->rcu_read_unlock_special.b.need_qs = true;
+ }
+ 
+-#ifdef CONFIG_RCU_BOOST
+-
+-static void rcu_preempt_do_callbacks(void)
+-{
+-	rcu_do_batch(&rcu_preempt_state, this_cpu_ptr(&rcu_preempt_data));
+-}
+-
+-#endif /* #ifdef CONFIG_RCU_BOOST */
+-
+ /*
+  * Queue a preemptible-RCU callback for invocation after a grace period.
+  */
+@@ -1072,6 +1055,19 @@ void exit_rcu(void)
+ 
+ #endif /* #else #ifdef CONFIG_TREE_PREEMPT_RCU */
+ 
++/*
++ * If boosting, set rcuc kthreads to realtime priority.
++ */
++static void rcu_cpu_kthread_setup(unsigned int cpu)
++{
++#ifdef CONFIG_RCU_BOOST
++	struct sched_param sp;
++
++	sp.sched_priority = RCU_KTHREAD_PRIO;
++	sched_setscheduler_nocheck(current, SCHED_FIFO, &sp);
++#endif /* #ifdef CONFIG_RCU_BOOST */
++}
++
+ #ifdef CONFIG_RCU_BOOST
+ 
+ #include "../locking/rtmutex_common.h"
+@@ -1103,16 +1099,6 @@ static void rcu_initiate_boost_trace(struct rcu_node *rnp)
+ 
+ #endif /* #else #ifdef CONFIG_RCU_TRACE */
+ 
+-static void rcu_wake_cond(struct task_struct *t, int status)
+-{
+-	/*
+-	 * If the thread is yielding, only wake it when this
+-	 * is invoked from idle
+-	 */
+-	if (status != RCU_KTHREAD_YIELDING || is_idle_task(current))
+-		wake_up_process(t);
+-}
+-
+ /*
+  * Carry out RCU priority boosting on the task indicated by ->exp_tasks
+  * or ->boost_tasks, advancing the pointer to the next task in the
+@@ -1175,15 +1161,11 @@ static int rcu_boost(struct rcu_node *rnp)
+ 	 */
+ 	t = container_of(tb, struct task_struct, rcu_node_entry);
+ 	rt_mutex_init_proxy_locked(&rnp->boost_mtx, t);
+-	init_completion(&rnp->boost_completion);
+ 	raw_spin_unlock_irqrestore(&rnp->lock, flags);
+ 	/* Lock only for side effect: boosts task t's priority. */
+ 	rt_mutex_lock(&rnp->boost_mtx);
+ 	rt_mutex_unlock(&rnp->boost_mtx);  /* Then keep lockdep happy. */
+ 
+-	/* Wait for boostee to be done w/boost_mtx before reinitializing. */
+-	wait_for_completion(&rnp->boost_completion);
+-
+ 	return ACCESS_ONCE(rnp->exp_tasks) != NULL ||
+ 	       ACCESS_ONCE(rnp->boost_tasks) != NULL;
+ }
+@@ -1261,23 +1243,6 @@ static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags)
+ }
+ 
+ /*
+- * Wake up the per-CPU kthread to invoke RCU callbacks.
+- */
+-static void invoke_rcu_callbacks_kthread(void)
+-{
+-	unsigned long flags;
+-
+-	local_irq_save(flags);
+-	__this_cpu_write(rcu_cpu_has_work, 1);
+-	if (__this_cpu_read(rcu_cpu_kthread_task) != NULL &&
+-	    current != __this_cpu_read(rcu_cpu_kthread_task)) {
+-		rcu_wake_cond(__this_cpu_read(rcu_cpu_kthread_task),
+-			      __this_cpu_read(rcu_cpu_kthread_status));
+-	}
+-	local_irq_restore(flags);
+-}
+-
+-/*
+  * Is the current CPU running the RCU-callbacks kthread?
+  * Caller must have preemption disabled.
+  */
+@@ -1332,67 +1297,6 @@ static int rcu_spawn_one_boost_kthread(struct rcu_state *rsp,
+ 	return 0;
+ }
+ 
+-static void rcu_kthread_do_work(void)
+-{
+-	rcu_do_batch(&rcu_sched_state, this_cpu_ptr(&rcu_sched_data));
+-	rcu_do_batch(&rcu_bh_state, this_cpu_ptr(&rcu_bh_data));
+-	rcu_preempt_do_callbacks();
+-}
+-
+-static void rcu_cpu_kthread_setup(unsigned int cpu)
+-{
+-	struct sched_param sp;
+-
+-	sp.sched_priority = RCU_KTHREAD_PRIO;
+-	sched_setscheduler_nocheck(current, SCHED_FIFO, &sp);
+-}
+-
+-static void rcu_cpu_kthread_park(unsigned int cpu)
+-{
+-	per_cpu(rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;
+-}
+-
+-static int rcu_cpu_kthread_should_run(unsigned int cpu)
+-{
+-	return __this_cpu_read(rcu_cpu_has_work);
+-}
+-
+-/*
+- * Per-CPU kernel thread that invokes RCU callbacks.  This replaces the
+- * RCU softirq used in flavors and configurations of RCU that do not
+- * support RCU priority boosting.
+- */
+-static void rcu_cpu_kthread(unsigned int cpu)
+-{
+-	unsigned int *statusp = this_cpu_ptr(&rcu_cpu_kthread_status);
+-	char work, *workp = this_cpu_ptr(&rcu_cpu_has_work);
+-	int spincnt;
+-
+-	for (spincnt = 0; spincnt < 10; spincnt++) {
+-		trace_rcu_utilization(TPS("Start CPU kthread@rcu_wait"));
+-		local_bh_disable();
+-		*statusp = RCU_KTHREAD_RUNNING;
+-		this_cpu_inc(rcu_cpu_kthread_loops);
+-		local_irq_disable();
+-		work = *workp;
+-		*workp = 0;
+-		local_irq_enable();
+-		if (work)
+-			rcu_kthread_do_work();
+-		local_bh_enable();
+-		if (*workp == 0) {
+-			trace_rcu_utilization(TPS("End CPU kthread@rcu_wait"));
+-			*statusp = RCU_KTHREAD_WAITING;
+-			return;
+-		}
+-	}
+-	*statusp = RCU_KTHREAD_YIELDING;
+-	trace_rcu_utilization(TPS("Start CPU kthread@rcu_yield"));
+-	schedule_timeout_interruptible(2);
+-	trace_rcu_utilization(TPS("End CPU kthread@rcu_yield"));
+-	*statusp = RCU_KTHREAD_WAITING;
+-}
+-
+ /*
+  * Set the per-rcu_node kthread's affinity to cover all CPUs that are
+  * served by the rcu_node in question.  The CPU hotplug lock is still
+@@ -1426,26 +1330,13 @@ static void rcu_boost_kthread_setaffinity(struct rcu_node *rnp, int outgoingcpu)
+ 	free_cpumask_var(cm);
+ }
+ 
+-static struct smp_hotplug_thread rcu_cpu_thread_spec = {
+-	.store			= &rcu_cpu_kthread_task,
+-	.thread_should_run	= rcu_cpu_kthread_should_run,
+-	.thread_fn		= rcu_cpu_kthread,
+-	.thread_comm		= "rcuc/%u",
+-	.setup			= rcu_cpu_kthread_setup,
+-	.park			= rcu_cpu_kthread_park,
+-};
+-
+ /*
+  * Spawn boost kthreads -- called as soon as the scheduler is running.
+  */
+ static void __init rcu_spawn_boost_kthreads(void)
+ {
+ 	struct rcu_node *rnp;
+-	int cpu;
+ 
+-	for_each_possible_cpu(cpu)
+-		per_cpu(rcu_cpu_has_work, cpu) = 0;
+-	BUG_ON(smpboot_register_percpu_thread(&rcu_cpu_thread_spec));
+ 	rnp = rcu_get_root(rcu_state_p);
+ 	(void)rcu_spawn_one_boost_kthread(rcu_state_p, rnp);
+ 	if (NUM_RCU_NODES > 1) {
+@@ -1472,11 +1363,6 @@ static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags)
+ 	raw_spin_unlock_irqrestore(&rnp->lock, flags);
+ }
+ 
+-static void invoke_rcu_callbacks_kthread(void)
+-{
+-	WARN_ON_ONCE(1);
+-}
+-
+ static bool rcu_is_callbacks_kthread(void)
+ {
+ 	return false;
+@@ -1500,7 +1386,7 @@ static void rcu_prepare_kthreads(int cpu)
+ 
+ #endif /* #else #ifdef CONFIG_RCU_BOOST */
+ 
+-#if !defined(CONFIG_RCU_FAST_NO_HZ)
++#if !defined(CONFIG_RCU_FAST_NO_HZ) || defined(CONFIG_PREEMPT_RT_FULL)
+ 
+ /*
+  * Check to see if any future RCU-related work will need to be done
+@@ -1518,7 +1404,9 @@ int rcu_needs_cpu(int cpu, unsigned long *delta_jiffies)
+ 	return rcu_cpu_has_callbacks(cpu, NULL);
+ }
+ #endif /* #ifndef CONFIG_RCU_NOCB_CPU_ALL */
++#endif /* !defined(CONFIG_RCU_FAST_NO_HZ) || defined(CONFIG_PREEMPT_RT_FULL) */
+ 
++#if !defined(CONFIG_RCU_FAST_NO_HZ)
+ /*
+  * Because we do not have RCU_FAST_NO_HZ, don't bother cleaning up
+  * after it.
+@@ -1615,6 +1503,8 @@ static bool __maybe_unused rcu_try_advance_all_cbs(void)
+ 	return cbs_ready;
+ }
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++
+ /*
+  * Allow the CPU to enter dyntick-idle mode unless it has callbacks ready
+  * to invoke.  If the CPU has callbacks, try to advance them.  Tell the
+@@ -1655,7 +1545,7 @@ int rcu_needs_cpu(int cpu, unsigned long *dj)
+ 	return 0;
+ }
+ #endif /* #ifndef CONFIG_RCU_NOCB_CPU_ALL */
+-
++#endif /* #ifndef CONFIG_PREEMPT_RT_FULL */
+ /*
+  * Prepare a CPU for idle from an RCU perspective.  The first major task
+  * is to sense whether nohz mode has been enabled or disabled via sysfs.
+@@ -2001,7 +1891,7 @@ early_param("rcu_nocb_poll", parse_rcu_nocb_poll);
+  */
+ static void rcu_nocb_gp_cleanup(struct rcu_state *rsp, struct rcu_node *rnp)
+ {
+-	wake_up_all(&rnp->nocb_gp_wq[rnp->completed & 0x1]);
++	swait_wake_all(&rnp->nocb_gp_wq[rnp->completed & 0x1]);
+ }
+ 
+ /*
+@@ -2019,8 +1909,8 @@ static void rcu_nocb_gp_set(struct rcu_node *rnp, int nrq)
+ 
+ static void rcu_init_one_nocb(struct rcu_node *rnp)
+ {
+-	init_waitqueue_head(&rnp->nocb_gp_wq[0]);
+-	init_waitqueue_head(&rnp->nocb_gp_wq[1]);
++	init_swait_head(&rnp->nocb_gp_wq[0]);
++	init_swait_head(&rnp->nocb_gp_wq[1]);
+ }
+ 
+ #ifndef CONFIG_RCU_NOCB_CPU_ALL
+@@ -2045,7 +1935,7 @@ static void wake_nocb_leader(struct rcu_data *rdp, bool force)
+ 	if (ACCESS_ONCE(rdp_leader->nocb_leader_sleep) || force) {
+ 		/* Prior smp_mb__after_atomic() orders against prior enqueue. */
+ 		ACCESS_ONCE(rdp_leader->nocb_leader_sleep) = false;
+-		wake_up(&rdp_leader->nocb_wq);
++		swait_wake(&rdp_leader->nocb_wq);
+ 	}
+ }
+ 
+@@ -2238,7 +2128,7 @@ static void rcu_nocb_wait_gp(struct rcu_data *rdp)
+ 	 */
+ 	trace_rcu_future_gp(rnp, rdp, c, TPS("StartWait"));
+ 	for (;;) {
+-		wait_event_interruptible(
++		swait_event_interruptible(
+ 			rnp->nocb_gp_wq[c & 0x1],
+ 			(d = ULONG_CMP_GE(ACCESS_ONCE(rnp->completed), c)));
+ 		if (likely(d))
+@@ -2266,7 +2156,7 @@ wait_again:
+ 	/* Wait for callbacks to appear. */
+ 	if (!rcu_nocb_poll) {
+ 		trace_rcu_nocb_wake(my_rdp->rsp->name, my_rdp->cpu, "Sleep");
+-		wait_event_interruptible(my_rdp->nocb_wq,
++		swait_event_interruptible(my_rdp->nocb_wq,
+ 				!ACCESS_ONCE(my_rdp->nocb_leader_sleep));
+ 		/* Memory barrier handled by smp_mb() calls below and repoll. */
+ 	} else if (firsttime) {
+@@ -2347,7 +2237,7 @@ wait_again:
+ 			 * List was empty, wake up the follower.
+ 			 * Memory barriers supplied by atomic_long_add().
+ 			 */
+-			wake_up(&rdp->nocb_wq);
++			swait_wake(&rdp->nocb_wq);
+ 		}
+ 	}
+ 
+@@ -2368,7 +2258,7 @@ static void nocb_follower_wait(struct rcu_data *rdp)
+ 		if (!rcu_nocb_poll) {
+ 			trace_rcu_nocb_wake(rdp->rsp->name, rdp->cpu,
+ 					    "FollowerSleep");
+-			wait_event_interruptible(rdp->nocb_wq,
++			swait_event_interruptible(rdp->nocb_wq,
+ 						 ACCESS_ONCE(rdp->nocb_follower_head));
+ 		} else if (firsttime) {
+ 			/* Don't drown trace log with "Poll"! */
+@@ -2539,7 +2429,7 @@ void __init rcu_init_nohz(void)
+ static void __init rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp)
+ {
+ 	rdp->nocb_tail = &rdp->nocb_head;
+-	init_waitqueue_head(&rdp->nocb_wq);
++	init_swait_head(&rdp->nocb_wq);
+ 	rdp->nocb_follower_tail = &rdp->nocb_follower_head;
+ }
+ 
+diff --git a/kernel/rcu/update.c b/kernel/rcu/update.c
+index 3ef8ba58694e..d80edcf92f94 100644
+--- a/kernel/rcu/update.c
++++ b/kernel/rcu/update.c
+@@ -170,6 +170,7 @@ int rcu_read_lock_held(void)
+ }
+ EXPORT_SYMBOL_GPL(rcu_read_lock_held);
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ /**
+  * rcu_read_lock_bh_held() - might we be in RCU-bh read-side critical section?
+  *
+@@ -196,6 +197,7 @@ int rcu_read_lock_bh_held(void)
+ 	return in_softirq() || irqs_disabled();
+ }
+ EXPORT_SYMBOL_GPL(rcu_read_lock_bh_held);
++#endif
+ 
+ #endif /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
+ 
+diff --git a/kernel/relay.c b/kernel/relay.c
+index 5a56d3c8dc03..822e7dab6a65 100644
+--- a/kernel/relay.c
++++ b/kernel/relay.c
+@@ -339,6 +339,10 @@ static void wakeup_readers(unsigned long data)
+ {
+ 	struct rchan_buf *buf = (struct rchan_buf *)data;
+ 	wake_up_interruptible(&buf->read_wait);
++	/*
++	 * Stupid polling for now:
++	 */
++	mod_timer(&buf->timer, jiffies + 1);
+ }
+ 
+ /**
+@@ -356,6 +360,7 @@ static void __relay_reset(struct rchan_buf *buf, unsigned int init)
+ 		init_waitqueue_head(&buf->read_wait);
+ 		kref_init(&buf->kref);
+ 		setup_timer(&buf->timer, wakeup_readers, (unsigned long)buf);
++		mod_timer(&buf->timer, jiffies + 1);
+ 	} else
+ 		del_timer_sync(&buf->timer);
+ 
+@@ -739,15 +744,6 @@ size_t relay_switch_subbuf(struct rchan_buf *buf, size_t length)
+ 		else
+ 			buf->early_bytes += buf->chan->subbuf_size -
+ 					    buf->padding[old_subbuf];
+-		smp_mb();
+-		if (waitqueue_active(&buf->read_wait))
+-			/*
+-			 * Calling wake_up_interruptible() from here
+-			 * will deadlock if we happen to be logging
+-			 * from the scheduler (trying to re-grab
+-			 * rq->lock), so defer it.
+-			 */
+-			mod_timer(&buf->timer, jiffies + 1);
+ 	}
+ 
+ 	old = buf->data;
+diff --git a/kernel/res_counter.c b/kernel/res_counter.c
+index e791130f85a7..75715182b1ac 100644
+--- a/kernel/res_counter.c
++++ b/kernel/res_counter.c
+@@ -59,7 +59,7 @@ static int __res_counter_charge(struct res_counter *counter, unsigned long val,
+ 
+ 	r = ret = 0;
+ 	*limit_fail_at = NULL;
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	for (c = counter; c != NULL; c = c->parent) {
+ 		spin_lock(&c->lock);
+ 		r = res_counter_charge_locked(c, val, force);
+@@ -79,7 +79,7 @@ static int __res_counter_charge(struct res_counter *counter, unsigned long val,
+ 			spin_unlock(&u->lock);
+ 		}
+ 	}
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 
+ 	return ret;
+ }
+@@ -104,7 +104,7 @@ u64 res_counter_uncharge_until(struct res_counter *counter,
+ 	struct res_counter *c;
+ 	u64 ret = 0;
+ 
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	for (c = counter; c != top; c = c->parent) {
+ 		u64 r;
+ 		spin_lock(&c->lock);
+@@ -113,7 +113,7 @@ u64 res_counter_uncharge_until(struct res_counter *counter,
+ 			ret = r;
+ 		spin_unlock(&c->lock);
+ 	}
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 	return ret;
+ }
+ 
+diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
+index ab32b7b0db5c..33d7bfc30a9b 100644
+--- a/kernel/sched/Makefile
++++ b/kernel/sched/Makefile
+@@ -13,7 +13,7 @@ endif
+ 
+ obj-y += core.o proc.o clock.o cputime.o
+ obj-y += idle_task.o fair.o rt.o deadline.o stop_task.o
+-obj-y += wait.o completion.o idle.o
++obj-y += wait.o wait-simple.o work-simple.o completion.o idle.o
+ obj-$(CONFIG_SMP) += cpupri.o cpudeadline.o
+ obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
+ obj-$(CONFIG_SCHEDSTATS) += stats.o
+diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
+index a63f4dc27909..e529e5fcedfb 100644
+--- a/kernel/sched/completion.c
++++ b/kernel/sched/completion.c
+@@ -30,10 +30,10 @@ void complete(struct completion *x)
+ {
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&x->wait.lock, flags);
++	raw_spin_lock_irqsave(&x->wait.lock, flags);
+ 	x->done++;
+-	__wake_up_locked(&x->wait, TASK_NORMAL, 1);
+-	spin_unlock_irqrestore(&x->wait.lock, flags);
++	__swait_wake_locked(&x->wait, TASK_NORMAL, 1);
++	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
+ }
+ EXPORT_SYMBOL(complete);
+ 
+@@ -50,10 +50,10 @@ void complete_all(struct completion *x)
+ {
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&x->wait.lock, flags);
++	raw_spin_lock_irqsave(&x->wait.lock, flags);
+ 	x->done += UINT_MAX/2;
+-	__wake_up_locked(&x->wait, TASK_NORMAL, 0);
+-	spin_unlock_irqrestore(&x->wait.lock, flags);
++	__swait_wake_locked(&x->wait, TASK_NORMAL, 0);
++	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
+ }
+ EXPORT_SYMBOL(complete_all);
+ 
+@@ -62,20 +62,20 @@ do_wait_for_common(struct completion *x,
+ 		   long (*action)(long), long timeout, int state)
+ {
+ 	if (!x->done) {
+-		DECLARE_WAITQUEUE(wait, current);
++		DEFINE_SWAITER(wait);
+ 
+-		__add_wait_queue_tail_exclusive(&x->wait, &wait);
++		swait_prepare_locked(&x->wait, &wait);
+ 		do {
+ 			if (signal_pending_state(state, current)) {
+ 				timeout = -ERESTARTSYS;
+ 				break;
+ 			}
+ 			__set_current_state(state);
+-			spin_unlock_irq(&x->wait.lock);
++			raw_spin_unlock_irq(&x->wait.lock);
+ 			timeout = action(timeout);
+-			spin_lock_irq(&x->wait.lock);
++			raw_spin_lock_irq(&x->wait.lock);
+ 		} while (!x->done && timeout);
+-		__remove_wait_queue(&x->wait, &wait);
++		swait_finish_locked(&x->wait, &wait);
+ 		if (!x->done)
+ 			return timeout;
+ 	}
+@@ -89,9 +89,9 @@ __wait_for_common(struct completion *x,
+ {
+ 	might_sleep();
+ 
+-	spin_lock_irq(&x->wait.lock);
++	raw_spin_lock_irq(&x->wait.lock);
+ 	timeout = do_wait_for_common(x, action, timeout, state);
+-	spin_unlock_irq(&x->wait.lock);
++	raw_spin_unlock_irq(&x->wait.lock);
+ 	return timeout;
+ }
+ 
+@@ -267,12 +267,12 @@ bool try_wait_for_completion(struct completion *x)
+ 	unsigned long flags;
+ 	int ret = 1;
+ 
+-	spin_lock_irqsave(&x->wait.lock, flags);
++	raw_spin_lock_irqsave(&x->wait.lock, flags);
+ 	if (!x->done)
+ 		ret = 0;
+ 	else
+ 		x->done--;
+-	spin_unlock_irqrestore(&x->wait.lock, flags);
++	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
+ 	return ret;
+ }
+ EXPORT_SYMBOL(try_wait_for_completion);
+@@ -290,10 +290,10 @@ bool completion_done(struct completion *x)
+ 	unsigned long flags;
+ 	int ret = 1;
+ 
+-	spin_lock_irqsave(&x->wait.lock, flags);
++	raw_spin_lock_irqsave(&x->wait.lock, flags);
+ 	if (!x->done)
+ 		ret = 0;
+-	spin_unlock_irqrestore(&x->wait.lock, flags);
++	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
+ 	return ret;
+ }
+ EXPORT_SYMBOL(completion_done);
+diff --git a/kernel/sched/core.c b/kernel/sched/core.c
+index d0eb16373bf0..4ce10a00f4a2 100644
+--- a/kernel/sched/core.c
++++ b/kernel/sched/core.c
+@@ -280,7 +280,11 @@ late_initcall(sched_init_debug);
+  * Number of tasks to iterate in a single balance run.
+  * Limited because this is done with IRQs disabled.
+  */
++#ifndef CONFIG_PREEMPT_RT_FULL
+ const_debug unsigned int sysctl_sched_nr_migrate = 32;
++#else
++const_debug unsigned int sysctl_sched_nr_migrate = 8;
++#endif
+ 
+ /*
+  * period over which we average the RT time consumption, measured
+@@ -516,6 +520,7 @@ static void init_rq_hrtick(struct rq *rq)
+ 
+ 	hrtimer_init(&rq->hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+ 	rq->hrtick_timer.function = hrtick;
++	rq->hrtick_timer.irqsafe = 1;
+ }
+ #else	/* CONFIG_SCHED_HRTICK */
+ static inline void hrtick_clear(struct rq *rq)
+@@ -596,6 +601,52 @@ static bool set_nr_if_polling(struct task_struct *p)
+ #endif
+ #endif
+ 
++void wake_q_add(struct wake_q_head *head, struct task_struct *task)
++{
++	struct wake_q_node *node = &task->wake_q;
++
++	/*
++	 * Atomically grab the task, if ->wake_q is !nil already it means
++	 * its already queued (either by us or someone else) and will get the
++	 * wakeup due to that.
++	 *
++	 * This cmpxchg() implies a full barrier, which pairs with the write
++	 * barrier implied by the wakeup in wake_up_list().
++	 */
++	if (cmpxchg(&node->next, NULL, WAKE_Q_TAIL))
++		return;
++
++	get_task_struct(task);
++
++	/*
++	 * The head is context local, there can be no concurrency.
++	 */
++	*head->lastp = node;
++	head->lastp = &node->next;
++}
++
++void wake_up_q(struct wake_q_head *head)
++{
++	struct wake_q_node *node = head->first;
++
++	while (node != WAKE_Q_TAIL) {
++		struct task_struct *task;
++
++		task = container_of(node, struct task_struct, wake_q);
++		BUG_ON(!task);
++		/* task can safely be re-inserted now */
++		node = node->next;
++		task->wake_q.next = NULL;
++
++		/*
++		 * wake_up_process() implies a wmb() to pair with the queueing
++		 * in wake_q_add() so as not to miss wakeups.
++		 */
++		wake_up_process(task);
++		put_task_struct(task);
++	}
++}
++
+ /*
+  * resched_curr - mark rq's current task 'to be rescheduled now'.
+  *
+@@ -627,6 +678,38 @@ void resched_curr(struct rq *rq)
+ 		trace_sched_wake_idle_without_ipi(cpu);
+ }
+ 
++#ifdef CONFIG_PREEMPT_LAZY
++void resched_curr_lazy(struct rq *rq)
++{
++	struct task_struct *curr = rq->curr;
++	int cpu;
++
++	if (!sched_feat(PREEMPT_LAZY)) {
++		resched_curr(rq);
++		return;
++	}
++
++	lockdep_assert_held(&rq->lock);
++
++	if (test_tsk_need_resched(curr))
++		return;
++
++	if (test_tsk_need_resched_lazy(curr))
++		return;
++
++	set_tsk_need_resched_lazy(curr);
++
++	cpu = cpu_of(rq);
++	if (cpu == smp_processor_id())
++		return;
++
++	/* NEED_RESCHED_LAZY must be visible before we test polling */
++	smp_mb();
++	if (!tsk_is_polling(curr))
++		smp_send_reschedule(cpu);
++}
++#endif
++
+ void resched_cpu(int cpu)
+ {
+ 	struct rq *rq = cpu_rq(cpu);
+@@ -650,12 +733,14 @@ void resched_cpu(int cpu)
+  */
+ int get_nohz_timer_target(int pinned)
+ {
+-	int cpu = smp_processor_id();
++	int cpu;
+ 	int i;
+ 	struct sched_domain *sd;
+ 
++	preempt_disable_rt();
++	cpu = smp_processor_id();
+ 	if (pinned || !get_sysctl_timer_migration() || !idle_cpu(cpu))
+-		return cpu;
++		goto preempt_en_rt;
+ 
+ 	rcu_read_lock();
+ 	for_each_domain(cpu, sd) {
+@@ -668,6 +753,8 @@ int get_nohz_timer_target(int pinned)
+ 	}
+ unlock:
+ 	rcu_read_unlock();
++preempt_en_rt:
++	preempt_enable_rt();
+ 	return cpu;
+ }
+ /*
+@@ -745,14 +832,29 @@ static inline bool got_nohz_idle_kick(void)
+ #endif /* CONFIG_NO_HZ_COMMON */
+ 
+ #ifdef CONFIG_NO_HZ_FULL
++
++static int ksoftirqd_running(void)
++{
++	struct task_struct *softirqd;
++
++	if (!IS_ENABLED(CONFIG_PREEMPT_RT_FULL))
++		return 0;
++	softirqd = this_cpu_ksoftirqd();
++	if (softirqd && softirqd->on_rq)
++		return 1;
++	return 0;
++}
++
+ bool sched_can_stop_tick(void)
+ {
+ 	/*
+ 	 * More than one running task need preemption.
+ 	 * nr_running update is assumed to be visible
+ 	 * after IPI is sent from wakers.
++	 *
++	 * NOTE, RT: if ksoftirqd is awake, subtract it.
+ 	 */
+-	if (this_rq()->nr_running > 1)
++	if (this_rq()->nr_running - ksoftirqd_running() > 1)
+ 		return false;
+ 
+ 	return true;
+@@ -1198,6 +1300,18 @@ struct migration_arg {
+ 
+ static int migration_cpu_stop(void *data);
+ 
++static bool check_task_state(struct task_struct *p, long match_state)
++{
++	bool match = false;
++
++	raw_spin_lock_irq(&p->pi_lock);
++	if (p->state == match_state || p->saved_state == match_state)
++		match = true;
++	raw_spin_unlock_irq(&p->pi_lock);
++
++	return match;
++}
++
+ /*
+  * wait_task_inactive - wait for a thread to unschedule.
+  *
+@@ -1242,7 +1356,7 @@ unsigned long wait_task_inactive(struct task_struct *p, long match_state)
+ 		 * is actually now running somewhere else!
+ 		 */
+ 		while (task_running(rq, p)) {
+-			if (match_state && unlikely(p->state != match_state))
++			if (match_state && !check_task_state(p, match_state))
+ 				return 0;
+ 			cpu_relax();
+ 		}
+@@ -1257,7 +1371,8 @@ unsigned long wait_task_inactive(struct task_struct *p, long match_state)
+ 		running = task_running(rq, p);
+ 		queued = task_on_rq_queued(p);
+ 		ncsw = 0;
+-		if (!match_state || p->state == match_state)
++		if (!match_state || p->state == match_state ||
++		    p->saved_state == match_state)
+ 			ncsw = p->nvcsw | LONG_MIN; /* sets MSB */
+ 		task_rq_unlock(rq, p, &flags);
+ 
+@@ -1482,10 +1597,6 @@ static void ttwu_activate(struct rq *rq, struct task_struct *p, int en_flags)
+ {
+ 	activate_task(rq, p, en_flags);
+ 	p->on_rq = TASK_ON_RQ_QUEUED;
+-
+-	/* if a worker is waking up, notify workqueue */
+-	if (p->flags & PF_WQ_WORKER)
+-		wq_worker_waking_up(p, cpu_of(rq));
+ }
+ 
+ /*
+@@ -1495,9 +1606,9 @@ static void
+ ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)
+ {
+ 	check_preempt_curr(rq, p, wake_flags);
+-	trace_sched_wakeup(p, true);
+-
+ 	p->state = TASK_RUNNING;
++	trace_sched_wakeup(p);
++
+ #ifdef CONFIG_SMP
+ 	if (p->sched_class->task_woken)
+ 		p->sched_class->task_woken(rq, p);
+@@ -1699,8 +1810,29 @@ try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
+ 	 */
+ 	smp_mb__before_spinlock();
+ 	raw_spin_lock_irqsave(&p->pi_lock, flags);
+-	if (!(p->state & state))
++	if (!(p->state & state)) {
++		/*
++		 * The task might be running due to a spinlock sleeper
++		 * wakeup. Check the saved state and set it to running
++		 * if the wakeup condition is true.
++		 */
++		if (!(wake_flags & WF_LOCK_SLEEPER)) {
++			if (p->saved_state & state) {
++				p->saved_state = TASK_RUNNING;
++				success = 1;
++			}
++		}
+ 		goto out;
++	}
++
++	/*
++	 * If this is a regular wakeup, then we can unconditionally
++	 * clear the saved state of a "lock sleeper".
++	 */
++	if (!(wake_flags & WF_LOCK_SLEEPER))
++		p->saved_state = TASK_RUNNING;
++
++	trace_sched_waking(p);
+ 
+ 	success = 1; /* we're going to change ->state */
+ 	cpu = task_cpu(p);
+@@ -1765,42 +1897,6 @@ out:
+ }
+ 
+ /**
+- * try_to_wake_up_local - try to wake up a local task with rq lock held
+- * @p: the thread to be awakened
+- *
+- * Put @p on the run-queue if it's not already there. The caller must
+- * ensure that this_rq() is locked, @p is bound to this_rq() and not
+- * the current task.
+- */
+-static void try_to_wake_up_local(struct task_struct *p)
+-{
+-	struct rq *rq = task_rq(p);
+-
+-	if (WARN_ON_ONCE(rq != this_rq()) ||
+-	    WARN_ON_ONCE(p == current))
+-		return;
+-
+-	lockdep_assert_held(&rq->lock);
+-
+-	if (!raw_spin_trylock(&p->pi_lock)) {
+-		raw_spin_unlock(&rq->lock);
+-		raw_spin_lock(&p->pi_lock);
+-		raw_spin_lock(&rq->lock);
+-	}
+-
+-	if (!(p->state & TASK_NORMAL))
+-		goto out;
+-
+-	if (!task_on_rq_queued(p))
+-		ttwu_activate(rq, p, ENQUEUE_WAKEUP);
+-
+-	ttwu_do_wakeup(rq, p, 0);
+-	ttwu_stat(p, smp_processor_id(), 0);
+-out:
+-	raw_spin_unlock(&p->pi_lock);
+-}
+-
+-/**
+  * wake_up_process - Wake up a specific process
+  * @p: The process to be woken up.
+  *
+@@ -1814,11 +1910,23 @@ out:
+  */
+ int wake_up_process(struct task_struct *p)
+ {
+-	WARN_ON(task_is_stopped_or_traced(p));
++	WARN_ON(__task_is_stopped_or_traced(p));
+ 	return try_to_wake_up(p, TASK_NORMAL, 0);
+ }
+ EXPORT_SYMBOL(wake_up_process);
+ 
++/**
++ * wake_up_lock_sleeper - Wake up a specific process blocked on a "sleeping lock"
++ * @p: The process to be woken up.
++ *
++ * Same as wake_up_process() above, but wake_flags=WF_LOCK_SLEEPER to indicate
++ * the nature of the wakeup.
++ */
++int wake_up_lock_sleeper(struct task_struct *p)
++{
++	return try_to_wake_up(p, TASK_ALL, WF_LOCK_SLEEPER);
++}
++
+ int wake_up_state(struct task_struct *p, unsigned int state)
+ {
+ 	return try_to_wake_up(p, state, 0);
+@@ -2009,6 +2117,9 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
+ 	p->on_cpu = 0;
+ #endif
+ 	init_task_preempt_count(p);
++#ifdef CONFIG_HAVE_PREEMPT_LAZY
++	task_thread_info(p)->preempt_lazy_count = 0;
++#endif
+ #ifdef CONFIG_SMP
+ 	plist_node_init(&p->pushable_tasks, MAX_PRIO);
+ 	RB_CLEAR_NODE(&p->pushable_dl_tasks);
+@@ -2160,7 +2271,7 @@ void wake_up_new_task(struct task_struct *p)
+ 	rq = __task_rq_lock(p);
+ 	activate_task(rq, p, 0);
+ 	p->on_rq = TASK_ON_RQ_QUEUED;
+-	trace_sched_wakeup_new(p, true);
++	trace_sched_wakeup_new(p);
+ 	check_preempt_curr(rq, p, WF_FORK);
+ #ifdef CONFIG_SMP
+ 	if (p->sched_class->task_woken)
+@@ -2292,8 +2403,12 @@ static void finish_task_switch(struct rq *rq, struct task_struct *prev)
+ 	finish_arch_post_lock_switch();
+ 
+ 	fire_sched_in_preempt_notifiers(current);
++	/*
++	 * We use mmdrop_delayed() here so we don't have to do the
++	 * full __mmdrop() when we are the last user.
++	 */
+ 	if (mm)
+-		mmdrop(mm);
++		mmdrop_delayed(mm);
+ 	if (unlikely(prev_state == TASK_DEAD)) {
+ 		if (prev->sched_class->task_dead)
+ 			prev->sched_class->task_dead(prev);
+@@ -2611,16 +2726,6 @@ u64 scheduler_tick_max_deferment(void)
+ }
+ #endif
+ 
+-notrace unsigned long get_parent_ip(unsigned long addr)
+-{
+-	if (in_lock_functions(addr)) {
+-		addr = CALLER_ADDR2;
+-		if (in_lock_functions(addr))
+-			addr = CALLER_ADDR3;
+-	}
+-	return addr;
+-}
+-
+ #if defined(CONFIG_PREEMPT) && (defined(CONFIG_DEBUG_PREEMPT) || \
+ 				defined(CONFIG_PREEMPT_TRACER))
+ 
+@@ -2642,7 +2747,7 @@ void preempt_count_add(int val)
+ 				PREEMPT_MASK - 10);
+ #endif
+ 	if (preempt_count() == val) {
+-		unsigned long ip = get_parent_ip(CALLER_ADDR1);
++		unsigned long ip = get_lock_parent_ip();
+ #ifdef CONFIG_DEBUG_PREEMPT
+ 		current->preempt_disable_ip = ip;
+ #endif
+@@ -2669,7 +2774,7 @@ void preempt_count_sub(int val)
+ #endif
+ 
+ 	if (preempt_count() == val)
+-		trace_preempt_on(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));
++		trace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());
+ 	__preempt_count_sub(val);
+ }
+ EXPORT_SYMBOL(preempt_count_sub);
+@@ -2726,6 +2831,133 @@ static inline void schedule_debug(struct task_struct *prev)
+ 	schedstat_inc(this_rq(), sched_count);
+ }
+ 
++#if defined(CONFIG_PREEMPT_RT_FULL) && defined(CONFIG_SMP)
++#define MIGRATE_DISABLE_SET_AFFIN	(1<<30) /* Can't make a negative */
++#define migrate_disabled_updated(p)	((p)->migrate_disable & MIGRATE_DISABLE_SET_AFFIN)
++#define migrate_disable_count(p)	((p)->migrate_disable & ~MIGRATE_DISABLE_SET_AFFIN)
++
++static inline void update_migrate_disable(struct task_struct *p)
++{
++	const struct cpumask *mask;
++
++	if (likely(!p->migrate_disable))
++		return;
++
++	/* Did we already update affinity? */
++	if (unlikely(migrate_disabled_updated(p)))
++		return;
++
++	/*
++	 * Since this is always current we can get away with only locking
++	 * rq->lock, the ->cpus_allowed value can normally only be changed
++	 * while holding both p->pi_lock and rq->lock, but seeing that this
++	 * is current, we cannot actually be waking up, so all code that
++	 * relies on serialization against p->pi_lock is out of scope.
++	 *
++	 * Having rq->lock serializes us against things like
++	 * set_cpus_allowed_ptr() that can still happen concurrently.
++	 */
++	mask = tsk_cpus_allowed(p);
++
++	if (p->sched_class->set_cpus_allowed)
++		p->sched_class->set_cpus_allowed(p, mask);
++	/* mask==cpumask_of(task_cpu(p)) which has a cpumask_weight==1 */
++	p->nr_cpus_allowed = 1;
++
++	/* Let migrate_enable know to fix things back up */
++	p->migrate_disable |= MIGRATE_DISABLE_SET_AFFIN;
++}
++
++void migrate_disable(void)
++{
++	struct task_struct *p = current;
++
++	if (in_atomic() || irqs_disabled()) {
++#ifdef CONFIG_SCHED_DEBUG
++		p->migrate_disable_atomic++;
++#endif
++		return;
++	}
++
++#ifdef CONFIG_SCHED_DEBUG
++	if (unlikely(p->migrate_disable_atomic)) {
++		tracing_off();
++		WARN_ON_ONCE(1);
++	}
++#endif
++
++	if (p->migrate_disable) {
++		p->migrate_disable++;
++		return;
++	}
++
++	preempt_disable();
++	preempt_lazy_disable();
++	pin_current_cpu();
++	p->migrate_disable = 1;
++	preempt_enable();
++}
++EXPORT_SYMBOL(migrate_disable);
++
++void migrate_enable(void)
++{
++	struct task_struct *p = current;
++	const struct cpumask *mask;
++	unsigned long flags;
++	struct rq *rq;
++
++	if (in_atomic() || irqs_disabled()) {
++#ifdef CONFIG_SCHED_DEBUG
++		p->migrate_disable_atomic--;
++#endif
++		return;
++	}
++
++#ifdef CONFIG_SCHED_DEBUG
++	if (unlikely(p->migrate_disable_atomic)) {
++		tracing_off();
++		WARN_ON_ONCE(1);
++	}
++#endif
++	WARN_ON_ONCE(p->migrate_disable <= 0);
++
++	if (migrate_disable_count(p) > 1) {
++		p->migrate_disable--;
++		return;
++	}
++
++	preempt_disable();
++	if (unlikely(migrate_disabled_updated(p))) {
++		/*
++		 * Undo whatever update_migrate_disable() did, also see there
++		 * about locking.
++		 */
++		rq = this_rq();
++		raw_spin_lock_irqsave(&rq->lock, flags);
++
++		/*
++		 * Clearing migrate_disable causes tsk_cpus_allowed to
++		 * show the tasks original cpu affinity.
++		 */
++		p->migrate_disable = 0;
++		mask = tsk_cpus_allowed(p);
++		if (p->sched_class->set_cpus_allowed)
++			p->sched_class->set_cpus_allowed(p, mask);
++		p->nr_cpus_allowed = cpumask_weight(mask);
++		raw_spin_unlock_irqrestore(&rq->lock, flags);
++	} else
++		p->migrate_disable = 0;
++
++	unpin_current_cpu();
++	preempt_enable();
++	preempt_lazy_enable();
++}
++EXPORT_SYMBOL(migrate_enable);
++#else
++static inline void update_migrate_disable(struct task_struct *p) { }
++#define migrate_disabled_updated(p)		0
++#endif
++
+ /*
+  * Pick up the highest-prio task:
+  */
+@@ -2829,6 +3061,8 @@ need_resched:
+ 	smp_mb__before_spinlock();
+ 	raw_spin_lock_irq(&rq->lock);
+ 
++	update_migrate_disable(prev);
++
+ 	switch_count = &prev->nivcsw;
+ 	if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {
+ 		if (unlikely(signal_pending_state(prev->state, prev))) {
+@@ -2836,19 +3070,6 @@ need_resched:
+ 		} else {
+ 			deactivate_task(rq, prev, DEQUEUE_SLEEP);
+ 			prev->on_rq = 0;
+-
+-			/*
+-			 * If a worker went to sleep, notify and ask workqueue
+-			 * whether it wants to wake up a task to maintain
+-			 * concurrency.
+-			 */
+-			if (prev->flags & PF_WQ_WORKER) {
+-				struct task_struct *to_wakeup;
+-
+-				to_wakeup = wq_worker_sleeping(prev, cpu);
+-				if (to_wakeup)
+-					try_to_wake_up_local(to_wakeup);
+-			}
+ 		}
+ 		switch_count = &prev->nvcsw;
+ 	}
+@@ -2858,6 +3079,7 @@ need_resched:
+ 
+ 	next = pick_next_task(rq, prev);
+ 	clear_tsk_need_resched(prev);
++	clear_tsk_need_resched_lazy(prev);
+ 	clear_preempt_need_resched();
+ 	rq->skip_clock_update = 0;
+ 
+@@ -2887,9 +3109,20 @@ need_resched:
+ 
+ static inline void sched_submit_work(struct task_struct *tsk)
+ {
+-	if (!tsk->state || tsk_is_pi_blocked(tsk))
++	if (!tsk->state)
+ 		return;
+ 	/*
++	 * If a worker went to sleep, notify and ask workqueue whether
++	 * it wants to wake up a task to maintain concurrency.
++	 */
++	if (tsk->flags & PF_WQ_WORKER)
++		wq_worker_sleeping(tsk);
++
++
++	if (tsk_is_pi_blocked(tsk))
++		return;
++
++	/*
+ 	 * If we are going to sleep and we have plugged IO queued,
+ 	 * make sure to submit it to avoid deadlocks.
+ 	 */
+@@ -2897,12 +3130,19 @@ static inline void sched_submit_work(struct task_struct *tsk)
+ 		blk_schedule_flush_plug(tsk);
+ }
+ 
++static inline void sched_update_worker(struct task_struct *tsk)
++{
++	if (tsk->flags & PF_WQ_WORKER)
++		wq_worker_running(tsk);
++}
++
+ asmlinkage __visible void __sched schedule(void)
+ {
+ 	struct task_struct *tsk = current;
+ 
+ 	sched_submit_work(tsk);
+ 	__schedule();
++	sched_update_worker(tsk);
+ }
+ EXPORT_SYMBOL(schedule);
+ 
+@@ -2937,6 +3177,30 @@ void __sched schedule_preempt_disabled(void)
+ 	preempt_disable();
+ }
+ 
++#ifdef CONFIG_PREEMPT_LAZY
++/*
++ * If TIF_NEED_RESCHED is then we allow to be scheduled away since this is
++ * set by a RT task. Oterwise we try to avoid beeing scheduled out as long as
++ * preempt_lazy_count counter >0.
++ */
++static __always_inline int preemptible_lazy(void)
++{
++	if (test_thread_flag(TIF_NEED_RESCHED))
++		return 1;
++	if (current_thread_info()->preempt_lazy_count)
++		return 0;
++	return 1;
++}
++
++#else
++
++static inline int preemptible_lazy(void)
++{
++	return 1;
++}
++
++#endif
++
+ #ifdef CONFIG_PREEMPT
+ /*
+  * this is the entry point to schedule() from in-kernel preemption
+@@ -2951,10 +3215,21 @@ asmlinkage __visible void __sched notrace preempt_schedule(void)
+ 	 */
+ 	if (likely(!preemptible()))
+ 		return;
++	if (!preemptible_lazy())
++		return;
+ 
+ 	do {
+ 		__preempt_count_add(PREEMPT_ACTIVE);
++		/*
++		 * The add/subtract must not be traced by the function
++		 * tracer. But we still want to account for the
++		 * preempt off latency tracer. Since the _notrace versions
++		 * of add/subtract skip the accounting for latency tracer
++		 * we must force it manually.
++		 */
++		start_critical_timings();
+ 		__schedule();
++		stop_critical_timings();
+ 		__preempt_count_sub(PREEMPT_ACTIVE);
+ 
+ 		/*
+@@ -4268,9 +4543,16 @@ SYSCALL_DEFINE0(sched_yield)
+ 
+ static void __cond_resched(void)
+ {
+-	__preempt_count_add(PREEMPT_ACTIVE);
+-	__schedule();
+-	__preempt_count_sub(PREEMPT_ACTIVE);
++	do {
++		__preempt_count_add(PREEMPT_ACTIVE);
++		__schedule();
++		__preempt_count_sub(PREEMPT_ACTIVE);
++		/*
++		 * Check again in case we missed a preemption
++		 * opportunity between schedule and now.
++		 */
++		barrier();
++	} while (need_resched());
+ }
+ 
+ int __sched _cond_resched(void)
+@@ -4311,6 +4593,7 @@ int __cond_resched_lock(spinlock_t *lock)
+ }
+ EXPORT_SYMBOL(__cond_resched_lock);
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ int __sched __cond_resched_softirq(void)
+ {
+ 	BUG_ON(!in_softirq());
+@@ -4324,6 +4607,7 @@ int __sched __cond_resched_softirq(void)
+ 	return 0;
+ }
+ EXPORT_SYMBOL(__cond_resched_softirq);
++#endif
+ 
+ /**
+  * yield - yield the current processor to other threads.
+@@ -4687,7 +4971,9 @@ void init_idle(struct task_struct *idle, int cpu)
+ 
+ 	/* Set the preempt count _outside_ the spinlocks! */
+ 	init_idle_preempt_count(idle, cpu);
+-
++#ifdef CONFIG_HAVE_PREEMPT_LAZY
++	task_thread_info(idle)->preempt_lazy_count = 0;
++#endif
+ 	/*
+ 	 * The idle tasks have their own, simple scheduling class:
+ 	 */
+@@ -4729,11 +5015,91 @@ static struct rq *move_queued_task(struct task_struct *p, int new_cpu)
+ 
+ void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)
+ {
+-	if (p->sched_class && p->sched_class->set_cpus_allowed)
+-		p->sched_class->set_cpus_allowed(p, new_mask);
++	if (!migrate_disabled_updated(p)) {
++		if (p->sched_class && p->sched_class->set_cpus_allowed)
++			p->sched_class->set_cpus_allowed(p, new_mask);
++		p->nr_cpus_allowed = cpumask_weight(new_mask);
++	}
+ 
+ 	cpumask_copy(&p->cpus_allowed, new_mask);
+-	p->nr_cpus_allowed = cpumask_weight(new_mask);
++}
++
++static DEFINE_PER_CPU(struct cpumask, sched_cpumasks);
++static DEFINE_MUTEX(sched_down_mutex);
++static cpumask_t sched_down_cpumask;
++
++void tell_sched_cpu_down_begin(int cpu)
++{
++	mutex_lock(&sched_down_mutex);
++	cpumask_set_cpu(cpu, &sched_down_cpumask);
++	mutex_unlock(&sched_down_mutex);
++}
++
++void tell_sched_cpu_down_done(int cpu)
++{
++	mutex_lock(&sched_down_mutex);
++	cpumask_clear_cpu(cpu, &sched_down_cpumask);
++	mutex_unlock(&sched_down_mutex);
++}
++
++/**
++ * migrate_me - try to move the current task off this cpu
++ *
++ * Used by the pin_current_cpu() code to try to get tasks
++ * to move off the current CPU as it is going down.
++ * It will only move the task if the task isn't pinned to
++ * the CPU (with migrate_disable, affinity or NO_SETAFFINITY)
++ * and the task has to be in a RUNNING state. Otherwise the
++ * movement of the task will wake it up (change its state
++ * to running) when the task did not expect it.
++ *
++ * Returns 1 if it succeeded in moving the current task
++ *         0 otherwise.
++ */
++int migrate_me(void)
++{
++	struct task_struct *p = current;
++	struct migration_arg arg;
++	struct cpumask *cpumask;
++	struct cpumask *mask;
++	unsigned long flags;
++	unsigned int dest_cpu;
++	struct rq *rq;
++
++	/*
++	 * We can not migrate tasks bounded to a CPU or tasks not
++	 * running. The movement of the task will wake it up.
++	 */
++	if (p->flags & PF_NO_SETAFFINITY || p->state)
++		return 0;
++
++	mutex_lock(&sched_down_mutex);
++	rq = task_rq_lock(p, &flags);
++
++	cpumask = &__get_cpu_var(sched_cpumasks);
++	mask = &p->cpus_allowed;
++
++	cpumask_andnot(cpumask, mask, &sched_down_cpumask);
++
++	if (!cpumask_weight(cpumask)) {
++		/* It's only on this CPU? */
++		task_rq_unlock(rq, p, &flags);
++		mutex_unlock(&sched_down_mutex);
++		return 0;
++	}
++
++	dest_cpu = cpumask_any_and(cpu_active_mask, cpumask);
++
++	arg.task = p;
++	arg.dest_cpu = dest_cpu;
++
++	task_rq_unlock(rq, p, &flags);
++
++	stop_one_cpu(cpu_of(rq), migration_cpu_stop, &arg);
++	tlb_migrate_finish(p->mm);
++	mutex_unlock(&sched_down_mutex);
++
++	return 1;
+ }
+ 
+ /*
+@@ -4779,7 +5145,7 @@ int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)
+ 	do_set_cpus_allowed(p, new_mask);
+ 
+ 	/* Can the task run on the task's current CPU? If so, we're done */
+-	if (cpumask_test_cpu(task_cpu(p), new_mask))
++	if (cpumask_test_cpu(task_cpu(p), new_mask) || __migrate_disabled(p))
+ 		goto out;
+ 
+ 	dest_cpu = cpumask_any_and(cpu_active_mask, new_mask);
+@@ -4919,6 +5285,8 @@ static int migration_cpu_stop(void *data)
+ 
+ #ifdef CONFIG_HOTPLUG_CPU
+ 
++static DEFINE_PER_CPU(struct mm_struct *, idle_last_mm);
++
+ /*
+  * Ensures that the idle task is using init_mm right before its cpu goes
+  * offline.
+@@ -4933,7 +5301,11 @@ void idle_task_exit(void)
+ 		switch_mm(mm, &init_mm, current);
+ 		finish_arch_post_lock_switch();
+ 	}
+-	mmdrop(mm);
++	/*
++	 * Defer the cleanup to an alive cpu. On RT we can neither
++	 * call mmdrop() nor mmdrop_delayed() from here.
++	 */
++	per_cpu(idle_last_mm, smp_processor_id()) = mm;
+ }
+ 
+ /*
+@@ -5276,6 +5648,10 @@ migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)
+ 
+ 	case CPU_DEAD:
+ 		calc_load_migrate(rq);
++		if (per_cpu(idle_last_mm, cpu)) {
++			mmdrop(per_cpu(idle_last_mm, cpu));
++			per_cpu(idle_last_mm, cpu) = NULL;
++		}
+ 		break;
+ #endif
+ 	}
+@@ -7241,7 +7617,8 @@ void __init sched_init(void)
+ #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
+ static inline int preempt_count_equals(int preempt_offset)
+ {
+-	int nested = (preempt_count() & ~PREEMPT_ACTIVE) + rcu_preempt_depth();
++	int nested = (preempt_count() & ~PREEMPT_ACTIVE) +
++		sched_rcu_preempt_depth();
+ 
+ 	return (nested == preempt_offset);
+ }
+diff --git a/kernel/sched/cputime.c b/kernel/sched/cputime.c
+index 87b8576cbd50..2ee44eb30f2b 100644
+--- a/kernel/sched/cputime.c
++++ b/kernel/sched/cputime.c
+@@ -675,37 +675,45 @@ static void __vtime_account_system(struct task_struct *tsk)
+ 
+ void vtime_account_system(struct task_struct *tsk)
+ {
+-	write_seqlock(&tsk->vtime_seqlock);
++	raw_spin_lock(&tsk->vtime_lock);
++	write_seqcount_begin(&tsk->vtime_seq);
+ 	__vtime_account_system(tsk);
+-	write_sequnlock(&tsk->vtime_seqlock);
++	write_seqcount_end(&tsk->vtime_seq);
++	raw_spin_unlock(&tsk->vtime_lock);
+ }
+ 
+ void vtime_gen_account_irq_exit(struct task_struct *tsk)
+ {
+-	write_seqlock(&tsk->vtime_seqlock);
++	raw_spin_lock(&tsk->vtime_lock);
++	write_seqcount_begin(&tsk->vtime_seq);
+ 	__vtime_account_system(tsk);
+ 	if (context_tracking_in_user())
+ 		tsk->vtime_snap_whence = VTIME_USER;
+-	write_sequnlock(&tsk->vtime_seqlock);
++	write_seqcount_end(&tsk->vtime_seq);
++	raw_spin_unlock(&tsk->vtime_lock);
+ }
+ 
+ void vtime_account_user(struct task_struct *tsk)
+ {
+ 	cputime_t delta_cpu;
+ 
+-	write_seqlock(&tsk->vtime_seqlock);
++	raw_spin_lock(&tsk->vtime_lock);
++	write_seqcount_begin(&tsk->vtime_seq);
+ 	delta_cpu = get_vtime_delta(tsk);
+ 	tsk->vtime_snap_whence = VTIME_SYS;
+ 	account_user_time(tsk, delta_cpu, cputime_to_scaled(delta_cpu));
+-	write_sequnlock(&tsk->vtime_seqlock);
++	write_seqcount_end(&tsk->vtime_seq);
++	raw_spin_unlock(&tsk->vtime_lock);
+ }
+ 
+ void vtime_user_enter(struct task_struct *tsk)
+ {
+-	write_seqlock(&tsk->vtime_seqlock);
++	raw_spin_lock(&tsk->vtime_lock);
++	write_seqcount_begin(&tsk->vtime_seq);
+ 	__vtime_account_system(tsk);
+ 	tsk->vtime_snap_whence = VTIME_USER;
+-	write_sequnlock(&tsk->vtime_seqlock);
++	write_seqcount_end(&tsk->vtime_seq);
++	raw_spin_unlock(&tsk->vtime_lock);
+ }
+ 
+ void vtime_guest_enter(struct task_struct *tsk)
+@@ -717,19 +725,23 @@ void vtime_guest_enter(struct task_struct *tsk)
+ 	 * synchronization against the reader (task_gtime())
+ 	 * that can thus safely catch up with a tickless delta.
+ 	 */
+-	write_seqlock(&tsk->vtime_seqlock);
++	raw_spin_lock(&tsk->vtime_lock);
++	write_seqcount_begin(&tsk->vtime_seq);
+ 	__vtime_account_system(tsk);
+ 	current->flags |= PF_VCPU;
+-	write_sequnlock(&tsk->vtime_seqlock);
++	write_seqcount_end(&tsk->vtime_seq);
++	raw_spin_unlock(&tsk->vtime_lock);
+ }
+ EXPORT_SYMBOL_GPL(vtime_guest_enter);
+ 
+ void vtime_guest_exit(struct task_struct *tsk)
+ {
+-	write_seqlock(&tsk->vtime_seqlock);
++	raw_spin_lock(&tsk->vtime_lock);
++	write_seqcount_begin(&tsk->vtime_seq);
+ 	__vtime_account_system(tsk);
+ 	current->flags &= ~PF_VCPU;
+-	write_sequnlock(&tsk->vtime_seqlock);
++	write_seqcount_end(&tsk->vtime_seq);
++	raw_spin_unlock(&tsk->vtime_lock);
+ }
+ EXPORT_SYMBOL_GPL(vtime_guest_exit);
+ 
+@@ -742,24 +754,30 @@ void vtime_account_idle(struct task_struct *tsk)
+ 
+ void arch_vtime_task_switch(struct task_struct *prev)
+ {
+-	write_seqlock(&prev->vtime_seqlock);
++	raw_spin_lock(&prev->vtime_lock);
++	write_seqcount_begin(&prev->vtime_seq);
+ 	prev->vtime_snap_whence = VTIME_SLEEPING;
+-	write_sequnlock(&prev->vtime_seqlock);
++	write_seqcount_end(&prev->vtime_seq);
++	raw_spin_unlock(&prev->vtime_lock);
+ 
+-	write_seqlock(&current->vtime_seqlock);
++	raw_spin_lock(&current->vtime_lock);
++	write_seqcount_begin(&current->vtime_seq);
+ 	current->vtime_snap_whence = VTIME_SYS;
+ 	current->vtime_snap = sched_clock_cpu(smp_processor_id());
+-	write_sequnlock(&current->vtime_seqlock);
++	write_seqcount_end(&current->vtime_seq);
++	raw_spin_unlock(&current->vtime_lock);
+ }
+ 
+ void vtime_init_idle(struct task_struct *t, int cpu)
+ {
+ 	unsigned long flags;
+ 
+-	write_seqlock_irqsave(&t->vtime_seqlock, flags);
++	raw_spin_lock_irqsave(&t->vtime_lock, flags);
++	write_seqcount_begin(&t->vtime_seq);
+ 	t->vtime_snap_whence = VTIME_SYS;
+ 	t->vtime_snap = sched_clock_cpu(cpu);
+-	write_sequnlock_irqrestore(&t->vtime_seqlock, flags);
++	write_seqcount_end(&t->vtime_seq);
++	raw_spin_unlock_irqrestore(&t->vtime_lock, flags);
+ }
+ 
+ cputime_t task_gtime(struct task_struct *t)
+@@ -768,13 +786,13 @@ cputime_t task_gtime(struct task_struct *t)
+ 	cputime_t gtime;
+ 
+ 	do {
+-		seq = read_seqbegin(&t->vtime_seqlock);
++		seq = read_seqcount_begin(&t->vtime_seq);
+ 
+ 		gtime = t->gtime;
+ 		if (t->flags & PF_VCPU)
+ 			gtime += vtime_delta(t);
+ 
+-	} while (read_seqretry(&t->vtime_seqlock, seq));
++	} while (read_seqcount_retry(&t->vtime_seq, seq));
+ 
+ 	return gtime;
+ }
+@@ -797,7 +815,7 @@ fetch_task_cputime(struct task_struct *t,
+ 		*udelta = 0;
+ 		*sdelta = 0;
+ 
+-		seq = read_seqbegin(&t->vtime_seqlock);
++		seq = read_seqcount_begin(&t->vtime_seq);
+ 
+ 		if (u_dst)
+ 			*u_dst = *u_src;
+@@ -821,7 +839,7 @@ fetch_task_cputime(struct task_struct *t,
+ 			if (t->vtime_snap_whence == VTIME_SYS)
+ 				*sdelta = delta;
+ 		}
+-	} while (read_seqretry(&t->vtime_seqlock, seq));
++	} while (read_seqcount_retry(&t->vtime_seq, seq));
+ }
+ 
+ 
+diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
+index 40a97c3d8aba..1ded49d7d652 100644
+--- a/kernel/sched/deadline.c
++++ b/kernel/sched/deadline.c
+@@ -570,6 +570,7 @@ void init_dl_task_timer(struct sched_dl_entity *dl_se)
+ 
+ 	hrtimer_init(timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+ 	timer->function = dl_task_timer;
++	timer->irqsafe = 1;
+ }
+ 
+ static
+diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
+index ce33780d8f20..20a1246da14d 100644
+--- a/kernel/sched/debug.c
++++ b/kernel/sched/debug.c
+@@ -256,6 +256,9 @@ void print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq)
+ 	P(rt_throttled);
+ 	PN(rt_time);
+ 	PN(rt_runtime);
++#ifdef CONFIG_SMP
++	P(rt_nr_migratory);
++#endif
+ 
+ #undef PN
+ #undef P
+@@ -634,6 +637,10 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
+ #endif
+ 	P(policy);
+ 	P(prio);
++#ifdef CONFIG_PREEMPT_RT_FULL
++	P(migrate_disable);
++#endif
++	P(nr_cpus_allowed);
+ #undef PN
+ #undef __PN
+ #undef P
+diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
+index 07a75c150eeb..2c04650654f7 100644
+--- a/kernel/sched/fair.c
++++ b/kernel/sched/fair.c
+@@ -2953,7 +2953,7 @@ check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
+ 	ideal_runtime = sched_slice(cfs_rq, curr);
+ 	delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
+ 	if (delta_exec > ideal_runtime) {
+-		resched_curr(rq_of(cfs_rq));
++		resched_curr_lazy(rq_of(cfs_rq));
+ 		/*
+ 		 * The current task ran long enough, ensure it doesn't get
+ 		 * re-elected due to buddy favours.
+@@ -2977,7 +2977,7 @@ check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
+ 		return;
+ 
+ 	if (delta > ideal_runtime)
+-		resched_curr(rq_of(cfs_rq));
++		resched_curr_lazy(rq_of(cfs_rq));
+ }
+ 
+ static void
+@@ -3117,7 +3117,7 @@ entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
+ 	 * validating it and just reschedule.
+ 	 */
+ 	if (queued) {
+-		resched_curr(rq_of(cfs_rq));
++		resched_curr_lazy(rq_of(cfs_rq));
+ 		return;
+ 	}
+ 	/*
+@@ -3308,7 +3308,7 @@ static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)
+ 	 * hierarchy can be throttled
+ 	 */
+ 	if (!assign_cfs_rq_runtime(cfs_rq) && likely(cfs_rq->curr))
+-		resched_curr(rq_of(cfs_rq));
++		resched_curr_lazy(rq_of(cfs_rq));
+ }
+ 
+ static __always_inline
+@@ -3927,7 +3927,7 @@ static void hrtick_start_fair(struct rq *rq, struct task_struct *p)
+ 
+ 		if (delta < 0) {
+ 			if (rq->curr == p)
+-				resched_curr(rq);
++				resched_curr_lazy(rq);
+ 			return;
+ 		}
+ 		hrtick_start(rq, delta);
+@@ -4794,7 +4794,7 @@ static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_
+ 	return;
+ 
+ preempt:
+-	resched_curr(rq);
++	resched_curr_lazy(rq);
+ 	/*
+ 	 * Only set the backward buddy when the current task is still
+ 	 * on the rq. This can happen when a wakeup gets interleaved
+@@ -7581,7 +7581,7 @@ static void task_fork_fair(struct task_struct *p)
+ 		 * 'current' within the tree based on its new key value.
+ 		 */
+ 		swap(curr->vruntime, se->vruntime);
+-		resched_curr(rq);
++		resched_curr_lazy(rq);
+ 	}
+ 
+ 	se->vruntime -= cfs_rq->min_vruntime;
+@@ -7606,7 +7606,7 @@ prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)
+ 	 */
+ 	if (rq->curr == p) {
+ 		if (p->prio > oldprio)
+-			resched_curr(rq);
++			resched_curr_lazy(rq);
+ 	} else
+ 		check_preempt_curr(rq, p, 0);
+ }
+diff --git a/kernel/sched/features.h b/kernel/sched/features.h
+index 90284d117fe6..fd6d61f1ee17 100644
+--- a/kernel/sched/features.h
++++ b/kernel/sched/features.h
+@@ -50,12 +50,18 @@ SCHED_FEAT(LB_BIAS, true)
+  */
+ SCHED_FEAT(NONTASK_CAPACITY, true)
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++SCHED_FEAT(TTWU_QUEUE, false)
++# ifdef CONFIG_PREEMPT_LAZY
++SCHED_FEAT(PREEMPT_LAZY, true)
++# endif
++#else
+ /*
+  * Queue remote wakeups on the target CPU and process them
+  * using the scheduler IPI. Reduces rq->lock contention/bounces.
+  */
+ SCHED_FEAT(TTWU_QUEUE, true)
+-
++#endif
+ SCHED_FEAT(FORCE_SD_OVERLAP, false)
+ SCHED_FEAT(RT_RUNTIME_SHARE, true)
+ SCHED_FEAT(LB_MIN, false)
+diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
+index 20bca398084a..fe2eef21ade8 100644
+--- a/kernel/sched/rt.c
++++ b/kernel/sched/rt.c
+@@ -43,6 +43,7 @@ void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)
+ 
+ 	hrtimer_init(&rt_b->rt_period_timer,
+ 			CLOCK_MONOTONIC, HRTIMER_MODE_REL);
++	rt_b->rt_period_timer.irqsafe = 1;
+ 	rt_b->rt_period_timer.function = sched_rt_period_timer;
+ }
+ 
+diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
+index f698089e10ca..c41de96c4563 100644
+--- a/kernel/sched/sched.h
++++ b/kernel/sched/sched.h
+@@ -1019,6 +1019,7 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
+ #define WF_SYNC		0x01		/* waker goes to sleep after wakeup */
+ #define WF_FORK		0x02		/* child wakeup after fork */
+ #define WF_MIGRATED	0x4		/* internal use, task got migrated */
++#define WF_LOCK_SLEEPER	0x08		/* wakeup spinlock "sleeper" */
+ 
+ /*
+  * To aid in avoiding the subversion of "niceness" due to uneven distribution
+@@ -1211,6 +1212,15 @@ extern void init_sched_dl_class(void);
+ extern void resched_curr(struct rq *rq);
+ extern void resched_cpu(int cpu);
+ 
++#ifdef CONFIG_PREEMPT_LAZY
++extern void resched_curr_lazy(struct rq *rq);
++#else
++static inline void resched_curr_lazy(struct rq *rq)
++{
++	resched_curr(rq);
++}
++#endif
++
+ extern struct rt_bandwidth def_rt_bandwidth;
+ extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
+ 
+diff --git a/kernel/sched/wait-simple.c b/kernel/sched/wait-simple.c
+new file mode 100644
+index 000000000000..7dfa86d1f654
+--- /dev/null
++++ b/kernel/sched/wait-simple.c
+@@ -0,0 +1,115 @@
++/*
++ * Simple waitqueues without fancy flags and callbacks
++ *
++ * (C) 2011 Thomas Gleixner <tglx@linutronix.de>
++ *
++ * Based on kernel/wait.c
++ *
++ * For licencing details see kernel-base/COPYING
++ */
++#include <linux/init.h>
++#include <linux/export.h>
++#include <linux/sched.h>
++#include <linux/wait-simple.h>
++
++/* Adds w to head->list. Must be called with head->lock locked. */
++static inline void __swait_enqueue(struct swait_head *head, struct swaiter *w)
++{
++	list_add(&w->node, &head->list);
++	/* We can't let the condition leak before the setting of head */
++	smp_mb();
++}
++
++/* Removes w from head->list. Must be called with head->lock locked. */
++static inline void __swait_dequeue(struct swaiter *w)
++{
++	list_del_init(&w->node);
++}
++
++void __init_swait_head(struct swait_head *head, struct lock_class_key *key)
++{
++	raw_spin_lock_init(&head->lock);
++	lockdep_set_class(&head->lock, key);
++	INIT_LIST_HEAD(&head->list);
++}
++EXPORT_SYMBOL(__init_swait_head);
++
++void swait_prepare_locked(struct swait_head *head, struct swaiter *w)
++{
++	w->task = current;
++	if (list_empty(&w->node))
++		__swait_enqueue(head, w);
++}
++
++void swait_prepare(struct swait_head *head, struct swaiter *w, int state)
++{
++	unsigned long flags;
++
++	raw_spin_lock_irqsave(&head->lock, flags);
++	swait_prepare_locked(head, w);
++	__set_current_state(state);
++	raw_spin_unlock_irqrestore(&head->lock, flags);
++}
++EXPORT_SYMBOL(swait_prepare);
++
++void swait_finish_locked(struct swait_head *head, struct swaiter *w)
++{
++	__set_current_state(TASK_RUNNING);
++	if (w->task)
++		__swait_dequeue(w);
++}
++
++void swait_finish(struct swait_head *head, struct swaiter *w)
++{
++	unsigned long flags;
++
++	__set_current_state(TASK_RUNNING);
++	if (w->task) {
++		raw_spin_lock_irqsave(&head->lock, flags);
++		__swait_dequeue(w);
++		raw_spin_unlock_irqrestore(&head->lock, flags);
++	}
++}
++EXPORT_SYMBOL(swait_finish);
++
++unsigned int
++__swait_wake_locked(struct swait_head *head, unsigned int state, unsigned int num)
++{
++	struct swaiter *curr, *next;
++	int woken = 0;
++
++	list_for_each_entry_safe(curr, next, &head->list, node) {
++		if (wake_up_state(curr->task, state)) {
++			__swait_dequeue(curr);
++			/*
++			 * The waiting task can free the waiter as
++			 * soon as curr->task = NULL is written,
++			 * without taking any locks. A memory barrier
++			 * is required here to prevent the following
++			 * store to curr->task from getting ahead of
++			 * the dequeue operation.
++			 */
++			smp_wmb();
++			curr->task = NULL;
++			if (++woken == num)
++				break;
++		}
++	}
++	return woken;
++}
++
++unsigned int
++__swait_wake(struct swait_head *head, unsigned int state, unsigned int num)
++{
++	unsigned long flags;
++	int woken;
++
++	if (!swaitqueue_active(head))
++		return 0;
++
++	raw_spin_lock_irqsave(&head->lock, flags);
++	woken = __swait_wake_locked(head, state, num);
++	raw_spin_unlock_irqrestore(&head->lock, flags);
++	return woken;
++}
++EXPORT_SYMBOL(__swait_wake);
+diff --git a/kernel/sched/work-simple.c b/kernel/sched/work-simple.c
+new file mode 100644
+index 000000000000..c996f755dba6
+--- /dev/null
++++ b/kernel/sched/work-simple.c
+@@ -0,0 +1,172 @@
++/*
++ * Copyright (C) 2014 BMW Car IT GmbH, Daniel Wagner daniel.wagner@bmw-carit.de
++ *
++ * Provides a framework for enqueuing callbacks from irq context
++ * PREEMPT_RT_FULL safe. The callbacks are executed in kthread context.
++ */
++
++#include <linux/wait-simple.h>
++#include <linux/work-simple.h>
++#include <linux/kthread.h>
++#include <linux/slab.h>
++#include <linux/spinlock.h>
++
++#define SWORK_EVENT_PENDING     (1 << 0)
++
++static DEFINE_MUTEX(worker_mutex);
++static struct sworker *glob_worker;
++
++struct sworker {
++	struct list_head events;
++	struct swait_head wq;
++
++	raw_spinlock_t lock;
++
++	struct task_struct *task;
++	int refs;
++};
++
++static bool swork_readable(struct sworker *worker)
++{
++	bool r;
++
++	if (kthread_should_stop())
++		return true;
++
++	raw_spin_lock_irq(&worker->lock);
++	r = !list_empty(&worker->events);
++	raw_spin_unlock_irq(&worker->lock);
++
++	return r;
++}
++
++static int swork_kthread(void *arg)
++{
++	struct sworker *worker = arg;
++
++	for (;;) {
++		swait_event_interruptible(worker->wq,
++					swork_readable(worker));
++		if (kthread_should_stop())
++			break;
++
++		raw_spin_lock_irq(&worker->lock);
++		while (!list_empty(&worker->events)) {
++			struct swork_event *sev;
++
++			sev = list_first_entry(&worker->events,
++					struct swork_event, item);
++			list_del(&sev->item);
++			raw_spin_unlock_irq(&worker->lock);
++
++			WARN_ON_ONCE(!test_and_clear_bit(SWORK_EVENT_PENDING,
++							 &sev->flags));
++			sev->func(sev);
++			raw_spin_lock_irq(&worker->lock);
++		}
++		raw_spin_unlock_irq(&worker->lock);
++	}
++	return 0;
++}
++
++static struct sworker *swork_create(void)
++{
++	struct sworker *worker;
++
++	worker = kzalloc(sizeof(*worker), GFP_KERNEL);
++	if (!worker)
++		return ERR_PTR(-ENOMEM);
++
++	INIT_LIST_HEAD(&worker->events);
++	raw_spin_lock_init(&worker->lock);
++	init_swait_head(&worker->wq);
++
++	worker->task = kthread_run(swork_kthread, worker, "kswork");
++	if (IS_ERR(worker->task)) {
++		kfree(worker);
++		return ERR_PTR(-ENOMEM);
++	}
++
++	return worker;
++}
++
++static void swork_destroy(struct sworker *worker)
++{
++	kthread_stop(worker->task);
++
++	WARN_ON(!list_empty(&worker->events));
++	kfree(worker);
++}
++
++/**
++ * swork_queue - queue swork
++ *
++ * Returns %false if @work was already on a queue, %true otherwise.
++ *
++ * The work is queued and processed on a random CPU
++ */
++bool swork_queue(struct swork_event *sev)
++{
++	unsigned long flags;
++
++	if (test_and_set_bit(SWORK_EVENT_PENDING, &sev->flags))
++		return false;
++
++	raw_spin_lock_irqsave(&glob_worker->lock, flags);
++	list_add_tail(&sev->item, &glob_worker->events);
++	raw_spin_unlock_irqrestore(&glob_worker->lock, flags);
++
++	swait_wake(&glob_worker->wq);
++	return true;
++}
++EXPORT_SYMBOL_GPL(swork_queue);
++
++/**
++ * swork_get - get an instance of the sworker
++ *
++ * Returns an negative error code if the initialization if the worker did not
++ * work, %0 otherwise.
++ *
++ */
++int swork_get(void)
++{
++	struct sworker *worker;
++
++	mutex_lock(&worker_mutex);
++	if (!glob_worker) {
++		worker = swork_create();
++		if (IS_ERR(worker)) {
++			mutex_unlock(&worker_mutex);
++			return -ENOMEM;
++		}
++
++		glob_worker = worker;
++	}
++
++	glob_worker->refs++;
++	mutex_unlock(&worker_mutex);
++
++	return 0;
++}
++EXPORT_SYMBOL_GPL(swork_get);
++
++/**
++ * swork_put - puts an instance of the sworker
++ *
++ * Will destroy the sworker thread. This function must not be called until all
++ * queued events have been completed.
++ */
++void swork_put(void)
++{
++	mutex_lock(&worker_mutex);
++
++	glob_worker->refs--;
++	if (glob_worker->refs > 0)
++		goto out;
++
++	swork_destroy(glob_worker);
++	glob_worker = NULL;
++out:
++	mutex_unlock(&worker_mutex);
++}
++EXPORT_SYMBOL_GPL(swork_put);
+diff --git a/kernel/signal.c b/kernel/signal.c
+index 2e1c5d375a0f..a99e77f01a95 100644
+--- a/kernel/signal.c
++++ b/kernel/signal.c
+@@ -14,6 +14,7 @@
+ #include <linux/export.h>
+ #include <linux/init.h>
+ #include <linux/sched.h>
++#include <linux/sched/rt.h>
+ #include <linux/fs.h>
+ #include <linux/tty.h>
+ #include <linux/binfmts.h>
+@@ -352,13 +353,45 @@ static bool task_participate_group_stop(struct task_struct *task)
+ 	return false;
+ }
+ 
++#ifdef __HAVE_ARCH_CMPXCHG
++static inline struct sigqueue *get_task_cache(struct task_struct *t)
++{
++	struct sigqueue *q = t->sigqueue_cache;
++
++	if (cmpxchg(&t->sigqueue_cache, q, NULL) != q)
++		return NULL;
++	return q;
++}
++
++static inline int put_task_cache(struct task_struct *t, struct sigqueue *q)
++{
++	if (cmpxchg(&t->sigqueue_cache, NULL, q) == NULL)
++		return 0;
++	return 1;
++}
++
++#else
++
++static inline struct sigqueue *get_task_cache(struct task_struct *t)
++{
++	return NULL;
++}
++
++static inline int put_task_cache(struct task_struct *t, struct sigqueue *q)
++{
++	return 1;
++}
++
++#endif
++
+ /*
+  * allocate a new signal queue record
+  * - this may be called without locks if and only if t == current, otherwise an
+  *   appropriate lock must be held to stop the target task from exiting
+  */
+ static struct sigqueue *
+-__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimit)
++__sigqueue_do_alloc(int sig, struct task_struct *t, gfp_t flags,
++		    int override_rlimit, int fromslab)
+ {
+ 	struct sigqueue *q = NULL;
+ 	struct user_struct *user;
+@@ -375,7 +408,10 @@ __sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimi
+ 	if (override_rlimit ||
+ 	    atomic_read(&user->sigpending) <=
+ 			task_rlimit(t, RLIMIT_SIGPENDING)) {
+-		q = kmem_cache_alloc(sigqueue_cachep, flags);
++		if (!fromslab)
++			q = get_task_cache(t);
++		if (!q)
++			q = kmem_cache_alloc(sigqueue_cachep, flags);
+ 	} else {
+ 		print_dropped_signal(sig);
+ 	}
+@@ -392,6 +428,13 @@ __sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags, int override_rlimi
+ 	return q;
+ }
+ 
++static struct sigqueue *
++__sigqueue_alloc(int sig, struct task_struct *t, gfp_t flags,
++		 int override_rlimit)
++{
++	return __sigqueue_do_alloc(sig, t, flags, override_rlimit, 0);
++}
++
+ static void __sigqueue_free(struct sigqueue *q)
+ {
+ 	if (q->flags & SIGQUEUE_PREALLOC)
+@@ -401,6 +444,21 @@ static void __sigqueue_free(struct sigqueue *q)
+ 	kmem_cache_free(sigqueue_cachep, q);
+ }
+ 
++static void sigqueue_free_current(struct sigqueue *q)
++{
++	struct user_struct *up;
++
++	if (q->flags & SIGQUEUE_PREALLOC)
++		return;
++
++	up = q->user;
++	if (rt_prio(current->normal_prio) && !put_task_cache(current, q)) {
++		atomic_dec(&up->sigpending);
++		free_uid(up);
++	} else
++		  __sigqueue_free(q);
++}
++
+ void flush_sigqueue(struct sigpending *queue)
+ {
+ 	struct sigqueue *q;
+@@ -414,6 +472,21 @@ void flush_sigqueue(struct sigpending *queue)
+ }
+ 
+ /*
++ * Called from __exit_signal. Flush tsk->pending and
++ * tsk->sigqueue_cache
++ */
++void flush_task_sigqueue(struct task_struct *tsk)
++{
++	struct sigqueue *q;
++
++	flush_sigqueue(&tsk->pending);
++
++	q = get_task_cache(tsk);
++	if (q)
++		kmem_cache_free(sigqueue_cachep, q);
++}
++
++/*
+  * Flush all pending signals for a task.
+  */
+ void __flush_signals(struct task_struct *t)
+@@ -572,7 +645,7 @@ still_pending:
+ 			(info->si_code == SI_TIMER) &&
+ 			(info->si_sys_private);
+ 
+-		__sigqueue_free(first);
++		sigqueue_free_current(first);
+ 	} else {
+ 		/*
+ 		 * Ok, it wasn't in the queue.  This must be
+@@ -619,6 +692,8 @@ int dequeue_signal(struct task_struct *tsk, sigset_t *mask, siginfo_t *info)
+ 	bool resched_timer = false;
+ 	int signr;
+ 
++	WARN_ON_ONCE(tsk != current);
++
+ 	/* We only dequeue private signals from ourselves, we don't let
+ 	 * signalfd steal them
+ 	 */
+@@ -1215,8 +1290,8 @@ int do_send_sig_info(int sig, struct siginfo *info, struct task_struct *p,
+  * We don't want to have recursive SIGSEGV's etc, for example,
+  * that is why we also clear SIGNAL_UNKILLABLE.
+  */
+-int
+-force_sig_info(int sig, struct siginfo *info, struct task_struct *t)
++static int
++do_force_sig_info(int sig, struct siginfo *info, struct task_struct *t)
+ {
+ 	unsigned long int flags;
+ 	int ret, blocked, ignored;
+@@ -1241,6 +1316,39 @@ force_sig_info(int sig, struct siginfo *info, struct task_struct *t)
+ 	return ret;
+ }
+ 
++int force_sig_info(int sig, struct siginfo *info, struct task_struct *t)
++{
++/*
++ * On some archs, PREEMPT_RT has to delay sending a signal from a trap
++ * since it can not enable preemption, and the signal code's spin_locks
++ * turn into mutexes. Instead, it must set TIF_NOTIFY_RESUME which will
++ * send the signal on exit of the trap.
++ */
++#ifdef ARCH_RT_DELAYS_SIGNAL_SEND
++	if (in_atomic()) {
++		if (WARN_ON_ONCE(t != current))
++			return 0;
++		if (WARN_ON_ONCE(t->forced_info.si_signo))
++			return 0;
++
++		if (is_si_special(info)) {
++			WARN_ON_ONCE(info != SEND_SIG_PRIV);
++			t->forced_info.si_signo = sig;
++			t->forced_info.si_errno = 0;
++			t->forced_info.si_code = SI_KERNEL;
++			t->forced_info.si_pid = 0;
++			t->forced_info.si_uid = 0;
++		} else {
++			t->forced_info = *info;
++		}
++
++		set_tsk_thread_flag(t, TIF_NOTIFY_RESUME);
++		return 0;
++	}
++#endif
++	return do_force_sig_info(sig, info, t);
++}
++
+ /*
+  * Nuke all other threads in the group.
+  */
+@@ -1275,12 +1383,12 @@ struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,
+ 		 * Disable interrupts early to avoid deadlocks.
+ 		 * See rcu_read_unlock() comment header for details.
+ 		 */
+-		local_irq_save(*flags);
++		local_irq_save_nort(*flags);
+ 		rcu_read_lock();
+ 		sighand = rcu_dereference(tsk->sighand);
+ 		if (unlikely(sighand == NULL)) {
+ 			rcu_read_unlock();
+-			local_irq_restore(*flags);
++			local_irq_restore_nort(*flags);
+ 			break;
+ 		}
+ 
+@@ -1291,7 +1399,7 @@ struct sighand_struct *__lock_task_sighand(struct task_struct *tsk,
+ 		}
+ 		spin_unlock(&sighand->siglock);
+ 		rcu_read_unlock();
+-		local_irq_restore(*flags);
++		local_irq_restore_nort(*flags);
+ 	}
+ 
+ 	return sighand;
+@@ -1536,7 +1644,8 @@ EXPORT_SYMBOL(kill_pid);
+  */
+ struct sigqueue *sigqueue_alloc(void)
+ {
+-	struct sigqueue *q = __sigqueue_alloc(-1, current, GFP_KERNEL, 0);
++	/* Preallocated sigqueue objects always from the slabcache ! */
++	struct sigqueue *q = __sigqueue_do_alloc(-1, current, GFP_KERNEL, 0, 1);
+ 
+ 	if (q)
+ 		q->flags |= SIGQUEUE_PREALLOC;
+@@ -1897,15 +2006,7 @@ static void ptrace_stop(int exit_code, int why, int clear_code, siginfo_t *info)
+ 		if (gstop_done && ptrace_reparented(current))
+ 			do_notify_parent_cldstop(current, false, why);
+ 
+-		/*
+-		 * Don't want to allow preemption here, because
+-		 * sys_ptrace() needs this task to be inactive.
+-		 *
+-		 * XXX: implement read_unlock_no_resched().
+-		 */
+-		preempt_disable();
+ 		read_unlock(&tasklist_lock);
+-		preempt_enable_no_resched();
+ 		freezable_schedule();
+ 	} else {
+ 		/*
+diff --git a/kernel/softirq.c b/kernel/softirq.c
+index 9e787d8311b5..89c490b405ad 100644
+--- a/kernel/softirq.c
++++ b/kernel/softirq.c
+@@ -21,10 +21,12 @@
+ #include <linux/freezer.h>
+ #include <linux/kthread.h>
+ #include <linux/rcupdate.h>
++#include <linux/delay.h>
+ #include <linux/ftrace.h>
+ #include <linux/smp.h>
+ #include <linux/smpboot.h>
+ #include <linux/tick.h>
++#include <linux/locallock.h>
+ #include <linux/irq.h>
+ 
+ #define CREATE_TRACE_POINTS
+@@ -56,12 +58,108 @@ EXPORT_SYMBOL(irq_stat);
+ static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;
+ 
+ DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
++#ifdef CONFIG_PREEMPT_RT_FULL
++#define TIMER_SOFTIRQS	((1 << TIMER_SOFTIRQ) | (1 << HRTIMER_SOFTIRQ))
++DEFINE_PER_CPU(struct task_struct *, ktimer_softirqd);
++#endif
+ 
+ const char * const softirq_to_name[NR_SOFTIRQS] = {
+ 	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL",
+ 	"TASKLET", "SCHED", "HRTIMER", "RCU"
+ };
+ 
++#ifdef CONFIG_NO_HZ_COMMON
++# ifdef CONFIG_PREEMPT_RT_FULL
++
++struct softirq_runner {
++	struct task_struct *runner[NR_SOFTIRQS];
++};
++
++static DEFINE_PER_CPU(struct softirq_runner, softirq_runners);
++
++static inline void softirq_set_runner(unsigned int sirq)
++{
++	struct softirq_runner *sr = &__get_cpu_var(softirq_runners);
++
++	sr->runner[sirq] = current;
++}
++
++static inline void softirq_clr_runner(unsigned int sirq)
++{
++	struct softirq_runner *sr = &__get_cpu_var(softirq_runners);
++
++	sr->runner[sirq] = NULL;
++}
++
++/*
++ * On preempt-rt a softirq running context might be blocked on a
++ * lock. There might be no other runnable task on this CPU because the
++ * lock owner runs on some other CPU. So we have to go into idle with
++ * the pending bit set. Therefor we need to check this otherwise we
++ * warn about false positives which confuses users and defeats the
++ * whole purpose of this test.
++ *
++ * This code is called with interrupts disabled.
++ */
++void softirq_check_pending_idle(void)
++{
++	static int rate_limit;
++	struct softirq_runner *sr = &__get_cpu_var(softirq_runners);
++	u32 warnpending;
++	int i;
++
++	if (rate_limit >= 10)
++		return;
++
++	warnpending = local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK;
++	for (i = 0; i < NR_SOFTIRQS; i++) {
++		struct task_struct *tsk = sr->runner[i];
++
++		/*
++		 * The wakeup code in rtmutex.c wakes up the task
++		 * _before_ it sets pi_blocked_on to NULL under
++		 * tsk->pi_lock. So we need to check for both: state
++		 * and pi_blocked_on.
++		 */
++		if (tsk) {
++			raw_spin_lock(&tsk->pi_lock);
++			if (tsk->pi_blocked_on || tsk->state == TASK_RUNNING) {
++				/* Clear all bits pending in that task */
++				warnpending &= ~(tsk->softirqs_raised);
++				warnpending &= ~(1 << i);
++			}
++			raw_spin_unlock(&tsk->pi_lock);
++		}
++	}
++
++	if (warnpending) {
++		printk(KERN_ERR "NOHZ: local_softirq_pending %02x\n",
++		       warnpending);
++		rate_limit++;
++	}
++}
++# else
++/*
++ * On !PREEMPT_RT we just printk rate limited:
++ */
++void softirq_check_pending_idle(void)
++{
++	static int rate_limit;
++
++	if (rate_limit < 10 &&
++			(local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK)) {
++		printk(KERN_ERR "NOHZ: local_softirq_pending %02x\n",
++		       local_softirq_pending());
++		rate_limit++;
++	}
++}
++# endif
++
++#else /* !CONFIG_NO_HZ_COMMON */
++static inline void softirq_set_runner(unsigned int sirq) { }
++static inline void softirq_clr_runner(unsigned int sirq) { }
++#endif
++
+ /*
+  * we cannot loop indefinitely here to avoid userspace starvation,
+  * but we also don't want to introduce a worst case 1/HZ latency
+@@ -77,6 +175,84 @@ static void wakeup_softirqd(void)
+ 		wake_up_process(tsk);
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++static void wakeup_timer_softirqd(void)
++{
++	/* Interrupts are disabled: no need to stop preemption */
++	struct task_struct *tsk = __this_cpu_read(ktimer_softirqd);
++
++	if (tsk && tsk->state != TASK_RUNNING)
++		wake_up_process(tsk);
++}
++#endif
++
++static void handle_softirq(unsigned int vec_nr)
++{
++	struct softirq_action *h = softirq_vec + vec_nr;
++	int prev_count;
++
++	prev_count = preempt_count();
++
++	kstat_incr_softirqs_this_cpu(vec_nr);
++
++	trace_softirq_entry(vec_nr);
++	h->action(h);
++	trace_softirq_exit(vec_nr);
++	if (unlikely(prev_count != preempt_count())) {
++		pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
++		       vec_nr, softirq_to_name[vec_nr], h->action,
++		       prev_count, preempt_count());
++		preempt_count_set(prev_count);
++	}
++}
++
++#ifndef CONFIG_PREEMPT_RT_FULL
++static inline int ksoftirqd_softirq_pending(void)
++{
++	return local_softirq_pending();
++}
++
++static void handle_pending_softirqs(u32 pending, int need_rcu_bh_qs)
++{
++	struct softirq_action *h = softirq_vec;
++	int softirq_bit;
++
++	local_irq_enable();
++
++	h = softirq_vec;
++
++	while ((softirq_bit = ffs(pending))) {
++		unsigned int vec_nr;
++
++		h += softirq_bit - 1;
++		vec_nr = h - softirq_vec;
++		handle_softirq(vec_nr);
++
++		h++;
++		pending >>= softirq_bit;
++	}
++
++	if (need_rcu_bh_qs)
++		rcu_bh_qs();
++	local_irq_disable();
++}
++
++static void run_ksoftirqd(unsigned int cpu)
++{
++	local_irq_disable();
++	if (ksoftirqd_softirq_pending()) {
++		__do_softirq();
++		local_irq_enable();
++		cond_resched();
++
++		preempt_disable();
++		rcu_note_context_switch(cpu);
++		preempt_enable();
++		return;
++	}
++	local_irq_enable();
++}
++
+ /*
+  * preempt_count and SOFTIRQ_OFFSET usage:
+  * - preempt_count is changed by SOFTIRQ_OFFSET on entering or leaving
+@@ -115,7 +291,7 @@ void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
+ 	raw_local_irq_restore(flags);
+ 
+ 	if (preempt_count() == cnt)
+-		trace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));
++		trace_preempt_off(CALLER_ADDR0, get_lock_parent_ip());
+ }
+ EXPORT_SYMBOL(__local_bh_disable_ip);
+ #endif /* CONFIG_TRACE_IRQFLAGS */
+@@ -228,10 +404,8 @@ asmlinkage __visible void __do_softirq(void)
+ 	unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
+ 	unsigned long old_flags = current->flags;
+ 	int max_restart = MAX_SOFTIRQ_RESTART;
+-	struct softirq_action *h;
+ 	bool in_hardirq;
+ 	__u32 pending;
+-	int softirq_bit;
+ 
+ 	/*
+ 	 * Mask out PF_MEMALLOC s current task context is borrowed for the
+@@ -250,36 +424,7 @@ restart:
+ 	/* Reset the pending bitmask before enabling irqs */
+ 	set_softirq_pending(0);
+ 
+-	local_irq_enable();
+-
+-	h = softirq_vec;
+-
+-	while ((softirq_bit = ffs(pending))) {
+-		unsigned int vec_nr;
+-		int prev_count;
+-
+-		h += softirq_bit - 1;
+-
+-		vec_nr = h - softirq_vec;
+-		prev_count = preempt_count();
+-
+-		kstat_incr_softirqs_this_cpu(vec_nr);
+-
+-		trace_softirq_entry(vec_nr);
+-		h->action(h);
+-		trace_softirq_exit(vec_nr);
+-		if (unlikely(prev_count != preempt_count())) {
+-			pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
+-			       vec_nr, softirq_to_name[vec_nr], h->action,
+-			       prev_count, preempt_count());
+-			preempt_count_set(prev_count);
+-		}
+-		h++;
+-		pending >>= softirq_bit;
+-	}
+-
+-	rcu_bh_qs();
+-	local_irq_disable();
++	handle_pending_softirqs(pending, 1);
+ 
+ 	pending = local_softirq_pending();
+ 	if (pending) {
+@@ -316,6 +461,340 @@ asmlinkage __visible void do_softirq(void)
+ }
+ 
+ /*
++ * This function must run with irqs disabled!
++ */
++void raise_softirq_irqoff(unsigned int nr)
++{
++	__raise_softirq_irqoff(nr);
++
++	/*
++	 * If we're in an interrupt or softirq, we're done
++	 * (this also catches softirq-disabled code). We will
++	 * actually run the softirq once we return from
++	 * the irq or softirq.
++	 *
++	 * Otherwise we wake up ksoftirqd to make sure we
++	 * schedule the softirq soon.
++	 */
++	if (!in_interrupt())
++		wakeup_softirqd();
++}
++
++void __raise_softirq_irqoff(unsigned int nr)
++{
++	trace_softirq_raise(nr);
++	or_softirq_pending(1UL << nr);
++}
++
++static inline void local_bh_disable_nort(void) { local_bh_disable(); }
++static inline void _local_bh_enable_nort(void) { _local_bh_enable(); }
++static void ksoftirqd_set_sched_params(unsigned int cpu) { }
++
++#else /* !PREEMPT_RT_FULL */
++
++/*
++ * On RT we serialize softirq execution with a cpu local lock per softirq
++ */
++static DEFINE_PER_CPU(struct local_irq_lock [NR_SOFTIRQS], local_softirq_locks);
++
++void __init softirq_early_init(void)
++{
++	int i;
++
++	for (i = 0; i < NR_SOFTIRQS; i++)
++		local_irq_lock_init(local_softirq_locks[i]);
++}
++
++static void lock_softirq(int which)
++{
++	local_lock(local_softirq_locks[which]);
++}
++
++static void unlock_softirq(int which)
++{
++	local_unlock(local_softirq_locks[which]);
++}
++
++static void do_single_softirq(int which, int need_rcu_bh_qs)
++{
++	unsigned long old_flags = current->flags;
++
++	current->flags &= ~PF_MEMALLOC;
++	vtime_account_irq_enter(current);
++	current->flags |= PF_IN_SOFTIRQ;
++	lockdep_softirq_enter();
++	local_irq_enable();
++	handle_softirq(which);
++	local_irq_disable();
++	lockdep_softirq_exit();
++	current->flags &= ~PF_IN_SOFTIRQ;
++	vtime_account_irq_enter(current);
++	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
++}
++
++/*
++ * Called with interrupts disabled. Process softirqs which were raised
++ * in current context (or on behalf of ksoftirqd).
++ */
++static void do_current_softirqs(int need_rcu_bh_qs)
++{
++	while (current->softirqs_raised) {
++		int i = __ffs(current->softirqs_raised);
++		unsigned int pending, mask = (1U << i);
++
++		current->softirqs_raised &= ~mask;
++		local_irq_enable();
++
++		/*
++		 * If the lock is contended, we boost the owner to
++		 * process the softirq or leave the critical section
++		 * now.
++		 */
++		lock_softirq(i);
++		local_irq_disable();
++		softirq_set_runner(i);
++		/*
++		 * Check with the local_softirq_pending() bits,
++		 * whether we need to process this still or if someone
++		 * else took care of it.
++		 */
++		pending = local_softirq_pending();
++		if (pending & mask) {
++			set_softirq_pending(pending & ~mask);
++			do_single_softirq(i, need_rcu_bh_qs);
++		}
++		softirq_clr_runner(i);
++		WARN_ON(current->softirq_nestcnt != 1);
++		local_irq_enable();
++		unlock_softirq(i);
++		local_irq_disable();
++	}
++}
++
++static void __local_bh_disable(void)
++{
++	if (++current->softirq_nestcnt == 1)
++		migrate_disable();
++}
++
++void local_bh_disable(void)
++{
++	__local_bh_disable();
++}
++EXPORT_SYMBOL(local_bh_disable);
++
++void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
++{
++	__local_bh_disable();
++	if (cnt & PREEMPT_CHECK_OFFSET)
++		preempt_disable();
++}
++
++static void __local_bh_enable(void)
++{
++	if (WARN_ON(current->softirq_nestcnt == 0))
++		return;
++
++	local_irq_disable();
++	if (current->softirq_nestcnt == 1 && current->softirqs_raised)
++		do_current_softirqs(1);
++	local_irq_enable();
++
++	if (--current->softirq_nestcnt == 0)
++		migrate_enable();
++}
++
++void local_bh_enable(void)
++{
++	__local_bh_enable();
++}
++EXPORT_SYMBOL(local_bh_enable);
++
++extern void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
++{
++	__local_bh_enable();
++	if (cnt & PREEMPT_CHECK_OFFSET)
++		preempt_enable();
++}
++
++void local_bh_enable_ip(unsigned long ip)
++{
++	local_bh_enable();
++}
++EXPORT_SYMBOL(local_bh_enable_ip);
++
++void _local_bh_enable(void)
++{
++	if (WARN_ON(current->softirq_nestcnt == 0))
++		return;
++	if (--current->softirq_nestcnt == 0)
++		migrate_enable();
++}
++EXPORT_SYMBOL(_local_bh_enable);
++
++int in_serving_softirq(void)
++{
++	return current->flags & PF_IN_SOFTIRQ;
++}
++EXPORT_SYMBOL(in_serving_softirq);
++
++/* Called with preemption disabled */
++static void run_ksoftirqd(unsigned int cpu)
++{
++	local_irq_disable();
++	current->softirq_nestcnt++;
++
++	do_current_softirqs(1);
++	current->softirq_nestcnt--;
++	rcu_note_context_switch(cpu);
++	local_irq_enable();
++}
++
++/*
++ * Called from netif_rx_ni(). Preemption enabled, but migration
++ * disabled. So the cpu can't go away under us.
++ */
++void thread_do_softirq(void)
++{
++	if (!in_serving_softirq() && current->softirqs_raised) {
++		current->softirq_nestcnt++;
++		do_current_softirqs(0);
++		current->softirq_nestcnt--;
++	}
++}
++
++static void do_raise_softirq_irqoff(unsigned int nr)
++{
++	unsigned int mask;
++
++	mask = 1UL << nr;
++
++	trace_softirq_raise(nr);
++	or_softirq_pending(mask);
++
++	/*
++	 * If we are not in a hard interrupt and inside a bh disabled
++	 * region, we simply raise the flag on current. local_bh_enable()
++	 * will make sure that the softirq is executed. Otherwise we
++	 * delegate it to ksoftirqd.
++	 */
++	if (!in_irq() && current->softirq_nestcnt)
++		current->softirqs_raised |= mask;
++	else if (!__this_cpu_read(ksoftirqd) || !__this_cpu_read(ktimer_softirqd))
++		return;
++
++	if (mask & TIMER_SOFTIRQS)
++		__this_cpu_read(ktimer_softirqd)->softirqs_raised |= mask;
++	else
++		__this_cpu_read(ksoftirqd)->softirqs_raised |= mask;
++}
++
++static void wakeup_proper_softirq(unsigned int nr)
++{
++	if ((1UL << nr) & TIMER_SOFTIRQS)
++		wakeup_timer_softirqd();
++	else
++		wakeup_softirqd();
++}
++
++
++void __raise_softirq_irqoff(unsigned int nr)
++{
++	do_raise_softirq_irqoff(nr);
++	if (!in_irq() && !current->softirq_nestcnt)
++		wakeup_proper_softirq(nr);
++}
++
++/*
++ * Same as __raise_softirq_irqoff() but will process them in ksoftirqd
++ */
++void __raise_softirq_irqoff_ksoft(unsigned int nr)
++{
++	unsigned int mask;
++
++	if (WARN_ON_ONCE(!__this_cpu_read(ksoftirqd) ||
++			 !__this_cpu_read(ktimer_softirqd)))
++		return;
++	mask = 1UL << nr;
++
++	trace_softirq_raise(nr);
++	or_softirq_pending(mask);
++	if (mask & TIMER_SOFTIRQS)
++		__this_cpu_read(ktimer_softirqd)->softirqs_raised |= mask;
++	else
++		__this_cpu_read(ksoftirqd)->softirqs_raised |= mask;
++	wakeup_proper_softirq(nr);
++}
++
++/*
++ * This function must run with irqs disabled!
++ */
++void raise_softirq_irqoff(unsigned int nr)
++{
++	do_raise_softirq_irqoff(nr);
++
++	/*
++	 * If we're in an hard interrupt we let irq return code deal
++	 * with the wakeup of ksoftirqd.
++	 */
++	if (in_irq())
++		return;
++	/*
++	 * If we are in thread context but outside of a bh disabled
++	 * region, we need to wake ksoftirqd as well.
++	 *
++	 * CHECKME: Some of the places which do that could be wrapped
++	 * into local_bh_disable/enable pairs. Though it's unclear
++	 * whether this is worth the effort. To find those places just
++	 * raise a WARN() if the condition is met.
++	 */
++	if (!current->softirq_nestcnt)
++		wakeup_proper_softirq(nr);
++}
++
++static inline int ksoftirqd_softirq_pending(void)
++{
++	return current->softirqs_raised;
++}
++
++static inline void local_bh_disable_nort(void) { }
++static inline void _local_bh_enable_nort(void) { }
++
++static inline void ksoftirqd_set_sched_params(unsigned int cpu)
++{
++	/* Take over all but timer pending softirqs when starting */
++	local_irq_disable();
++	current->softirqs_raised = local_softirq_pending() & ~TIMER_SOFTIRQS;
++	local_irq_enable();
++}
++
++static inline void ktimer_softirqd_set_sched_params(unsigned int cpu)
++{
++	struct sched_param param = { .sched_priority = 1 };
++
++	sched_setscheduler(current, SCHED_FIFO, &param);
++
++	/* Take over timer pending softirqs when starting */
++	local_irq_disable();
++	current->softirqs_raised = local_softirq_pending() & TIMER_SOFTIRQS;
++	local_irq_enable();
++}
++
++static inline void ktimer_softirqd_clr_sched_params(unsigned int cpu,
++						    bool online)
++{
++	struct sched_param param = { .sched_priority = 0 };
++
++	sched_setscheduler(current, SCHED_NORMAL, &param);
++}
++
++static int ktimer_softirqd_should_run(unsigned int cpu)
++{
++	return current->softirqs_raised;
++}
++
++#endif /* PREEMPT_RT_FULL */
++/*
+  * Enter an interrupt context.
+  */
+ void irq_enter(void)
+@@ -326,9 +805,9 @@ void irq_enter(void)
+ 		 * Prevent raise_softirq from needlessly waking up ksoftirqd
+ 		 * here, as softirq will be serviced on return from interrupt.
+ 		 */
+-		local_bh_disable();
++		local_bh_disable_nort();
+ 		tick_irq_enter();
+-		_local_bh_enable();
++		_local_bh_enable_nort();
+ 	}
+ 
+ 	__irq_enter();
+@@ -336,6 +815,7 @@ void irq_enter(void)
+ 
+ static inline void invoke_softirq(void)
+ {
++#ifndef CONFIG_PREEMPT_RT_FULL
+ 	if (!force_irqthreads) {
+ #ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
+ 		/*
+@@ -355,6 +835,18 @@ static inline void invoke_softirq(void)
+ 	} else {
+ 		wakeup_softirqd();
+ 	}
++#else /* PREEMPT_RT_FULL */
++	unsigned long flags;
++
++	local_irq_save(flags);
++	if (__this_cpu_read(ksoftirqd) &&
++			__this_cpu_read(ksoftirqd)->softirqs_raised)
++		wakeup_softirqd();
++	if (__this_cpu_read(ktimer_softirqd) &&
++			__this_cpu_read(ktimer_softirqd)->softirqs_raised)
++		wakeup_timer_softirqd();
++	local_irq_restore(flags);
++#endif
+ }
+ 
+ static inline void tick_irq_exit(void)
+@@ -391,26 +883,6 @@ void irq_exit(void)
+ 	trace_hardirq_exit(); /* must be last! */
+ }
+ 
+-/*
+- * This function must run with irqs disabled!
+- */
+-inline void raise_softirq_irqoff(unsigned int nr)
+-{
+-	__raise_softirq_irqoff(nr);
+-
+-	/*
+-	 * If we're in an interrupt or softirq, we're done
+-	 * (this also catches softirq-disabled code). We will
+-	 * actually run the softirq once we return from
+-	 * the irq or softirq.
+-	 *
+-	 * Otherwise we wake up ksoftirqd to make sure we
+-	 * schedule the softirq soon.
+-	 */
+-	if (!in_interrupt())
+-		wakeup_softirqd();
+-}
+-
+ void raise_softirq(unsigned int nr)
+ {
+ 	unsigned long flags;
+@@ -420,12 +892,6 @@ void raise_softirq(unsigned int nr)
+ 	local_irq_restore(flags);
+ }
+ 
+-void __raise_softirq_irqoff(unsigned int nr)
+-{
+-	trace_softirq_raise(nr);
+-	or_softirq_pending(1UL << nr);
+-}
+-
+ void open_softirq(int nr, void (*action)(struct softirq_action *))
+ {
+ 	softirq_vec[nr].action = action;
+@@ -442,15 +908,45 @@ struct tasklet_head {
+ static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
+ static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);
+ 
++static void inline
++__tasklet_common_schedule(struct tasklet_struct *t, struct tasklet_head *head, unsigned int nr)
++{
++	if (tasklet_trylock(t)) {
++again:
++		/* We may have been preempted before tasklet_trylock
++		 * and __tasklet_action may have already run.
++		 * So double check the sched bit while the takslet
++		 * is locked before adding it to the list.
++		 */
++		if (test_bit(TASKLET_STATE_SCHED, &t->state)) {
++			t->next = NULL;
++			*head->tail = t;
++			head->tail = &(t->next);
++			raise_softirq_irqoff(nr);
++			tasklet_unlock(t);
++		} else {
++			/* This is subtle. If we hit the corner case above
++			 * It is possible that we get preempted right here,
++			 * and another task has successfully called
++			 * tasklet_schedule(), then this function, and
++			 * failed on the trylock. Thus we must be sure
++			 * before releasing the tasklet lock, that the
++			 * SCHED_BIT is clear. Otherwise the tasklet
++			 * may get its SCHED_BIT set, but not added to the
++			 * list
++			 */
++			if (!tasklet_tryunlock(t))
++				goto again;
++		}
++	}
++}
++
+ void __tasklet_schedule(struct tasklet_struct *t)
+ {
+ 	unsigned long flags;
+ 
+ 	local_irq_save(flags);
+-	t->next = NULL;
+-	*__this_cpu_read(tasklet_vec.tail) = t;
+-	__this_cpu_write(tasklet_vec.tail, &(t->next));
+-	raise_softirq_irqoff(TASKLET_SOFTIRQ);
++	__tasklet_common_schedule(t, &__get_cpu_var(tasklet_vec), TASKLET_SOFTIRQ);
+ 	local_irq_restore(flags);
+ }
+ EXPORT_SYMBOL(__tasklet_schedule);
+@@ -460,10 +956,7 @@ void __tasklet_hi_schedule(struct tasklet_struct *t)
+ 	unsigned long flags;
+ 
+ 	local_irq_save(flags);
+-	t->next = NULL;
+-	*__this_cpu_read(tasklet_hi_vec.tail) = t;
+-	__this_cpu_write(tasklet_hi_vec.tail,  &(t->next));
+-	raise_softirq_irqoff(HI_SOFTIRQ);
++	__tasklet_common_schedule(t, &__get_cpu_var(tasklet_hi_vec), HI_SOFTIRQ);
+ 	local_irq_restore(flags);
+ }
+ EXPORT_SYMBOL(__tasklet_hi_schedule);
+@@ -472,48 +965,116 @@ void __tasklet_hi_schedule_first(struct tasklet_struct *t)
+ {
+ 	BUG_ON(!irqs_disabled());
+ 
+-	t->next = __this_cpu_read(tasklet_hi_vec.head);
+-	__this_cpu_write(tasklet_hi_vec.head, t);
+-	__raise_softirq_irqoff(HI_SOFTIRQ);
++	__tasklet_hi_schedule(t);
+ }
+ EXPORT_SYMBOL(__tasklet_hi_schedule_first);
+ 
+-static void tasklet_action(struct softirq_action *a)
++void  tasklet_enable(struct tasklet_struct *t)
+ {
+-	struct tasklet_struct *list;
++	if (!atomic_dec_and_test(&t->count))
++		return;
++	if (test_and_clear_bit(TASKLET_STATE_PENDING, &t->state))
++		tasklet_schedule(t);
++}
++EXPORT_SYMBOL(tasklet_enable);
+ 
+-	local_irq_disable();
+-	list = __this_cpu_read(tasklet_vec.head);
+-	__this_cpu_write(tasklet_vec.head, NULL);
+-	__this_cpu_write(tasklet_vec.tail, this_cpu_ptr(&tasklet_vec.head));
+-	local_irq_enable();
++void  tasklet_hi_enable(struct tasklet_struct *t)
++{
++	if (!atomic_dec_and_test(&t->count))
++		return;
++	if (test_and_clear_bit(TASKLET_STATE_PENDING, &t->state))
++		tasklet_hi_schedule(t);
++}
++EXPORT_SYMBOL(tasklet_hi_enable);
++
++static void __tasklet_action(struct softirq_action *a,
++			     struct tasklet_struct *list)
++{
++	int loops = 1000000;
+ 
+ 	while (list) {
+ 		struct tasklet_struct *t = list;
+ 
+ 		list = list->next;
+ 
+-		if (tasklet_trylock(t)) {
+-			if (!atomic_read(&t->count)) {
+-				if (!test_and_clear_bit(TASKLET_STATE_SCHED,
+-							&t->state))
+-					BUG();
+-				t->func(t->data);
+-				tasklet_unlock(t);
+-				continue;
+-			}
+-			tasklet_unlock(t);
++		/*
++		 * Should always succeed - after a tasklist got on the
++		 * list (after getting the SCHED bit set from 0 to 1),
++		 * nothing but the tasklet softirq it got queued to can
++		 * lock it:
++		 */
++		if (!tasklet_trylock(t)) {
++			WARN_ON(1);
++			continue;
+ 		}
+ 
+-		local_irq_disable();
+ 		t->next = NULL;
+-		*__this_cpu_read(tasklet_vec.tail) = t;
+-		__this_cpu_write(tasklet_vec.tail, &(t->next));
+-		__raise_softirq_irqoff(TASKLET_SOFTIRQ);
+-		local_irq_enable();
++
++		/*
++		 * If we cannot handle the tasklet because it's disabled,
++		 * mark it as pending. tasklet_enable() will later
++		 * re-schedule the tasklet.
++		 */
++		if (unlikely(atomic_read(&t->count))) {
++out_disabled:
++			/* implicit unlock: */
++			wmb();
++			t->state = TASKLET_STATEF_PENDING;
++			continue;
++		}
++
++		/*
++		 * After this point on the tasklet might be rescheduled
++		 * on another CPU, but it can only be added to another
++		 * CPU's tasklet list if we unlock the tasklet (which we
++		 * dont do yet).
++		 */
++		if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
++			WARN_ON(1);
++
++again:
++		t->func(t->data);
++
++		/*
++		 * Try to unlock the tasklet. We must use cmpxchg, because
++		 * another CPU might have scheduled or disabled the tasklet.
++		 * We only allow the STATE_RUN -> 0 transition here.
++		 */
++		while (!tasklet_tryunlock(t)) {
++			/*
++			 * If it got disabled meanwhile, bail out:
++			 */
++			if (atomic_read(&t->count))
++				goto out_disabled;
++			/*
++			 * If it got scheduled meanwhile, re-execute
++			 * the tasklet function:
++			 */
++			if (test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
++				goto again;
++			if (!--loops) {
++				printk("hm, tasklet state: %08lx\n", t->state);
++				WARN_ON(1);
++				tasklet_unlock(t);
++				break;
++			}
++		}
+ 	}
+ }
+ 
++static void tasklet_action(struct softirq_action *a)
++{
++	struct tasklet_struct *list;
++
++	local_irq_disable();
++	list = __get_cpu_var(tasklet_vec).head;
++	__get_cpu_var(tasklet_vec).head = NULL;
++	__get_cpu_var(tasklet_vec).tail = &__get_cpu_var(tasklet_vec).head;
++	local_irq_enable();
++
++	__tasklet_action(a, list);
++}
++
+ static void tasklet_hi_action(struct softirq_action *a)
+ {
+ 	struct tasklet_struct *list;
+@@ -524,30 +1085,7 @@ static void tasklet_hi_action(struct softirq_action *a)
+ 	__this_cpu_write(tasklet_hi_vec.tail, this_cpu_ptr(&tasklet_hi_vec.head));
+ 	local_irq_enable();
+ 
+-	while (list) {
+-		struct tasklet_struct *t = list;
+-
+-		list = list->next;
+-
+-		if (tasklet_trylock(t)) {
+-			if (!atomic_read(&t->count)) {
+-				if (!test_and_clear_bit(TASKLET_STATE_SCHED,
+-							&t->state))
+-					BUG();
+-				t->func(t->data);
+-				tasklet_unlock(t);
+-				continue;
+-			}
+-			tasklet_unlock(t);
+-		}
+-
+-		local_irq_disable();
+-		t->next = NULL;
+-		*__this_cpu_read(tasklet_hi_vec.tail) = t;
+-		__this_cpu_write(tasklet_hi_vec.tail, &(t->next));
+-		__raise_softirq_irqoff(HI_SOFTIRQ);
+-		local_irq_enable();
+-	}
++	__tasklet_action(a, list);
+ }
+ 
+ void tasklet_init(struct tasklet_struct *t,
+@@ -568,7 +1106,7 @@ void tasklet_kill(struct tasklet_struct *t)
+ 
+ 	while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
+ 		do {
+-			yield();
++			msleep(1);
+ 		} while (test_bit(TASKLET_STATE_SCHED, &t->state));
+ 	}
+ 	tasklet_unlock_wait(t);
+@@ -642,30 +1180,26 @@ void __init softirq_init(void)
+ 	open_softirq(HI_SOFTIRQ, tasklet_hi_action);
+ }
+ 
+-static int ksoftirqd_should_run(unsigned int cpu)
+-{
+-	return local_softirq_pending();
+-}
+-
+-static void run_ksoftirqd(unsigned int cpu)
++#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT_FULL)
++void tasklet_unlock_wait(struct tasklet_struct *t)
+ {
+-	local_irq_disable();
+-	if (local_softirq_pending()) {
++	while (test_bit(TASKLET_STATE_RUN, &(t)->state)) {
+ 		/*
+-		 * We can safely run softirq on inline stack, as we are not deep
+-		 * in the task stack here.
++		 * Hack for now to avoid this busy-loop:
+ 		 */
+-		__do_softirq();
+-		local_irq_enable();
+-		cond_resched();
+-
+-		preempt_disable();
+-		rcu_note_context_switch(cpu);
+-		preempt_enable();
+-
+-		return;
++#ifdef CONFIG_PREEMPT_RT_FULL
++		msleep(1);
++#else
++		barrier();
++#endif
+ 	}
+-	local_irq_enable();
++}
++EXPORT_SYMBOL(tasklet_unlock_wait);
++#endif
++
++static int ksoftirqd_should_run(unsigned int cpu)
++{
++	return ksoftirqd_softirq_pending();
+ }
+ 
+ #ifdef CONFIG_HOTPLUG_CPU
+@@ -747,16 +1281,31 @@ static struct notifier_block cpu_nfb = {
+ 
+ static struct smp_hotplug_thread softirq_threads = {
+ 	.store			= &ksoftirqd,
++	.setup			= ksoftirqd_set_sched_params,
+ 	.thread_should_run	= ksoftirqd_should_run,
+ 	.thread_fn		= run_ksoftirqd,
+ 	.thread_comm		= "ksoftirqd/%u",
+ };
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++static struct smp_hotplug_thread softirq_timer_threads = {
++	.store			= &ktimer_softirqd,
++	.setup			= ktimer_softirqd_set_sched_params,
++	.cleanup		= ktimer_softirqd_clr_sched_params,
++	.thread_should_run	= ktimer_softirqd_should_run,
++	.thread_fn		= run_ksoftirqd,
++	.thread_comm		= "ktimersoftd/%u",
++};
++#endif
++
+ static __init int spawn_ksoftirqd(void)
+ {
+ 	register_cpu_notifier(&cpu_nfb);
+ 
+ 	BUG_ON(smpboot_register_percpu_thread(&softirq_threads));
++#ifdef CONFIG_PREEMPT_RT_FULL
++	BUG_ON(smpboot_register_percpu_thread(&softirq_timer_threads));
++#endif
+ 
+ 	return 0;
+ }
+diff --git a/kernel/stop_machine.c b/kernel/stop_machine.c
+index 695f0c6cd169..d3ea2452e291 100644
+--- a/kernel/stop_machine.c
++++ b/kernel/stop_machine.c
+@@ -35,7 +35,7 @@ struct cpu_stop_done {
+ 
+ /* the actual stopper, one per every possible cpu, enabled on online cpus */
+ struct cpu_stopper {
+-	spinlock_t		lock;
++	raw_spinlock_t		lock;
+ 	bool			enabled;	/* is this stopper enabled? */
+ 	struct list_head	works;		/* list of pending works */
+ };
+@@ -78,7 +78,7 @@ static void cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)
+ 
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&stopper->lock, flags);
++	raw_spin_lock_irqsave(&stopper->lock, flags);
+ 
+ 	if (stopper->enabled) {
+ 		list_add_tail(&work->list, &stopper->works);
+@@ -86,7 +86,7 @@ static void cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)
+ 	} else
+ 		cpu_stop_signal_done(work->done, false);
+ 
+-	spin_unlock_irqrestore(&stopper->lock, flags);
++	raw_spin_unlock_irqrestore(&stopper->lock, flags);
+ }
+ 
+ /**
+@@ -248,7 +248,7 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
+ 	struct irq_cpu_stop_queue_work_info call_args;
+ 	struct multi_stop_data msdata;
+ 
+-	preempt_disable();
++	preempt_disable_nort();
+ 	msdata = (struct multi_stop_data){
+ 		.fn = fn,
+ 		.data = arg,
+@@ -281,7 +281,7 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
+ 	 * This relies on the stopper workqueues to be FIFO.
+ 	 */
+ 	if (!cpu_active(cpu1) || !cpu_active(cpu2)) {
+-		preempt_enable();
++		preempt_enable_nort();
+ 		return -ENOENT;
+ 	}
+ 
+@@ -295,7 +295,7 @@ int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *
+ 				 &irq_cpu_stop_queue_work,
+ 				 &call_args, 1);
+ 	lg_local_unlock(&stop_cpus_lock);
+-	preempt_enable();
++	preempt_enable_nort();
+ 
+ 	wait_for_completion(&done.completion);
+ 
+@@ -329,7 +329,7 @@ static DEFINE_PER_CPU(struct cpu_stop_work, stop_cpus_work);
+ 
+ static void queue_stop_cpus_work(const struct cpumask *cpumask,
+ 				 cpu_stop_fn_t fn, void *arg,
+-				 struct cpu_stop_done *done)
++				 struct cpu_stop_done *done, bool inactive)
+ {
+ 	struct cpu_stop_work *work;
+ 	unsigned int cpu;
+@@ -343,11 +343,13 @@ static void queue_stop_cpus_work(const struct cpumask *cpumask,
+ 	}
+ 
+ 	/*
+-	 * Disable preemption while queueing to avoid getting
+-	 * preempted by a stopper which might wait for other stoppers
+-	 * to enter @fn which can lead to deadlock.
++	 * Make sure that all work is queued on all cpus before
++	 * any of the cpus can execute it.
+ 	 */
+-	lg_global_lock(&stop_cpus_lock);
++	if (!inactive)
++		lg_global_lock(&stop_cpus_lock);
++	else
++		lg_global_trylock_relax(&stop_cpus_lock);
+ 	for_each_cpu(cpu, cpumask)
+ 		cpu_stop_queue_work(cpu, &per_cpu(stop_cpus_work, cpu));
+ 	lg_global_unlock(&stop_cpus_lock);
+@@ -359,7 +361,7 @@ static int __stop_cpus(const struct cpumask *cpumask,
+ 	struct cpu_stop_done done;
+ 
+ 	cpu_stop_init_done(&done, cpumask_weight(cpumask));
+-	queue_stop_cpus_work(cpumask, fn, arg, &done);
++	queue_stop_cpus_work(cpumask, fn, arg, &done, false);
+ 	wait_for_completion(&done.completion);
+ 	return done.executed ? done.ret : -ENOENT;
+ }
+@@ -439,9 +441,9 @@ static int cpu_stop_should_run(unsigned int cpu)
+ 	unsigned long flags;
+ 	int run;
+ 
+-	spin_lock_irqsave(&stopper->lock, flags);
++	raw_spin_lock_irqsave(&stopper->lock, flags);
+ 	run = !list_empty(&stopper->works);
+-	spin_unlock_irqrestore(&stopper->lock, flags);
++	raw_spin_unlock_irqrestore(&stopper->lock, flags);
+ 	return run;
+ }
+ 
+@@ -453,13 +455,13 @@ static void cpu_stopper_thread(unsigned int cpu)
+ 
+ repeat:
+ 	work = NULL;
+-	spin_lock_irq(&stopper->lock);
++	raw_spin_lock_irq(&stopper->lock);
+ 	if (!list_empty(&stopper->works)) {
+ 		work = list_first_entry(&stopper->works,
+ 					struct cpu_stop_work, list);
+ 		list_del_init(&work->list);
+ 	}
+-	spin_unlock_irq(&stopper->lock);
++	raw_spin_unlock_irq(&stopper->lock);
+ 
+ 	if (work) {
+ 		cpu_stop_fn_t fn = work->fn;
+@@ -467,6 +469,16 @@ repeat:
+ 		struct cpu_stop_done *done = work->done;
+ 		char ksym_buf[KSYM_NAME_LEN] __maybe_unused;
+ 
++		/*
++		 * Wait until the stopper finished scheduling on all
++		 * cpus
++		 */
++		lg_global_lock(&stop_cpus_lock);
++		/*
++		 * Let other cpu threads continue as well
++		 */
++		lg_global_unlock(&stop_cpus_lock);
++
+ 		/* cpu stop callbacks are not allowed to sleep */
+ 		preempt_disable();
+ 
+@@ -500,20 +512,20 @@ static void cpu_stop_park(unsigned int cpu)
+ 	unsigned long flags;
+ 
+ 	/* drain remaining works */
+-	spin_lock_irqsave(&stopper->lock, flags);
++	raw_spin_lock_irqsave(&stopper->lock, flags);
+ 	list_for_each_entry(work, &stopper->works, list)
+ 		cpu_stop_signal_done(work->done, false);
+ 	stopper->enabled = false;
+-	spin_unlock_irqrestore(&stopper->lock, flags);
++	raw_spin_unlock_irqrestore(&stopper->lock, flags);
+ }
+ 
+ static void cpu_stop_unpark(unsigned int cpu)
+ {
+ 	struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ 
+-	spin_lock_irq(&stopper->lock);
++	raw_spin_lock_irq(&stopper->lock);
+ 	stopper->enabled = true;
+-	spin_unlock_irq(&stopper->lock);
++	raw_spin_unlock_irq(&stopper->lock);
+ }
+ 
+ static struct smp_hotplug_thread cpu_stop_threads = {
+@@ -535,10 +547,12 @@ static int __init cpu_stop_init(void)
+ 	for_each_possible_cpu(cpu) {
+ 		struct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);
+ 
+-		spin_lock_init(&stopper->lock);
++		raw_spin_lock_init(&stopper->lock);
+ 		INIT_LIST_HEAD(&stopper->works);
+ 	}
+ 
++	lg_lock_init(&stop_cpus_lock, "stop_cpus_lock");
++
+ 	BUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));
+ 	stop_machine_initialized = true;
+ 	return 0;
+@@ -634,7 +648,7 @@ int stop_machine_from_inactive_cpu(int (*fn)(void *), void *data,
+ 	set_state(&msdata, MULTI_STOP_PREPARE);
+ 	cpu_stop_init_done(&done, num_active_cpus());
+ 	queue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,
+-			     &done);
++			     &done, true);
+ 	ret = multi_cpu_stop(&msdata);
+ 
+ 	/* Busy wait for completion. */
+diff --git a/kernel/time/hrtimer.c b/kernel/time/hrtimer.c
+index 210b84882935..2c1597d2bd6d 100644
+--- a/kernel/time/hrtimer.c
++++ b/kernel/time/hrtimer.c
+@@ -48,11 +48,13 @@
+ #include <linux/sched/rt.h>
+ #include <linux/sched/deadline.h>
+ #include <linux/timer.h>
++#include <linux/kthread.h>
+ #include <linux/freezer.h>
+ 
+ #include <asm/uaccess.h>
+ 
+ #include <trace/events/timer.h>
++#include <trace/events/hist.h>
+ 
+ #include "timekeeping.h"
+ 
+@@ -570,8 +572,7 @@ static int hrtimer_reprogram(struct hrtimer *timer,
+ 	 * When the callback is running, we do not reprogram the clock event
+ 	 * device. The timer callback is either running on a different CPU or
+ 	 * the callback is executed in the hrtimer_interrupt context. The
+-	 * reprogramming is handled either by the softirq, which called the
+-	 * callback or at the end of the hrtimer_interrupt.
++	 * reprogramming is handled at the end of the hrtimer_interrupt.
+ 	 */
+ 	if (hrtimer_callback_running(timer))
+ 		return 0;
+@@ -606,6 +607,9 @@ static int hrtimer_reprogram(struct hrtimer *timer,
+ 	return res;
+ }
+ 
++static void __run_hrtimer(struct hrtimer *timer, ktime_t *now);
++static int hrtimer_rt_defer(struct hrtimer *timer);
++
+ /*
+  * Initialize the high resolution related parts of cpu_base
+  */
+@@ -615,6 +619,21 @@ static inline void hrtimer_init_hres(struct hrtimer_cpu_base *base)
+ 	base->hres_active = 0;
+ }
+ 
++static inline int hrtimer_enqueue_reprogram(struct hrtimer *timer,
++					    struct hrtimer_clock_base *base,
++					    int wakeup)
++{
++	if (!hrtimer_reprogram(timer, base))
++		return 0;
++	if (!wakeup)
++		return -ETIME;
++#ifdef CONFIG_PREEMPT_RT_BASE
++	if (!hrtimer_rt_defer(timer))
++		return -ETIME;
++#endif
++	return 1;
++}
++
+ static inline ktime_t hrtimer_update_base(struct hrtimer_cpu_base *base)
+ {
+ 	ktime_t *offs_real = &base->clock_base[HRTIMER_BASE_REALTIME].offset;
+@@ -680,6 +699,44 @@ static void clock_was_set_work(struct work_struct *work)
+ 
+ static DECLARE_WORK(hrtimer_work, clock_was_set_work);
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++/*
++ * RT can not call schedule_work from real interrupt context.
++ * Need to make a thread to do the real work.
++ */
++static struct task_struct *clock_set_delay_thread;
++static bool do_clock_set_delay;
++
++static int run_clock_set_delay(void *ignore)
++{
++	while (!kthread_should_stop()) {
++		set_current_state(TASK_INTERRUPTIBLE);
++		if (do_clock_set_delay) {
++			do_clock_set_delay = false;
++			schedule_work(&hrtimer_work);
++		}
++		schedule();
++	}
++	__set_current_state(TASK_RUNNING);
++	return 0;
++}
++
++void clock_was_set_delayed(void)
++{
++	do_clock_set_delay = true;
++	/* Make visible before waking up process */
++	smp_wmb();
++	wake_up_process(clock_set_delay_thread);
++}
++
++static __init int create_clock_set_delay_thread(void)
++{
++	clock_set_delay_thread = kthread_run(run_clock_set_delay, NULL, "kclksetdelayd");
++	BUG_ON(!clock_set_delay_thread);
++	return 0;
++}
++early_initcall(create_clock_set_delay_thread);
++#else /* PREEMPT_RT_FULL */
+ /*
+  * Called from timekeeping and resume code to reprogramm the hrtimer
+  * interrupt device on all cpus.
+@@ -688,6 +745,7 @@ void clock_was_set_delayed(void)
+ {
+ 	schedule_work(&hrtimer_work);
+ }
++#endif
+ 
+ #else
+ 
+@@ -696,6 +754,13 @@ static inline int hrtimer_is_hres_enabled(void) { return 0; }
+ static inline int hrtimer_switch_to_hres(void) { return 0; }
+ static inline void
+ hrtimer_force_reprogram(struct hrtimer_cpu_base *base, int skip_equal) { }
++static inline int hrtimer_enqueue_reprogram(struct hrtimer *timer,
++					    struct hrtimer_clock_base *base,
++					    int wakeup)
++{
++	return 0;
++}
++
+ static inline int hrtimer_reprogram(struct hrtimer *timer,
+ 				    struct hrtimer_clock_base *base)
+ {
+@@ -703,7 +768,6 @@ static inline int hrtimer_reprogram(struct hrtimer *timer,
+ }
+ static inline void hrtimer_init_hres(struct hrtimer_cpu_base *base) { }
+ static inline void retrigger_next_event(void *arg) { }
+-
+ #endif /* CONFIG_HIGH_RES_TIMERS */
+ 
+ /*
+@@ -821,6 +885,32 @@ u64 hrtimer_forward(struct hrtimer *timer, ktime_t now, ktime_t interval)
+ }
+ EXPORT_SYMBOL_GPL(hrtimer_forward);
+ 
++#ifdef CONFIG_PREEMPT_RT_BASE
++# define wake_up_timer_waiters(b)	wake_up(&(b)->wait)
++
++/**
++ * hrtimer_wait_for_timer - Wait for a running timer
++ *
++ * @timer:	timer to wait for
++ *
++ * The function waits in case the timers callback function is
++ * currently executed on the waitqueue of the timer base. The
++ * waitqueue is woken up after the timer callback function has
++ * finished execution.
++ */
++void hrtimer_wait_for_timer(const struct hrtimer *timer)
++{
++	struct hrtimer_clock_base *base = timer->base;
++
++	if (base && base->cpu_base && !timer->irqsafe)
++		wait_event(base->cpu_base->wait,
++			   !(timer->state & HRTIMER_STATE_CALLBACK));
++}
++
++#else
++# define wake_up_timer_waiters(b)	do { } while (0)
++#endif
++
+ /*
+  * enqueue_hrtimer - internal function to (re)start a timer
+  *
+@@ -864,6 +954,11 @@ static void __remove_hrtimer(struct hrtimer *timer,
+ 	if (!(timer->state & HRTIMER_STATE_ENQUEUED))
+ 		goto out;
+ 
++	if (unlikely(!list_empty(&timer->cb_entry))) {
++		list_del_init(&timer->cb_entry);
++		goto out;
++	}
++
+ 	next_timer = timerqueue_getnext(&base->active);
+ 	timerqueue_del(&base->active, &timer->node);
+ 	if (&timer->node == next_timer) {
+@@ -951,7 +1046,16 @@ int __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
+ 	new_base = switch_hrtimer_base(timer, base, mode & HRTIMER_MODE_PINNED);
+ 
+ 	timer_stats_hrtimer_set_start_info(timer);
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++	{
++		ktime_t now = new_base->get_time();
+ 
++		if (ktime_to_ns(tim) < ktime_to_ns(now))
++			timer->praecox = now;
++		else
++			timer->praecox = ktime_set(0, 0);
++	}
++#endif
+ 	leftmost = enqueue_hrtimer(timer, new_base);
+ 
+ 	if (!leftmost) {
+@@ -965,15 +1069,26 @@ int __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
+ 		 * on dynticks target.
+ 		 */
+ 		wake_up_nohz_cpu(new_base->cpu_base->cpu);
+-	} else if (new_base->cpu_base == this_cpu_ptr(&hrtimer_bases) &&
+-			hrtimer_reprogram(timer, new_base)) {
++	} else if (new_base->cpu_base == this_cpu_ptr(&hrtimer_bases)) {
++
++		ret = hrtimer_enqueue_reprogram(timer, new_base, wakeup);
++		if (ret < 0) {
++			/*
++			 * In case we failed to reprogram the timer (mostly
++			 * because out current timer is already elapsed),
++			 * remove it again and report a failure. This avoids
++			 * stale base->first entries.
++			 */
++			debug_deactivate(timer);
++			__remove_hrtimer(timer, new_base,
++				timer->state & HRTIMER_STATE_CALLBACK, 0);
++		} else if (ret > 0) {
+ 		/*
+ 		 * Only allow reprogramming if the new base is on this CPU.
+ 		 * (it might still be on another CPU if the timer was pending)
+ 		 *
+ 		 * XXX send_remote_softirq() ?
+ 		 */
+-		if (wakeup) {
+ 			/*
+ 			 * We need to drop cpu_base->lock to avoid a
+ 			 * lock ordering issue vs. rq->lock.
+@@ -981,9 +1096,7 @@ int __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,
+ 			raw_spin_unlock(&new_base->cpu_base->lock);
+ 			raise_softirq_irqoff(HRTIMER_SOFTIRQ);
+ 			local_irq_restore(flags);
+-			return ret;
+-		} else {
+-			__raise_softirq_irqoff(HRTIMER_SOFTIRQ);
++			return 0;
+ 		}
+ 	}
+ 
+@@ -1074,7 +1187,7 @@ int hrtimer_cancel(struct hrtimer *timer)
+ 
+ 		if (ret >= 0)
+ 			return ret;
+-		cpu_relax();
++		hrtimer_wait_for_timer(timer);
+ 	}
+ }
+ EXPORT_SYMBOL_GPL(hrtimer_cancel);
+@@ -1153,6 +1266,7 @@ static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
+ 
+ 	base = hrtimer_clockid_to_base(clock_id);
+ 	timer->base = &cpu_base->clock_base[base];
++	INIT_LIST_HEAD(&timer->cb_entry);
+ 	timerqueue_init(&timer->node);
+ 
+ #ifdef CONFIG_TIMER_STATS
+@@ -1236,6 +1350,126 @@ static void __run_hrtimer(struct hrtimer *timer, ktime_t *now)
+ 	timer->state &= ~HRTIMER_STATE_CALLBACK;
+ }
+ 
++static enum hrtimer_restart hrtimer_wakeup(struct hrtimer *timer);
++
++#ifdef CONFIG_PREEMPT_RT_BASE
++static void hrtimer_rt_reprogram(int restart, struct hrtimer *timer,
++				 struct hrtimer_clock_base *base)
++{
++	/*
++	 * Note, we clear the callback flag before we requeue the
++	 * timer otherwise we trigger the callback_running() check
++	 * in hrtimer_reprogram().
++	 */
++	timer->state &= ~HRTIMER_STATE_CALLBACK;
++
++	if (restart != HRTIMER_NORESTART) {
++		BUG_ON(hrtimer_active(timer));
++		/*
++		 * Enqueue the timer, if it's the leftmost timer then
++		 * we need to reprogram it.
++		 */
++		if (!enqueue_hrtimer(timer, base))
++			return;
++
++#ifndef CONFIG_HIGH_RES_TIMERS
++	}
++#else
++		if (base->cpu_base->hres_active &&
++		    hrtimer_reprogram(timer, base))
++			goto requeue;
++
++	} else if (hrtimer_active(timer)) {
++		/*
++		 * If the timer was rearmed on another CPU, reprogram
++		 * the event device.
++		 */
++		if (&timer->node == base->active.next &&
++		    base->cpu_base->hres_active &&
++		    hrtimer_reprogram(timer, base))
++			goto requeue;
++	}
++	return;
++
++requeue:
++	/*
++	 * Timer is expired. Thus move it from tree to pending list
++	 * again.
++	 */
++	__remove_hrtimer(timer, base, timer->state, 0);
++	list_add_tail(&timer->cb_entry, &base->expired);
++#endif
++}
++
++/*
++ * The changes in mainline which removed the callback modes from
++ * hrtimer are not yet working with -rt. The non wakeup_process()
++ * based callbacks which involve sleeping locks need to be treated
++ * seperately.
++ */
++static void hrtimer_rt_run_pending(void)
++{
++	enum hrtimer_restart (*fn)(struct hrtimer *);
++	struct hrtimer_cpu_base *cpu_base;
++	struct hrtimer_clock_base *base;
++	struct hrtimer *timer;
++	int index, restart;
++
++	local_irq_disable();
++	cpu_base = &per_cpu(hrtimer_bases, smp_processor_id());
++
++	raw_spin_lock(&cpu_base->lock);
++
++	for (index = 0; index < HRTIMER_MAX_CLOCK_BASES; index++) {
++		base = &cpu_base->clock_base[index];
++
++		while (!list_empty(&base->expired)) {
++			timer = list_first_entry(&base->expired,
++						 struct hrtimer, cb_entry);
++
++			/*
++			 * Same as the above __run_hrtimer function
++			 * just we run with interrupts enabled.
++			 */
++			debug_hrtimer_deactivate(timer);
++			__remove_hrtimer(timer, base, HRTIMER_STATE_CALLBACK, 0);
++			timer_stats_account_hrtimer(timer);
++			fn = timer->function;
++
++			raw_spin_unlock_irq(&cpu_base->lock);
++			restart = fn(timer);
++			raw_spin_lock_irq(&cpu_base->lock);
++
++			hrtimer_rt_reprogram(restart, timer, base);
++		}
++	}
++
++	raw_spin_unlock_irq(&cpu_base->lock);
++
++	wake_up_timer_waiters(cpu_base);
++}
++
++static int hrtimer_rt_defer(struct hrtimer *timer)
++{
++	if (timer->irqsafe)
++		return 0;
++
++	__remove_hrtimer(timer, timer->base, timer->state, 0);
++	list_add_tail(&timer->cb_entry, &timer->base->expired);
++	return 1;
++}
++
++#else
++
++static inline void hrtimer_rt_run_pending(void)
++{
++	hrtimer_peek_ahead_timers();
++}
++
++static inline int hrtimer_rt_defer(struct hrtimer *timer) { return 0; }
++
++#endif
++
+ #ifdef CONFIG_HIGH_RES_TIMERS
+ 
+ /*
+@@ -1246,7 +1480,7 @@ void hrtimer_interrupt(struct clock_event_device *dev)
+ {
+ 	struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);
+ 	ktime_t expires_next, now, entry_time, delta;
+-	int i, retries = 0;
++	int i, retries = 0, raise = 0;
+ 
+ 	BUG_ON(!cpu_base->hres_active);
+ 	cpu_base->nr_events++;
+@@ -1281,6 +1515,15 @@ retry:
+ 
+ 			timer = container_of(node, struct hrtimer, node);
+ 
++			trace_hrtimer_interrupt(raw_smp_processor_id(),
++			    ktime_to_ns(ktime_sub(ktime_to_ns(timer->praecox) ?
++				timer->praecox : hrtimer_get_expires(timer),
++				basenow)),
++			    current,
++			    timer->function == hrtimer_wakeup ?
++			    container_of(timer, struct hrtimer_sleeper,
++				timer)->task : NULL);
++
+ 			/*
+ 			 * The immediate goal for using the softexpires is
+ 			 * minimizing wakeups, not running timers at the
+@@ -1306,7 +1549,10 @@ retry:
+ 				break;
+ 			}
+ 
+-			__run_hrtimer(timer, &basenow);
++			if (!hrtimer_rt_defer(timer))
++				__run_hrtimer(timer, &basenow);
++			else
++				raise = 1;
+ 		}
+ 	}
+ 
+@@ -1321,7 +1567,7 @@ retry:
+ 	if (expires_next.tv64 == KTIME_MAX ||
+ 	    !tick_program_event(expires_next, 0)) {
+ 		cpu_base->hang_detected = 0;
+-		return;
++		goto out;
+ 	}
+ 
+ 	/*
+@@ -1365,6 +1611,9 @@ retry:
+ 	tick_program_event(expires_next, 1);
+ 	printk_once(KERN_WARNING "hrtimer: interrupt took %llu ns\n",
+ 		    ktime_to_ns(delta));
++out:
++	if (raise)
++		raise_softirq_irqoff(HRTIMER_SOFTIRQ);
+ }
+ 
+ /*
+@@ -1400,18 +1649,18 @@ void hrtimer_peek_ahead_timers(void)
+ 	__hrtimer_peek_ahead_timers();
+ 	local_irq_restore(flags);
+ }
+-
+-static void run_hrtimer_softirq(struct softirq_action *h)
+-{
+-	hrtimer_peek_ahead_timers();
+-}
+-
+ #else /* CONFIG_HIGH_RES_TIMERS */
+ 
+ static inline void __hrtimer_peek_ahead_timers(void) { }
+ 
+ #endif	/* !CONFIG_HIGH_RES_TIMERS */
+ 
++
++static void run_hrtimer_softirq(struct softirq_action *h)
++{
++	hrtimer_rt_run_pending();
++}
++
+ /*
+  * Called from timer softirq every jiffy, expire hrtimers:
+  *
+@@ -1444,7 +1693,7 @@ void hrtimer_run_queues(void)
+ 	struct timerqueue_node *node;
+ 	struct hrtimer_cpu_base *cpu_base = this_cpu_ptr(&hrtimer_bases);
+ 	struct hrtimer_clock_base *base;
+-	int index, gettime = 1;
++	int index, gettime = 1, raise = 0;
+ 
+ 	if (hrtimer_hres_active())
+ 		return;
+@@ -1469,10 +1718,16 @@ void hrtimer_run_queues(void)
+ 					hrtimer_get_expires_tv64(timer))
+ 				break;
+ 
+-			__run_hrtimer(timer, &base->softirq_time);
++			if (!hrtimer_rt_defer(timer))
++				__run_hrtimer(timer, &base->softirq_time);
++			else
++				raise = 1;
+ 		}
+ 		raw_spin_unlock(&cpu_base->lock);
+ 	}
++
++	if (raise)
++		raise_softirq_irqoff(HRTIMER_SOFTIRQ);
+ }
+ 
+ /*
+@@ -1494,16 +1749,18 @@ static enum hrtimer_restart hrtimer_wakeup(struct hrtimer *timer)
+ void hrtimer_init_sleeper(struct hrtimer_sleeper *sl, struct task_struct *task)
+ {
+ 	sl->timer.function = hrtimer_wakeup;
++	sl->timer.irqsafe = 1;
+ 	sl->task = task;
+ }
+ EXPORT_SYMBOL_GPL(hrtimer_init_sleeper);
+ 
+-static int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode)
++static int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode,
++				unsigned long state)
+ {
+ 	hrtimer_init_sleeper(t, current);
+ 
+ 	do {
+-		set_current_state(TASK_INTERRUPTIBLE);
++		set_current_state(state);
+ 		hrtimer_start_expires(&t->timer, mode);
+ 		if (!hrtimer_active(&t->timer))
+ 			t->task = NULL;
+@@ -1547,7 +1804,8 @@ long __sched hrtimer_nanosleep_restart(struct restart_block *restart)
+ 				HRTIMER_MODE_ABS);
+ 	hrtimer_set_expires_tv64(&t.timer, restart->nanosleep.expires);
+ 
+-	if (do_nanosleep(&t, HRTIMER_MODE_ABS))
++	/* cpu_chill() does not care about restart state. */
++	if (do_nanosleep(&t, HRTIMER_MODE_ABS, TASK_INTERRUPTIBLE))
+ 		goto out;
+ 
+ 	rmtp = restart->nanosleep.rmtp;
+@@ -1564,8 +1822,10 @@ out:
+ 	return ret;
+ }
+ 
+-long hrtimer_nanosleep(struct timespec *rqtp, struct timespec __user *rmtp,
+-		       const enum hrtimer_mode mode, const clockid_t clockid)
++static long
++__hrtimer_nanosleep(struct timespec *rqtp, struct timespec __user *rmtp,
++		    const enum hrtimer_mode mode, const clockid_t clockid,
++		    unsigned long state)
+ {
+ 	struct restart_block *restart;
+ 	struct hrtimer_sleeper t;
+@@ -1578,7 +1838,7 @@ long hrtimer_nanosleep(struct timespec *rqtp, struct timespec __user *rmtp,
+ 
+ 	hrtimer_init_on_stack(&t.timer, clockid, mode);
+ 	hrtimer_set_expires_range_ns(&t.timer, timespec_to_ktime(*rqtp), slack);
+-	if (do_nanosleep(&t, mode))
++	if (do_nanosleep(&t, mode, state))
+ 		goto out;
+ 
+ 	/* Absolute timers do not update the rmtp value and restart: */
+@@ -1605,6 +1865,12 @@ out:
+ 	return ret;
+ }
+ 
++long hrtimer_nanosleep(struct timespec *rqtp, struct timespec __user *rmtp,
++		       const enum hrtimer_mode mode, const clockid_t clockid)
++{
++	return __hrtimer_nanosleep(rqtp, rmtp, mode, clockid, TASK_INTERRUPTIBLE);
++}
++
+ SYSCALL_DEFINE2(nanosleep, struct timespec __user *, rqtp,
+ 		struct timespec __user *, rmtp)
+ {
+@@ -1619,6 +1885,26 @@ SYSCALL_DEFINE2(nanosleep, struct timespec __user *, rqtp,
+ 	return hrtimer_nanosleep(&tu, rmtp, HRTIMER_MODE_REL, CLOCK_MONOTONIC);
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++/*
++ * Sleep for 1 ms in hope whoever holds what we want will let it go.
++ */
++void cpu_chill(void)
++{
++	struct timespec tu = {
++		.tv_nsec = NSEC_PER_MSEC,
++	};
++	unsigned int freeze_flag = current->flags & PF_NOFREEZE;
++
++	current->flags |= PF_NOFREEZE;
++	__hrtimer_nanosleep(&tu, NULL, HRTIMER_MODE_REL, CLOCK_MONOTONIC,
++			    TASK_UNINTERRUPTIBLE);
++	if (!freeze_flag)
++		current->flags &= ~PF_NOFREEZE;
++}
++EXPORT_SYMBOL(cpu_chill);
++#endif
++
+ /*
+  * Functions related to boot-time initialization:
+  */
+@@ -1630,10 +1916,14 @@ static void init_hrtimers_cpu(int cpu)
+ 	for (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {
+ 		cpu_base->clock_base[i].cpu_base = cpu_base;
+ 		timerqueue_init_head(&cpu_base->clock_base[i].active);
++		INIT_LIST_HEAD(&cpu_base->clock_base[i].expired);
+ 	}
+ 
+ 	cpu_base->cpu = cpu;
+ 	hrtimer_init_hres(cpu_base);
++#ifdef CONFIG_PREEMPT_RT_BASE
++	init_waitqueue_head(&cpu_base->wait);
++#endif
+ }
+ 
+ #ifdef CONFIG_HOTPLUG_CPU
+@@ -1746,9 +2036,7 @@ void __init hrtimers_init(void)
+ 	hrtimer_cpu_notify(&hrtimers_nb, (unsigned long)CPU_UP_PREPARE,
+ 			  (void *)(long)smp_processor_id());
+ 	register_cpu_notifier(&hrtimers_nb);
+-#ifdef CONFIG_HIGH_RES_TIMERS
+ 	open_softirq(HRTIMER_SOFTIRQ, run_hrtimer_softirq);
+-#endif
+ }
+ 
+ /**
+diff --git a/kernel/time/itimer.c b/kernel/time/itimer.c
+index 8d262b467573..d0513909d663 100644
+--- a/kernel/time/itimer.c
++++ b/kernel/time/itimer.c
+@@ -213,6 +213,7 @@ again:
+ 		/* We are sharing ->siglock with it_real_fn() */
+ 		if (hrtimer_try_to_cancel(timer) < 0) {
+ 			spin_unlock_irq(&tsk->sighand->siglock);
++			hrtimer_wait_for_timer(&tsk->signal->real_timer);
+ 			goto again;
+ 		}
+ 		expires = timeval_to_ktime(value->it_value);
+diff --git a/kernel/time/jiffies.c b/kernel/time/jiffies.c
+index a6a5bf53e86d..23d7203cc8af 100644
+--- a/kernel/time/jiffies.c
++++ b/kernel/time/jiffies.c
+@@ -73,7 +73,8 @@ static struct clocksource clocksource_jiffies = {
+ 	.shift		= JIFFIES_SHIFT,
+ };
+ 
+-__cacheline_aligned_in_smp DEFINE_SEQLOCK(jiffies_lock);
++__cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(jiffies_lock);
++__cacheline_aligned_in_smp seqcount_t jiffies_seq;
+ 
+ #if (BITS_PER_LONG < 64)
+ u64 get_jiffies_64(void)
+@@ -82,9 +83,9 @@ u64 get_jiffies_64(void)
+ 	u64 ret;
+ 
+ 	do {
+-		seq = read_seqbegin(&jiffies_lock);
++		seq = read_seqcount_begin(&jiffies_seq);
+ 		ret = jiffies_64;
+-	} while (read_seqretry(&jiffies_lock, seq));
++	} while (read_seqcount_retry(&jiffies_seq, seq));
+ 	return ret;
+ }
+ EXPORT_SYMBOL(get_jiffies_64);
+diff --git a/kernel/time/ntp.c b/kernel/time/ntp.c
+index 85fb3d632bd8..02897eb749d3 100644
+--- a/kernel/time/ntp.c
++++ b/kernel/time/ntp.c
+@@ -10,6 +10,7 @@
+ #include <linux/workqueue.h>
+ #include <linux/hrtimer.h>
+ #include <linux/jiffies.h>
++#include <linux/kthread.h>
+ #include <linux/math64.h>
+ #include <linux/timex.h>
+ #include <linux/time.h>
+@@ -519,10 +520,52 @@ static void sync_cmos_clock(struct work_struct *work)
+ 			   &sync_cmos_work, timespec_to_jiffies(&next));
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++/*
++ * RT can not call schedule_delayed_work from real interrupt context.
++ * Need to make a thread to do the real work.
++ */
++static struct task_struct *cmos_delay_thread;
++static bool do_cmos_delay;
++
++static int run_cmos_delay(void *ignore)
++{
++	while (!kthread_should_stop()) {
++		set_current_state(TASK_INTERRUPTIBLE);
++		if (do_cmos_delay) {
++			do_cmos_delay = false;
++			queue_delayed_work(system_power_efficient_wq,
++					   &sync_cmos_work, 0);
++		}
++		schedule();
++	}
++	__set_current_state(TASK_RUNNING);
++	return 0;
++}
++
++void ntp_notify_cmos_timer(void)
++{
++	do_cmos_delay = true;
++	/* Make visible before waking up process */
++	smp_wmb();
++	wake_up_process(cmos_delay_thread);
++}
++
++static __init int create_cmos_delay_thread(void)
++{
++	cmos_delay_thread = kthread_run(run_cmos_delay, NULL, "kcmosdelayd");
++	BUG_ON(!cmos_delay_thread);
++	return 0;
++}
++early_initcall(create_cmos_delay_thread);
++
++#else
++
+ void ntp_notify_cmos_timer(void)
+ {
+ 	queue_delayed_work(system_power_efficient_wq, &sync_cmos_work, 0);
+ }
++#endif /* CONFIG_PREEMPT_RT_FULL */
+ 
+ #else
+ void ntp_notify_cmos_timer(void) { }
+diff --git a/kernel/time/posix-cpu-timers.c b/kernel/time/posix-cpu-timers.c
+index 731d7a342670..d7b794998144 100644
+--- a/kernel/time/posix-cpu-timers.c
++++ b/kernel/time/posix-cpu-timers.c
+@@ -3,6 +3,7 @@
+  */
+ 
+ #include <linux/sched.h>
++#include <linux/sched/rt.h>
+ #include <linux/posix-timers.h>
+ #include <linux/errno.h>
+ #include <linux/math64.h>
+@@ -626,7 +627,7 @@ static int posix_cpu_timer_set(struct k_itimer *timer, int timer_flags,
+ 	/*
+ 	 * Disarm any old timer after extracting its expiry time.
+ 	 */
+-	WARN_ON_ONCE(!irqs_disabled());
++	WARN_ON_ONCE_NONRT(!irqs_disabled());
+ 
+ 	ret = 0;
+ 	old_incr = timer->it.cpu.incr;
+@@ -1048,7 +1049,7 @@ void posix_cpu_timer_schedule(struct k_itimer *timer)
+ 	/*
+ 	 * Now re-arm for the new expiry time.
+ 	 */
+-	WARN_ON_ONCE(!irqs_disabled());
++	WARN_ON_ONCE_NONRT(!irqs_disabled());
+ 	arm_timer(timer);
+ 	unlock_task_sighand(p, &flags);
+ 
+@@ -1114,10 +1115,11 @@ static inline int fastpath_timer_check(struct task_struct *tsk)
+ 	sig = tsk->signal;
+ 	if (sig->cputimer.running) {
+ 		struct task_cputime group_sample;
++		unsigned long flags;
+ 
+-		raw_spin_lock(&sig->cputimer.lock);
++		raw_spin_lock_irqsave(&sig->cputimer.lock, flags);
+ 		group_sample = sig->cputimer.cputime;
+-		raw_spin_unlock(&sig->cputimer.lock);
++		raw_spin_unlock_irqrestore(&sig->cputimer.lock, flags);
+ 
+ 		if (task_cputime_expired(&group_sample, &sig->cputime_expires))
+ 			return 1;
+@@ -1131,13 +1133,13 @@ static inline int fastpath_timer_check(struct task_struct *tsk)
+  * already updated our counts.  We need to check if any timers fire now.
+  * Interrupts are disabled.
+  */
+-void run_posix_cpu_timers(struct task_struct *tsk)
++static void __run_posix_cpu_timers(struct task_struct *tsk)
+ {
+ 	LIST_HEAD(firing);
+ 	struct k_itimer *timer, *next;
+ 	unsigned long flags;
+ 
+-	WARN_ON_ONCE(!irqs_disabled());
++	WARN_ON_ONCE_NONRT(!irqs_disabled());
+ 
+ 	/*
+ 	 * The fast path checks that there are no expired thread or thread
+@@ -1195,6 +1197,190 @@ void run_posix_cpu_timers(struct task_struct *tsk)
+ 	}
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_BASE
++#include <linux/kthread.h>
++#include <linux/cpu.h>
++DEFINE_PER_CPU(struct task_struct *, posix_timer_task);
++DEFINE_PER_CPU(struct task_struct *, posix_timer_tasklist);
++
++static int posix_cpu_timers_thread(void *data)
++{
++	int cpu = (long)data;
++
++	BUG_ON(per_cpu(posix_timer_task,cpu) != current);
++
++	while (!kthread_should_stop()) {
++		struct task_struct *tsk = NULL;
++		struct task_struct *next = NULL;
++
++		if (cpu_is_offline(cpu))
++			goto wait_to_die;
++
++		/* grab task list */
++		raw_local_irq_disable();
++		tsk = per_cpu(posix_timer_tasklist, cpu);
++		per_cpu(posix_timer_tasklist, cpu) = NULL;
++		raw_local_irq_enable();
++
++		/* its possible the list is empty, just return */
++		if (!tsk) {
++			set_current_state(TASK_INTERRUPTIBLE);
++			schedule();
++			__set_current_state(TASK_RUNNING);
++			continue;
++		}
++
++		/* Process task list */
++		while (1) {
++			/* save next */
++			next = tsk->posix_timer_list;
++
++			/* run the task timers, clear its ptr and
++			 * unreference it
++			 */
++			__run_posix_cpu_timers(tsk);
++			tsk->posix_timer_list = NULL;
++			put_task_struct(tsk);
++
++			/* check if this is the last on the list */
++			if (next == tsk)
++				break;
++			tsk = next;
++		}
++	}
++	return 0;
++
++wait_to_die:
++	/* Wait for kthread_stop */
++	set_current_state(TASK_INTERRUPTIBLE);
++	while (!kthread_should_stop()) {
++		schedule();
++		set_current_state(TASK_INTERRUPTIBLE);
++	}
++	__set_current_state(TASK_RUNNING);
++	return 0;
++}
++
++static inline int __fastpath_timer_check(struct task_struct *tsk)
++{
++	/* tsk == current, ensure it is safe to use ->signal/sighand */
++	if (unlikely(tsk->exit_state))
++		return 0;
++
++	if (!task_cputime_zero(&tsk->cputime_expires))
++			return 1;
++
++	if (!task_cputime_zero(&tsk->signal->cputime_expires))
++			return 1;
++
++	return 0;
++}
++
++void run_posix_cpu_timers(struct task_struct *tsk)
++{
++	unsigned long cpu = smp_processor_id();
++	struct task_struct *tasklist;
++
++	BUG_ON(!irqs_disabled());
++	if(!per_cpu(posix_timer_task, cpu))
++		return;
++	/* get per-cpu references */
++	tasklist = per_cpu(posix_timer_tasklist, cpu);
++
++	/* check to see if we're already queued */
++	if (!tsk->posix_timer_list && __fastpath_timer_check(tsk)) {
++		get_task_struct(tsk);
++		if (tasklist) {
++			tsk->posix_timer_list = tasklist;
++		} else {
++			/*
++			 * The list is terminated by a self-pointing
++			 * task_struct
++			 */
++			tsk->posix_timer_list = tsk;
++		}
++		per_cpu(posix_timer_tasklist, cpu) = tsk;
++
++		wake_up_process(per_cpu(posix_timer_task, cpu));
++	}
++}
++
++/*
++ * posix_cpu_thread_call - callback that gets triggered when a CPU is added.
++ * Here we can start up the necessary migration thread for the new CPU.
++ */
++static int posix_cpu_thread_call(struct notifier_block *nfb,
++				 unsigned long action, void *hcpu)
++{
++	int cpu = (long)hcpu;
++	struct task_struct *p;
++	struct sched_param param;
++
++	switch (action) {
++	case CPU_UP_PREPARE:
++		p = kthread_create(posix_cpu_timers_thread, hcpu,
++					"posixcputmr/%d",cpu);
++		if (IS_ERR(p))
++			return NOTIFY_BAD;
++		p->flags |= PF_NOFREEZE;
++		kthread_bind(p, cpu);
++		/* Must be high prio to avoid getting starved */
++		param.sched_priority = MAX_RT_PRIO-1;
++		sched_setscheduler(p, SCHED_FIFO, &param);
++		per_cpu(posix_timer_task,cpu) = p;
++		break;
++	case CPU_ONLINE:
++		/* Strictly unneccessary, as first user will wake it. */
++		wake_up_process(per_cpu(posix_timer_task,cpu));
++		break;
++#ifdef CONFIG_HOTPLUG_CPU
++	case CPU_UP_CANCELED:
++		/* Unbind it from offline cpu so it can run.  Fall thru. */
++		kthread_bind(per_cpu(posix_timer_task, cpu),
++			     cpumask_any(cpu_online_mask));
++		kthread_stop(per_cpu(posix_timer_task,cpu));
++		per_cpu(posix_timer_task,cpu) = NULL;
++		break;
++	case CPU_DEAD:
++		kthread_stop(per_cpu(posix_timer_task,cpu));
++		per_cpu(posix_timer_task,cpu) = NULL;
++		break;
++#endif
++	}
++	return NOTIFY_OK;
++}
++
++/* Register at highest priority so that task migration (migrate_all_tasks)
++ * happens before everything else.
++ */
++static struct notifier_block posix_cpu_thread_notifier = {
++	.notifier_call = posix_cpu_thread_call,
++	.priority = 10
++};
++
++static int __init posix_cpu_thread_init(void)
++{
++	void *hcpu = (void *)(long)smp_processor_id();
++	/* Start one for boot CPU. */
++	unsigned long cpu;
++
++	/* init the per-cpu posix_timer_tasklets */
++	for_each_possible_cpu(cpu)
++		per_cpu(posix_timer_tasklist, cpu) = NULL;
++
++	posix_cpu_thread_call(&posix_cpu_thread_notifier, CPU_UP_PREPARE, hcpu);
++	posix_cpu_thread_call(&posix_cpu_thread_notifier, CPU_ONLINE, hcpu);
++	register_cpu_notifier(&posix_cpu_thread_notifier);
++	return 0;
++}
++early_initcall(posix_cpu_thread_init);
++#else /* CONFIG_PREEMPT_RT_BASE */
++void run_posix_cpu_timers(struct task_struct *tsk)
++{
++	__run_posix_cpu_timers(tsk);
++}
++#endif /* CONFIG_PREEMPT_RT_BASE */
++
+ /*
+  * Set one of the process-wide special case CPU timers or RLIMIT_CPU.
+  * The tsk->sighand->siglock must be held by the caller.
+diff --git a/kernel/time/posix-timers.c b/kernel/time/posix-timers.c
+index 31ea01f42e1f..0f5d7eae61f0 100644
+--- a/kernel/time/posix-timers.c
++++ b/kernel/time/posix-timers.c
+@@ -499,6 +499,7 @@ static enum hrtimer_restart posix_timer_fn(struct hrtimer *timer)
+ static struct pid *good_sigevent(sigevent_t * event)
+ {
+ 	struct task_struct *rtn = current->group_leader;
++	int sig = event->sigev_signo;
+ 
+ 	if ((event->sigev_notify & SIGEV_THREAD_ID ) &&
+ 		(!(rtn = find_task_by_vpid(event->sigev_notify_thread_id)) ||
+@@ -507,7 +508,8 @@ static struct pid *good_sigevent(sigevent_t * event)
+ 		return NULL;
+ 
+ 	if (((event->sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_NONE) &&
+-	    ((event->sigev_signo <= 0) || (event->sigev_signo > SIGRTMAX)))
++	    (sig <= 0 || sig > SIGRTMAX || sig_kernel_only(sig) ||
++	     sig_kernel_coredump(sig)))
+ 		return NULL;
+ 
+ 	return task_pid(rtn);
+@@ -819,6 +821,20 @@ SYSCALL_DEFINE1(timer_getoverrun, timer_t, timer_id)
+ 	return overrun;
+ }
+ 
++/*
++ * Protected by RCU!
++ */
++static void timer_wait_for_callback(struct k_clock *kc, struct k_itimer *timr)
++{
++#ifdef CONFIG_PREEMPT_RT_FULL
++	if (kc->timer_set == common_timer_set)
++		hrtimer_wait_for_timer(&timr->it.real.timer);
++	else
++		/* FIXME: Whacky hack for posix-cpu-timers */
++		schedule_timeout(1);
++#endif
++}
++
+ /* Set a POSIX.1b interval timer. */
+ /* timr->it_lock is taken. */
+ static int
+@@ -896,6 +912,7 @@ retry:
+ 	if (!timr)
+ 		return -EINVAL;
+ 
++	rcu_read_lock();
+ 	kc = clockid_to_kclock(timr->it_clock);
+ 	if (WARN_ON_ONCE(!kc || !kc->timer_set))
+ 		error = -EINVAL;
+@@ -904,9 +921,12 @@ retry:
+ 
+ 	unlock_timer(timr, flag);
+ 	if (error == TIMER_RETRY) {
++		timer_wait_for_callback(kc, timr);
+ 		rtn = NULL;	// We already got the old time...
++		rcu_read_unlock();
+ 		goto retry;
+ 	}
++	rcu_read_unlock();
+ 
+ 	if (old_setting && !error &&
+ 	    copy_to_user(old_setting, &old_spec, sizeof (old_spec)))
+@@ -944,10 +964,15 @@ retry_delete:
+ 	if (!timer)
+ 		return -EINVAL;
+ 
++	rcu_read_lock();
+ 	if (timer_delete_hook(timer) == TIMER_RETRY) {
+ 		unlock_timer(timer, flags);
++		timer_wait_for_callback(clockid_to_kclock(timer->it_clock),
++					timer);
++		rcu_read_unlock();
+ 		goto retry_delete;
+ 	}
++	rcu_read_unlock();
+ 
+ 	spin_lock(&current->sighand->siglock);
+ 	list_del(&timer->list);
+@@ -973,8 +998,18 @@ static void itimer_delete(struct k_itimer *timer)
+ retry_delete:
+ 	spin_lock_irqsave(&timer->it_lock, flags);
+ 
++	/* On RT we can race with a deletion */
++	if (!timer->it_signal) {
++		unlock_timer(timer, flags);
++		return;
++	}
++
+ 	if (timer_delete_hook(timer) == TIMER_RETRY) {
++		rcu_read_lock();
+ 		unlock_timer(timer, flags);
++		timer_wait_for_callback(clockid_to_kclock(timer->it_clock),
++					timer);
++		rcu_read_unlock();
+ 		goto retry_delete;
+ 	}
+ 	list_del(&timer->list);
+diff --git a/kernel/time/tick-broadcast-hrtimer.c b/kernel/time/tick-broadcast-hrtimer.c
+index 6aac4beedbbe..943c03395e46 100644
+--- a/kernel/time/tick-broadcast-hrtimer.c
++++ b/kernel/time/tick-broadcast-hrtimer.c
+@@ -109,5 +109,6 @@ void tick_setup_hrtimer_broadcast(void)
+ {
+ 	hrtimer_init(&bctimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
+ 	bctimer.function = bc_handler;
++	bctimer.irqsafe = true;
+ 	clockevents_register_device(&ce_broadcast_hrtimer);
+ }
+diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
+index 7efeedf53ebd..e3dd07d0e96b 100644
+--- a/kernel/time/tick-common.c
++++ b/kernel/time/tick-common.c
+@@ -78,13 +78,15 @@ int tick_is_oneshot_available(void)
+ static void tick_periodic(int cpu)
+ {
+ 	if (tick_do_timer_cpu == cpu) {
+-		write_seqlock(&jiffies_lock);
++		raw_spin_lock(&jiffies_lock);
++		write_seqcount_begin(&jiffies_seq);
+ 
+ 		/* Keep track of the next tick event */
+ 		tick_next_period = ktime_add(tick_next_period, tick_period);
+ 
+ 		do_timer(1);
+-		write_sequnlock(&jiffies_lock);
++		write_seqcount_end(&jiffies_seq);
++		raw_spin_unlock(&jiffies_lock);
+ 		update_wall_time();
+ 	}
+ 
+@@ -146,9 +148,9 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
+ 		ktime_t next;
+ 
+ 		do {
+-			seq = read_seqbegin(&jiffies_lock);
++			seq = read_seqcount_begin(&jiffies_seq);
+ 			next = tick_next_period;
+-		} while (read_seqretry(&jiffies_lock, seq));
++		} while (read_seqcount_retry(&jiffies_seq, seq));
+ 
+ 		clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
+ 
+diff --git a/kernel/time/tick-internal.h b/kernel/time/tick-internal.h
+index 366aeb4f2c66..8e118bb8ee1c 100644
+--- a/kernel/time/tick-internal.h
++++ b/kernel/time/tick-internal.h
+@@ -6,7 +6,8 @@
+ 
+ #include "timekeeping.h"
+ 
+-extern seqlock_t jiffies_lock;
++extern raw_spinlock_t jiffies_lock;
++extern seqcount_t jiffies_seq;
+ 
+ #define CS_NAME_LEN	32
+ 
+diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c
+index 8c30ef7a2b70..d7cef36a443c 100644
+--- a/kernel/time/tick-sched.c
++++ b/kernel/time/tick-sched.c
+@@ -62,7 +62,8 @@ static void tick_do_update_jiffies64(ktime_t now)
+ 		return;
+ 
+ 	/* Reevalute with jiffies_lock held */
+-	write_seqlock(&jiffies_lock);
++	raw_spin_lock(&jiffies_lock);
++	write_seqcount_begin(&jiffies_seq);
+ 
+ 	delta = ktime_sub(now, last_jiffies_update);
+ 	if (delta.tv64 >= tick_period.tv64) {
+@@ -85,10 +86,12 @@ static void tick_do_update_jiffies64(ktime_t now)
+ 		/* Keep the tick_next_period variable up to date */
+ 		tick_next_period = ktime_add(last_jiffies_update, tick_period);
+ 	} else {
+-		write_sequnlock(&jiffies_lock);
++		write_seqcount_end(&jiffies_seq);
++		raw_spin_unlock(&jiffies_lock);
+ 		return;
+ 	}
+-	write_sequnlock(&jiffies_lock);
++	write_seqcount_end(&jiffies_seq);
++	raw_spin_unlock(&jiffies_lock);
+ 	update_wall_time();
+ }
+ 
+@@ -99,12 +102,14 @@ static ktime_t tick_init_jiffy_update(void)
+ {
+ 	ktime_t period;
+ 
+-	write_seqlock(&jiffies_lock);
++	raw_spin_lock(&jiffies_lock);
++	write_seqcount_begin(&jiffies_seq);
+ 	/* Did we start the jiffies update yet ? */
+ 	if (last_jiffies_update.tv64 == 0)
+ 		last_jiffies_update = tick_next_period;
+ 	period = last_jiffies_update;
+-	write_sequnlock(&jiffies_lock);
++	write_seqcount_end(&jiffies_seq);
++	raw_spin_unlock(&jiffies_lock);
+ 	return period;
+ }
+ 
+@@ -176,6 +181,11 @@ static bool can_stop_full_tick(void)
+ 		return false;
+ 	}
+ 
++	if (!arch_irq_work_has_interrupt()) {
++		trace_tick_stop(0, "missing irq work interrupt\n");
++		return false;
++	}
++
+ 	/* sched_clock_tick() needs us? */
+ #ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
+ 	/*
+@@ -217,11 +227,17 @@ void __tick_nohz_full_check(void)
+ 
+ static void nohz_full_kick_work_func(struct irq_work *work)
+ {
++	unsigned long flags;
++
++	/* ksoftirqd processes sirqs with interrupts enabled */
++	local_irq_save(flags);
+ 	__tick_nohz_full_check();
++	local_irq_restore(flags);
+ }
+ 
+ static DEFINE_PER_CPU(struct irq_work, nohz_full_kick_work) = {
+ 	.func = nohz_full_kick_work_func,
++	.flags = IRQ_WORK_HARD_IRQ,
+ };
+ 
+ /*
+@@ -580,10 +596,10 @@ static ktime_t tick_nohz_stop_sched_tick(struct tick_sched *ts,
+ 
+ 	/* Read jiffies and the time when jiffies were updated last */
+ 	do {
+-		seq = read_seqbegin(&jiffies_lock);
++		seq = read_seqcount_begin(&jiffies_seq);
+ 		last_update = last_jiffies_update;
+ 		last_jiffies = jiffies;
+-	} while (read_seqretry(&jiffies_lock, seq));
++	} while (read_seqcount_retry(&jiffies_seq, seq));
+ 
+ 	if (rcu_needs_cpu(cpu, &rcu_delta_jiffies) ||
+ 	    arch_needs_cpu() || irq_work_needs_cpu()) {
+@@ -761,14 +777,7 @@ static bool can_stop_idle_tick(int cpu, struct tick_sched *ts)
+ 		return false;
+ 
+ 	if (unlikely(local_softirq_pending() && cpu_online(cpu))) {
+-		static int ratelimit;
+-
+-		if (ratelimit < 10 &&
+-		    (local_softirq_pending() & SOFTIRQ_STOP_IDLE_MASK)) {
+-			pr_warn("NOHZ: local_softirq_pending %02x\n",
+-				(unsigned int) local_softirq_pending());
+-			ratelimit++;
+-		}
++		softirq_check_pending_idle();
+ 		return false;
+ 	}
+ 
+@@ -1156,6 +1165,7 @@ void tick_setup_sched_timer(void)
+ 	 * Emulate tick processing via per-CPU hrtimers:
+ 	 */
+ 	hrtimer_init(&ts->sched_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
++	ts->sched_timer.irqsafe = 1;
+ 	ts->sched_timer.function = tick_sched_timer;
+ 
+ 	/* Get the next period (per cpu) */
+diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
+index c596236607ae..481b70fdd7eb 100644
+--- a/kernel/time/timekeeping.c
++++ b/kernel/time/timekeeping.c
+@@ -1813,8 +1813,10 @@ EXPORT_SYMBOL(hardpps);
+  */
+ void xtime_update(unsigned long ticks)
+ {
+-	write_seqlock(&jiffies_lock);
++	raw_spin_lock(&jiffies_lock);
++	write_seqcount_begin(&jiffies_seq);
+ 	do_timer(ticks);
+-	write_sequnlock(&jiffies_lock);
++	write_seqcount_end(&jiffies_seq);
++	raw_spin_unlock(&jiffies_lock);
+ 	update_wall_time();
+ }
+diff --git a/kernel/time/timer.c b/kernel/time/timer.c
+index 3c4e3116cdb1..0df5cbfbcb5b 100644
+--- a/kernel/time/timer.c
++++ b/kernel/time/timer.c
+@@ -78,6 +78,9 @@ struct tvec_root {
+ struct tvec_base {
+ 	spinlock_t lock;
+ 	struct timer_list *running_timer;
++#ifdef CONFIG_PREEMPT_RT_FULL
++	wait_queue_head_t wait_for_running_timer;
++#endif
+ 	unsigned long timer_jiffies;
+ 	unsigned long next_timer;
+ 	unsigned long active_timers;
+@@ -758,6 +761,36 @@ static struct tvec_base *lock_timer_base(struct timer_list *timer,
+ 	}
+ }
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++static inline struct tvec_base *switch_timer_base(struct timer_list *timer,
++						  struct tvec_base *old,
++						  struct tvec_base *new)
++{
++	/* See the comment in lock_timer_base() */
++	timer_set_base(timer, NULL);
++	spin_unlock(&old->lock);
++	spin_lock(&new->lock);
++	timer_set_base(timer, new);
++	return new;
++}
++#else
++static inline struct tvec_base *switch_timer_base(struct timer_list *timer,
++						  struct tvec_base *old,
++						  struct tvec_base *new)
++{
++	/*
++	 * We cannot do the above because we might be preempted and
++	 * then the preempter would see NULL and loop forever.
++	 */
++	if (spin_trylock(&new->lock)) {
++		timer_set_base(timer, new);
++		spin_unlock(&old->lock);
++		return new;
++	}
++	return old;
++}
++#endif
++
+ static inline int
+ __mod_timer(struct timer_list *timer, unsigned long expires,
+ 						bool pending_only, int pinned)
+@@ -788,14 +821,8 @@ __mod_timer(struct timer_list *timer, unsigned long expires,
+ 		 * handler yet has not finished. This also guarantees that
+ 		 * the timer is serialized wrt itself.
+ 		 */
+-		if (likely(base->running_timer != timer)) {
+-			/* See the comment in lock_timer_base() */
+-			timer_set_base(timer, NULL);
+-			spin_unlock(&base->lock);
+-			base = new_base;
+-			spin_lock(&base->lock);
+-			timer_set_base(timer, base);
+-		}
++		if (likely(base->running_timer != timer))
++			base = switch_timer_base(timer, base, new_base);
+ 	}
+ 
+ 	timer->expires = expires;
+@@ -983,6 +1010,29 @@ void add_timer_on(struct timer_list *timer, int cpu)
+ }
+ EXPORT_SYMBOL_GPL(add_timer_on);
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++/*
++ * Wait for a running timer
++ */
++static void wait_for_running_timer(struct timer_list *timer)
++{
++	struct tvec_base *base = timer->base;
++
++	if (base->running_timer == timer)
++		wait_event(base->wait_for_running_timer,
++			   base->running_timer != timer);
++}
++
++# define wakeup_timer_waiters(b)	wake_up_all(&(b)->wait_for_running_timer)
++#else
++static inline void wait_for_running_timer(struct timer_list *timer)
++{
++	cpu_relax();
++}
++
++# define wakeup_timer_waiters(b)	do { } while (0)
++#endif
++
+ /**
+  * del_timer - deactive a timer.
+  * @timer: the timer to be deactivated
+@@ -1040,7 +1090,7 @@ int try_to_del_timer_sync(struct timer_list *timer)
+ }
+ EXPORT_SYMBOL(try_to_del_timer_sync);
+ 
+-#ifdef CONFIG_SMP
++#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT_FULL)
+ /**
+  * del_timer_sync - deactivate a timer and wait for the handler to finish.
+  * @timer: the timer to be deactivated
+@@ -1100,7 +1150,7 @@ int del_timer_sync(struct timer_list *timer)
+ 		int ret = try_to_del_timer_sync(timer);
+ 		if (ret >= 0)
+ 			return ret;
+-		cpu_relax();
++		wait_for_running_timer(timer);
+ 	}
+ }
+ EXPORT_SYMBOL(del_timer_sync);
+@@ -1221,16 +1271,18 @@ static inline void __run_timers(struct tvec_base *base)
+ 			if (irqsafe) {
+ 				spin_unlock(&base->lock);
+ 				call_timer_fn(timer, fn, data);
++				base->running_timer = NULL;
+ 				spin_lock(&base->lock);
+ 			} else {
+ 				spin_unlock_irq(&base->lock);
+ 				call_timer_fn(timer, fn, data);
++				base->running_timer = NULL;
+ 				spin_lock_irq(&base->lock);
+ 			}
+ 		}
+ 	}
+-	base->running_timer = NULL;
+ 	spin_unlock_irq(&base->lock);
++	wakeup_timer_waiters(base);
+ }
+ 
+ #ifdef CONFIG_NO_HZ_COMMON
+@@ -1369,17 +1421,31 @@ unsigned long get_next_timer_interrupt(unsigned long now)
+ 	if (cpu_is_offline(smp_processor_id()))
+ 		return expires;
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++	/*
++	 * On PREEMPT_RT we cannot sleep here. If the trylock does not
++	 * succeed then we return the worst-case 'expires in 1 tick'
++	 * value.  We use the rt functions here directly to avoid a
++	 * migrate_disable() call.
++	 */
++	if (!spin_do_trylock(&base->lock))
++		return  now + 1;
++#else
+ 	spin_lock(&base->lock);
++#endif
+ 	if (base->active_timers) {
+ 		if (time_before_eq(base->next_timer, base->timer_jiffies))
+ 			base->next_timer = __next_timer_interrupt(base);
+ 		expires = base->next_timer;
+ 	}
++#ifdef CONFIG_PREEMPT_RT_FULL
++	rt_spin_unlock_after_trylock_in_irq(&base->lock);
++#else
+ 	spin_unlock(&base->lock);
++#endif
+ 
+ 	if (time_before_eq(expires, now))
+ 		return now;
+-
+ 	return cmp_next_hrtimer_event(now, expires);
+ }
+ #endif
+@@ -1395,13 +1461,14 @@ void update_process_times(int user_tick)
+ 
+ 	/* Note: this timer irq context must be accounted for as well. */
+ 	account_process_tick(p, user_tick);
++	scheduler_tick();
+ 	run_local_timers();
+ 	rcu_check_callbacks(cpu, user_tick);
+-#ifdef CONFIG_IRQ_WORK
++
++#if defined(CONFIG_IRQ_WORK)
+ 	if (in_irq())
+ 		irq_work_tick();
+ #endif
+-	scheduler_tick();
+ 	run_posix_cpu_timers(p);
+ }
+ 
+@@ -1414,6 +1481,8 @@ static void run_timer_softirq(struct softirq_action *h)
+ 
+ 	hrtimer_run_pending();
+ 
++	irq_work_tick_soft();
++
+ 	if (time_after_eq(jiffies, base->timer_jiffies))
+ 		__run_timers(base);
+ }
+@@ -1588,6 +1657,9 @@ static int init_timers_cpu(int cpu)
+ 		base = per_cpu(tvec_bases, cpu);
+ 	}
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++	init_waitqueue_head(&base->wait_for_running_timer);
++#endif
+ 
+ 	for (j = 0; j < TVN_SIZE; j++) {
+ 		INIT_LIST_HEAD(base->tv5.vec + j);
+@@ -1627,7 +1699,7 @@ static void migrate_timers(int cpu)
+ 
+ 	BUG_ON(cpu_online(cpu));
+ 	old_base = per_cpu(tvec_bases, cpu);
+-	new_base = get_cpu_var(tvec_bases);
++	new_base = get_local_var(tvec_bases);
+ 	/*
+ 	 * The caller is globally serialized and nobody else
+ 	 * takes two locks at once, deadlock is not possible.
+@@ -1648,7 +1720,7 @@ static void migrate_timers(int cpu)
+ 
+ 	spin_unlock(&old_base->lock);
+ 	spin_unlock_irq(&new_base->lock);
+-	put_cpu_var(tvec_bases);
++	put_local_var(tvec_bases);
+ }
+ #endif /* CONFIG_HOTPLUG_CPU */
+ 
+diff --git a/kernel/trace/Kconfig b/kernel/trace/Kconfig
+index a5da09c899dd..20505a806a2b 100644
+--- a/kernel/trace/Kconfig
++++ b/kernel/trace/Kconfig
+@@ -187,6 +187,24 @@ config IRQSOFF_TRACER
+ 	  enabled. This option and the preempt-off timing option can be
+ 	  used together or separately.)
+ 
++config INTERRUPT_OFF_HIST
++	bool "Interrupts-off Latency Histogram"
++	depends on IRQSOFF_TRACER
++	help
++	  This option generates continuously updated histograms (one per cpu)
++	  of the duration of time periods with interrupts disabled. The
++	  histograms are disabled by default. To enable them, write a non-zero
++	  number to
++
++	      /sys/kernel/debug/tracing/latency_hist/enable/preemptirqsoff
++
++	  If PREEMPT_OFF_HIST is also selected, additional histograms (one
++	  per cpu) are generated that accumulate the duration of time periods
++	  when both interrupts and preemption are disabled. The histogram data
++	  will be located in the debug file system at
++
++	      /sys/kernel/debug/tracing/latency_hist/irqsoff
++
+ config PREEMPT_TRACER
+ 	bool "Preemption-off Latency Tracer"
+ 	default n
+@@ -211,6 +229,24 @@ config PREEMPT_TRACER
+ 	  enabled. This option and the irqs-off timing option can be
+ 	  used together or separately.)
+ 
++config PREEMPT_OFF_HIST
++	bool "Preemption-off Latency Histogram"
++	depends on PREEMPT_TRACER
++	help
++	  This option generates continuously updated histograms (one per cpu)
++	  of the duration of time periods with preemption disabled. The
++	  histograms are disabled by default. To enable them, write a non-zero
++	  number to
++
++	      /sys/kernel/debug/tracing/latency_hist/enable/preemptirqsoff
++
++	  If INTERRUPT_OFF_HIST is also selected, additional histograms (one
++	  per cpu) are generated that accumulate the duration of time periods
++	  when both interrupts and preemption are disabled. The histogram data
++	  will be located in the debug file system at
++
++	      /sys/kernel/debug/tracing/latency_hist/preemptoff
++
+ config SCHED_TRACER
+ 	bool "Scheduling Latency Tracer"
+ 	select GENERIC_TRACER
+@@ -221,6 +257,74 @@ config SCHED_TRACER
+ 	  This tracer tracks the latency of the highest priority task
+ 	  to be scheduled in, starting from the point it has woken up.
+ 
++config WAKEUP_LATENCY_HIST
++	bool "Scheduling Latency Histogram"
++	depends on SCHED_TRACER
++	help
++	  This option generates continuously updated histograms (one per cpu)
++	  of the scheduling latency of the highest priority task.
++	  The histograms are disabled by default. To enable them, write a
++	  non-zero number to
++
++	      /sys/kernel/debug/tracing/latency_hist/enable/wakeup
++
++	  Two different algorithms are used, one to determine the latency of
++	  processes that exclusively use the highest priority of the system and
++	  another one to determine the latency of processes that share the
++	  highest system priority with other processes. The former is used to
++	  improve hardware and system software, the latter to optimize the
++	  priority design of a given system. The histogram data will be
++	  located in the debug file system at
++
++	      /sys/kernel/debug/tracing/latency_hist/wakeup
++
++	  and
++
++	      /sys/kernel/debug/tracing/latency_hist/wakeup/sharedprio
++
++	  If both Scheduling Latency Histogram and Missed Timer Offsets
++	  Histogram are selected, additional histogram data will be collected
++	  that contain, in addition to the wakeup latency, the timer latency, in
++	  case the wakeup was triggered by an expired timer. These histograms
++	  are available in the
++
++	      /sys/kernel/debug/tracing/latency_hist/timerandwakeup
++
++	  directory. They reflect the apparent interrupt and scheduling latency
++	  and are best suitable to determine the worst-case latency of a given
++	  system. To enable these histograms, write a non-zero number to
++
++	      /sys/kernel/debug/tracing/latency_hist/enable/timerandwakeup
++
++config MISSED_TIMER_OFFSETS_HIST
++	depends on HIGH_RES_TIMERS
++	select GENERIC_TRACER
++	bool "Missed Timer Offsets Histogram"
++	help
++	  Generate a histogram of missed timer offsets in microseconds. The
++	  histograms are disabled by default. To enable them, write a non-zero
++	  number to
++
++	      /sys/kernel/debug/tracing/latency_hist/enable/missed_timer_offsets
++
++	  The histogram data will be located in the debug file system at
++
++	      /sys/kernel/debug/tracing/latency_hist/missed_timer_offsets
++
++	  If both Scheduling Latency Histogram and Missed Timer Offsets
++	  Histogram are selected, additional histogram data will be collected
++	  that contain, in addition to the wakeup latency, the timer latency, in
++	  case the wakeup was triggered by an expired timer. These histograms
++	  are available in the
++
++	      /sys/kernel/debug/tracing/latency_hist/timerandwakeup
++
++	  directory. They reflect the apparent interrupt and scheduling latency
++	  and are best suitable to determine the worst-case latency of a given
++	  system. To enable these histograms, write a non-zero number to
++
++	      /sys/kernel/debug/tracing/latency_hist/enable/timerandwakeup
++
+ config ENABLE_DEFAULT_TRACERS
+ 	bool "Trace process context switches and events"
+ 	depends on !GENERIC_TRACER
+diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
+index 67d6369ddf83..b896e048e96d 100644
+--- a/kernel/trace/Makefile
++++ b/kernel/trace/Makefile
+@@ -36,6 +36,10 @@ obj-$(CONFIG_FUNCTION_TRACER) += trace_functions.o
+ obj-$(CONFIG_IRQSOFF_TRACER) += trace_irqsoff.o
+ obj-$(CONFIG_PREEMPT_TRACER) += trace_irqsoff.o
+ obj-$(CONFIG_SCHED_TRACER) += trace_sched_wakeup.o
++obj-$(CONFIG_INTERRUPT_OFF_HIST) += latency_hist.o
++obj-$(CONFIG_PREEMPT_OFF_HIST) += latency_hist.o
++obj-$(CONFIG_WAKEUP_LATENCY_HIST) += latency_hist.o
++obj-$(CONFIG_MISSED_TIMER_OFFSETS_HIST) += latency_hist.o
+ obj-$(CONFIG_NOP_TRACER) += trace_nop.o
+ obj-$(CONFIG_STACK_TRACER) += trace_stack.o
+ obj-$(CONFIG_MMIOTRACE) += trace_mmiotrace.o
+diff --git a/kernel/trace/latency_hist.c b/kernel/trace/latency_hist.c
+new file mode 100644
+index 000000000000..b6c1d14b71c4
+--- /dev/null
++++ b/kernel/trace/latency_hist.c
+@@ -0,0 +1,1178 @@
++/*
++ * kernel/trace/latency_hist.c
++ *
++ * Add support for histograms of preemption-off latency and
++ * interrupt-off latency and wakeup latency, it depends on
++ * Real-Time Preemption Support.
++ *
++ *  Copyright (C) 2005 MontaVista Software, Inc.
++ *  Yi Yang <yyang@ch.mvista.com>
++ *
++ *  Converted to work with the new latency tracer.
++ *  Copyright (C) 2008 Red Hat, Inc.
++ *    Steven Rostedt <srostedt@redhat.com>
++ *
++ */
++#include <linux/module.h>
++#include <linux/debugfs.h>
++#include <linux/seq_file.h>
++#include <linux/percpu.h>
++#include <linux/kallsyms.h>
++#include <linux/uaccess.h>
++#include <linux/sched.h>
++#include <linux/sched/rt.h>
++#include <linux/slab.h>
++#include <linux/atomic.h>
++#include <asm/div64.h>
++
++#include "trace.h"
++#include <trace/events/sched.h>
++
++#define NSECS_PER_USECS 1000L
++
++#define CREATE_TRACE_POINTS
++#include <trace/events/hist.h>
++
++enum {
++	IRQSOFF_LATENCY = 0,
++	PREEMPTOFF_LATENCY,
++	PREEMPTIRQSOFF_LATENCY,
++	WAKEUP_LATENCY,
++	WAKEUP_LATENCY_SHAREDPRIO,
++	MISSED_TIMER_OFFSETS,
++	TIMERANDWAKEUP_LATENCY,
++	MAX_LATENCY_TYPE,
++};
++
++#define MAX_ENTRY_NUM 10240
++
++struct hist_data {
++	atomic_t hist_mode; /* 0 log, 1 don't log */
++	long offset; /* set it to MAX_ENTRY_NUM/2 for a bipolar scale */
++	long min_lat;
++	long max_lat;
++	unsigned long long below_hist_bound_samples;
++	unsigned long long above_hist_bound_samples;
++	long long accumulate_lat;
++	unsigned long long total_samples;
++	unsigned long long hist_array[MAX_ENTRY_NUM];
++};
++
++struct enable_data {
++	int latency_type;
++	int enabled;
++};
++
++static char *latency_hist_dir_root = "latency_hist";
++
++#ifdef CONFIG_INTERRUPT_OFF_HIST
++static DEFINE_PER_CPU(struct hist_data, irqsoff_hist);
++static char *irqsoff_hist_dir = "irqsoff";
++static DEFINE_PER_CPU(cycles_t, hist_irqsoff_start);
++static DEFINE_PER_CPU(int, hist_irqsoff_counting);
++#endif
++
++#ifdef CONFIG_PREEMPT_OFF_HIST
++static DEFINE_PER_CPU(struct hist_data, preemptoff_hist);
++static char *preemptoff_hist_dir = "preemptoff";
++static DEFINE_PER_CPU(cycles_t, hist_preemptoff_start);
++static DEFINE_PER_CPU(int, hist_preemptoff_counting);
++#endif
++
++#if defined(CONFIG_PREEMPT_OFF_HIST) && defined(CONFIG_INTERRUPT_OFF_HIST)
++static DEFINE_PER_CPU(struct hist_data, preemptirqsoff_hist);
++static char *preemptirqsoff_hist_dir = "preemptirqsoff";
++static DEFINE_PER_CPU(cycles_t, hist_preemptirqsoff_start);
++static DEFINE_PER_CPU(int, hist_preemptirqsoff_counting);
++#endif
++
++#if defined(CONFIG_PREEMPT_OFF_HIST) || defined(CONFIG_INTERRUPT_OFF_HIST)
++static notrace void probe_preemptirqsoff_hist(void *v, int reason, int start);
++static struct enable_data preemptirqsoff_enabled_data = {
++	.latency_type = PREEMPTIRQSOFF_LATENCY,
++	.enabled = 0,
++};
++#endif
++
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++struct maxlatproc_data {
++	char comm[FIELD_SIZEOF(struct task_struct, comm)];
++	char current_comm[FIELD_SIZEOF(struct task_struct, comm)];
++	int pid;
++	int current_pid;
++	int prio;
++	int current_prio;
++	long latency;
++	long timeroffset;
++	cycle_t timestamp;
++};
++#endif
++
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++static DEFINE_PER_CPU(struct hist_data, wakeup_latency_hist);
++static DEFINE_PER_CPU(struct hist_data, wakeup_latency_hist_sharedprio);
++static char *wakeup_latency_hist_dir = "wakeup";
++static char *wakeup_latency_hist_dir_sharedprio = "sharedprio";
++static notrace void probe_wakeup_latency_hist_start(void *v,
++	struct task_struct *p);
++static notrace void probe_wakeup_latency_hist_stop(void *v,
++	struct task_struct *prev, struct task_struct *next);
++static notrace void probe_sched_migrate_task(void *,
++	struct task_struct *task, int cpu);
++static struct enable_data wakeup_latency_enabled_data = {
++	.latency_type = WAKEUP_LATENCY,
++	.enabled = 0,
++};
++static DEFINE_PER_CPU(struct maxlatproc_data, wakeup_maxlatproc);
++static DEFINE_PER_CPU(struct maxlatproc_data, wakeup_maxlatproc_sharedprio);
++static DEFINE_PER_CPU(struct task_struct *, wakeup_task);
++static DEFINE_PER_CPU(int, wakeup_sharedprio);
++static unsigned long wakeup_pid;
++#endif
++
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++static DEFINE_PER_CPU(struct hist_data, missed_timer_offsets);
++static char *missed_timer_offsets_dir = "missed_timer_offsets";
++static notrace void probe_hrtimer_interrupt(void *v, int cpu,
++	long long offset, struct task_struct *curr, struct task_struct *task);
++static struct enable_data missed_timer_offsets_enabled_data = {
++	.latency_type = MISSED_TIMER_OFFSETS,
++	.enabled = 0,
++};
++static DEFINE_PER_CPU(struct maxlatproc_data, missed_timer_offsets_maxlatproc);
++static unsigned long missed_timer_offsets_pid;
++#endif
++
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) && \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++static DEFINE_PER_CPU(struct hist_data, timerandwakeup_latency_hist);
++static char *timerandwakeup_latency_hist_dir = "timerandwakeup";
++static struct enable_data timerandwakeup_enabled_data = {
++	.latency_type = TIMERANDWAKEUP_LATENCY,
++	.enabled = 0,
++};
++static DEFINE_PER_CPU(struct maxlatproc_data, timerandwakeup_maxlatproc);
++#endif
++
++void notrace latency_hist(int latency_type, int cpu, long latency,
++			  long timeroffset, cycle_t stop,
++			  struct task_struct *p)
++{
++	struct hist_data *my_hist;
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++	struct maxlatproc_data *mp = NULL;
++#endif
++
++	if (!cpu_possible(cpu) || latency_type < 0 ||
++	    latency_type >= MAX_LATENCY_TYPE)
++		return;
++
++	switch (latency_type) {
++#ifdef CONFIG_INTERRUPT_OFF_HIST
++	case IRQSOFF_LATENCY:
++		my_hist = &per_cpu(irqsoff_hist, cpu);
++		break;
++#endif
++#ifdef CONFIG_PREEMPT_OFF_HIST
++	case PREEMPTOFF_LATENCY:
++		my_hist = &per_cpu(preemptoff_hist, cpu);
++		break;
++#endif
++#if defined(CONFIG_PREEMPT_OFF_HIST) && defined(CONFIG_INTERRUPT_OFF_HIST)
++	case PREEMPTIRQSOFF_LATENCY:
++		my_hist = &per_cpu(preemptirqsoff_hist, cpu);
++		break;
++#endif
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++	case WAKEUP_LATENCY:
++		my_hist = &per_cpu(wakeup_latency_hist, cpu);
++		mp = &per_cpu(wakeup_maxlatproc, cpu);
++		break;
++	case WAKEUP_LATENCY_SHAREDPRIO:
++		my_hist = &per_cpu(wakeup_latency_hist_sharedprio, cpu);
++		mp = &per_cpu(wakeup_maxlatproc_sharedprio, cpu);
++		break;
++#endif
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++	case MISSED_TIMER_OFFSETS:
++		my_hist = &per_cpu(missed_timer_offsets, cpu);
++		mp = &per_cpu(missed_timer_offsets_maxlatproc, cpu);
++		break;
++#endif
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) && \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++	case TIMERANDWAKEUP_LATENCY:
++		my_hist = &per_cpu(timerandwakeup_latency_hist, cpu);
++		mp = &per_cpu(timerandwakeup_maxlatproc, cpu);
++		break;
++#endif
++
++	default:
++		return;
++	}
++
++	latency += my_hist->offset;
++
++	if (atomic_read(&my_hist->hist_mode) == 0)
++		return;
++
++	if (latency < 0 || latency >= MAX_ENTRY_NUM) {
++		if (latency < 0)
++			my_hist->below_hist_bound_samples++;
++		else
++			my_hist->above_hist_bound_samples++;
++	} else
++		my_hist->hist_array[latency]++;
++
++	if (unlikely(latency > my_hist->max_lat ||
++	    my_hist->min_lat == LONG_MAX)) {
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++		if (latency_type == WAKEUP_LATENCY ||
++		    latency_type == WAKEUP_LATENCY_SHAREDPRIO ||
++		    latency_type == MISSED_TIMER_OFFSETS ||
++		    latency_type == TIMERANDWAKEUP_LATENCY) {
++			strncpy(mp->comm, p->comm, sizeof(mp->comm));
++			strncpy(mp->current_comm, current->comm,
++			    sizeof(mp->current_comm));
++			mp->pid = task_pid_nr(p);
++			mp->current_pid = task_pid_nr(current);
++			mp->prio = p->prio;
++			mp->current_prio = current->prio;
++			mp->latency = latency;
++			mp->timeroffset = timeroffset;
++			mp->timestamp = stop;
++		}
++#endif
++		my_hist->max_lat = latency;
++	}
++	if (unlikely(latency < my_hist->min_lat))
++		my_hist->min_lat = latency;
++	my_hist->total_samples++;
++	my_hist->accumulate_lat += latency;
++}
++
++static void *l_start(struct seq_file *m, loff_t *pos)
++{
++	loff_t *index_ptr = NULL;
++	loff_t index = *pos;
++	struct hist_data *my_hist = m->private;
++
++	if (index == 0) {
++		char minstr[32], avgstr[32], maxstr[32];
++
++		atomic_dec(&my_hist->hist_mode);
++
++		if (likely(my_hist->total_samples)) {
++			long avg = (long) div64_s64(my_hist->accumulate_lat,
++			    my_hist->total_samples);
++			snprintf(minstr, sizeof(minstr), "%ld",
++			    my_hist->min_lat - my_hist->offset);
++			snprintf(avgstr, sizeof(avgstr), "%ld",
++			    avg - my_hist->offset);
++			snprintf(maxstr, sizeof(maxstr), "%ld",
++			    my_hist->max_lat - my_hist->offset);
++		} else {
++			strcpy(minstr, "<undef>");
++			strcpy(avgstr, minstr);
++			strcpy(maxstr, minstr);
++		}
++
++		seq_printf(m, "#Minimum latency: %s microseconds\n"
++			   "#Average latency: %s microseconds\n"
++			   "#Maximum latency: %s microseconds\n"
++			   "#Total samples: %llu\n"
++			   "#There are %llu samples lower than %ld"
++			   " microseconds.\n"
++			   "#There are %llu samples greater or equal"
++			   " than %ld microseconds.\n"
++			   "#usecs\t%16s\n",
++			   minstr, avgstr, maxstr,
++			   my_hist->total_samples,
++			   my_hist->below_hist_bound_samples,
++			   -my_hist->offset,
++			   my_hist->above_hist_bound_samples,
++			   MAX_ENTRY_NUM - my_hist->offset,
++			   "samples");
++	}
++	if (index < MAX_ENTRY_NUM) {
++		index_ptr = kmalloc(sizeof(loff_t), GFP_KERNEL);
++		if (index_ptr)
++			*index_ptr = index;
++	}
++
++	return index_ptr;
++}
++
++static void *l_next(struct seq_file *m, void *p, loff_t *pos)
++{
++	loff_t *index_ptr = p;
++	struct hist_data *my_hist = m->private;
++
++	if (++*pos >= MAX_ENTRY_NUM) {
++		atomic_inc(&my_hist->hist_mode);
++		return NULL;
++	}
++	*index_ptr = *pos;
++	return index_ptr;
++}
++
++static void l_stop(struct seq_file *m, void *p)
++{
++	kfree(p);
++}
++
++static int l_show(struct seq_file *m, void *p)
++{
++	int index = *(loff_t *) p;
++	struct hist_data *my_hist = m->private;
++
++	seq_printf(m, "%6ld\t%16llu\n", index - my_hist->offset,
++	    my_hist->hist_array[index]);
++	return 0;
++}
++
++static const struct seq_operations latency_hist_seq_op = {
++	.start = l_start,
++	.next  = l_next,
++	.stop  = l_stop,
++	.show  = l_show
++};
++
++static int latency_hist_open(struct inode *inode, struct file *file)
++{
++	int ret;
++
++	ret = seq_open(file, &latency_hist_seq_op);
++	if (!ret) {
++		struct seq_file *seq = file->private_data;
++		seq->private = inode->i_private;
++	}
++	return ret;
++}
++
++static const struct file_operations latency_hist_fops = {
++	.open = latency_hist_open,
++	.read = seq_read,
++	.llseek = seq_lseek,
++	.release = seq_release,
++};
++
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++static void clear_maxlatprocdata(struct maxlatproc_data *mp)
++{
++	mp->comm[0] = mp->current_comm[0] = '\0';
++	mp->prio = mp->current_prio = mp->pid = mp->current_pid =
++	    mp->latency = mp->timeroffset = -1;
++	mp->timestamp = 0;
++}
++#endif
++
++static void hist_reset(struct hist_data *hist)
++{
++	atomic_dec(&hist->hist_mode);
++
++	memset(hist->hist_array, 0, sizeof(hist->hist_array));
++	hist->below_hist_bound_samples = 0ULL;
++	hist->above_hist_bound_samples = 0ULL;
++	hist->min_lat = LONG_MAX;
++	hist->max_lat = LONG_MIN;
++	hist->total_samples = 0ULL;
++	hist->accumulate_lat = 0LL;
++
++	atomic_inc(&hist->hist_mode);
++}
++
++static ssize_t
++latency_hist_reset(struct file *file, const char __user *a,
++		   size_t size, loff_t *off)
++{
++	int cpu;
++	struct hist_data *hist = NULL;
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++	struct maxlatproc_data *mp = NULL;
++#endif
++	off_t latency_type = (off_t) file->private_data;
++
++	for_each_online_cpu(cpu) {
++
++		switch (latency_type) {
++#ifdef CONFIG_PREEMPT_OFF_HIST
++		case PREEMPTOFF_LATENCY:
++			hist = &per_cpu(preemptoff_hist, cpu);
++			break;
++#endif
++#ifdef CONFIG_INTERRUPT_OFF_HIST
++		case IRQSOFF_LATENCY:
++			hist = &per_cpu(irqsoff_hist, cpu);
++			break;
++#endif
++#if defined(CONFIG_INTERRUPT_OFF_HIST) && defined(CONFIG_PREEMPT_OFF_HIST)
++		case PREEMPTIRQSOFF_LATENCY:
++			hist = &per_cpu(preemptirqsoff_hist, cpu);
++			break;
++#endif
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++		case WAKEUP_LATENCY:
++			hist = &per_cpu(wakeup_latency_hist, cpu);
++			mp = &per_cpu(wakeup_maxlatproc, cpu);
++			break;
++		case WAKEUP_LATENCY_SHAREDPRIO:
++			hist = &per_cpu(wakeup_latency_hist_sharedprio, cpu);
++			mp = &per_cpu(wakeup_maxlatproc_sharedprio, cpu);
++			break;
++#endif
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++		case MISSED_TIMER_OFFSETS:
++			hist = &per_cpu(missed_timer_offsets, cpu);
++			mp = &per_cpu(missed_timer_offsets_maxlatproc, cpu);
++			break;
++#endif
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) && \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++		case TIMERANDWAKEUP_LATENCY:
++			hist = &per_cpu(timerandwakeup_latency_hist, cpu);
++			mp = &per_cpu(timerandwakeup_maxlatproc, cpu);
++			break;
++#endif
++		}
++
++		hist_reset(hist);
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++		if (latency_type == WAKEUP_LATENCY ||
++		    latency_type == WAKEUP_LATENCY_SHAREDPRIO ||
++		    latency_type == MISSED_TIMER_OFFSETS ||
++		    latency_type == TIMERANDWAKEUP_LATENCY)
++			clear_maxlatprocdata(mp);
++#endif
++	}
++
++	return size;
++}
++
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++static ssize_t
++show_pid(struct file *file, char __user *ubuf, size_t cnt, loff_t *ppos)
++{
++	char buf[64];
++	int r;
++	unsigned long *this_pid = file->private_data;
++
++	r = snprintf(buf, sizeof(buf), "%lu\n", *this_pid);
++	return simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
++}
++
++static ssize_t do_pid(struct file *file, const char __user *ubuf,
++		      size_t cnt, loff_t *ppos)
++{
++	char buf[64];
++	unsigned long pid;
++	unsigned long *this_pid = file->private_data;
++
++	if (cnt >= sizeof(buf))
++		return -EINVAL;
++
++	if (copy_from_user(&buf, ubuf, cnt))
++		return -EFAULT;
++
++	buf[cnt] = '\0';
++
++	if (kstrtoul(buf, 10, &pid))
++		return -EINVAL;
++
++	*this_pid = pid;
++
++	return cnt;
++}
++#endif
++
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++static ssize_t
++show_maxlatproc(struct file *file, char __user *ubuf, size_t cnt, loff_t *ppos)
++{
++	int r;
++	struct maxlatproc_data *mp = file->private_data;
++	int strmaxlen = (TASK_COMM_LEN * 2) + (8 * 8);
++	unsigned long long t;
++	unsigned long usecs, secs;
++	char *buf;
++
++	if (mp->pid == -1 || mp->current_pid == -1) {
++		buf = "(none)\n";
++		return simple_read_from_buffer(ubuf, cnt, ppos, buf,
++		    strlen(buf));
++	}
++
++	buf = kmalloc(strmaxlen, GFP_KERNEL);
++	if (buf == NULL)
++		return -ENOMEM;
++
++	t = ns2usecs(mp->timestamp);
++	usecs = do_div(t, USEC_PER_SEC);
++	secs = (unsigned long) t;
++	r = snprintf(buf, strmaxlen,
++	    "%d %d %ld (%ld) %s <- %d %d %s %lu.%06lu\n", mp->pid,
++	    MAX_RT_PRIO-1 - mp->prio, mp->latency, mp->timeroffset, mp->comm,
++	    mp->current_pid, MAX_RT_PRIO-1 - mp->current_prio, mp->current_comm,
++	    secs, usecs);
++	r = simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
++	kfree(buf);
++	return r;
++}
++#endif
++
++static ssize_t
++show_enable(struct file *file, char __user *ubuf, size_t cnt, loff_t *ppos)
++{
++	char buf[64];
++	struct enable_data *ed = file->private_data;
++	int r;
++
++	r = snprintf(buf, sizeof(buf), "%d\n", ed->enabled);
++	return simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
++}
++
++static ssize_t
++do_enable(struct file *file, const char __user *ubuf, size_t cnt, loff_t *ppos)
++{
++	char buf[64];
++	long enable;
++	struct enable_data *ed = file->private_data;
++
++	if (cnt >= sizeof(buf))
++		return -EINVAL;
++
++	if (copy_from_user(&buf, ubuf, cnt))
++		return -EFAULT;
++
++	buf[cnt] = 0;
++
++	if (kstrtoul(buf, 10, &enable))
++		return -EINVAL;
++
++	if ((enable && ed->enabled) || (!enable && !ed->enabled))
++		return cnt;
++
++	if (enable) {
++		int ret;
++
++		switch (ed->latency_type) {
++#if defined(CONFIG_INTERRUPT_OFF_HIST) || defined(CONFIG_PREEMPT_OFF_HIST)
++		case PREEMPTIRQSOFF_LATENCY:
++			ret = register_trace_preemptirqsoff_hist(
++			    probe_preemptirqsoff_hist, NULL);
++			if (ret) {
++				pr_info("wakeup trace: Couldn't assign "
++				    "probe_preemptirqsoff_hist "
++				    "to trace_preemptirqsoff_hist\n");
++				return ret;
++			}
++			break;
++#endif
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++		case WAKEUP_LATENCY:
++			ret = register_trace_sched_wakeup(
++			    probe_wakeup_latency_hist_start, NULL);
++			if (ret) {
++				pr_info("wakeup trace: Couldn't assign "
++				    "probe_wakeup_latency_hist_start "
++				    "to trace_sched_wakeup\n");
++				return ret;
++			}
++			ret = register_trace_sched_wakeup_new(
++			    probe_wakeup_latency_hist_start, NULL);
++			if (ret) {
++				pr_info("wakeup trace: Couldn't assign "
++				    "probe_wakeup_latency_hist_start "
++				    "to trace_sched_wakeup_new\n");
++				unregister_trace_sched_wakeup(
++				    probe_wakeup_latency_hist_start, NULL);
++				return ret;
++			}
++			ret = register_trace_sched_switch(
++			    probe_wakeup_latency_hist_stop, NULL);
++			if (ret) {
++				pr_info("wakeup trace: Couldn't assign "
++				    "probe_wakeup_latency_hist_stop "
++				    "to trace_sched_switch\n");
++				unregister_trace_sched_wakeup(
++				    probe_wakeup_latency_hist_start, NULL);
++				unregister_trace_sched_wakeup_new(
++				    probe_wakeup_latency_hist_start, NULL);
++				return ret;
++			}
++			ret = register_trace_sched_migrate_task(
++			    probe_sched_migrate_task, NULL);
++			if (ret) {
++				pr_info("wakeup trace: Couldn't assign "
++				    "probe_sched_migrate_task "
++				    "to trace_sched_migrate_task\n");
++				unregister_trace_sched_wakeup(
++				    probe_wakeup_latency_hist_start, NULL);
++				unregister_trace_sched_wakeup_new(
++				    probe_wakeup_latency_hist_start, NULL);
++				unregister_trace_sched_switch(
++				    probe_wakeup_latency_hist_stop, NULL);
++				return ret;
++			}
++			break;
++#endif
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++		case MISSED_TIMER_OFFSETS:
++			ret = register_trace_hrtimer_interrupt(
++			    probe_hrtimer_interrupt, NULL);
++			if (ret) {
++				pr_info("wakeup trace: Couldn't assign "
++				    "probe_hrtimer_interrupt "
++				    "to trace_hrtimer_interrupt\n");
++				return ret;
++			}
++			break;
++#endif
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) && \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++		case TIMERANDWAKEUP_LATENCY:
++			if (!wakeup_latency_enabled_data.enabled ||
++			    !missed_timer_offsets_enabled_data.enabled)
++				return -EINVAL;
++			break;
++#endif
++		default:
++			break;
++		}
++	} else {
++		switch (ed->latency_type) {
++#if defined(CONFIG_INTERRUPT_OFF_HIST) || defined(CONFIG_PREEMPT_OFF_HIST)
++		case PREEMPTIRQSOFF_LATENCY:
++			{
++				int cpu;
++
++				unregister_trace_preemptirqsoff_hist(
++				    probe_preemptirqsoff_hist, NULL);
++				for_each_online_cpu(cpu) {
++#ifdef CONFIG_INTERRUPT_OFF_HIST
++					per_cpu(hist_irqsoff_counting,
++					    cpu) = 0;
++#endif
++#ifdef CONFIG_PREEMPT_OFF_HIST
++					per_cpu(hist_preemptoff_counting,
++					    cpu) = 0;
++#endif
++#if defined(CONFIG_INTERRUPT_OFF_HIST) && defined(CONFIG_PREEMPT_OFF_HIST)
++					per_cpu(hist_preemptirqsoff_counting,
++					    cpu) = 0;
++#endif
++				}
++			}
++			break;
++#endif
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++		case WAKEUP_LATENCY:
++			{
++				int cpu;
++
++				unregister_trace_sched_wakeup(
++				    probe_wakeup_latency_hist_start, NULL);
++				unregister_trace_sched_wakeup_new(
++				    probe_wakeup_latency_hist_start, NULL);
++				unregister_trace_sched_switch(
++				    probe_wakeup_latency_hist_stop, NULL);
++				unregister_trace_sched_migrate_task(
++				    probe_sched_migrate_task, NULL);
++
++				for_each_online_cpu(cpu) {
++					per_cpu(wakeup_task, cpu) = NULL;
++					per_cpu(wakeup_sharedprio, cpu) = 0;
++				}
++			}
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++			timerandwakeup_enabled_data.enabled = 0;
++#endif
++			break;
++#endif
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++		case MISSED_TIMER_OFFSETS:
++			unregister_trace_hrtimer_interrupt(
++			    probe_hrtimer_interrupt, NULL);
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++			timerandwakeup_enabled_data.enabled = 0;
++#endif
++			break;
++#endif
++		default:
++			break;
++		}
++	}
++	ed->enabled = enable;
++	return cnt;
++}
++
++static const struct file_operations latency_hist_reset_fops = {
++	.open = tracing_open_generic,
++	.write = latency_hist_reset,
++};
++
++static const struct file_operations enable_fops = {
++	.open = tracing_open_generic,
++	.read = show_enable,
++	.write = do_enable,
++};
++
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++static const struct file_operations pid_fops = {
++	.open = tracing_open_generic,
++	.read = show_pid,
++	.write = do_pid,
++};
++
++static const struct file_operations maxlatproc_fops = {
++	.open = tracing_open_generic,
++	.read = show_maxlatproc,
++};
++#endif
++
++#if defined(CONFIG_INTERRUPT_OFF_HIST) || defined(CONFIG_PREEMPT_OFF_HIST)
++static notrace void probe_preemptirqsoff_hist(void *v, int reason,
++	int starthist)
++{
++	int cpu = raw_smp_processor_id();
++	int time_set = 0;
++
++	if (starthist) {
++		cycle_t uninitialized_var(start);
++
++		if (!preempt_count() && !irqs_disabled())
++			return;
++
++#ifdef CONFIG_INTERRUPT_OFF_HIST
++		if ((reason == IRQS_OFF || reason == TRACE_START) &&
++		    !per_cpu(hist_irqsoff_counting, cpu)) {
++			per_cpu(hist_irqsoff_counting, cpu) = 1;
++			start = ftrace_now(cpu);
++			time_set++;
++			per_cpu(hist_irqsoff_start, cpu) = start;
++		}
++#endif
++
++#ifdef CONFIG_PREEMPT_OFF_HIST
++		if ((reason == PREEMPT_OFF || reason == TRACE_START) &&
++		    !per_cpu(hist_preemptoff_counting, cpu)) {
++			per_cpu(hist_preemptoff_counting, cpu) = 1;
++			if (!(time_set++))
++				start = ftrace_now(cpu);
++			per_cpu(hist_preemptoff_start, cpu) = start;
++		}
++#endif
++
++#if defined(CONFIG_INTERRUPT_OFF_HIST) && defined(CONFIG_PREEMPT_OFF_HIST)
++		if (per_cpu(hist_irqsoff_counting, cpu) &&
++		    per_cpu(hist_preemptoff_counting, cpu) &&
++		    !per_cpu(hist_preemptirqsoff_counting, cpu)) {
++			per_cpu(hist_preemptirqsoff_counting, cpu) = 1;
++			if (!time_set)
++				start = ftrace_now(cpu);
++			per_cpu(hist_preemptirqsoff_start, cpu) = start;
++		}
++#endif
++	} else {
++		cycle_t uninitialized_var(stop);
++
++#ifdef CONFIG_INTERRUPT_OFF_HIST
++		if ((reason == IRQS_ON || reason == TRACE_STOP) &&
++		    per_cpu(hist_irqsoff_counting, cpu)) {
++			cycle_t start = per_cpu(hist_irqsoff_start, cpu);
++
++			stop = ftrace_now(cpu);
++			time_set++;
++			if (start) {
++				long latency = ((long) (stop - start)) /
++				    NSECS_PER_USECS;
++
++				latency_hist(IRQSOFF_LATENCY, cpu, latency, 0,
++				    stop, NULL);
++			}
++			per_cpu(hist_irqsoff_counting, cpu) = 0;
++		}
++#endif
++
++#ifdef CONFIG_PREEMPT_OFF_HIST
++		if ((reason == PREEMPT_ON || reason == TRACE_STOP) &&
++		    per_cpu(hist_preemptoff_counting, cpu)) {
++			cycle_t start = per_cpu(hist_preemptoff_start, cpu);
++
++			if (!(time_set++))
++				stop = ftrace_now(cpu);
++			if (start) {
++				long latency = ((long) (stop - start)) /
++				    NSECS_PER_USECS;
++
++				latency_hist(PREEMPTOFF_LATENCY, cpu, latency,
++				    0, stop, NULL);
++			}
++			per_cpu(hist_preemptoff_counting, cpu) = 0;
++		}
++#endif
++
++#if defined(CONFIG_INTERRUPT_OFF_HIST) && defined(CONFIG_PREEMPT_OFF_HIST)
++		if ((!per_cpu(hist_irqsoff_counting, cpu) ||
++		     !per_cpu(hist_preemptoff_counting, cpu)) &&
++		   per_cpu(hist_preemptirqsoff_counting, cpu)) {
++			cycle_t start = per_cpu(hist_preemptirqsoff_start, cpu);
++
++			if (!time_set)
++				stop = ftrace_now(cpu);
++			if (start) {
++				long latency = ((long) (stop - start)) /
++				    NSECS_PER_USECS;
++
++				latency_hist(PREEMPTIRQSOFF_LATENCY, cpu,
++				    latency, 0, stop, NULL);
++			}
++			per_cpu(hist_preemptirqsoff_counting, cpu) = 0;
++		}
++#endif
++	}
++}
++#endif
++
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++static DEFINE_RAW_SPINLOCK(wakeup_lock);
++static notrace void probe_sched_migrate_task(void *v, struct task_struct *task,
++	int cpu)
++{
++	int old_cpu = task_cpu(task);
++
++	if (cpu != old_cpu) {
++		unsigned long flags;
++		struct task_struct *cpu_wakeup_task;
++
++		raw_spin_lock_irqsave(&wakeup_lock, flags);
++
++		cpu_wakeup_task = per_cpu(wakeup_task, old_cpu);
++		if (task == cpu_wakeup_task) {
++			put_task_struct(cpu_wakeup_task);
++			per_cpu(wakeup_task, old_cpu) = NULL;
++			cpu_wakeup_task = per_cpu(wakeup_task, cpu) = task;
++			get_task_struct(cpu_wakeup_task);
++		}
++
++		raw_spin_unlock_irqrestore(&wakeup_lock, flags);
++	}
++}
++
++static notrace void probe_wakeup_latency_hist_start(void *v,
++	struct task_struct *p)
++{
++	unsigned long flags;
++	struct task_struct *curr = current;
++	int cpu = task_cpu(p);
++	struct task_struct *cpu_wakeup_task;
++
++	raw_spin_lock_irqsave(&wakeup_lock, flags);
++
++	cpu_wakeup_task = per_cpu(wakeup_task, cpu);
++
++	if (wakeup_pid) {
++		if ((cpu_wakeup_task && p->prio == cpu_wakeup_task->prio) ||
++		    p->prio == curr->prio)
++			per_cpu(wakeup_sharedprio, cpu) = 1;
++		if (likely(wakeup_pid != task_pid_nr(p)))
++			goto out;
++	} else {
++		if (likely(!rt_task(p)) ||
++		    (cpu_wakeup_task && p->prio > cpu_wakeup_task->prio) ||
++		    p->prio > curr->prio)
++			goto out;
++		if ((cpu_wakeup_task && p->prio == cpu_wakeup_task->prio) ||
++		    p->prio == curr->prio)
++			per_cpu(wakeup_sharedprio, cpu) = 1;
++	}
++
++	if (cpu_wakeup_task)
++		put_task_struct(cpu_wakeup_task);
++	cpu_wakeup_task = per_cpu(wakeup_task, cpu) = p;
++	get_task_struct(cpu_wakeup_task);
++	cpu_wakeup_task->preempt_timestamp_hist =
++		ftrace_now(raw_smp_processor_id());
++out:
++	raw_spin_unlock_irqrestore(&wakeup_lock, flags);
++}
++
++static notrace void probe_wakeup_latency_hist_stop(void *v,
++	struct task_struct *prev, struct task_struct *next)
++{
++	unsigned long flags;
++	int cpu = task_cpu(next);
++	long latency;
++	cycle_t stop;
++	struct task_struct *cpu_wakeup_task;
++
++	raw_spin_lock_irqsave(&wakeup_lock, flags);
++
++	cpu_wakeup_task = per_cpu(wakeup_task, cpu);
++
++	if (cpu_wakeup_task == NULL)
++		goto out;
++
++	/* Already running? */
++	if (unlikely(current == cpu_wakeup_task))
++		goto out_reset;
++
++	if (next != cpu_wakeup_task) {
++		if (next->prio < cpu_wakeup_task->prio)
++			goto out_reset;
++
++		if (next->prio == cpu_wakeup_task->prio)
++			per_cpu(wakeup_sharedprio, cpu) = 1;
++
++		goto out;
++	}
++
++	if (current->prio == cpu_wakeup_task->prio)
++		per_cpu(wakeup_sharedprio, cpu) = 1;
++
++	/*
++	 * The task we are waiting for is about to be switched to.
++	 * Calculate latency and store it in histogram.
++	 */
++	stop = ftrace_now(raw_smp_processor_id());
++
++	latency = ((long) (stop - next->preempt_timestamp_hist)) /
++	    NSECS_PER_USECS;
++
++	if (per_cpu(wakeup_sharedprio, cpu)) {
++		latency_hist(WAKEUP_LATENCY_SHAREDPRIO, cpu, latency, 0, stop,
++		    next);
++		per_cpu(wakeup_sharedprio, cpu) = 0;
++	} else {
++		latency_hist(WAKEUP_LATENCY, cpu, latency, 0, stop, next);
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++		if (timerandwakeup_enabled_data.enabled) {
++			latency_hist(TIMERANDWAKEUP_LATENCY, cpu,
++			    next->timer_offset + latency, next->timer_offset,
++			    stop, next);
++		}
++#endif
++	}
++
++out_reset:
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++	next->timer_offset = 0;
++#endif
++	put_task_struct(cpu_wakeup_task);
++	per_cpu(wakeup_task, cpu) = NULL;
++out:
++	raw_spin_unlock_irqrestore(&wakeup_lock, flags);
++}
++#endif
++
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++static notrace void probe_hrtimer_interrupt(void *v, int cpu,
++	long long latency_ns, struct task_struct *curr,
++	struct task_struct *task)
++{
++	if (latency_ns <= 0 && task != NULL && rt_task(task) &&
++	    (task->prio < curr->prio ||
++	    (task->prio == curr->prio &&
++	    !cpumask_test_cpu(cpu, &task->cpus_allowed)))) {
++		long latency;
++		cycle_t now;
++
++		if (missed_timer_offsets_pid) {
++			if (likely(missed_timer_offsets_pid !=
++			    task_pid_nr(task)))
++				return;
++		}
++
++		now = ftrace_now(cpu);
++		latency = (long) div_s64(-latency_ns, NSECS_PER_USECS);
++		latency_hist(MISSED_TIMER_OFFSETS, cpu, latency, latency, now,
++		    task);
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++		task->timer_offset = latency;
++#endif
++	}
++}
++#endif
++
++static __init int latency_hist_init(void)
++{
++	struct dentry *latency_hist_root = NULL;
++	struct dentry *dentry;
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++	struct dentry *dentry_sharedprio;
++#endif
++	struct dentry *entry;
++	struct dentry *enable_root;
++	int i = 0;
++	struct hist_data *my_hist;
++	char name[64];
++	char *cpufmt = "CPU%d";
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) || \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++	char *cpufmt_maxlatproc = "max_latency-CPU%d";
++	struct maxlatproc_data *mp = NULL;
++#endif
++
++	dentry = tracing_init_dentry();
++	latency_hist_root = debugfs_create_dir(latency_hist_dir_root, dentry);
++	enable_root = debugfs_create_dir("enable", latency_hist_root);
++
++#ifdef CONFIG_INTERRUPT_OFF_HIST
++	dentry = debugfs_create_dir(irqsoff_hist_dir, latency_hist_root);
++	for_each_possible_cpu(i) {
++		sprintf(name, cpufmt, i);
++		entry = debugfs_create_file(name, 0444, dentry,
++		    &per_cpu(irqsoff_hist, i), &latency_hist_fops);
++		my_hist = &per_cpu(irqsoff_hist, i);
++		atomic_set(&my_hist->hist_mode, 1);
++		my_hist->min_lat = LONG_MAX;
++	}
++	entry = debugfs_create_file("reset", 0644, dentry,
++	    (void *)IRQSOFF_LATENCY, &latency_hist_reset_fops);
++#endif
++
++#ifdef CONFIG_PREEMPT_OFF_HIST
++	dentry = debugfs_create_dir(preemptoff_hist_dir,
++	    latency_hist_root);
++	for_each_possible_cpu(i) {
++		sprintf(name, cpufmt, i);
++		entry = debugfs_create_file(name, 0444, dentry,
++		    &per_cpu(preemptoff_hist, i), &latency_hist_fops);
++		my_hist = &per_cpu(preemptoff_hist, i);
++		atomic_set(&my_hist->hist_mode, 1);
++		my_hist->min_lat = LONG_MAX;
++	}
++	entry = debugfs_create_file("reset", 0644, dentry,
++	    (void *)PREEMPTOFF_LATENCY, &latency_hist_reset_fops);
++#endif
++
++#if defined(CONFIG_INTERRUPT_OFF_HIST) && defined(CONFIG_PREEMPT_OFF_HIST)
++	dentry = debugfs_create_dir(preemptirqsoff_hist_dir,
++	    latency_hist_root);
++	for_each_possible_cpu(i) {
++		sprintf(name, cpufmt, i);
++		entry = debugfs_create_file(name, 0444, dentry,
++		    &per_cpu(preemptirqsoff_hist, i), &latency_hist_fops);
++		my_hist = &per_cpu(preemptirqsoff_hist, i);
++		atomic_set(&my_hist->hist_mode, 1);
++		my_hist->min_lat = LONG_MAX;
++	}
++	entry = debugfs_create_file("reset", 0644, dentry,
++	    (void *)PREEMPTIRQSOFF_LATENCY, &latency_hist_reset_fops);
++#endif
++
++#if defined(CONFIG_INTERRUPT_OFF_HIST) || defined(CONFIG_PREEMPT_OFF_HIST)
++	entry = debugfs_create_file("preemptirqsoff", 0644,
++	    enable_root, (void *)&preemptirqsoff_enabled_data,
++	    &enable_fops);
++#endif
++
++#ifdef CONFIG_WAKEUP_LATENCY_HIST
++	dentry = debugfs_create_dir(wakeup_latency_hist_dir,
++	    latency_hist_root);
++	dentry_sharedprio = debugfs_create_dir(
++	    wakeup_latency_hist_dir_sharedprio, dentry);
++	for_each_possible_cpu(i) {
++		sprintf(name, cpufmt, i);
++
++		entry = debugfs_create_file(name, 0444, dentry,
++		    &per_cpu(wakeup_latency_hist, i),
++		    &latency_hist_fops);
++		my_hist = &per_cpu(wakeup_latency_hist, i);
++		atomic_set(&my_hist->hist_mode, 1);
++		my_hist->min_lat = LONG_MAX;
++
++		entry = debugfs_create_file(name, 0444, dentry_sharedprio,
++		    &per_cpu(wakeup_latency_hist_sharedprio, i),
++		    &latency_hist_fops);
++		my_hist = &per_cpu(wakeup_latency_hist_sharedprio, i);
++		atomic_set(&my_hist->hist_mode, 1);
++		my_hist->min_lat = LONG_MAX;
++
++		sprintf(name, cpufmt_maxlatproc, i);
++
++		mp = &per_cpu(wakeup_maxlatproc, i);
++		entry = debugfs_create_file(name, 0444, dentry, mp,
++		    &maxlatproc_fops);
++		clear_maxlatprocdata(mp);
++
++		mp = &per_cpu(wakeup_maxlatproc_sharedprio, i);
++		entry = debugfs_create_file(name, 0444, dentry_sharedprio, mp,
++		    &maxlatproc_fops);
++		clear_maxlatprocdata(mp);
++	}
++	entry = debugfs_create_file("pid", 0644, dentry,
++	    (void *)&wakeup_pid, &pid_fops);
++	entry = debugfs_create_file("reset", 0644, dentry,
++	    (void *)WAKEUP_LATENCY, &latency_hist_reset_fops);
++	entry = debugfs_create_file("reset", 0644, dentry_sharedprio,
++	    (void *)WAKEUP_LATENCY_SHAREDPRIO, &latency_hist_reset_fops);
++	entry = debugfs_create_file("wakeup", 0644,
++	    enable_root, (void *)&wakeup_latency_enabled_data,
++	    &enable_fops);
++#endif
++
++#ifdef CONFIG_MISSED_TIMER_OFFSETS_HIST
++	dentry = debugfs_create_dir(missed_timer_offsets_dir,
++	    latency_hist_root);
++	for_each_possible_cpu(i) {
++		sprintf(name, cpufmt, i);
++		entry = debugfs_create_file(name, 0444, dentry,
++		    &per_cpu(missed_timer_offsets, i), &latency_hist_fops);
++		my_hist = &per_cpu(missed_timer_offsets, i);
++		atomic_set(&my_hist->hist_mode, 1);
++		my_hist->min_lat = LONG_MAX;
++
++		sprintf(name, cpufmt_maxlatproc, i);
++		mp = &per_cpu(missed_timer_offsets_maxlatproc, i);
++		entry = debugfs_create_file(name, 0444, dentry, mp,
++		    &maxlatproc_fops);
++		clear_maxlatprocdata(mp);
++	}
++	entry = debugfs_create_file("pid", 0644, dentry,
++	    (void *)&missed_timer_offsets_pid, &pid_fops);
++	entry = debugfs_create_file("reset", 0644, dentry,
++	    (void *)MISSED_TIMER_OFFSETS, &latency_hist_reset_fops);
++	entry = debugfs_create_file("missed_timer_offsets", 0644,
++	    enable_root, (void *)&missed_timer_offsets_enabled_data,
++	    &enable_fops);
++#endif
++
++#if defined(CONFIG_WAKEUP_LATENCY_HIST) && \
++	defined(CONFIG_MISSED_TIMER_OFFSETS_HIST)
++	dentry = debugfs_create_dir(timerandwakeup_latency_hist_dir,
++	    latency_hist_root);
++	for_each_possible_cpu(i) {
++		sprintf(name, cpufmt, i);
++		entry = debugfs_create_file(name, 0444, dentry,
++		    &per_cpu(timerandwakeup_latency_hist, i),
++		    &latency_hist_fops);
++		my_hist = &per_cpu(timerandwakeup_latency_hist, i);
++		atomic_set(&my_hist->hist_mode, 1);
++		my_hist->min_lat = LONG_MAX;
++
++		sprintf(name, cpufmt_maxlatproc, i);
++		mp = &per_cpu(timerandwakeup_maxlatproc, i);
++		entry = debugfs_create_file(name, 0444, dentry, mp,
++		    &maxlatproc_fops);
++		clear_maxlatprocdata(mp);
++	}
++	entry = debugfs_create_file("reset", 0644, dentry,
++	    (void *)TIMERANDWAKEUP_LATENCY, &latency_hist_reset_fops);
++	entry = debugfs_create_file("timerandwakeup", 0644,
++	    enable_root, (void *)&timerandwakeup_enabled_data,
++	    &enable_fops);
++#endif
++	return 0;
++}
++
++device_initcall(latency_hist_init);
+diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
+index 111dfd986dbf..f159c8d211b5 100644
+--- a/kernel/trace/trace.c
++++ b/kernel/trace/trace.c
+@@ -1579,6 +1579,7 @@ tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,
+ 	struct task_struct *tsk = current;
+ 
+ 	entry->preempt_count		= pc & 0xff;
++	entry->preempt_lazy_count	= preempt_lazy_count();
+ 	entry->pid			= (tsk) ? tsk->pid : 0;
+ 	entry->flags =
+ #ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT
+@@ -1588,8 +1589,11 @@ tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,
+ #endif
+ 		((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |
+ 		((pc & SOFTIRQ_OFFSET) ? TRACE_FLAG_SOFTIRQ : 0) |
+-		(tif_need_resched() ? TRACE_FLAG_NEED_RESCHED : 0) |
++		(tif_need_resched_now() ? TRACE_FLAG_NEED_RESCHED : 0) |
++		(need_resched_lazy() ? TRACE_FLAG_NEED_RESCHED_LAZY : 0) |
+ 		(test_preempt_need_resched() ? TRACE_FLAG_PREEMPT_RESCHED : 0);
++
++	entry->migrate_disable = (tsk) ? __migrate_disabled(tsk) & 0xFF : 0;
+ }
+ EXPORT_SYMBOL_GPL(tracing_generic_entry_update);
+ 
+@@ -2509,14 +2513,17 @@ get_total_entries(struct trace_buffer *buf,
+ 
+ static void print_lat_help_header(struct seq_file *m)
+ {
+-	seq_puts(m, "#                  _------=> CPU#            \n");
+-	seq_puts(m, "#                 / _-----=> irqs-off        \n");
+-	seq_puts(m, "#                | / _----=> need-resched    \n");
+-	seq_puts(m, "#                || / _---=> hardirq/softirq \n");
+-	seq_puts(m, "#                ||| / _--=> preempt-depth   \n");
+-	seq_puts(m, "#                |||| /     delay             \n");
+-	seq_puts(m, "#  cmd     pid   ||||| time  |   caller      \n");
+-	seq_puts(m, "#     \\   /      |||||  \\    |   /           \n");
++	seq_puts(m, "#                  _--------=> CPU#              \n"
++		    "#                 / _-------=> irqs-off          \n"
++		    "#                | / _------=> need-resched      \n"
++		    "#                || / _-----=> need-resched_lazy \n"
++		    "#                ||| / _----=> hardirq/softirq   \n"
++		    "#                |||| / _---=> preempt-depth     \n"
++		    "#                ||||| / _--=> preempt-lazy-depth\n"
++		    "#                |||||| / _-=> migrate-disable   \n"
++		    "#                ||||||| /     delay             \n"
++		    "# cmd     pid    |||||||| time   |  caller       \n"
++		    "#     \\   /      ||||||||   \\    |  /            \n");
+ }
+ 
+ static void print_event_info(struct trace_buffer *buf, struct seq_file *m)
+@@ -2540,13 +2547,16 @@ static void print_func_help_header(struct trace_buffer *buf, struct seq_file *m)
+ static void print_func_help_header_irq(struct trace_buffer *buf, struct seq_file *m)
+ {
+ 	print_event_info(buf, m);
+-	seq_puts(m, "#                              _-----=> irqs-off\n");
+-	seq_puts(m, "#                             / _----=> need-resched\n");
+-	seq_puts(m, "#                            | / _---=> hardirq/softirq\n");
+-	seq_puts(m, "#                            || / _--=> preempt-depth\n");
+-	seq_puts(m, "#                            ||| /     delay\n");
+-	seq_puts(m, "#           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION\n");
+-	seq_puts(m, "#              | |       |   ||||       |         |\n");
++	seq_puts(m, "#                              _-------=> irqs-off          \n");
++	seq_puts(m, "#                            /  _------=> need-resched      \n");
++	seq_puts(m, "#                            |/  _-----=> need-resched_lazy \n");
++	seq_puts(m, "#                            ||/  _----=> hardirq/softirq   \n");
++	seq_puts(m, "#                            |||/  _---=> preempt-depth     \n");
++	seq_puts(m, "#                            ||||/  _--=> preempt-lazy-depth\n");
++	seq_puts(m, "#                            ||||| / _-=> migrate-disable   \n");
++	seq_puts(m, "#                            |||||| /     delay\n");
++	seq_puts(m, "#           TASK-PID   CPU#  ||||||  TIMESTAMP  FUNCTION\n");
++	seq_puts(m, "#              | |       |   ||||||     |         |\n");
+ }
+ 
+ void
+diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
+index 5642436bf97e..e759a36e7870 100644
+--- a/kernel/trace/trace.h
++++ b/kernel/trace/trace.h
+@@ -119,6 +119,7 @@ struct kretprobe_trace_entry_head {
+  *  NEED_RESCHED	- reschedule is requested
+  *  HARDIRQ		- inside an interrupt handler
+  *  SOFTIRQ		- inside a softirq handler
++ *  NEED_RESCHED_LAZY	- lazy reschedule is requested
+  */
+ enum trace_flag_type {
+ 	TRACE_FLAG_IRQS_OFF		= 0x01,
+@@ -127,6 +128,7 @@ enum trace_flag_type {
+ 	TRACE_FLAG_HARDIRQ		= 0x08,
+ 	TRACE_FLAG_SOFTIRQ		= 0x10,
+ 	TRACE_FLAG_PREEMPT_RESCHED	= 0x20,
++	TRACE_FLAG_NEED_RESCHED_LAZY    = 0x40,
+ };
+ 
+ #define TRACE_BUF_SIZE		1024
+diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
+index 51c47bc832d4..7bf7478c3e38 100644
+--- a/kernel/trace/trace_events.c
++++ b/kernel/trace/trace_events.c
+@@ -162,6 +162,8 @@ static int trace_define_common_fields(void)
+ 	__common_field(unsigned char, flags);
+ 	__common_field(unsigned char, preempt_count);
+ 	__common_field(int, pid);
++	__common_field(unsigned short, migrate_disable);
++	__common_field(unsigned short, padding);
+ 
+ 	return ret;
+ }
+@@ -198,6 +200,14 @@ void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
+ 
+ 	local_save_flags(fbuffer->flags);
+ 	fbuffer->pc = preempt_count();
++	/*
++	 * If CONFIG_PREEMPT is enabled, then the tracepoint itself disables
++	 * preemption (adding one to the preempt_count). Since we are
++	 * interested in the preempt_count at the time the tracepoint was
++	 * hit, we need to subtract one to offset the increment.
++	 */
++	if (IS_ENABLED(CONFIG_PREEMPT))
++		fbuffer->pc--;
+ 	fbuffer->ftrace_file = ftrace_file;
+ 
+ 	fbuffer->event =
+diff --git a/kernel/trace/trace_irqsoff.c b/kernel/trace/trace_irqsoff.c
+index 9bb104f748d0..d1940b095d85 100644
+--- a/kernel/trace/trace_irqsoff.c
++++ b/kernel/trace/trace_irqsoff.c
+@@ -17,6 +17,7 @@
+ #include <linux/fs.h>
+ 
+ #include "trace.h"
++#include <trace/events/hist.h>
+ 
+ static struct trace_array		*irqsoff_trace __read_mostly;
+ static int				tracer_enabled __read_mostly;
+@@ -435,11 +436,13 @@ void start_critical_timings(void)
+ {
+ 	if (preempt_trace() || irq_trace())
+ 		start_critical_timing(CALLER_ADDR0, CALLER_ADDR1);
++	trace_preemptirqsoff_hist_rcuidle(TRACE_START, 1);
+ }
+ EXPORT_SYMBOL_GPL(start_critical_timings);
+ 
+ void stop_critical_timings(void)
+ {
++	trace_preemptirqsoff_hist_rcuidle(TRACE_STOP, 0);
+ 	if (preempt_trace() || irq_trace())
+ 		stop_critical_timing(CALLER_ADDR0, CALLER_ADDR1);
+ }
+@@ -449,6 +452,7 @@ EXPORT_SYMBOL_GPL(stop_critical_timings);
+ #ifdef CONFIG_PROVE_LOCKING
+ void time_hardirqs_on(unsigned long a0, unsigned long a1)
+ {
++	trace_preemptirqsoff_hist(IRQS_ON, 0);
+ 	if (!preempt_trace() && irq_trace())
+ 		stop_critical_timing(a0, a1);
+ }
+@@ -457,6 +461,7 @@ void time_hardirqs_off(unsigned long a0, unsigned long a1)
+ {
+ 	if (!preempt_trace() && irq_trace())
+ 		start_critical_timing(a0, a1);
++	trace_preemptirqsoff_hist(IRQS_OFF, 1);
+ }
+ 
+ #else /* !CONFIG_PROVE_LOCKING */
+@@ -482,6 +487,7 @@ inline void print_irqtrace_events(struct task_struct *curr)
+  */
+ void trace_hardirqs_on(void)
+ {
++	trace_preemptirqsoff_hist_rcuidle(IRQS_ON, 0);
+ 	if (!preempt_trace() && irq_trace())
+ 		stop_critical_timing(CALLER_ADDR0, CALLER_ADDR1);
+ }
+@@ -491,11 +497,13 @@ void trace_hardirqs_off(void)
+ {
+ 	if (!preempt_trace() && irq_trace())
+ 		start_critical_timing(CALLER_ADDR0, CALLER_ADDR1);
++	trace_preemptirqsoff_hist_rcuidle(IRQS_OFF, 1);
+ }
+ EXPORT_SYMBOL(trace_hardirqs_off);
+ 
+ __visible void trace_hardirqs_on_caller(unsigned long caller_addr)
+ {
++	trace_preemptirqsoff_hist(IRQS_ON, 0);
+ 	if (!preempt_trace() && irq_trace())
+ 		stop_critical_timing(CALLER_ADDR0, caller_addr);
+ }
+@@ -505,6 +513,7 @@ __visible void trace_hardirqs_off_caller(unsigned long caller_addr)
+ {
+ 	if (!preempt_trace() && irq_trace())
+ 		start_critical_timing(CALLER_ADDR0, caller_addr);
++	trace_preemptirqsoff_hist(IRQS_OFF, 1);
+ }
+ EXPORT_SYMBOL(trace_hardirqs_off_caller);
+ 
+@@ -514,12 +523,14 @@ EXPORT_SYMBOL(trace_hardirqs_off_caller);
+ #ifdef CONFIG_PREEMPT_TRACER
+ void trace_preempt_on(unsigned long a0, unsigned long a1)
+ {
++	trace_preemptirqsoff_hist(PREEMPT_ON, 0);
+ 	if (preempt_trace() && !irq_trace())
+ 		stop_critical_timing(a0, a1);
+ }
+ 
+ void trace_preempt_off(unsigned long a0, unsigned long a1)
+ {
++	trace_preemptirqsoff_hist(PREEMPT_ON, 1);
+ 	if (preempt_trace() && !irq_trace())
+ 		start_critical_timing(a0, a1);
+ }
+diff --git a/kernel/trace/trace_output.c b/kernel/trace/trace_output.c
+index c6977d5a9b12..d72684443868 100644
+--- a/kernel/trace/trace_output.c
++++ b/kernel/trace/trace_output.c
+@@ -410,6 +410,7 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
+ {
+ 	char hardsoft_irq;
+ 	char need_resched;
++	char need_resched_lazy;
+ 	char irqs_off;
+ 	int hardirq;
+ 	int softirq;
+@@ -438,6 +439,8 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
+ 		need_resched = '.';
+ 		break;
+ 	}
++	need_resched_lazy =
++		(entry->flags & TRACE_FLAG_NEED_RESCHED_LAZY) ? 'L' : '.';
+ 
+ 	hardsoft_irq =
+ 		(hardirq && softirq) ? 'H' :
+@@ -445,8 +448,9 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
+ 		softirq ? 's' :
+ 		'.';
+ 
+-	if (!trace_seq_printf(s, "%c%c%c",
+-			      irqs_off, need_resched, hardsoft_irq))
++	if (!trace_seq_printf(s, "%c%c%c%c",
++			      irqs_off, need_resched, need_resched_lazy,
++			      hardsoft_irq))
+ 		return 0;
+ 
+ 	if (entry->preempt_count)
+@@ -454,6 +458,16 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
+ 	else
+ 		ret = trace_seq_putc(s, '.');
+ 
++	if (entry->preempt_lazy_count)
++		ret = trace_seq_printf(s, "%x", entry->preempt_lazy_count);
++	else
++		ret = trace_seq_putc(s, '.');
++
++	if (entry->migrate_disable)
++		ret = trace_seq_printf(s, "%x", entry->migrate_disable);
++	else
++		ret = trace_seq_putc(s, '.');
++
+ 	return ret;
+ }
+ 
+diff --git a/kernel/trace/trace_sched_switch.c b/kernel/trace/trace_sched_switch.c
+index 3f34dc9b40f3..9586cde520b0 100644
+--- a/kernel/trace/trace_sched_switch.c
++++ b/kernel/trace/trace_sched_switch.c
+@@ -106,7 +106,7 @@ tracing_sched_wakeup_trace(struct trace_array *tr,
+ }
+ 
+ static void
+-probe_sched_wakeup(void *ignore, struct task_struct *wakee, int success)
++probe_sched_wakeup(void *ignore, struct task_struct *wakee)
+ {
+ 	struct trace_array_cpu *data;
+ 	unsigned long flags;
+diff --git a/kernel/trace/trace_sched_wakeup.c b/kernel/trace/trace_sched_wakeup.c
+index 19bd8928ce94..808258ccf6c5 100644
+--- a/kernel/trace/trace_sched_wakeup.c
++++ b/kernel/trace/trace_sched_wakeup.c
+@@ -460,7 +460,7 @@ static void wakeup_reset(struct trace_array *tr)
+ }
+ 
+ static void
+-probe_wakeup(void *ignore, struct task_struct *p, int success)
++probe_wakeup(void *ignore, struct task_struct *p)
+ {
+ 	struct trace_array_cpu *data;
+ 	int cpu = smp_processor_id();
+diff --git a/kernel/user.c b/kernel/user.c
+index 2d09940c9632..ea7d0ec75f81 100644
+--- a/kernel/user.c
++++ b/kernel/user.c
+@@ -158,11 +158,11 @@ void free_uid(struct user_struct *up)
+ 	if (!up)
+ 		return;
+ 
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 	if (atomic_dec_and_lock(&up->__count, &uidhash_lock))
+ 		free_user(up, flags);
+ 	else
+-		local_irq_restore(flags);
++		local_irq_restore_nort(flags);
+ }
+ 
+ struct user_struct *alloc_uid(kuid_t uid)
+diff --git a/kernel/watchdog.c b/kernel/watchdog.c
+index 70bf11815f84..64f44cfb3bdc 100644
+--- a/kernel/watchdog.c
++++ b/kernel/watchdog.c
+@@ -248,6 +248,8 @@ static int is_softlockup(unsigned long touch_ts)
+ 
+ #ifdef CONFIG_HARDLOCKUP_DETECTOR
+ 
++static DEFINE_RAW_SPINLOCK(watchdog_output_lock);
++
+ static struct perf_event_attr wd_hw_attr = {
+ 	.type		= PERF_TYPE_HARDWARE,
+ 	.config		= PERF_COUNT_HW_CPU_CYCLES,
+@@ -281,13 +283,21 @@ static void watchdog_overflow_callback(struct perf_event *event,
+ 		/* only print hardlockups once */
+ 		if (__this_cpu_read(hard_watchdog_warn) == true)
+ 			return;
++		/*
++		 * If early-printk is enabled then make sure we do not
++		 * lock up in printk() and kill console logging:
++		 */
++		printk_kill();
+ 
+-		if (hardlockup_panic)
++		if (hardlockup_panic) {
+ 			panic("Watchdog detected hard LOCKUP on cpu %d",
+ 			      this_cpu);
+-		else
++		} else {
++			raw_spin_lock(&watchdog_output_lock);
+ 			WARN(1, "Watchdog detected hard LOCKUP on cpu %d",
+ 			     this_cpu);
++			raw_spin_unlock(&watchdog_output_lock);
++		}
+ 
+ 		__this_cpu_write(hard_watchdog_warn, true);
+ 		return;
+@@ -430,6 +440,7 @@ static void watchdog_enable(unsigned int cpu)
+ 	/* kick off the timer for the hardlockup detector */
+ 	hrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+ 	hrtimer->function = watchdog_timer_fn;
++	hrtimer->irqsafe = 1;
+ 
+ 	/* Enable the perf event */
+ 	watchdog_nmi_enable(cpu);
+diff --git a/kernel/workqueue.c b/kernel/workqueue.c
+index a636e38c0813..6f6c6254f41b 100644
+--- a/kernel/workqueue.c
++++ b/kernel/workqueue.c
+@@ -48,6 +48,8 @@
+ #include <linux/nodemask.h>
+ #include <linux/moduleparam.h>
+ #include <linux/uaccess.h>
++#include <linux/locallock.h>
++#include <linux/delay.h>
+ 
+ #include "workqueue_internal.h"
+ 
+@@ -121,15 +123,20 @@ enum {
+  *    cpu or grabbing pool->lock is enough for read access.  If
+  *    POOL_DISASSOCIATED is set, it's identical to L.
+  *
++ *    On RT we need the extra protection via rt_lock_idle_list() for
++ *    the list manipulations against read access from
++ *    wq_worker_sleeping(). All other places are nicely serialized via
++ *    pool->lock.
++ *
+  * A: pool->attach_mutex protected.
+  *
+  * PL: wq_pool_mutex protected.
+  *
+- * PR: wq_pool_mutex protected for writes.  Sched-RCU protected for reads.
++ * PR: wq_pool_mutex protected for writes.  RCU protected for reads.
+  *
+  * WQ: wq->mutex protected.
+  *
+- * WR: wq->mutex protected for writes.  Sched-RCU protected for reads.
++ * WR: wq->mutex protected for writes.  RCU protected for reads.
+  *
+  * MD: wq_mayday_lock protected.
+  */
+@@ -177,7 +184,7 @@ struct worker_pool {
+ 	atomic_t		nr_running ____cacheline_aligned_in_smp;
+ 
+ 	/*
+-	 * Destruction of pool is sched-RCU protected to allow dereferences
++	 * Destruction of pool is RCU protected to allow dereferences
+ 	 * from get_work_pool().
+ 	 */
+ 	struct rcu_head		rcu;
+@@ -206,7 +213,7 @@ struct pool_workqueue {
+ 	/*
+ 	 * Release of unbound pwq is punted to system_wq.  See put_pwq()
+ 	 * and pwq_unbound_release_workfn() for details.  pool_workqueue
+-	 * itself is also sched-RCU protected so that the first pwq can be
++	 * itself is also RCU protected so that the first pwq can be
+ 	 * determined without grabbing wq->mutex.
+ 	 */
+ 	struct work_struct	unbound_release_work;
+@@ -321,6 +328,8 @@ EXPORT_SYMBOL_GPL(system_power_efficient_wq);
+ struct workqueue_struct *system_freezable_power_efficient_wq __read_mostly;
+ EXPORT_SYMBOL_GPL(system_freezable_power_efficient_wq);
+ 
++static DEFINE_LOCAL_IRQ_LOCK(pendingb_lock);
++
+ static int worker_thread(void *__worker);
+ static void copy_workqueue_attrs(struct workqueue_attrs *to,
+ 				 const struct workqueue_attrs *from);
+@@ -329,14 +338,14 @@ static void copy_workqueue_attrs(struct workqueue_attrs *to,
+ #include <trace/events/workqueue.h>
+ 
+ #define assert_rcu_or_pool_mutex()					\
+-	rcu_lockdep_assert(rcu_read_lock_sched_held() ||		\
++	rcu_lockdep_assert(rcu_read_lock_held() ||			\
+ 			   lockdep_is_held(&wq_pool_mutex),		\
+-			   "sched RCU or wq_pool_mutex should be held")
++			   "RCU or wq_pool_mutex should be held")
+ 
+ #define assert_rcu_or_wq_mutex(wq)					\
+-	rcu_lockdep_assert(rcu_read_lock_sched_held() ||		\
++	rcu_lockdep_assert(rcu_read_lock_held() ||			\
+ 			   lockdep_is_held(&wq->mutex),			\
+-			   "sched RCU or wq->mutex should be held")
++			   "RCU or wq->mutex should be held")
+ 
+ #define for_each_cpu_worker_pool(pool, cpu)				\
+ 	for ((pool) = &per_cpu(cpu_worker_pools, cpu)[0];		\
+@@ -348,7 +357,7 @@ static void copy_workqueue_attrs(struct workqueue_attrs *to,
+  * @pool: iteration cursor
+  * @pi: integer used for iteration
+  *
+- * This must be called either with wq_pool_mutex held or sched RCU read
++ * This must be called either with wq_pool_mutex held or RCU read
+  * locked.  If the pool needs to be used beyond the locking in effect, the
+  * caller is responsible for guaranteeing that the pool stays online.
+  *
+@@ -380,7 +389,7 @@ static void copy_workqueue_attrs(struct workqueue_attrs *to,
+  * @pwq: iteration cursor
+  * @wq: the target workqueue
+  *
+- * This must be called either with wq->mutex held or sched RCU read locked.
++ * This must be called either with wq->mutex held or RCU read locked.
+  * If the pwq needs to be used beyond the locking in effect, the caller is
+  * responsible for guaranteeing that the pwq stays online.
+  *
+@@ -392,6 +401,31 @@ static void copy_workqueue_attrs(struct workqueue_attrs *to,
+ 		if (({ assert_rcu_or_wq_mutex(wq); false; })) { }	\
+ 		else
+ 
++#ifdef CONFIG_PREEMPT_RT_BASE
++static inline void rt_lock_idle_list(struct worker_pool *pool)
++{
++	preempt_disable();
++}
++static inline void rt_unlock_idle_list(struct worker_pool *pool)
++{
++	preempt_enable();
++}
++static inline void sched_lock_idle_list(struct worker_pool *pool) { }
++static inline void sched_unlock_idle_list(struct worker_pool *pool) { }
++#else
++static inline void rt_lock_idle_list(struct worker_pool *pool) { }
++static inline void rt_unlock_idle_list(struct worker_pool *pool) { }
++static inline void sched_lock_idle_list(struct worker_pool *pool)
++{
++	spin_lock_irq(&pool->lock);
++}
++static inline void sched_unlock_idle_list(struct worker_pool *pool)
++{
++	spin_unlock_irq(&pool->lock);
++}
++#endif
++
++
+ #ifdef CONFIG_DEBUG_OBJECTS_WORK
+ 
+ static struct debug_obj_descr work_debug_descr;
+@@ -542,7 +576,7 @@ static int worker_pool_assign_id(struct worker_pool *pool)
+  * @wq: the target workqueue
+  * @node: the node ID
+  *
+- * This must be called either with pwq_lock held or sched RCU read locked.
++ * This must be called either with pwq_lock held or RCU read locked.
+  * If the pwq needs to be used beyond the locking in effect, the caller is
+  * responsible for guaranteeing that the pwq stays online.
+  *
+@@ -675,8 +709,8 @@ static struct pool_workqueue *get_work_pwq(struct work_struct *work)
+  * @work: the work item of interest
+  *
+  * Pools are created and destroyed under wq_pool_mutex, and allows read
+- * access under sched-RCU read lock.  As such, this function should be
+- * called under wq_pool_mutex or with preemption disabled.
++ * access under RCU read lock.  As such, this function should be
++ * called under wq_pool_mutex or inside of a rcu_read_lock() region.
+  *
+  * All fields of the returned pool are accessible as long as the above
+  * mentioned locking is in effect.  If the returned pool needs to be used
+@@ -813,51 +847,44 @@ static struct worker *first_idle_worker(struct worker_pool *pool)
+  */
+ static void wake_up_worker(struct worker_pool *pool)
+ {
+-	struct worker *worker = first_idle_worker(pool);
++	struct worker *worker;
++
++	rt_lock_idle_list(pool);
++
++	worker = first_idle_worker(pool);
+ 
+ 	if (likely(worker))
+ 		wake_up_process(worker->task);
++
++	rt_unlock_idle_list(pool);
+ }
+ 
+ /**
+- * wq_worker_waking_up - a worker is waking up
+- * @task: task waking up
+- * @cpu: CPU @task is waking up to
+- *
+- * This function is called during try_to_wake_up() when a worker is
+- * being awoken.
++ * wq_worker_running - a worker is running again
++ * @task: task returning from sleep
+  *
+- * CONTEXT:
+- * spin_lock_irq(rq->lock)
++ * This function is called when a worker returns from schedule()
+  */
+-void wq_worker_waking_up(struct task_struct *task, int cpu)
++void wq_worker_running(struct task_struct *task)
+ {
+ 	struct worker *worker = kthread_data(task);
+ 
+-	if (!(worker->flags & WORKER_NOT_RUNNING)) {
+-		WARN_ON_ONCE(worker->pool->cpu != cpu);
++	if (!worker->sleeping)
++		return;
++	if (!(worker->flags & WORKER_NOT_RUNNING))
+ 		atomic_inc(&worker->pool->nr_running);
+-	}
++	worker->sleeping = 0;
+ }
+ 
+ /**
+  * wq_worker_sleeping - a worker is going to sleep
+  * @task: task going to sleep
+- * @cpu: CPU in question, must be the current CPU number
+- *
+- * This function is called during schedule() when a busy worker is
+- * going to sleep.  Worker on the same cpu can be woken up by
+- * returning pointer to its task.
+- *
+- * CONTEXT:
+- * spin_lock_irq(rq->lock)
+- *
+- * Return:
+- * Worker task on @cpu to wake up, %NULL if none.
++ * This function is called from schedule() when a busy worker is
++ * going to sleep.
+  */
+-struct task_struct *wq_worker_sleeping(struct task_struct *task, int cpu)
++void wq_worker_sleeping(struct task_struct *task)
+ {
+-	struct worker *worker = kthread_data(task), *to_wakeup = NULL;
++	struct worker *worker = kthread_data(task);
+ 	struct worker_pool *pool;
+ 
+ 	/*
+@@ -866,29 +893,26 @@ struct task_struct *wq_worker_sleeping(struct task_struct *task, int cpu)
+ 	 * checking NOT_RUNNING.
+ 	 */
+ 	if (worker->flags & WORKER_NOT_RUNNING)
+-		return NULL;
++		return;
+ 
+ 	pool = worker->pool;
+ 
+-	/* this can only happen on the local cpu */
+-	if (WARN_ON_ONCE(cpu != raw_smp_processor_id() || pool->cpu != cpu))
+-		return NULL;
++	if (WARN_ON_ONCE(worker->sleeping))
++		return;
++
++	worker->sleeping = 1;
+ 
+ 	/*
+ 	 * The counterpart of the following dec_and_test, implied mb,
+ 	 * worklist not empty test sequence is in insert_work().
+ 	 * Please read comment there.
+-	 *
+-	 * NOT_RUNNING is clear.  This means that we're bound to and
+-	 * running on the local cpu w/ rq lock held and preemption
+-	 * disabled, which in turn means that none else could be
+-	 * manipulating idle_list, so dereferencing idle_list without pool
+-	 * lock is safe.
+ 	 */
+ 	if (atomic_dec_and_test(&pool->nr_running) &&
+-	    !list_empty(&pool->worklist))
+-		to_wakeup = first_idle_worker(pool);
+-	return to_wakeup ? to_wakeup->task : NULL;
++	    !list_empty(&pool->worklist)) {
++		sched_lock_idle_list(pool);
++		wake_up_worker(pool);
++		sched_unlock_idle_list(pool);
++	}
+ }
+ 
+ /**
+@@ -1082,12 +1106,14 @@ static void put_pwq_unlocked(struct pool_workqueue *pwq)
+ {
+ 	if (pwq) {
+ 		/*
+-		 * As both pwqs and pools are sched-RCU protected, the
++		 * As both pwqs and pools are RCU protected, the
+ 		 * following lock operations are safe.
+ 		 */
+-		spin_lock_irq(&pwq->pool->lock);
++		rcu_read_lock();
++		local_spin_lock_irq(pendingb_lock, &pwq->pool->lock);
+ 		put_pwq(pwq);
+-		spin_unlock_irq(&pwq->pool->lock);
++		local_spin_unlock_irq(pendingb_lock, &pwq->pool->lock);
++		rcu_read_unlock();
+ 	}
+ }
+ 
+@@ -1189,7 +1215,7 @@ static int try_to_grab_pending(struct work_struct *work, bool is_dwork,
+ 	struct worker_pool *pool;
+ 	struct pool_workqueue *pwq;
+ 
+-	local_irq_save(*flags);
++	local_lock_irqsave(pendingb_lock, *flags);
+ 
+ 	/* try to steal the timer if it exists */
+ 	if (is_dwork) {
+@@ -1208,6 +1234,7 @@ static int try_to_grab_pending(struct work_struct *work, bool is_dwork,
+ 	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work)))
+ 		return 0;
+ 
++	rcu_read_lock();
+ 	/*
+ 	 * The queueing is in progress, or it is already queued. Try to
+ 	 * steal it from ->worklist without clearing WORK_STRUCT_PENDING.
+@@ -1246,14 +1273,16 @@ static int try_to_grab_pending(struct work_struct *work, bool is_dwork,
+ 		set_work_pool_and_keep_pending(work, pool->id);
+ 
+ 		spin_unlock(&pool->lock);
++		rcu_read_unlock();
+ 		return 1;
+ 	}
+ 	spin_unlock(&pool->lock);
+ fail:
+-	local_irq_restore(*flags);
++	rcu_read_unlock();
++	local_unlock_irqrestore(pendingb_lock, *flags);
+ 	if (work_is_canceling(work))
+ 		return -ENOENT;
+-	cpu_relax();
++	cpu_chill();
+ 	return -EAGAIN;
+ }
+ 
+@@ -1322,7 +1351,7 @@ static void __queue_work(int cpu, struct workqueue_struct *wq,
+ 	 * queued or lose PENDING.  Grabbing PENDING and queueing should
+ 	 * happen with IRQ disabled.
+ 	 */
+-	WARN_ON_ONCE(!irqs_disabled());
++	WARN_ON_ONCE_NONRT(!irqs_disabled());
+ 
+ 	debug_work_activate(work);
+ 
+@@ -1330,6 +1359,8 @@ static void __queue_work(int cpu, struct workqueue_struct *wq,
+ 	if (unlikely(wq->flags & __WQ_DRAINING) &&
+ 	    WARN_ON_ONCE(!is_chained_work(wq)))
+ 		return;
++
++	rcu_read_lock();
+ retry:
+ 	if (req_cpu == WORK_CPU_UNBOUND)
+ 		cpu = raw_smp_processor_id();
+@@ -1386,10 +1417,8 @@ retry:
+ 	/* pwq determined, queue */
+ 	trace_workqueue_queue_work(req_cpu, pwq, work);
+ 
+-	if (WARN_ON(!list_empty(&work->entry))) {
+-		spin_unlock(&pwq->pool->lock);
+-		return;
+-	}
++	if (WARN_ON(!list_empty(&work->entry)))
++		goto out;
+ 
+ 	pwq->nr_in_flight[pwq->work_color]++;
+ 	work_flags = work_color_to_flags(pwq->work_color);
+@@ -1405,7 +1434,9 @@ retry:
+ 
+ 	insert_work(pwq, work, worklist, work_flags);
+ 
++out:
+ 	spin_unlock(&pwq->pool->lock);
++	rcu_read_unlock();
+ }
+ 
+ /**
+@@ -1425,14 +1456,14 @@ bool queue_work_on(int cpu, struct workqueue_struct *wq,
+ 	bool ret = false;
+ 	unsigned long flags;
+ 
+-	local_irq_save(flags);
++	local_lock_irqsave(pendingb_lock,flags);
+ 
+ 	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {
+ 		__queue_work(cpu, wq, work);
+ 		ret = true;
+ 	}
+ 
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(pendingb_lock, flags);
+ 	return ret;
+ }
+ EXPORT_SYMBOL(queue_work_on);
+@@ -1499,14 +1530,14 @@ bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
+ 	unsigned long flags;
+ 
+ 	/* read the comment in __queue_work() */
+-	local_irq_save(flags);
++	local_lock_irqsave(pendingb_lock, flags);
+ 
+ 	if (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {
+ 		__queue_delayed_work(cpu, wq, dwork, delay);
+ 		ret = true;
+ 	}
+ 
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(pendingb_lock, flags);
+ 	return ret;
+ }
+ EXPORT_SYMBOL(queue_delayed_work_on);
+@@ -1541,7 +1572,7 @@ bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
+ 
+ 	if (likely(ret >= 0)) {
+ 		__queue_delayed_work(cpu, wq, dwork, delay);
+-		local_irq_restore(flags);
++		local_unlock_irqrestore(pendingb_lock, flags);
+ 	}
+ 
+ 	/* -ENOENT from try_to_grab_pending() becomes %true */
+@@ -1574,7 +1605,9 @@ static void worker_enter_idle(struct worker *worker)
+ 	worker->last_active = jiffies;
+ 
+ 	/* idle_list is LIFO */
++	rt_lock_idle_list(pool);
+ 	list_add(&worker->entry, &pool->idle_list);
++	rt_unlock_idle_list(pool);
+ 
+ 	if (too_many_workers(pool) && !timer_pending(&pool->idle_timer))
+ 		mod_timer(&pool->idle_timer, jiffies + IDLE_WORKER_TIMEOUT);
+@@ -1607,7 +1640,9 @@ static void worker_leave_idle(struct worker *worker)
+ 		return;
+ 	worker_clr_flags(worker, WORKER_IDLE);
+ 	pool->nr_idle--;
++	rt_lock_idle_list(pool);
+ 	list_del_init(&worker->entry);
++	rt_unlock_idle_list(pool);
+ }
+ 
+ static struct worker *alloc_worker(int node)
+@@ -1775,7 +1810,9 @@ static void destroy_worker(struct worker *worker)
+ 	pool->nr_workers--;
+ 	pool->nr_idle--;
+ 
++	rt_lock_idle_list(pool);
+ 	list_del_init(&worker->entry);
++	rt_unlock_idle_list(pool);
+ 	worker->flags |= WORKER_DIE;
+ 	wake_up_process(worker->task);
+ }
+@@ -2670,14 +2707,14 @@ static bool start_flush_work(struct work_struct *work, struct wq_barrier *barr)
+ 
+ 	might_sleep();
+ 
+-	local_irq_disable();
++	rcu_read_lock();
+ 	pool = get_work_pool(work);
+ 	if (!pool) {
+-		local_irq_enable();
++		rcu_read_unlock();
+ 		return false;
+ 	}
+ 
+-	spin_lock(&pool->lock);
++	spin_lock_irq(&pool->lock);
+ 	/* see the comment in try_to_grab_pending() with the same code */
+ 	pwq = get_work_pwq(work);
+ 	if (pwq) {
+@@ -2704,10 +2741,11 @@ static bool start_flush_work(struct work_struct *work, struct wq_barrier *barr)
+ 	else
+ 		lock_map_acquire_read(&pwq->wq->lockdep_map);
+ 	lock_map_release(&pwq->wq->lockdep_map);
+-
++	rcu_read_unlock();
+ 	return true;
+ already_gone:
+ 	spin_unlock_irq(&pool->lock);
++	rcu_read_unlock();
+ 	return false;
+ }
+ 
+@@ -2794,7 +2832,7 @@ static bool __cancel_work_timer(struct work_struct *work, bool is_dwork)
+ 
+ 	/* tell other tasks trying to grab @work to back off */
+ 	mark_work_canceling(work);
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(pendingb_lock, flags);
+ 
+ 	flush_work(work);
+ 	clear_work_data(work);
+@@ -2849,10 +2887,10 @@ EXPORT_SYMBOL_GPL(cancel_work_sync);
+  */
+ bool flush_delayed_work(struct delayed_work *dwork)
+ {
+-	local_irq_disable();
++	local_lock_irq(pendingb_lock);
+ 	if (del_timer_sync(&dwork->timer))
+ 		__queue_work(dwork->cpu, dwork->wq, &dwork->work);
+-	local_irq_enable();
++	local_unlock_irq(pendingb_lock);
+ 	return flush_work(&dwork->work);
+ }
+ EXPORT_SYMBOL(flush_delayed_work);
+@@ -2887,7 +2925,7 @@ bool cancel_delayed_work(struct delayed_work *dwork)
+ 
+ 	set_work_pool_and_clear_pending(&dwork->work,
+ 					get_work_pool_id(&dwork->work));
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(pendingb_lock, flags);
+ 	return ret;
+ }
+ EXPORT_SYMBOL(cancel_delayed_work);
+@@ -3073,7 +3111,8 @@ static ssize_t wq_pool_ids_show(struct device *dev,
+ 	const char *delim = "";
+ 	int node, written = 0;
+ 
+-	rcu_read_lock_sched();
++	get_online_cpus();
++	rcu_read_lock();
+ 	for_each_node(node) {
+ 		written += scnprintf(buf + written, PAGE_SIZE - written,
+ 				     "%s%d:%d", delim, node,
+@@ -3081,7 +3120,8 @@ static ssize_t wq_pool_ids_show(struct device *dev,
+ 		delim = " ";
+ 	}
+ 	written += scnprintf(buf + written, PAGE_SIZE - written, "\n");
+-	rcu_read_unlock_sched();
++	rcu_read_unlock();
++	put_online_cpus();
+ 
+ 	return written;
+ }
+@@ -3449,7 +3489,7 @@ static void rcu_free_pool(struct rcu_head *rcu)
+  * put_unbound_pool - put a worker_pool
+  * @pool: worker_pool to put
+  *
+- * Put @pool.  If its refcnt reaches zero, it gets destroyed in sched-RCU
++ * Put @pool.  If its refcnt reaches zero, it gets destroyed in RCU
+  * safe manner.  get_unbound_pool() calls this function on its failure path
+  * and this function should be able to release pools which went through,
+  * successfully or not, init_worker_pool().
+@@ -3503,8 +3543,8 @@ static void put_unbound_pool(struct worker_pool *pool)
+ 	del_timer_sync(&pool->idle_timer);
+ 	del_timer_sync(&pool->mayday_timer);
+ 
+-	/* sched-RCU protected to allow dereferences from get_work_pool() */
+-	call_rcu_sched(&pool->rcu, rcu_free_pool);
++	/* RCU protected to allow dereferences from get_work_pool() */
++	call_rcu(&pool->rcu, rcu_free_pool);
+ }
+ 
+ /**
+@@ -3609,7 +3649,7 @@ static void pwq_unbound_release_workfn(struct work_struct *work)
+ 	put_unbound_pool(pool);
+ 	mutex_unlock(&wq_pool_mutex);
+ 
+-	call_rcu_sched(&pwq->rcu, rcu_free_pwq);
++	call_rcu(&pwq->rcu, rcu_free_pwq);
+ 
+ 	/*
+ 	 * If we're the last pwq going away, @wq is already dead and no one
+@@ -4336,7 +4376,8 @@ bool workqueue_congested(int cpu, struct workqueue_struct *wq)
+ 	struct pool_workqueue *pwq;
+ 	bool ret;
+ 
+-	rcu_read_lock_sched();
++	rcu_read_lock();
++	preempt_disable();
+ 
+ 	if (cpu == WORK_CPU_UNBOUND)
+ 		cpu = smp_processor_id();
+@@ -4347,7 +4388,8 @@ bool workqueue_congested(int cpu, struct workqueue_struct *wq)
+ 		pwq = unbound_pwq_by_node(wq, cpu_to_node(cpu));
+ 
+ 	ret = !list_empty(&pwq->delayed_works);
+-	rcu_read_unlock_sched();
++	preempt_enable();
++	rcu_read_unlock();
+ 
+ 	return ret;
+ }
+@@ -4373,16 +4415,15 @@ unsigned int work_busy(struct work_struct *work)
+ 	if (work_pending(work))
+ 		ret |= WORK_BUSY_PENDING;
+ 
+-	local_irq_save(flags);
++	rcu_read_lock();
+ 	pool = get_work_pool(work);
+ 	if (pool) {
+-		spin_lock(&pool->lock);
++		spin_lock_irqsave(&pool->lock, flags);
+ 		if (find_worker_executing_work(pool, work))
+ 			ret |= WORK_BUSY_RUNNING;
+-		spin_unlock(&pool->lock);
++		spin_unlock_irqrestore(&pool->lock, flags);
+ 	}
+-	local_irq_restore(flags);
+-
++	rcu_read_unlock();
+ 	return ret;
+ }
+ EXPORT_SYMBOL_GPL(work_busy);
+@@ -4822,16 +4863,16 @@ bool freeze_workqueues_busy(void)
+ 		 * nr_active is monotonically decreasing.  It's safe
+ 		 * to peek without lock.
+ 		 */
+-		rcu_read_lock_sched();
++		rcu_read_lock();
+ 		for_each_pwq(pwq, wq) {
+ 			WARN_ON_ONCE(pwq->nr_active < 0);
+ 			if (pwq->nr_active) {
+ 				busy = true;
+-				rcu_read_unlock_sched();
++				rcu_read_unlock();
+ 				goto out_unlock;
+ 			}
+ 		}
+-		rcu_read_unlock_sched();
++		rcu_read_unlock();
+ 	}
+ out_unlock:
+ 	mutex_unlock(&wq_pool_mutex);
+diff --git a/kernel/workqueue_internal.h b/kernel/workqueue_internal.h
+index 45215870ac6c..f000c4d6917e 100644
+--- a/kernel/workqueue_internal.h
++++ b/kernel/workqueue_internal.h
+@@ -43,6 +43,7 @@ struct worker {
+ 	unsigned long		last_active;	/* L: last active timestamp */
+ 	unsigned int		flags;		/* X: flags */
+ 	int			id;		/* I: worker id */
++	int			sleeping;	/* None */
+ 
+ 	/*
+ 	 * Opaque string set with work_set_desc().  Printed out with task
+@@ -68,7 +69,7 @@ static inline struct worker *current_wq_worker(void)
+  * Scheduler hooks for concurrency managed workqueue.  Only to be used from
+  * sched/core.c and workqueue.c.
+  */
+-void wq_worker_waking_up(struct task_struct *task, int cpu);
+-struct task_struct *wq_worker_sleeping(struct task_struct *task, int cpu);
++void wq_worker_running(struct task_struct *task);
++void wq_worker_sleeping(struct task_struct *task);
+ 
+ #endif /* _KERNEL_WORKQUEUE_INTERNAL_H */
+diff --git a/lib/Kconfig b/lib/Kconfig
+index 54cf309a92a5..b06d5444fb11 100644
+--- a/lib/Kconfig
++++ b/lib/Kconfig
+@@ -383,6 +383,7 @@ config CHECK_SIGNATURE
+ 
+ config CPUMASK_OFFSTACK
+ 	bool "Force CPU masks off stack" if DEBUG_PER_CPU_MAPS
++	depends on !PREEMPT_RT_FULL
+ 	help
+ 	  Use dynamic allocation for cpumask_var_t, instead of putting
+ 	  them on the stack.  This is a bit more expensive, but avoids
+diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
+index aab836755e21..2d0f0d4352dd 100644
+--- a/lib/Kconfig.debug
++++ b/lib/Kconfig.debug
+@@ -639,7 +639,7 @@ endmenu # "Memory Debugging"
+ 
+ config DEBUG_SHIRQ
+ 	bool "Debug shared IRQ handlers"
+-	depends on DEBUG_KERNEL
++	depends on DEBUG_KERNEL && !PREEMPT_RT_BASE
+ 	help
+ 	  Enable this to generate a spurious interrupt as soon as a shared
+ 	  interrupt handler is registered, and just before one is deregistered.
+diff --git a/lib/debugobjects.c b/lib/debugobjects.c
+index 547f7f923dbc..8fcdbc2fc6d0 100644
+--- a/lib/debugobjects.c
++++ b/lib/debugobjects.c
+@@ -309,7 +309,10 @@ __debug_object_init(void *addr, struct debug_obj_descr *descr, int onstack)
+ 	struct debug_obj *obj;
+ 	unsigned long flags;
+ 
+-	fill_pool();
++#ifdef CONFIG_PREEMPT_RT_FULL
++	if (preempt_count() == 0 && !irqs_disabled())
++#endif
++		fill_pool();
+ 
+ 	db = get_bucket((unsigned long) addr);
+ 
+diff --git a/lib/dump_stack.c b/lib/dump_stack.c
+index c30d07e99dba..01ca6dae9414 100644
+--- a/lib/dump_stack.c
++++ b/lib/dump_stack.c
+@@ -25,7 +25,6 @@ static atomic_t dump_lock = ATOMIC_INIT(-1);
+ 
+ asmlinkage __visible void dump_stack(void)
+ {
+-	unsigned long flags;
+ 	int was_locked;
+ 	int old;
+ 	int cpu;
+@@ -34,8 +33,8 @@ asmlinkage __visible void dump_stack(void)
+ 	 * Permit this cpu to perform nested stack dumps while serialising
+ 	 * against other CPUs
+ 	 */
++	migrate_disable();
+ retry:
+-	local_irq_save(flags);
+ 	cpu = smp_processor_id();
+ 	old = atomic_cmpxchg(&dump_lock, -1, cpu);
+ 	if (old == -1) {
+@@ -43,7 +42,6 @@ retry:
+ 	} else if (old == cpu) {
+ 		was_locked = 1;
+ 	} else {
+-		local_irq_restore(flags);
+ 		cpu_relax();
+ 		goto retry;
+ 	}
+@@ -53,7 +51,7 @@ retry:
+ 	if (!was_locked)
+ 		atomic_set(&dump_lock, -1);
+ 
+-	local_irq_restore(flags);
++	migrate_enable();
+ }
+ #else
+ asmlinkage __visible void dump_stack(void)
+diff --git a/lib/idr.c b/lib/idr.c
+index e654aebd5f80..83758924cba7 100644
+--- a/lib/idr.c
++++ b/lib/idr.c
+@@ -31,6 +31,7 @@
+ #include <linux/spinlock.h>
+ #include <linux/percpu.h>
+ #include <linux/hardirq.h>
++#include <linux/locallock.h>
+ 
+ #define MAX_IDR_SHIFT		(sizeof(int) * 8 - 1)
+ #define MAX_IDR_BIT		(1U << MAX_IDR_SHIFT)
+@@ -367,6 +368,35 @@ static void idr_fill_slot(struct idr *idr, void *ptr, int id,
+ 	idr_mark_full(pa, id);
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++static DEFINE_LOCAL_IRQ_LOCK(idr_lock);
++
++static inline void idr_preload_lock(void)
++{
++	local_lock(idr_lock);
++}
++
++static inline void idr_preload_unlock(void)
++{
++	local_unlock(idr_lock);
++}
++
++void idr_preload_end(void)
++{
++	idr_preload_unlock();
++}
++EXPORT_SYMBOL(idr_preload_end);
++#else
++static inline void idr_preload_lock(void)
++{
++	preempt_disable();
++}
++
++static inline void idr_preload_unlock(void)
++{
++	preempt_enable();
++}
++#endif
+ 
+ /**
+  * idr_preload - preload for idr_alloc()
+@@ -402,7 +432,7 @@ void idr_preload(gfp_t gfp_mask)
+ 	WARN_ON_ONCE(in_interrupt());
+ 	might_sleep_if(gfp_mask & __GFP_WAIT);
+ 
+-	preempt_disable();
++	idr_preload_lock();
+ 
+ 	/*
+ 	 * idr_alloc() is likely to succeed w/o full idr_layer buffer and
+@@ -414,9 +444,9 @@ void idr_preload(gfp_t gfp_mask)
+ 	while (__this_cpu_read(idr_preload_cnt) < MAX_IDR_FREE) {
+ 		struct idr_layer *new;
+ 
+-		preempt_enable();
++		idr_preload_unlock();
+ 		new = kmem_cache_zalloc(idr_layer_cache, gfp_mask);
+-		preempt_disable();
++		idr_preload_lock();
+ 		if (!new)
+ 			break;
+ 
+diff --git a/lib/locking-selftest.c b/lib/locking-selftest.c
+index 872a15a2a637..b93a6103fa4d 100644
+--- a/lib/locking-selftest.c
++++ b/lib/locking-selftest.c
+@@ -590,6 +590,8 @@ GENERATE_TESTCASE(init_held_rsem)
+ #include "locking-selftest-spin-hardirq.h"
+ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_hard_spin)
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++
+ #include "locking-selftest-rlock-hardirq.h"
+ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_hard_rlock)
+ 
+@@ -605,9 +607,12 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_soft_rlock)
+ #include "locking-selftest-wlock-softirq.h"
+ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe1_soft_wlock)
+ 
++#endif
++
+ #undef E1
+ #undef E2
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ /*
+  * Enabling hardirqs with a softirq-safe lock held:
+  */
+@@ -640,6 +645,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2A_rlock)
+ #undef E1
+ #undef E2
+ 
++#endif
++
+ /*
+  * Enabling irqs with an irq-safe lock held:
+  */
+@@ -663,6 +670,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2A_rlock)
+ #include "locking-selftest-spin-hardirq.h"
+ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_hard_spin)
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++
+ #include "locking-selftest-rlock-hardirq.h"
+ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_hard_rlock)
+ 
+@@ -678,6 +687,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_rlock)
+ #include "locking-selftest-wlock-softirq.h"
+ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_wlock)
+ 
++#endif
++
+ #undef E1
+ #undef E2
+ 
+@@ -709,6 +720,8 @@ GENERATE_PERMUTATIONS_2_EVENTS(irqsafe2B_soft_wlock)
+ #include "locking-selftest-spin-hardirq.h"
+ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_hard_spin)
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++
+ #include "locking-selftest-rlock-hardirq.h"
+ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_hard_rlock)
+ 
+@@ -724,6 +737,8 @@ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_rlock)
+ #include "locking-selftest-wlock-softirq.h"
+ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_wlock)
+ 
++#endif
++
+ #undef E1
+ #undef E2
+ #undef E3
+@@ -757,6 +772,8 @@ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe3_soft_wlock)
+ #include "locking-selftest-spin-hardirq.h"
+ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_hard_spin)
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++
+ #include "locking-selftest-rlock-hardirq.h"
+ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_hard_rlock)
+ 
+@@ -772,10 +789,14 @@ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_soft_rlock)
+ #include "locking-selftest-wlock-softirq.h"
+ GENERATE_PERMUTATIONS_3_EVENTS(irqsafe4_soft_wlock)
+ 
++#endif
++
+ #undef E1
+ #undef E2
+ #undef E3
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
++
+ /*
+  * read-lock / write-lock irq inversion.
+  *
+@@ -838,6 +859,10 @@ GENERATE_PERMUTATIONS_3_EVENTS(irq_inversion_soft_wlock)
+ #undef E2
+ #undef E3
+ 
++#endif
++
++#ifndef CONFIG_PREEMPT_RT_FULL
++
+ /*
+  * read-lock / write-lock recursion that is actually safe.
+  */
+@@ -876,6 +901,8 @@ GENERATE_PERMUTATIONS_3_EVENTS(irq_read_recursion_soft)
+ #undef E2
+ #undef E3
+ 
++#endif
++
+ /*
+  * read-lock / write-lock recursion that is unsafe.
+  */
+@@ -1858,6 +1885,7 @@ void locking_selftest(void)
+ 
+ 	printk("  --------------------------------------------------------------------------\n");
+ 
++#ifndef CONFIG_PREEMPT_RT_FULL
+ 	/*
+ 	 * irq-context testcases:
+ 	 */
+@@ -1870,6 +1898,28 @@ void locking_selftest(void)
+ 
+ 	DO_TESTCASE_6x2("irq read-recursion", irq_read_recursion);
+ //	DO_TESTCASE_6x2B("irq read-recursion #2", irq_read_recursion2);
++#else
++	/* On -rt, we only do hardirq context test for raw spinlock */
++	DO_TESTCASE_1B("hard-irqs-on + irq-safe-A", irqsafe1_hard_spin, 12);
++	DO_TESTCASE_1B("hard-irqs-on + irq-safe-A", irqsafe1_hard_spin, 21);
++
++	DO_TESTCASE_1B("hard-safe-A + irqs-on", irqsafe2B_hard_spin, 12);
++	DO_TESTCASE_1B("hard-safe-A + irqs-on", irqsafe2B_hard_spin, 21);
++
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 123);
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 132);
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 213);
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 231);
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 312);
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #1", irqsafe3_hard_spin, 321);
++
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 123);
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 132);
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 213);
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 231);
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 312);
++	DO_TESTCASE_1B("hard-safe-A + unsafe-B #2", irqsafe4_hard_spin, 321);
++#endif
+ 
+ 	ww_tests();
+ 
+diff --git a/lib/percpu_ida.c b/lib/percpu_ida.c
+index 93d145e5539c..32a608ddbe3b 100644
+--- a/lib/percpu_ida.c
++++ b/lib/percpu_ida.c
+@@ -29,6 +29,9 @@
+ #include <linux/string.h>
+ #include <linux/spinlock.h>
+ #include <linux/percpu_ida.h>
++#include <linux/locallock.h>
++
++static DEFINE_LOCAL_IRQ_LOCK(irq_off_lock);
+ 
+ struct percpu_ida_cpu {
+ 	/*
+@@ -151,13 +154,13 @@ int percpu_ida_alloc(struct percpu_ida *pool, int state)
+ 	unsigned long flags;
+ 	int tag;
+ 
+-	local_irq_save(flags);
++	local_lock_irqsave(irq_off_lock, flags);
+ 	tags = this_cpu_ptr(pool->tag_cpu);
+ 
+ 	/* Fastpath */
+ 	tag = alloc_local_tag(tags);
+ 	if (likely(tag >= 0)) {
+-		local_irq_restore(flags);
++		local_unlock_irqrestore(irq_off_lock, flags);
+ 		return tag;
+ 	}
+ 
+@@ -176,6 +179,7 @@ int percpu_ida_alloc(struct percpu_ida *pool, int state)
+ 
+ 		if (!tags->nr_free)
+ 			alloc_global_tags(pool, tags);
++
+ 		if (!tags->nr_free)
+ 			steal_tags(pool, tags);
+ 
+@@ -187,7 +191,7 @@ int percpu_ida_alloc(struct percpu_ida *pool, int state)
+ 		}
+ 
+ 		spin_unlock(&pool->lock);
+-		local_irq_restore(flags);
++		local_unlock_irqrestore(irq_off_lock, flags);
+ 
+ 		if (tag >= 0 || state == TASK_RUNNING)
+ 			break;
+@@ -199,7 +203,7 @@ int percpu_ida_alloc(struct percpu_ida *pool, int state)
+ 
+ 		schedule();
+ 
+-		local_irq_save(flags);
++		local_lock_irqsave(irq_off_lock, flags);
+ 		tags = this_cpu_ptr(pool->tag_cpu);
+ 	}
+ 	if (state != TASK_RUNNING)
+@@ -224,7 +228,7 @@ void percpu_ida_free(struct percpu_ida *pool, unsigned tag)
+ 
+ 	BUG_ON(tag >= pool->nr_tags);
+ 
+-	local_irq_save(flags);
++	local_lock_irqsave(irq_off_lock, flags);
+ 	tags = this_cpu_ptr(pool->tag_cpu);
+ 
+ 	spin_lock(&tags->lock);
+@@ -256,7 +260,7 @@ void percpu_ida_free(struct percpu_ida *pool, unsigned tag)
+ 		spin_unlock(&pool->lock);
+ 	}
+ 
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(irq_off_lock, flags);
+ }
+ EXPORT_SYMBOL_GPL(percpu_ida_free);
+ 
+@@ -348,7 +352,7 @@ int percpu_ida_for_each_free(struct percpu_ida *pool, percpu_ida_cb fn,
+ 	struct percpu_ida_cpu *remote;
+ 	unsigned cpu, i, err = 0;
+ 
+-	local_irq_save(flags);
++	local_lock_irqsave(irq_off_lock, flags);
+ 	for_each_possible_cpu(cpu) {
+ 		remote = per_cpu_ptr(pool->tag_cpu, cpu);
+ 		spin_lock(&remote->lock);
+@@ -370,7 +374,7 @@ int percpu_ida_for_each_free(struct percpu_ida *pool, percpu_ida_cb fn,
+ 	}
+ 	spin_unlock(&pool->lock);
+ out:
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(irq_off_lock, flags);
+ 	return err;
+ }
+ EXPORT_SYMBOL_GPL(percpu_ida_for_each_free);
+diff --git a/lib/radix-tree.c b/lib/radix-tree.c
+index 8399002aa0f0..14192936ee69 100644
+--- a/lib/radix-tree.c
++++ b/lib/radix-tree.c
+@@ -34,7 +34,7 @@
+ #include <linux/bitops.h>
+ #include <linux/rcupdate.h>
+ #include <linux/preempt_mask.h>		/* in_interrupt() */
+-
++#include <linux/locallock.h>
+ 
+ /*
+  * The height_to_maxindex array needs to be one deeper than the maximum
+@@ -68,6 +68,7 @@ struct radix_tree_preload {
+ 	struct radix_tree_node *nodes[RADIX_TREE_PRELOAD_SIZE];
+ };
+ static DEFINE_PER_CPU(struct radix_tree_preload, radix_tree_preloads) = { 0, };
++static DEFINE_LOCAL_IRQ_LOCK(radix_tree_preloads_lock);
+ 
+ static inline void *ptr_to_indirect(void *ptr)
+ {
+@@ -195,12 +196,13 @@ radix_tree_node_alloc(struct radix_tree_root *root)
+ 		 * succeed in getting a node here (and never reach
+ 		 * kmem_cache_alloc)
+ 		 */
+-		rtp = this_cpu_ptr(&radix_tree_preloads);
++		rtp = &get_locked_var(radix_tree_preloads_lock, radix_tree_preloads);
+ 		if (rtp->nr) {
+ 			ret = rtp->nodes[rtp->nr - 1];
+ 			rtp->nodes[rtp->nr - 1] = NULL;
+ 			rtp->nr--;
+ 		}
++		put_locked_var(radix_tree_preloads_lock, radix_tree_preloads);
+ 		/*
+ 		 * Update the allocation stack trace as this is more useful
+ 		 * for debugging.
+@@ -255,14 +257,14 @@ static int __radix_tree_preload(gfp_t gfp_mask)
+ 	struct radix_tree_node *node;
+ 	int ret = -ENOMEM;
+ 
+-	preempt_disable();
++	local_lock(radix_tree_preloads_lock);
+ 	rtp = this_cpu_ptr(&radix_tree_preloads);
+ 	while (rtp->nr < ARRAY_SIZE(rtp->nodes)) {
+-		preempt_enable();
++		local_unlock(radix_tree_preloads_lock);
+ 		node = kmem_cache_alloc(radix_tree_node_cachep, gfp_mask);
+ 		if (node == NULL)
+ 			goto out;
+-		preempt_disable();
++		local_lock(radix_tree_preloads_lock);
+ 		rtp = this_cpu_ptr(&radix_tree_preloads);
+ 		if (rtp->nr < ARRAY_SIZE(rtp->nodes))
+ 			rtp->nodes[rtp->nr++] = node;
+@@ -301,11 +303,17 @@ int radix_tree_maybe_preload(gfp_t gfp_mask)
+ 	if (gfp_mask & __GFP_WAIT)
+ 		return __radix_tree_preload(gfp_mask);
+ 	/* Preloading doesn't help anything with this gfp mask, skip it */
+-	preempt_disable();
++	local_lock(radix_tree_preloads_lock);
+ 	return 0;
+ }
+ EXPORT_SYMBOL(radix_tree_maybe_preload);
+ 
++void radix_tree_preload_end(void)
++{
++	local_unlock(radix_tree_preloads_lock);
++}
++EXPORT_SYMBOL(radix_tree_preload_end);
++
+ /*
+  *	Return the maximum key which can be store into a
+  *	radix tree with height HEIGHT.
+diff --git a/lib/scatterlist.c b/lib/scatterlist.c
+index c9f2e8c6ccc9..f6d1f8899dca 100644
+--- a/lib/scatterlist.c
++++ b/lib/scatterlist.c
+@@ -592,7 +592,7 @@ void sg_miter_stop(struct sg_mapping_iter *miter)
+ 			flush_kernel_dcache_page(miter->page);
+ 
+ 		if (miter->__flags & SG_MITER_ATOMIC) {
+-			WARN_ON_ONCE(preemptible());
++			WARN_ON_ONCE(!pagefault_disabled());
+ 			kunmap_atomic(miter->addr);
+ 		} else
+ 			kunmap(miter->page);
+@@ -637,7 +637,7 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
+ 	if (!sg_miter_skip(&miter, skip))
+ 		return false;
+ 
+-	local_irq_save(flags);
++	local_irq_save_nort(flags);
+ 
+ 	while (sg_miter_next(&miter) && offset < buflen) {
+ 		unsigned int len;
+@@ -654,7 +654,7 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
+ 
+ 	sg_miter_stop(&miter);
+ 
+-	local_irq_restore(flags);
++	local_irq_restore_nort(flags);
+ 	return offset;
+ }
+ 
+diff --git a/lib/smp_processor_id.c b/lib/smp_processor_id.c
+index 1afec32de6f2..11fa431046a8 100644
+--- a/lib/smp_processor_id.c
++++ b/lib/smp_processor_id.c
+@@ -39,8 +39,9 @@ notrace static unsigned int check_preemption_disabled(const char *what1,
+ 	if (!printk_ratelimit())
+ 		goto out_enable;
+ 
+-	printk(KERN_ERR "BUG: using %s%s() in preemptible [%08x] code: %s/%d\n",
+-		what1, what2, preempt_count() - 1, current->comm, current->pid);
++	printk(KERN_ERR "BUG: using %s%s() in preemptible [%08x %08x] code: %s/%d\n",
++		what1, what2, preempt_count() - 1, __migrate_disabled(current),
++		current->comm, current->pid);
+ 
+ 	print_symbol("caller is %s\n", (long)__builtin_return_address(0));
+ 	dump_stack();
+diff --git a/localversion-rt b/localversion-rt
+new file mode 100644
+index 000000000000..54e7da6f49fb
+--- /dev/null
++++ b/localversion-rt
+@@ -0,0 +1 @@
++-rt75
+diff --git a/mm/Kconfig b/mm/Kconfig
+index 1d1ae6b078fd..296e4e048db8 100644
+--- a/mm/Kconfig
++++ b/mm/Kconfig
+@@ -408,7 +408,7 @@ config NOMMU_INITIAL_TRIM_EXCESS
+ 
+ config TRANSPARENT_HUGEPAGE
+ 	bool "Transparent Hugepage Support"
+-	depends on HAVE_ARCH_TRANSPARENT_HUGEPAGE
++	depends on HAVE_ARCH_TRANSPARENT_HUGEPAGE && !PREEMPT_RT_FULL
+ 	select COMPACTION
+ 	help
+ 	  Transparent Hugepages allows the kernel to use huge pages and
+diff --git a/mm/filemap.c b/mm/filemap.c
+index 7e6ab98d4d3c..98ed28f9f9f2 100644
+--- a/mm/filemap.c
++++ b/mm/filemap.c
+@@ -168,7 +168,9 @@ static void page_cache_tree_delete(struct address_space *mapping,
+ 	if (!workingset_node_pages(node) &&
+ 	    list_empty(&node->private_list)) {
+ 		node->private_data = mapping;
+-		list_lru_add(&workingset_shadow_nodes, &node->private_list);
++		local_lock(workingset_shadow_lock);
++		list_lru_add(&__workingset_shadow_nodes, &node->private_list);
++		local_unlock(workingset_shadow_lock);
+ 	}
+ }
+ 
+@@ -535,9 +537,12 @@ static int page_cache_tree_insert(struct address_space *mapping,
+ 		 * node->private_list is protected by
+ 		 * mapping->tree_lock.
+ 		 */
+-		if (!list_empty(&node->private_list))
+-			list_lru_del(&workingset_shadow_nodes,
++		if (!list_empty(&node->private_list)) {
++			local_lock(workingset_shadow_lock);
++			list_lru_del(&__workingset_shadow_nodes,
+ 				     &node->private_list);
++			local_unlock(workingset_shadow_lock);
++		}
+ 	}
+ 	return 0;
+ }
+diff --git a/mm/highmem.c b/mm/highmem.c
+index 123bcd3ed4f2..16e8cf26d38a 100644
+--- a/mm/highmem.c
++++ b/mm/highmem.c
+@@ -29,10 +29,11 @@
+ #include <linux/kgdb.h>
+ #include <asm/tlbflush.h>
+ 
+-
++#ifndef CONFIG_PREEMPT_RT_FULL
+ #if defined(CONFIG_HIGHMEM) || defined(CONFIG_X86_32)
+ DEFINE_PER_CPU(int, __kmap_atomic_idx);
+ #endif
++#endif
+ 
+ /*
+  * Virtual_count is not a pure "count".
+@@ -107,8 +108,9 @@ static inline wait_queue_head_t *get_pkmap_wait_queue_head(unsigned int color)
+ unsigned long totalhigh_pages __read_mostly;
+ EXPORT_SYMBOL(totalhigh_pages);
+ 
+-
++#ifndef CONFIG_PREEMPT_RT_FULL
+ EXPORT_PER_CPU_SYMBOL(__kmap_atomic_idx);
++#endif
+ 
+ unsigned int nr_free_highpages (void)
+ {
+diff --git a/mm/memcontrol.c b/mm/memcontrol.c
+index 0afd8444a8d6..30037f330bf2 100644
+--- a/mm/memcontrol.c
++++ b/mm/memcontrol.c
+@@ -60,6 +60,8 @@
+ #include <net/sock.h>
+ #include <net/ip.h>
+ #include <net/tcp_memcontrol.h>
++#include <linux/locallock.h>
++
+ #include "slab.h"
+ 
+ #include <asm/uaccess.h>
+@@ -87,6 +89,7 @@ static int really_do_swap_account __initdata;
+ #define do_swap_account		0
+ #endif
+ 
++static DEFINE_LOCAL_IRQ_LOCK(event_lock);
+ 
+ static const char * const mem_cgroup_stat_names[] = {
+ 	"cache",
+@@ -2376,14 +2379,17 @@ static void __init memcg_stock_init(void)
+  */
+ static void refill_stock(struct mem_cgroup *memcg, unsigned int nr_pages)
+ {
+-	struct memcg_stock_pcp *stock = &get_cpu_var(memcg_stock);
++	struct memcg_stock_pcp *stock;
++	int cpu = get_cpu_light();
++
++	stock = &per_cpu(memcg_stock, cpu);
+ 
+ 	if (stock->cached != memcg) { /* reset if necessary */
+ 		drain_stock(stock);
+ 		stock->cached = memcg;
+ 	}
+ 	stock->nr_pages += nr_pages;
+-	put_cpu_var(memcg_stock);
++	put_cpu_light();
+ }
+ 
+ /*
+@@ -2397,7 +2403,7 @@ static void drain_all_stock(struct mem_cgroup *root_memcg, bool sync)
+ 
+ 	/* Notify other cpus that system-wide "drain" is running */
+ 	get_online_cpus();
+-	curcpu = get_cpu();
++	curcpu = get_cpu_light();
+ 	for_each_online_cpu(cpu) {
+ 		struct memcg_stock_pcp *stock = &per_cpu(memcg_stock, cpu);
+ 		struct mem_cgroup *memcg;
+@@ -2414,7 +2420,7 @@ static void drain_all_stock(struct mem_cgroup *root_memcg, bool sync)
+ 				schedule_work_on(cpu, &stock->work);
+ 		}
+ 	}
+-	put_cpu();
++	put_cpu_light();
+ 
+ 	if (!sync)
+ 		goto out;
+@@ -3419,12 +3425,12 @@ static int mem_cgroup_move_account(struct page *page,
+ 	move_unlock_mem_cgroup(from, &flags);
+ 	ret = 0;
+ 
+-	local_irq_disable();
++	local_lock_irq(event_lock);
+ 	mem_cgroup_charge_statistics(to, page, nr_pages);
+ 	memcg_check_events(to, page);
+ 	mem_cgroup_charge_statistics(from, page, -nr_pages);
+ 	memcg_check_events(from, page);
+-	local_irq_enable();
++	local_unlock_irq(event_lock);
+ out_unlock:
+ 	unlock_page(page);
+ out:
+@@ -6407,10 +6413,10 @@ void mem_cgroup_commit_charge(struct page *page, struct mem_cgroup *memcg,
+ 		VM_BUG_ON_PAGE(!PageTransHuge(page), page);
+ 	}
+ 
+-	local_irq_disable();
++	local_lock_irq(event_lock);
+ 	mem_cgroup_charge_statistics(memcg, page, nr_pages);
+ 	memcg_check_events(memcg, page);
+-	local_irq_enable();
++	local_unlock_irq(event_lock);
+ 
+ 	if (do_swap_account && PageSwapCache(page)) {
+ 		swp_entry_t entry = { .val = page_private(page) };
+@@ -6469,14 +6475,14 @@ static void uncharge_batch(struct mem_cgroup *memcg, unsigned long pgpgout,
+ 		memcg_oom_recover(memcg);
+ 	}
+ 
+-	local_irq_save(flags);
++	local_lock_irqsave(event_lock, flags);
+ 	__this_cpu_sub(memcg->stat->count[MEM_CGROUP_STAT_RSS], nr_anon);
+ 	__this_cpu_sub(memcg->stat->count[MEM_CGROUP_STAT_CACHE], nr_file);
+ 	__this_cpu_sub(memcg->stat->count[MEM_CGROUP_STAT_RSS_HUGE], nr_huge);
+ 	__this_cpu_add(memcg->stat->events[MEM_CGROUP_EVENTS_PGPGOUT], pgpgout);
+ 	__this_cpu_add(memcg->stat->nr_page_events, nr_anon + nr_file);
+ 	memcg_check_events(memcg, dummy_page);
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(event_lock, flags);
+ }
+ 
+ static void uncharge_list(struct list_head *page_list)
+diff --git a/mm/memory.c b/mm/memory.c
+index 0c4f5e36b155..65bab76f35c2 100644
+--- a/mm/memory.c
++++ b/mm/memory.c
+@@ -3211,6 +3211,32 @@ unlock:
+ 	return 0;
+ }
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++void pagefault_disable(void)
++{
++	migrate_disable();
++	current->pagefault_disabled++;
++	/*
++	 * make sure to have issued the store before a pagefault
++	 * can hit.
++	 */
++	barrier();
++}
++EXPORT_SYMBOL(pagefault_disable);
++
++void pagefault_enable(void)
++{
++	/*
++	 * make sure to issue those last loads/stores before enabling
++	 * the pagefault handler again.
++	 */
++	barrier();
++	current->pagefault_disabled--;
++	migrate_enable();
++}
++EXPORT_SYMBOL(pagefault_enable);
++#endif
++
+ /*
+  * By the time we get here, we already hold the mm semaphore
+  *
+diff --git a/mm/mmu_context.c b/mm/mmu_context.c
+index f802c2d216a7..b1b6f238e42d 100644
+--- a/mm/mmu_context.c
++++ b/mm/mmu_context.c
+@@ -23,6 +23,7 @@ void use_mm(struct mm_struct *mm)
+ 	struct task_struct *tsk = current;
+ 
+ 	task_lock(tsk);
++	preempt_disable_rt();
+ 	active_mm = tsk->active_mm;
+ 	if (active_mm != mm) {
+ 		atomic_inc(&mm->mm_count);
+@@ -30,6 +31,7 @@ void use_mm(struct mm_struct *mm)
+ 	}
+ 	tsk->mm = mm;
+ 	switch_mm(active_mm, mm, tsk);
++	preempt_enable_rt();
+ 	task_unlock(tsk);
+ #ifdef finish_arch_post_lock_switch
+ 	finish_arch_post_lock_switch();
+diff --git a/mm/page_alloc.c b/mm/page_alloc.c
+index fcd8a8ce5cc0..24d4a1b28a18 100644
+--- a/mm/page_alloc.c
++++ b/mm/page_alloc.c
+@@ -59,6 +59,7 @@
+ #include <linux/page-debug-flags.h>
+ #include <linux/hugetlb.h>
+ #include <linux/sched/rt.h>
++#include <linux/locallock.h>
+ 
+ #include <asm/sections.h>
+ #include <asm/tlbflush.h>
+@@ -231,6 +232,18 @@ EXPORT_SYMBOL(nr_node_ids);
+ EXPORT_SYMBOL(nr_online_nodes);
+ #endif
+ 
++static DEFINE_LOCAL_IRQ_LOCK(pa_lock);
++
++#ifdef CONFIG_PREEMPT_RT_BASE
++# define cpu_lock_irqsave(cpu, flags)		\
++	local_lock_irqsave_on(pa_lock, flags, cpu)
++# define cpu_unlock_irqrestore(cpu, flags)	\
++	local_unlock_irqrestore_on(pa_lock, flags, cpu)
++#else
++# define cpu_lock_irqsave(cpu, flags)		local_irq_save(flags)
++# define cpu_unlock_irqrestore(cpu, flags)	local_irq_restore(flags)
++#endif
++
+ int page_group_by_mobility_disabled __read_mostly;
+ 
+ void set_pageblock_migratetype(struct page *page, int migratetype)
+@@ -675,7 +688,7 @@ static inline int free_pages_check(struct page *page)
+ }
+ 
+ /*
+- * Frees a number of pages from the PCP lists
++ * Frees a number of pages which have been collected from the pcp lists.
+  * Assumes all pages on list are in same zone, and of same order.
+  * count is the number of pages to free.
+  *
+@@ -686,18 +699,51 @@ static inline int free_pages_check(struct page *page)
+  * pinned" detection logic.
+  */
+ static void free_pcppages_bulk(struct zone *zone, int count,
+-					struct per_cpu_pages *pcp)
++			       struct list_head *list)
+ {
+-	int migratetype = 0;
+-	int batch_free = 0;
+ 	int to_free = count;
+ 	unsigned long nr_scanned;
++	unsigned long flags;
++
++	spin_lock_irqsave(&zone->lock, flags);
+ 
+-	spin_lock(&zone->lock);
+ 	nr_scanned = zone_page_state(zone, NR_PAGES_SCANNED);
+ 	if (nr_scanned)
+ 		__mod_zone_page_state(zone, NR_PAGES_SCANNED, -nr_scanned);
+ 
++	while (!list_empty(list)) {
++		struct page *page = list_first_entry(list, struct page, lru);
++		int mt;	/* migratetype of the to-be-freed page */
++
++		/* must delete as __free_one_page list manipulates */
++		list_del(&page->lru);
++
++		mt = get_freepage_migratetype(page);
++		if (unlikely(has_isolate_pageblock(zone)))
++			mt = get_pageblock_migratetype(page);
++
++		/* MIGRATE_MOVABLE list may include MIGRATE_RESERVEs */
++		__free_one_page(page, page_to_pfn(page), zone, 0, mt);
++		trace_mm_page_pcpu_drain(page, 0, mt);
++		to_free--;
++	}
++	WARN_ON(to_free != 0);
++	spin_unlock_irqrestore(&zone->lock, flags);
++}
++
++/*
++ * Moves a number of pages from the PCP lists to free list which
++ * is freed outside of the locked region.
++ *
++ * Assumes all pages on list are in same zone, and of same order.
++ * count is the number of pages to free.
++ */
++static void isolate_pcp_pages(int to_free, struct per_cpu_pages *src,
++			      struct list_head *dst)
++{
++	int migratetype = 0;
++	int batch_free = 0;
++
+ 	while (to_free) {
+ 		struct page *page;
+ 		struct list_head *list;
+@@ -713,7 +759,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,
+ 			batch_free++;
+ 			if (++migratetype == MIGRATE_PCPTYPES)
+ 				migratetype = 0;
+-			list = &pcp->lists[migratetype];
++			list = &src->lists[migratetype];
+ 		} while (list_empty(list));
+ 
+ 		/* This is the only non-empty list. Free them all. */
+@@ -721,21 +767,11 @@ static void free_pcppages_bulk(struct zone *zone, int count,
+ 			batch_free = to_free;
+ 
+ 		do {
+-			int mt;	/* migratetype of the to-be-freed page */
+-
+-			page = list_entry(list->prev, struct page, lru);
+-			/* must delete as __free_one_page list manipulates */
++			page = list_last_entry(list, struct page, lru);
+ 			list_del(&page->lru);
+-			mt = get_freepage_migratetype(page);
+-			if (unlikely(has_isolate_pageblock(zone)))
+-				mt = get_pageblock_migratetype(page);
+-
+-			/* MIGRATE_MOVABLE list may include MIGRATE_RESERVEs */
+-			__free_one_page(page, page_to_pfn(page), zone, 0, mt);
+-			trace_mm_page_pcpu_drain(page, 0, mt);
++			list_add(&page->lru, dst);
+ 		} while (--to_free && --batch_free && !list_empty(list));
+ 	}
+-	spin_unlock(&zone->lock);
+ }
+ 
+ static void free_one_page(struct zone *zone,
+@@ -744,7 +780,9 @@ static void free_one_page(struct zone *zone,
+ 				int migratetype)
+ {
+ 	unsigned long nr_scanned;
+-	spin_lock(&zone->lock);
++	unsigned long flags;
++
++	spin_lock_irqsave(&zone->lock, flags);
+ 	nr_scanned = zone_page_state(zone, NR_PAGES_SCANNED);
+ 	if (nr_scanned)
+ 		__mod_zone_page_state(zone, NR_PAGES_SCANNED, -nr_scanned);
+@@ -754,7 +792,7 @@ static void free_one_page(struct zone *zone,
+ 		migratetype = get_pfnblock_migratetype(page, pfn);
+ 	}
+ 	__free_one_page(page, pfn, zone, order, migratetype);
+-	spin_unlock(&zone->lock);
++	spin_unlock_irqrestore(&zone->lock, flags);
+ }
+ 
+ static bool free_pages_prepare(struct page *page, unsigned int order)
+@@ -794,11 +832,11 @@ static void __free_pages_ok(struct page *page, unsigned int order)
+ 		return;
+ 
+ 	migratetype = get_pfnblock_migratetype(page, pfn);
+-	local_irq_save(flags);
++	local_lock_irqsave(pa_lock, flags);
+ 	__count_vm_events(PGFREE, 1 << order);
+ 	set_freepage_migratetype(page, migratetype);
+ 	free_one_page(page_zone(page), page, pfn, order, migratetype);
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(pa_lock, flags);
+ }
+ 
+ void __init __free_pages_bootmem(struct page *page, unsigned long pfn,
+@@ -1273,16 +1311,18 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,
+ void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp)
+ {
+ 	unsigned long flags;
++	LIST_HEAD(dst);
+ 	int to_drain, batch;
+ 
+-	local_irq_save(flags);
++	local_lock_irqsave(pa_lock, flags);
+ 	batch = ACCESS_ONCE(pcp->batch);
+ 	to_drain = min(pcp->count, batch);
+ 	if (to_drain > 0) {
+-		free_pcppages_bulk(zone, to_drain, pcp);
++		isolate_pcp_pages(to_drain, pcp, &dst);
+ 		pcp->count -= to_drain;
+ 	}
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(pa_lock, flags);
++	free_pcppages_bulk(zone, to_drain, &dst);
+ }
+ #endif
+ 
+@@ -1301,16 +1341,21 @@ static void drain_pages(unsigned int cpu)
+ 	for_each_populated_zone(zone) {
+ 		struct per_cpu_pageset *pset;
+ 		struct per_cpu_pages *pcp;
++		LIST_HEAD(dst);
++		int count;
+ 
+-		local_irq_save(flags);
++		cpu_lock_irqsave(cpu, flags);
+ 		pset = per_cpu_ptr(zone->pageset, cpu);
+ 
+ 		pcp = &pset->pcp;
+-		if (pcp->count) {
+-			free_pcppages_bulk(zone, pcp->count, pcp);
++		count = pcp->count;
++		if (count) {
++			isolate_pcp_pages(count, pcp, &dst);
+ 			pcp->count = 0;
+ 		}
+-		local_irq_restore(flags);
++		cpu_unlock_irqrestore(cpu, flags);
++		if (count)
++			free_pcppages_bulk(zone, count, &dst);
+ 	}
+ }
+ 
+@@ -1363,7 +1408,12 @@ void drain_all_pages(void)
+ 		else
+ 			cpumask_clear_cpu(cpu, &cpus_with_pcps);
+ 	}
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	on_each_cpu_mask(&cpus_with_pcps, drain_local_pages, NULL, 1);
++#else
++	for_each_cpu(cpu, &cpus_with_pcps)
++		drain_pages(cpu);
++#endif
+ }
+ 
+ #ifdef CONFIG_HIBERNATION
+@@ -1419,7 +1469,7 @@ void free_hot_cold_page(struct page *page, bool cold)
+ 
+ 	migratetype = get_pfnblock_migratetype(page, pfn);
+ 	set_freepage_migratetype(page, migratetype);
+-	local_irq_save(flags);
++	local_lock_irqsave(pa_lock, flags);
+ 	__count_vm_event(PGFREE);
+ 
+ 	/*
+@@ -1445,12 +1495,17 @@ void free_hot_cold_page(struct page *page, bool cold)
+ 	pcp->count++;
+ 	if (pcp->count >= pcp->high) {
+ 		unsigned long batch = ACCESS_ONCE(pcp->batch);
+-		free_pcppages_bulk(zone, batch, pcp);
++		LIST_HEAD(dst);
++
++		isolate_pcp_pages(batch, pcp, &dst);
+ 		pcp->count -= batch;
++		local_unlock_irqrestore(pa_lock, flags);
++		free_pcppages_bulk(zone, batch, &dst);
++		return;
+ 	}
+ 
+ out:
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(pa_lock, flags);
+ }
+ 
+ /*
+@@ -1580,7 +1635,7 @@ again:
+ 		struct per_cpu_pages *pcp;
+ 		struct list_head *list;
+ 
+-		local_irq_save(flags);
++		local_lock_irqsave(pa_lock, flags);
+ 		pcp = &this_cpu_ptr(zone->pageset)->pcp;
+ 		list = &pcp->lists[migratetype];
+ 		if (list_empty(list)) {
+@@ -1612,13 +1667,15 @@ again:
+ 			 */
+ 			WARN_ON_ONCE(order > 1);
+ 		}
+-		spin_lock_irqsave(&zone->lock, flags);
++		local_spin_lock_irqsave(pa_lock, &zone->lock, flags);
+ 		page = __rmqueue(zone, order, migratetype);
+-		spin_unlock(&zone->lock);
+-		if (!page)
++		if (!page) {
++			spin_unlock(&zone->lock);
+ 			goto failed;
++		}
+ 		__mod_zone_freepage_state(zone, -(1 << order),
+ 					  get_freepage_migratetype(page));
++		spin_unlock(&zone->lock);
+ 	}
+ 
+ 	__mod_zone_page_state(zone, NR_ALLOC_BATCH, -(1 << order));
+@@ -1628,7 +1685,7 @@ again:
+ 
+ 	__count_zone_vm_events(PGALLOC, zone, 1 << order);
+ 	zone_statistics(preferred_zone, zone, gfp_flags);
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(pa_lock, flags);
+ 
+ 	VM_BUG_ON_PAGE(bad_range(zone, page), page);
+ 	if (prep_new_page(page, order, gfp_flags))
+@@ -1636,7 +1693,7 @@ again:
+ 	return page;
+ 
+ failed:
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(pa_lock, flags);
+ 	return NULL;
+ }
+ 
+@@ -2347,8 +2404,8 @@ __alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,
+ 	count_vm_event(COMPACTSTALL);
+ 
+ 	/* Page migration frees to the PCP lists but we want merging */
+-	drain_pages(get_cpu());
+-	put_cpu();
++	drain_pages(get_cpu_light());
++	put_cpu_light();
+ 
+ 	page = get_page_from_freelist(gfp_mask, nodemask,
+ 			order, zonelist, high_zoneidx,
+@@ -5593,6 +5650,7 @@ static int page_alloc_cpu_notify(struct notifier_block *self,
+ void __init page_alloc_init(void)
+ {
+ 	hotcpu_notifier(page_alloc_cpu_notify, 0);
++	local_irq_lock_init(pa_lock);
+ }
+ 
+ /*
+@@ -6488,7 +6546,7 @@ void zone_pcp_reset(struct zone *zone)
+ 	struct per_cpu_pageset *pset;
+ 
+ 	/* avoid races with drain_pages()  */
+-	local_irq_save(flags);
++	local_lock_irqsave(pa_lock, flags);
+ 	if (zone->pageset != &boot_pageset) {
+ 		for_each_online_cpu(cpu) {
+ 			pset = per_cpu_ptr(zone->pageset, cpu);
+@@ -6497,7 +6555,7 @@ void zone_pcp_reset(struct zone *zone)
+ 		free_percpu(zone->pageset);
+ 		zone->pageset = &boot_pageset;
+ 	}
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(pa_lock, flags);
+ }
+ 
+ #ifdef CONFIG_MEMORY_HOTREMOVE
+diff --git a/mm/percpu.c b/mm/percpu.c
+index f7da3a3070c4..809b1547d35d 100644
+--- a/mm/percpu.c
++++ b/mm/percpu.c
+@@ -1285,18 +1285,7 @@ void free_percpu(void __percpu *ptr)
+ }
+ EXPORT_SYMBOL_GPL(free_percpu);
+ 
+-/**
+- * is_kernel_percpu_address - test whether address is from static percpu area
+- * @addr: address to test
+- *
+- * Test whether @addr belongs to in-kernel static percpu area.  Module
+- * static percpu areas are not considered.  For those, use
+- * is_module_percpu_address().
+- *
+- * RETURNS:
+- * %true if @addr is from in-kernel static percpu area, %false otherwise.
+- */
+-bool is_kernel_percpu_address(unsigned long addr)
++bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr)
+ {
+ #ifdef CONFIG_SMP
+ 	const size_t static_size = __per_cpu_end - __per_cpu_start;
+@@ -1305,16 +1294,36 @@ bool is_kernel_percpu_address(unsigned long addr)
+ 
+ 	for_each_possible_cpu(cpu) {
+ 		void *start = per_cpu_ptr(base, cpu);
++		void *va = (void *)addr;
+ 
+-		if ((void *)addr >= start && (void *)addr < start + static_size)
++		if (va >= start && va < start + static_size) {
++			if (can_addr)
++				*can_addr = (unsigned long) (va - start);
+ 			return true;
+-        }
++		}
++	}
+ #endif
+ 	/* on UP, can't distinguish from other static vars, always false */
+ 	return false;
+ }
+ 
+ /**
++ * is_kernel_percpu_address - test whether address is from static percpu area
++ * @addr: address to test
++ *
++ * Test whether @addr belongs to in-kernel static percpu area.  Module
++ * static percpu areas are not considered.  For those, use
++ * is_module_percpu_address().
++ *
++ * RETURNS:
++ * %true if @addr is from in-kernel static percpu area, %false otherwise.
++ */
++bool is_kernel_percpu_address(unsigned long addr)
++{
++	return __is_kernel_percpu_address(addr, NULL);
++}
++
++/**
+  * per_cpu_ptr_to_phys - convert translated percpu address to physical address
+  * @addr: the address to be converted to physical address
+  *
+diff --git a/mm/slab.h b/mm/slab.h
+index ab019e63e3c2..c5d1c194f2ca 100644
+--- a/mm/slab.h
++++ b/mm/slab.h
+@@ -315,7 +315,11 @@ static inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)
+  * The slab lists for all objects.
+  */
+ struct kmem_cache_node {
++#ifdef CONFIG_SLUB
++	raw_spinlock_t list_lock;
++#else
+ 	spinlock_t list_lock;
++#endif
+ 
+ #ifdef CONFIG_SLAB
+ 	struct list_head slabs_partial;	/* partial list first, better asm code */
+diff --git a/mm/slub.c b/mm/slub.c
+index 332ce18463cb..a38d7608baee 100644
+--- a/mm/slub.c
++++ b/mm/slub.c
+@@ -1044,7 +1044,7 @@ static noinline struct kmem_cache_node *free_debug_processing(
+ {
+ 	struct kmem_cache_node *n = get_node(s, page_to_nid(page));
+ 
+-	spin_lock_irqsave(&n->list_lock, *flags);
++	raw_spin_lock_irqsave(&n->list_lock, *flags);
+ 	slab_lock(page);
+ 
+ 	if (!check_slab(s, page))
+@@ -1091,7 +1091,7 @@ out:
+ 
+ fail:
+ 	slab_unlock(page);
+-	spin_unlock_irqrestore(&n->list_lock, *flags);
++	raw_spin_unlock_irqrestore(&n->list_lock, *flags);
+ 	slab_fix(s, "Object at 0x%p not freed", object);
+ 	return NULL;
+ }
+@@ -1219,6 +1219,12 @@ static inline void dec_slabs_node(struct kmem_cache *s, int node,
+ 
+ #endif /* CONFIG_SLUB_DEBUG */
+ 
++struct slub_free_list {
++	raw_spinlock_t		lock;
++	struct list_head	list;
++};
++static DEFINE_PER_CPU(struct slub_free_list, slub_free_list);
++
+ /*
+  * Hooks for other subsystems that check memory allocations. In a typical
+  * production configuration these hooks all should produce no code at all.
+@@ -1273,6 +1279,13 @@ static inline void slab_free_hook(struct kmem_cache *s, void *x)
+ 		debug_check_no_obj_freed(x, s->object_size);
+ }
+ 
++static void setup_object(struct kmem_cache *s, struct page *page,
++				void *object)
++{
++	setup_object_debug(s, page, object);
++	if (unlikely(s->ctor))
++		s->ctor(object);
++}
+ /*
+  * Slab allocation and freeing
+  */
+@@ -1303,10 +1316,17 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
+ 	struct page *page;
+ 	struct kmem_cache_order_objects oo = s->oo;
+ 	gfp_t alloc_gfp;
++	bool enableirqs;
++	void *start, *p;
++	int idx, order;
+ 
+ 	flags &= gfp_allowed_mask;
+ 
+-	if (flags & __GFP_WAIT)
++	enableirqs = (flags & __GFP_WAIT) != 0;
++#ifdef CONFIG_PREEMPT_RT_FULL
++	enableirqs |= system_state == SYSTEM_RUNNING;
++#endif
++	if (enableirqs)
+ 		local_irq_enable();
+ 
+ 	flags |= s->allocflags;
+@@ -1326,13 +1346,13 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
+ 		 * Try a lower order alloc if possible
+ 		 */
+ 		page = alloc_slab_page(s, alloc_gfp, node, oo);
+-
+-		if (page)
+-			stat(s, ORDER_FALLBACK);
++		if (unlikely(!page))
++			goto out;
++		stat(s, ORDER_FALLBACK);
+ 	}
+ 
+-	if (kmemcheck_enabled && page
+-		&& !(s->flags & (SLAB_NOTRACK | DEBUG_DEFAULT_FLAGS))) {
++	if (kmemcheck_enabled &&
++	    !(s->flags & (SLAB_NOTRACK | DEBUG_DEFAULT_FLAGS))) {
+ 		int pages = 1 << oo_order(oo);
+ 
+ 		kmemcheck_alloc_shadow(page, oo_order(oo), alloc_gfp, node);
+@@ -1347,45 +1367,9 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
+ 			kmemcheck_mark_unallocated_pages(page, pages);
+ 	}
+ 
+-	if (flags & __GFP_WAIT)
+-		local_irq_disable();
+-	if (!page)
+-		return NULL;
+-
+ 	page->objects = oo_objects(oo);
+-	mod_zone_page_state(page_zone(page),
+-		(s->flags & SLAB_RECLAIM_ACCOUNT) ?
+-		NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
+-		1 << oo_order(oo));
+-
+-	return page;
+-}
+-
+-static void setup_object(struct kmem_cache *s, struct page *page,
+-				void *object)
+-{
+-	setup_object_debug(s, page, object);
+-	if (unlikely(s->ctor))
+-		s->ctor(object);
+-}
+-
+-static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
+-{
+-	struct page *page;
+-	void *start;
+-	void *p;
+-	int order;
+-	int idx;
+-
+-	BUG_ON(flags & GFP_SLAB_BUG_MASK);
+-
+-	page = allocate_slab(s,
+-		flags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node);
+-	if (!page)
+-		goto out;
+ 
+ 	order = compound_order(page);
+-	inc_slabs_node(s, page_to_nid(page), page->objects);
+ 	page->slab_cache = s;
+ 	__SetPageSlab(page);
+ 	if (page->pfmemalloc)
+@@ -1407,10 +1391,34 @@ static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
+ 	page->freelist = start;
+ 	page->inuse = page->objects;
+ 	page->frozen = 1;
++
+ out:
++	if (enableirqs)
++		local_irq_disable();
++	if (!page)
++		return NULL;
++
++	mod_zone_page_state(page_zone(page),
++		(s->flags & SLAB_RECLAIM_ACCOUNT) ?
++		NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
++		1 << oo_order(oo));
++
++	inc_slabs_node(s, page_to_nid(page), page->objects);
++
+ 	return page;
+ }
+ 
++static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
++{
++	if (unlikely(flags & GFP_SLAB_BUG_MASK)) {
++		pr_emerg("gfp: %u\n", flags & GFP_SLAB_BUG_MASK);
++		BUG();
++	}
++
++	return allocate_slab(s,
++		flags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node);
++}
++
+ static void __free_slab(struct kmem_cache *s, struct page *page)
+ {
+ 	int order = compound_order(page);
+@@ -1442,6 +1450,16 @@ static void __free_slab(struct kmem_cache *s, struct page *page)
+ 	memcg_uncharge_slab(s, order);
+ }
+ 
++static void free_delayed(struct list_head *h)
++{
++	while(!list_empty(h)) {
++		struct page *page = list_first_entry(h, struct page, lru);
++
++		list_del(&page->lru);
++		__free_slab(page->slab_cache, page);
++	}
++}
++
+ #define need_reserve_slab_rcu						\
+ 	(sizeof(((struct page *)NULL)->lru) < sizeof(struct rcu_head))
+ 
+@@ -1476,6 +1494,12 @@ static void free_slab(struct kmem_cache *s, struct page *page)
+ 		}
+ 
+ 		call_rcu(head, rcu_free_slab);
++	} else if (irqs_disabled()) {
++		struct slub_free_list *f = &__get_cpu_var(slub_free_list);
++
++		raw_spin_lock(&f->lock);
++		list_add(&page->lru, &f->list);
++		raw_spin_unlock(&f->lock);
+ 	} else
+ 		__free_slab(s, page);
+ }
+@@ -1589,7 +1613,7 @@ static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
+ 	if (!n || !n->nr_partial)
+ 		return NULL;
+ 
+-	spin_lock(&n->list_lock);
++	raw_spin_lock(&n->list_lock);
+ 	list_for_each_entry_safe(page, page2, &n->partial, lru) {
+ 		void *t;
+ 
+@@ -1614,7 +1638,7 @@ static void *get_partial_node(struct kmem_cache *s, struct kmem_cache_node *n,
+ 			break;
+ 
+ 	}
+-	spin_unlock(&n->list_lock);
++	raw_spin_unlock(&n->list_lock);
+ 	return object;
+ }
+ 
+@@ -1860,7 +1884,7 @@ redo:
+ 			 * that acquire_slab() will see a slab page that
+ 			 * is frozen
+ 			 */
+-			spin_lock(&n->list_lock);
++			raw_spin_lock(&n->list_lock);
+ 		}
+ 	} else {
+ 		m = M_FULL;
+@@ -1871,7 +1895,7 @@ redo:
+ 			 * slabs from diagnostic functions will not see
+ 			 * any frozen slabs.
+ 			 */
+-			spin_lock(&n->list_lock);
++			raw_spin_lock(&n->list_lock);
+ 		}
+ 	}
+ 
+@@ -1906,7 +1930,7 @@ redo:
+ 		goto redo;
+ 
+ 	if (lock)
+-		spin_unlock(&n->list_lock);
++		raw_spin_unlock(&n->list_lock);
+ 
+ 	if (m == M_FREE) {
+ 		stat(s, DEACTIVATE_EMPTY);
+@@ -1938,10 +1962,10 @@ static void unfreeze_partials(struct kmem_cache *s,
+ 		n2 = get_node(s, page_to_nid(page));
+ 		if (n != n2) {
+ 			if (n)
+-				spin_unlock(&n->list_lock);
++				raw_spin_unlock(&n->list_lock);
+ 
+ 			n = n2;
+-			spin_lock(&n->list_lock);
++			raw_spin_lock(&n->list_lock);
+ 		}
+ 
+ 		do {
+@@ -1970,7 +1994,7 @@ static void unfreeze_partials(struct kmem_cache *s,
+ 	}
+ 
+ 	if (n)
+-		spin_unlock(&n->list_lock);
++		raw_spin_unlock(&n->list_lock);
+ 
+ 	while (discard_page) {
+ 		page = discard_page;
+@@ -2008,14 +2032,21 @@ static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
+ 			pobjects = oldpage->pobjects;
+ 			pages = oldpage->pages;
+ 			if (drain && pobjects > s->cpu_partial) {
++				struct slub_free_list *f;
+ 				unsigned long flags;
++				LIST_HEAD(tofree);
+ 				/*
+ 				 * partial array is full. Move the existing
+ 				 * set to the per node partial list.
+ 				 */
+ 				local_irq_save(flags);
+ 				unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
++				f = &__get_cpu_var(slub_free_list);
++				raw_spin_lock(&f->lock);
++				list_splice_init(&f->list, &tofree);
++				raw_spin_unlock(&f->lock);
+ 				local_irq_restore(flags);
++				free_delayed(&tofree);
+ 				oldpage = NULL;
+ 				pobjects = 0;
+ 				pages = 0;
+@@ -2079,7 +2110,22 @@ static bool has_cpu_slab(int cpu, void *info)
+ 
+ static void flush_all(struct kmem_cache *s)
+ {
++	LIST_HEAD(tofree);
++	int cpu;
++
+ 	on_each_cpu_cond(has_cpu_slab, flush_cpu_slab, s, 1, GFP_ATOMIC);
++	for_each_online_cpu(cpu) {
++		struct slub_free_list *f;
++
++		if (!has_cpu_slab(cpu, s))
++			continue;
++
++		f = &per_cpu(slub_free_list, cpu);
++		raw_spin_lock_irq(&f->lock);
++		list_splice_init(&f->list, &tofree);
++		raw_spin_unlock_irq(&f->lock);
++		free_delayed(&tofree);
++	}
+ }
+ 
+ /*
+@@ -2115,10 +2161,10 @@ static unsigned long count_partial(struct kmem_cache_node *n,
+ 	unsigned long x = 0;
+ 	struct page *page;
+ 
+-	spin_lock_irqsave(&n->list_lock, flags);
++	raw_spin_lock_irqsave(&n->list_lock, flags);
+ 	list_for_each_entry(page, &n->partial, lru)
+ 		x += get_count(page);
+-	spin_unlock_irqrestore(&n->list_lock, flags);
++	raw_spin_unlock_irqrestore(&n->list_lock, flags);
+ 	return x;
+ }
+ #endif /* CONFIG_SLUB_DEBUG || CONFIG_SYSFS */
+@@ -2255,9 +2301,11 @@ static inline void *get_freelist(struct kmem_cache *s, struct page *page)
+ static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
+ 			  unsigned long addr, struct kmem_cache_cpu *c)
+ {
++	struct slub_free_list *f;
+ 	void *freelist;
+ 	struct page *page;
+ 	unsigned long flags;
++	LIST_HEAD(tofree);
+ 
+ 	local_irq_save(flags);
+ #ifdef CONFIG_PREEMPT
+@@ -2325,7 +2373,13 @@ load_freelist:
+ 	VM_BUG_ON(!c->page->frozen);
+ 	c->freelist = get_freepointer(s, freelist);
+ 	c->tid = next_tid(c->tid);
++out:
++	f = &__get_cpu_var(slub_free_list);
++	raw_spin_lock(&f->lock);
++	list_splice_init(&f->list, &tofree);
++	raw_spin_unlock(&f->lock);
+ 	local_irq_restore(flags);
++	free_delayed(&tofree);
+ 	return freelist;
+ 
+ new_slab:
+@@ -2342,8 +2396,7 @@ new_slab:
+ 
+ 	if (unlikely(!freelist)) {
+ 		slab_out_of_memory(s, gfpflags, node);
+-		local_irq_restore(flags);
+-		return NULL;
++		goto out;
+ 	}
+ 
+ 	page = c->page;
+@@ -2358,8 +2411,7 @@ new_slab:
+ 	deactivate_slab(s, page, get_freepointer(s, freelist));
+ 	c->page = NULL;
+ 	c->freelist = NULL;
+-	local_irq_restore(flags);
+-	return freelist;
++	goto out;
+ }
+ 
+ /*
+@@ -2531,7 +2583,7 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
+ 
+ 	do {
+ 		if (unlikely(n)) {
+-			spin_unlock_irqrestore(&n->list_lock, flags);
++			raw_spin_unlock_irqrestore(&n->list_lock, flags);
+ 			n = NULL;
+ 		}
+ 		prior = page->freelist;
+@@ -2563,7 +2615,7 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
+ 				 * Otherwise the list_lock will synchronize with
+ 				 * other processors updating the list of slabs.
+ 				 */
+-				spin_lock_irqsave(&n->list_lock, flags);
++				raw_spin_lock_irqsave(&n->list_lock, flags);
+ 
+ 			}
+ 		}
+@@ -2605,7 +2657,7 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
+ 		add_partial(n, page, DEACTIVATE_TO_TAIL);
+ 		stat(s, FREE_ADD_PARTIAL);
+ 	}
+-	spin_unlock_irqrestore(&n->list_lock, flags);
++	raw_spin_unlock_irqrestore(&n->list_lock, flags);
+ 	return;
+ 
+ slab_empty:
+@@ -2620,7 +2672,7 @@ slab_empty:
+ 		remove_full(s, n, page);
+ 	}
+ 
+-	spin_unlock_irqrestore(&n->list_lock, flags);
++	raw_spin_unlock_irqrestore(&n->list_lock, flags);
+ 	stat(s, FREE_SLAB);
+ 	discard_slab(s, page);
+ }
+@@ -2816,7 +2868,7 @@ static void
+ init_kmem_cache_node(struct kmem_cache_node *n)
+ {
+ 	n->nr_partial = 0;
+-	spin_lock_init(&n->list_lock);
++	raw_spin_lock_init(&n->list_lock);
+ 	INIT_LIST_HEAD(&n->partial);
+ #ifdef CONFIG_SLUB_DEBUG
+ 	atomic_long_set(&n->nr_slabs, 0);
+@@ -3373,7 +3425,7 @@ int __kmem_cache_shrink(struct kmem_cache *s)
+ 		for (i = 0; i < objects; i++)
+ 			INIT_LIST_HEAD(slabs_by_inuse + i);
+ 
+-		spin_lock_irqsave(&n->list_lock, flags);
++		raw_spin_lock_irqsave(&n->list_lock, flags);
+ 
+ 		/*
+ 		 * Build lists indexed by the items in use in each slab.
+@@ -3394,7 +3446,7 @@ int __kmem_cache_shrink(struct kmem_cache *s)
+ 		for (i = objects - 1; i > 0; i--)
+ 			list_splice(slabs_by_inuse + i, n->partial.prev);
+ 
+-		spin_unlock_irqrestore(&n->list_lock, flags);
++		raw_spin_unlock_irqrestore(&n->list_lock, flags);
+ 
+ 		/* Release empty slabs */
+ 		list_for_each_entry_safe(page, t, slabs_by_inuse, lru)
+@@ -3567,6 +3619,12 @@ void __init kmem_cache_init(void)
+ {
+ 	static __initdata struct kmem_cache boot_kmem_cache,
+ 		boot_kmem_cache_node;
++	int cpu;
++
++	for_each_possible_cpu(cpu) {
++		raw_spin_lock_init(&per_cpu(slub_free_list, cpu).lock);
++		INIT_LIST_HEAD(&per_cpu(slub_free_list, cpu).list);
++	}
+ 
+ 	if (debug_guardpage_minorder())
+ 		slub_max_order = 0;
+@@ -3815,7 +3873,7 @@ static int validate_slab_node(struct kmem_cache *s,
+ 	struct page *page;
+ 	unsigned long flags;
+ 
+-	spin_lock_irqsave(&n->list_lock, flags);
++	raw_spin_lock_irqsave(&n->list_lock, flags);
+ 
+ 	list_for_each_entry(page, &n->partial, lru) {
+ 		validate_slab_slab(s, page, map);
+@@ -3837,7 +3895,7 @@ static int validate_slab_node(struct kmem_cache *s,
+ 		       s->name, count, atomic_long_read(&n->nr_slabs));
+ 
+ out:
+-	spin_unlock_irqrestore(&n->list_lock, flags);
++	raw_spin_unlock_irqrestore(&n->list_lock, flags);
+ 	return count;
+ }
+ 
+@@ -4025,12 +4083,12 @@ static int list_locations(struct kmem_cache *s, char *buf,
+ 		if (!atomic_long_read(&n->nr_slabs))
+ 			continue;
+ 
+-		spin_lock_irqsave(&n->list_lock, flags);
++		raw_spin_lock_irqsave(&n->list_lock, flags);
+ 		list_for_each_entry(page, &n->partial, lru)
+ 			process_slab(&t, s, page, alloc, map);
+ 		list_for_each_entry(page, &n->full, lru)
+ 			process_slab(&t, s, page, alloc, map);
+-		spin_unlock_irqrestore(&n->list_lock, flags);
++		raw_spin_unlock_irqrestore(&n->list_lock, flags);
+ 	}
+ 
+ 	for (i = 0; i < t.count; i++) {
+diff --git a/mm/swap.c b/mm/swap.c
+index e657ba642e5e..8f8fa64e2d74 100644
+--- a/mm/swap.c
++++ b/mm/swap.c
+@@ -31,6 +31,7 @@
+ #include <linux/memcontrol.h>
+ #include <linux/gfp.h>
+ #include <linux/uio.h>
++#include <linux/locallock.h>
+ 
+ #include "internal.h"
+ 
+@@ -44,6 +45,9 @@ static DEFINE_PER_CPU(struct pagevec, lru_add_pvec);
+ static DEFINE_PER_CPU(struct pagevec, lru_rotate_pvecs);
+ static DEFINE_PER_CPU(struct pagevec, lru_deactivate_file_pvecs);
+ 
++static DEFINE_LOCAL_IRQ_LOCK(rotate_lock);
++static DEFINE_LOCAL_IRQ_LOCK(swapvec_lock);
++
+ /*
+  * This path almost never happens for VM activity - pages are normally
+  * freed via pagevecs.  But it gets used by networking.
+@@ -473,11 +477,11 @@ void rotate_reclaimable_page(struct page *page)
+ 		unsigned long flags;
+ 
+ 		page_cache_get(page);
+-		local_irq_save(flags);
++		local_lock_irqsave(rotate_lock, flags);
+ 		pvec = this_cpu_ptr(&lru_rotate_pvecs);
+ 		if (!pagevec_add(pvec, page) || PageCompound(page))
+ 			pagevec_move_tail(pvec);
+-		local_irq_restore(flags);
++		local_unlock_irqrestore(rotate_lock, flags);
+ 	}
+ }
+ 
+@@ -528,12 +532,13 @@ static bool need_activate_page_drain(int cpu)
+ void activate_page(struct page *page)
+ {
+ 	if (PageLRU(page) && !PageActive(page) && !PageUnevictable(page)) {
+-		struct pagevec *pvec = &get_cpu_var(activate_page_pvecs);
++		struct pagevec *pvec = &get_locked_var(swapvec_lock,
++						       activate_page_pvecs);
+ 
+ 		page_cache_get(page);
+ 		if (!pagevec_add(pvec, page) || PageCompound(page))
+ 			pagevec_lru_move_fn(pvec, __activate_page, NULL);
+-		put_cpu_var(activate_page_pvecs);
++		put_locked_var(swapvec_lock, activate_page_pvecs);
+ 	}
+ }
+ 
+@@ -559,7 +564,7 @@ void activate_page(struct page *page)
+ 
+ static void __lru_cache_activate_page(struct page *page)
+ {
+-	struct pagevec *pvec = &get_cpu_var(lru_add_pvec);
++	struct pagevec *pvec = &get_locked_var(swapvec_lock, lru_add_pvec);
+ 	int i;
+ 
+ 	/*
+@@ -581,7 +586,7 @@ static void __lru_cache_activate_page(struct page *page)
+ 		}
+ 	}
+ 
+-	put_cpu_var(lru_add_pvec);
++	put_locked_var(swapvec_lock, lru_add_pvec);
+ }
+ 
+ /*
+@@ -620,12 +625,12 @@ EXPORT_SYMBOL(mark_page_accessed);
+ 
+ static void __lru_cache_add(struct page *page)
+ {
+-	struct pagevec *pvec = &get_cpu_var(lru_add_pvec);
++	struct pagevec *pvec = &get_locked_var(swapvec_lock, lru_add_pvec);
+ 
+ 	page_cache_get(page);
+ 	if (!pagevec_add(pvec, page) || PageCompound(page))
+ 		__pagevec_lru_add(pvec);
+-	put_cpu_var(lru_add_pvec);
++	put_locked_var(swapvec_lock, lru_add_pvec);
+ }
+ 
+ /**
+@@ -805,9 +810,15 @@ void lru_add_drain_cpu(int cpu)
+ 		unsigned long flags;
+ 
+ 		/* No harm done if a racing interrupt already did this */
+-		local_irq_save(flags);
++#ifdef CONFIG_PREEMPT_RT_BASE
++		local_lock_irqsave_on(rotate_lock, flags, cpu);
++		pagevec_move_tail(pvec);
++		local_unlock_irqrestore_on(rotate_lock, flags, cpu);
++#else
++		local_lock_irqsave(rotate_lock, flags);
+ 		pagevec_move_tail(pvec);
+-		local_irq_restore(flags);
++		local_unlock_irqrestore(rotate_lock, flags);
++#endif
+ 	}
+ 
+ 	pvec = &per_cpu(lru_deactivate_file_pvecs, cpu);
+@@ -835,26 +846,47 @@ void deactivate_file_page(struct page *page)
+ 		return;
+ 
+ 	if (likely(get_page_unless_zero(page))) {
+-		struct pagevec *pvec = &get_cpu_var(lru_deactivate_file_pvecs);
++		struct pagevec *pvec = &get_locked_var(swapvec_lock,
++						       lru_deactivate_file_pvecs);
+ 
+-		if (!pagevec_add(pvec, page) || PageCompound(page))
++		if (!pagevec_add(pvec, page))
+ 			pagevec_lru_move_fn(pvec, lru_deactivate_file_fn, NULL);
+-		put_cpu_var(lru_deactivate_file_pvecs);
++		put_locked_var(swapvec_lock, lru_deactivate_file_pvecs);
+ 	}
+ }
+ 
+ void lru_add_drain(void)
+ {
+-	lru_add_drain_cpu(get_cpu());
+-	put_cpu();
++	lru_add_drain_cpu(local_lock_cpu(swapvec_lock));
++	local_unlock_cpu(swapvec_lock);
++}
++
++
++#ifdef CONFIG_PREEMPT_RT_BASE
++static inline void remote_lru_add_drain(int cpu, struct cpumask *has_work)
++{
++	local_lock_on(swapvec_lock, cpu);
++	lru_add_drain_cpu(cpu);
++	local_unlock_on(swapvec_lock, cpu);
+ }
+ 
++#else
++
+ static void lru_add_drain_per_cpu(struct work_struct *dummy)
+ {
+ 	lru_add_drain();
+ }
+ 
+ static DEFINE_PER_CPU(struct work_struct, lru_add_drain_work);
++static inline void remote_lru_add_drain(int cpu, struct cpumask *has_work)
++{
++	struct work_struct *work = &per_cpu(lru_add_drain_work, cpu);
++
++	INIT_WORK(work, lru_add_drain_per_cpu);
++	schedule_work_on(cpu, work);
++	cpumask_set_cpu(cpu, has_work);
++}
++#endif
+ 
+ void lru_add_drain_all(void)
+ {
+@@ -867,20 +899,17 @@ void lru_add_drain_all(void)
+ 	cpumask_clear(&has_work);
+ 
+ 	for_each_online_cpu(cpu) {
+-		struct work_struct *work = &per_cpu(lru_add_drain_work, cpu);
+-
+ 		if (pagevec_count(&per_cpu(lru_add_pvec, cpu)) ||
+ 		    pagevec_count(&per_cpu(lru_rotate_pvecs, cpu)) ||
+ 		    pagevec_count(&per_cpu(lru_deactivate_file_pvecs, cpu)) ||
+-		    need_activate_page_drain(cpu)) {
+-			INIT_WORK(work, lru_add_drain_per_cpu);
+-			schedule_work_on(cpu, work);
+-			cpumask_set_cpu(cpu, &has_work);
+-		}
++		    need_activate_page_drain(cpu))
++			remote_lru_add_drain(cpu, &has_work);
+ 	}
+ 
++#ifndef CONFIG_PREEMPT_RT_BASE
+ 	for_each_cpu(cpu, &has_work)
+ 		flush_work(&per_cpu(lru_add_drain_work, cpu));
++#endif
+ 
+ 	put_online_cpus();
+ 	mutex_unlock(&lock);
+diff --git a/mm/truncate.c b/mm/truncate.c
+index dff252c03f3b..d83247ec256d 100644
+--- a/mm/truncate.c
++++ b/mm/truncate.c
+@@ -56,8 +56,11 @@ static void clear_exceptional_entry(struct address_space *mapping,
+ 	 * protected by mapping->tree_lock.
+ 	 */
+ 	if (!workingset_node_shadows(node) &&
+-	    !list_empty(&node->private_list))
+-		list_lru_del(&workingset_shadow_nodes, &node->private_list);
++	    !list_empty(&node->private_list)) {
++		local_lock(workingset_shadow_lock);
++		list_lru_del(&__workingset_shadow_nodes, &node->private_list);
++		local_unlock(workingset_shadow_lock);
++	}
+ 	__radix_tree_delete_node(&mapping->page_tree, node);
+ unlock:
+ 	spin_unlock_irq(&mapping->tree_lock);
+diff --git a/mm/vmalloc.c b/mm/vmalloc.c
+index 90520af7f186..43122b6c46ed 100644
+--- a/mm/vmalloc.c
++++ b/mm/vmalloc.c
+@@ -798,7 +798,7 @@ static struct vmap_block *new_vmap_block(gfp_t gfp_mask)
+ 	struct vmap_block *vb;
+ 	struct vmap_area *va;
+ 	unsigned long vb_idx;
+-	int node, err;
++	int node, err, cpu;
+ 
+ 	node = numa_node_id();
+ 
+@@ -836,11 +836,12 @@ static struct vmap_block *new_vmap_block(gfp_t gfp_mask)
+ 	BUG_ON(err);
+ 	radix_tree_preload_end();
+ 
+-	vbq = &get_cpu_var(vmap_block_queue);
++	cpu = get_cpu_light();
++	vbq = &__get_cpu_var(vmap_block_queue);
+ 	spin_lock(&vbq->lock);
+ 	list_add_rcu(&vb->free_list, &vbq->free);
+ 	spin_unlock(&vbq->lock);
+-	put_cpu_var(vmap_block_queue);
++	put_cpu_light();
+ 
+ 	return vb;
+ }
+@@ -908,6 +909,7 @@ static void *vb_alloc(unsigned long size, gfp_t gfp_mask)
+ 	struct vmap_block *vb;
+ 	unsigned long addr = 0;
+ 	unsigned int order;
++	int cpu = 0;
+ 
+ 	BUG_ON(size & ~PAGE_MASK);
+ 	BUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);
+@@ -923,7 +925,8 @@ static void *vb_alloc(unsigned long size, gfp_t gfp_mask)
+ 
+ again:
+ 	rcu_read_lock();
+-	vbq = &get_cpu_var(vmap_block_queue);
++	cpu = get_cpu_light();
++	vbq = &__get_cpu_var(vmap_block_queue);
+ 	list_for_each_entry_rcu(vb, &vbq->free, free_list) {
+ 		int i;
+ 
+@@ -947,7 +950,7 @@ next:
+ 		spin_unlock(&vb->lock);
+ 	}
+ 
+-	put_cpu_var(vmap_block_queue);
++	put_cpu_light();
+ 	rcu_read_unlock();
+ 
+ 	if (!addr) {
+diff --git a/mm/vmstat.c b/mm/vmstat.c
+index 4590aa42b6cd..d65d0c30a536 100644
+--- a/mm/vmstat.c
++++ b/mm/vmstat.c
+@@ -221,6 +221,7 @@ void __mod_zone_page_state(struct zone *zone, enum zone_stat_item item,
+ 	long x;
+ 	long t;
+ 
++	preempt_disable_rt();
+ 	x = delta + __this_cpu_read(*p);
+ 
+ 	t = __this_cpu_read(pcp->stat_threshold);
+@@ -230,6 +231,7 @@ void __mod_zone_page_state(struct zone *zone, enum zone_stat_item item,
+ 		x = 0;
+ 	}
+ 	__this_cpu_write(*p, x);
++	preempt_enable_rt();
+ }
+ EXPORT_SYMBOL(__mod_zone_page_state);
+ 
+@@ -262,6 +264,7 @@ void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
+ 	s8 __percpu *p = pcp->vm_stat_diff + item;
+ 	s8 v, t;
+ 
++	preempt_disable_rt();
+ 	v = __this_cpu_inc_return(*p);
+ 	t = __this_cpu_read(pcp->stat_threshold);
+ 	if (unlikely(v > t)) {
+@@ -270,6 +273,7 @@ void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
+ 		zone_page_state_add(v + overstep, zone, item);
+ 		__this_cpu_write(*p, -overstep);
+ 	}
++	preempt_enable_rt();
+ }
+ 
+ void __inc_zone_page_state(struct page *page, enum zone_stat_item item)
+@@ -284,6 +288,7 @@ void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
+ 	s8 __percpu *p = pcp->vm_stat_diff + item;
+ 	s8 v, t;
+ 
++	preempt_disable_rt();
+ 	v = __this_cpu_dec_return(*p);
+ 	t = __this_cpu_read(pcp->stat_threshold);
+ 	if (unlikely(v < - t)) {
+@@ -292,6 +297,7 @@ void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
+ 		zone_page_state_add(v - overstep, zone, item);
+ 		__this_cpu_write(*p, overstep);
+ 	}
++	preempt_enable_rt();
+ }
+ 
+ void __dec_zone_page_state(struct page *page, enum zone_stat_item item)
+diff --git a/mm/workingset.c b/mm/workingset.c
+index f7216fa7da27..0decb9ca8db8 100644
+--- a/mm/workingset.c
++++ b/mm/workingset.c
+@@ -264,7 +264,8 @@ void workingset_activation(struct page *page)
+  * point where they would still be useful.
+  */
+ 
+-struct list_lru workingset_shadow_nodes;
++struct list_lru __workingset_shadow_nodes;
++DEFINE_LOCAL_IRQ_LOCK(workingset_shadow_lock);
+ 
+ static unsigned long count_shadow_nodes(struct shrinker *shrinker,
+ 					struct shrink_control *sc)
+@@ -274,9 +275,9 @@ static unsigned long count_shadow_nodes(struct shrinker *shrinker,
+ 	unsigned long pages;
+ 
+ 	/* list_lru lock nests inside IRQ-safe mapping->tree_lock */
+-	local_irq_disable();
+-	shadow_nodes = list_lru_count_node(&workingset_shadow_nodes, sc->nid);
+-	local_irq_enable();
++	local_lock_irq(workingset_shadow_lock);
++	shadow_nodes = list_lru_count_node(&__workingset_shadow_nodes, sc->nid);
++	local_unlock_irq(workingset_shadow_lock);
+ 
+ 	pages = node_present_pages(sc->nid);
+ 	/*
+@@ -362,9 +363,9 @@ static enum lru_status shadow_lru_isolate(struct list_head *item,
+ 	spin_unlock(&mapping->tree_lock);
+ 	ret = LRU_REMOVED_RETRY;
+ out:
+-	local_irq_enable();
++	local_unlock_irq(workingset_shadow_lock);
+ 	cond_resched();
+-	local_irq_disable();
++	local_lock_irq(workingset_shadow_lock);
+ 	spin_lock(lru_lock);
+ 	return ret;
+ }
+@@ -375,10 +376,10 @@ static unsigned long scan_shadow_nodes(struct shrinker *shrinker,
+ 	unsigned long ret;
+ 
+ 	/* list_lru lock nests inside IRQ-safe mapping->tree_lock */
+-	local_irq_disable();
+-	ret =  list_lru_walk_node(&workingset_shadow_nodes, sc->nid,
++	local_lock_irq(workingset_shadow_lock);
++	ret =  list_lru_walk_node(&__workingset_shadow_nodes, sc->nid,
+ 				  shadow_lru_isolate, NULL, &sc->nr_to_scan);
+-	local_irq_enable();
++	local_unlock_irq(workingset_shadow_lock);
+ 	return ret;
+ }
+ 
+@@ -399,7 +400,7 @@ static int __init workingset_init(void)
+ {
+ 	int ret;
+ 
+-	ret = list_lru_init_key(&workingset_shadow_nodes, &shadow_nodes_key);
++	ret = list_lru_init_key(&__workingset_shadow_nodes, &shadow_nodes_key);
+ 	if (ret)
+ 		goto err;
+ 	ret = register_shrinker(&workingset_shadow_shrinker);
+@@ -407,7 +408,7 @@ static int __init workingset_init(void)
+ 		goto err_list_lru;
+ 	return 0;
+ err_list_lru:
+-	list_lru_destroy(&workingset_shadow_nodes);
++	list_lru_destroy(&__workingset_shadow_nodes);
+ err:
+ 	return ret;
+ }
+diff --git a/mm/zsmalloc.c b/mm/zsmalloc.c
+index 839a48c3ca27..85028f8a5ed3 100644
+--- a/mm/zsmalloc.c
++++ b/mm/zsmalloc.c
+@@ -93,6 +93,7 @@
+ #include <linux/types.h>
+ #include <linux/zsmalloc.h>
+ #include <linux/zpool.h>
++#include <linux/locallock.h>
+ 
+ /*
+  * This must be power of 2 and greater than of equal to sizeof(link_free).
+@@ -318,6 +319,7 @@ MODULE_ALIAS("zpool-zsmalloc");
+ 
+ /* per-cpu VM mapping areas for zspage accesses that cross page boundaries */
+ static DEFINE_PER_CPU(struct mapping_area, zs_map_area);
++static DEFINE_LOCAL_IRQ_LOCK(zs_map_area_lock);
+ 
+ static int is_first_page(struct page *page)
+ {
+@@ -1127,7 +1129,7 @@ void *zs_map_object(struct zs_pool *pool, unsigned long handle,
+ 	class = &pool->size_class[class_idx];
+ 	off = obj_idx_to_offset(page, obj_idx, class->size);
+ 
+-	area = &get_cpu_var(zs_map_area);
++	area = &get_locked_var(zs_map_area_lock, zs_map_area);
+ 	area->vm_mm = mm;
+ 	if (off + class->size <= PAGE_SIZE) {
+ 		/* this object is contained entirely within a page */
+@@ -1173,7 +1175,7 @@ void zs_unmap_object(struct zs_pool *pool, unsigned long handle)
+ 
+ 		__zs_unmap_object(area, pages, off, class->size);
+ 	}
+-	put_cpu_var(zs_map_area);
++	put_locked_var(zs_map_area_lock, zs_map_area);
+ }
+ EXPORT_SYMBOL_GPL(zs_unmap_object);
+ 
+diff --git a/net/core/dev.c b/net/core/dev.c
+index 22ad4e433b3a..dc05b6cbf23c 100644
+--- a/net/core/dev.c
++++ b/net/core/dev.c
+@@ -182,6 +182,7 @@ static unsigned int napi_gen_id;
+ static DEFINE_HASHTABLE(napi_hash, 8);
+ 
+ static seqcount_t devnet_rename_seq;
++static DEFINE_MUTEX(devnet_rename_mutex);
+ 
+ static inline void dev_base_seq_inc(struct net *net)
+ {
+@@ -203,14 +204,14 @@ static inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)
+ static inline void rps_lock(struct softnet_data *sd)
+ {
+ #ifdef CONFIG_RPS
+-	spin_lock(&sd->input_pkt_queue.lock);
++	raw_spin_lock(&sd->input_pkt_queue.raw_lock);
+ #endif
+ }
+ 
+ static inline void rps_unlock(struct softnet_data *sd)
+ {
+ #ifdef CONFIG_RPS
+-	spin_unlock(&sd->input_pkt_queue.lock);
++	raw_spin_unlock(&sd->input_pkt_queue.raw_lock);
+ #endif
+ }
+ 
+@@ -832,7 +833,8 @@ retry:
+ 	strcpy(name, dev->name);
+ 	rcu_read_unlock();
+ 	if (read_seqcount_retry(&devnet_rename_seq, seq)) {
+-		cond_resched();
++		mutex_lock(&devnet_rename_mutex);
++		mutex_unlock(&devnet_rename_mutex);
+ 		goto retry;
+ 	}
+ 
+@@ -1101,20 +1103,17 @@ int dev_change_name(struct net_device *dev, const char *newname)
+ 	if (dev->flags & IFF_UP)
+ 		return -EBUSY;
+ 
+-	write_seqcount_begin(&devnet_rename_seq);
++	mutex_lock(&devnet_rename_mutex);
++	__raw_write_seqcount_begin(&devnet_rename_seq);
+ 
+-	if (strncmp(newname, dev->name, IFNAMSIZ) == 0) {
+-		write_seqcount_end(&devnet_rename_seq);
+-		return 0;
+-	}
++	if (strncmp(newname, dev->name, IFNAMSIZ) == 0)
++		goto outunlock;
+ 
+ 	memcpy(oldname, dev->name, IFNAMSIZ);
+ 
+ 	err = dev_get_valid_name(net, dev, newname);
+-	if (err < 0) {
+-		write_seqcount_end(&devnet_rename_seq);
+-		return err;
+-	}
++	if (err < 0)
++		goto outunlock;
+ 
+ 	if (oldname[0] && !strchr(oldname, '%'))
+ 		netdev_info(dev, "renamed from %s\n", oldname);
+@@ -1127,11 +1126,12 @@ rollback:
+ 	if (ret) {
+ 		memcpy(dev->name, oldname, IFNAMSIZ);
+ 		dev->name_assign_type = old_assign_type;
+-		write_seqcount_end(&devnet_rename_seq);
+-		return ret;
++		err = ret;
++		goto outunlock;
+ 	}
+ 
+-	write_seqcount_end(&devnet_rename_seq);
++	__raw_write_seqcount_end(&devnet_rename_seq);
++	mutex_unlock(&devnet_rename_mutex);
+ 
+ 	netdev_adjacent_rename_links(dev, oldname);
+ 
+@@ -1152,7 +1152,8 @@ rollback:
+ 		/* err >= 0 after dev_alloc_name() or stores the first errno */
+ 		if (err >= 0) {
+ 			err = ret;
+-			write_seqcount_begin(&devnet_rename_seq);
++			mutex_lock(&devnet_rename_mutex);
++			__raw_write_seqcount_begin(&devnet_rename_seq);
+ 			memcpy(dev->name, oldname, IFNAMSIZ);
+ 			memcpy(oldname, newname, IFNAMSIZ);
+ 			dev->name_assign_type = old_assign_type;
+@@ -1165,6 +1166,11 @@ rollback:
+ 	}
+ 
+ 	return err;
++
++outunlock:
++	__raw_write_seqcount_end(&devnet_rename_seq);
++	mutex_unlock(&devnet_rename_mutex);
++	return err;
+ }
+ 
+ /**
+@@ -2156,6 +2162,7 @@ static inline void __netif_reschedule(struct Qdisc *q)
+ 	sd->output_queue_tailp = &q->next_sched;
+ 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
+ 	local_irq_restore(flags);
++	preempt_check_resched_rt();
+ }
+ 
+ void __netif_schedule(struct Qdisc *q)
+@@ -2237,6 +2244,7 @@ void __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason)
+ 	__this_cpu_write(softnet_data.completion_queue, skb);
+ 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
+ 	local_irq_restore(flags);
++	preempt_check_resched_rt();
+ }
+ EXPORT_SYMBOL(__dev_kfree_skb_irq);
+ 
+@@ -2797,7 +2805,11 @@ static inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,
+ 	 * This permits __QDISC___STATE_RUNNING owner to get the lock more
+ 	 * often and dequeue packets faster.
+ 	 */
++#ifdef CONFIG_PREEMPT_RT_FULL
++	contended = true;
++#else
+ 	contended = qdisc_is_running(q);
++#endif
+ 	if (unlikely(contended))
+ 		spin_lock(&q->busylock);
+ 
+@@ -2857,9 +2869,44 @@ static void skb_update_prio(struct sk_buff *skb)
+ #define skb_update_prio(skb)
+ #endif
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++
++static inline int xmit_rec_read(void)
++{
++       return current->xmit_recursion;
++}
++
++static inline void xmit_rec_inc(void)
++{
++       current->xmit_recursion++;
++}
++
++static inline void xmit_rec_dec(void)
++{
++       current->xmit_recursion--;
++}
++
++#else
++
+ DEFINE_PER_CPU(int, xmit_recursion);
+ EXPORT_SYMBOL(xmit_recursion);
+ 
++static inline int xmit_rec_read(void)
++{
++	return __this_cpu_read(xmit_recursion);
++}
++
++static inline void xmit_rec_inc(void)
++{
++	__this_cpu_inc(xmit_recursion);
++}
++
++static inline int xmit_rec_dec(void)
++{
++	__this_cpu_dec(xmit_recursion);
++}
++#endif
++
+ #define RECURSION_LIMIT 10
+ 
+ /**
+@@ -2961,7 +3008,7 @@ static int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv)
+ 
+ 		if (txq->xmit_lock_owner != cpu) {
+ 
+-			if (__this_cpu_read(xmit_recursion) > RECURSION_LIMIT)
++			if (xmit_rec_read() > RECURSION_LIMIT)
+ 				goto recursion_alert;
+ 
+ 			skb = validate_xmit_skb(skb, dev);
+@@ -2971,9 +3018,9 @@ static int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv)
+ 			HARD_TX_LOCK(dev, txq, cpu);
+ 
+ 			if (!netif_xmit_stopped(txq)) {
+-				__this_cpu_inc(xmit_recursion);
++				xmit_rec_inc();
+ 				skb = dev_hard_start_xmit(skb, dev, txq, &rc);
+-				__this_cpu_dec(xmit_recursion);
++				xmit_rec_dec();
+ 				if (dev_xmit_complete(rc)) {
+ 					HARD_TX_UNLOCK(dev, txq);
+ 					goto out;
+@@ -3341,6 +3388,7 @@ drop:
+ 	rps_unlock(sd);
+ 
+ 	local_irq_restore(flags);
++	preempt_check_resched_rt();
+ 
+ 	atomic_long_inc(&skb->dev->rx_dropped);
+ 	kfree_skb(skb);
+@@ -3359,7 +3407,7 @@ static int netif_rx_internal(struct sk_buff *skb)
+ 		struct rps_dev_flow voidflow, *rflow = &voidflow;
+ 		int cpu;
+ 
+-		preempt_disable();
++		migrate_disable();
+ 		rcu_read_lock();
+ 
+ 		cpu = get_rps_cpu(skb->dev, skb, &rflow);
+@@ -3369,13 +3417,13 @@ static int netif_rx_internal(struct sk_buff *skb)
+ 		ret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);
+ 
+ 		rcu_read_unlock();
+-		preempt_enable();
++		migrate_enable();
+ 	} else
+ #endif
+ 	{
+ 		unsigned int qtail;
+-		ret = enqueue_to_backlog(skb, get_cpu(), &qtail);
+-		put_cpu();
++		ret = enqueue_to_backlog(skb, get_cpu_light(), &qtail);
++		put_cpu_light();
+ 	}
+ 	return ret;
+ }
+@@ -3409,16 +3457,44 @@ int netif_rx_ni(struct sk_buff *skb)
+ 
+ 	trace_netif_rx_ni_entry(skb);
+ 
+-	preempt_disable();
++	local_bh_disable();
+ 	err = netif_rx_internal(skb);
+-	if (local_softirq_pending())
+-		do_softirq();
+-	preempt_enable();
++	local_bh_enable();
+ 
+ 	return err;
+ }
+ EXPORT_SYMBOL(netif_rx_ni);
+ 
++#ifdef CONFIG_PREEMPT_RT_FULL
++/*
++ * RT runs ksoftirqd as a real time thread and the root_lock is a
++ * "sleeping spinlock". If the trylock fails then we can go into an
++ * infinite loop when ksoftirqd preempted the task which actually
++ * holds the lock, because we requeue q and raise NET_TX softirq
++ * causing ksoftirqd to loop forever.
++ *
++ * It's safe to use spin_lock on RT here as softirqs run in thread
++ * context and cannot deadlock against the thread which is holding
++ * root_lock.
++ *
++ * On !RT the trylock might fail, but there we bail out from the
++ * softirq loop after 10 attempts which we can't do on RT. And the
++ * task holding root_lock cannot be preempted, so the only downside of
++ * that trylock is that we need 10 loops to decide that we should have
++ * given up in the first one :)
++ */
++static inline int take_root_lock(spinlock_t *lock)
++{
++	spin_lock(lock);
++	return 1;
++}
++#else
++static inline int take_root_lock(spinlock_t *lock)
++{
++	return spin_trylock(lock);
++}
++#endif
++
+ static void net_tx_action(struct softirq_action *h)
+ {
+ 	struct softnet_data *sd = this_cpu_ptr(&softnet_data);
+@@ -3460,7 +3536,7 @@ static void net_tx_action(struct softirq_action *h)
+ 			head = head->next_sched;
+ 
+ 			root_lock = qdisc_lock(q);
+-			if (spin_trylock(root_lock)) {
++			if (take_root_lock(root_lock)) {
+ 				smp_mb__before_atomic();
+ 				clear_bit(__QDISC_STATE_SCHED,
+ 					  &q->state);
+@@ -3851,7 +3927,7 @@ static void flush_backlog(void *arg)
+ 	skb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {
+ 		if (skb->dev == dev) {
+ 			__skb_unlink(skb, &sd->input_pkt_queue);
+-			kfree_skb(skb);
++			__skb_queue_tail(&sd->tofree_queue, skb);
+ 			input_queue_head_incr(sd);
+ 		}
+ 	}
+@@ -3860,10 +3936,13 @@ static void flush_backlog(void *arg)
+ 	skb_queue_walk_safe(&sd->process_queue, skb, tmp) {
+ 		if (skb->dev == dev) {
+ 			__skb_unlink(skb, &sd->process_queue);
+-			kfree_skb(skb);
++			__skb_queue_tail(&sd->tofree_queue, skb);
+ 			input_queue_head_incr(sd);
+ 		}
+ 	}
++
++	if (!skb_queue_empty(&sd->tofree_queue))
++		raise_softirq_irqoff(NET_RX_SOFTIRQ);
+ }
+ 
+ static int napi_gro_complete(struct sk_buff *skb)
+@@ -4326,6 +4405,7 @@ static void net_rps_action_and_irq_enable(struct softnet_data *sd)
+ 	} else
+ #endif
+ 		local_irq_enable();
++	preempt_check_resched_rt();
+ }
+ 
+ static int process_backlog(struct napi_struct *napi, int quota)
+@@ -4399,6 +4479,7 @@ void __napi_schedule(struct napi_struct *n)
+ 	local_irq_save(flags);
+ 	____napi_schedule(this_cpu_ptr(&softnet_data), n);
+ 	local_irq_restore(flags);
++	preempt_check_resched_rt();
+ }
+ EXPORT_SYMBOL(__napi_schedule);
+ 
+@@ -4521,10 +4602,17 @@ static void net_rx_action(struct softirq_action *h)
+ 	struct softnet_data *sd = this_cpu_ptr(&softnet_data);
+ 	unsigned long time_limit = jiffies + 2;
+ 	int budget = netdev_budget;
++	struct sk_buff *skb;
+ 	void *have;
+ 
+ 	local_irq_disable();
+ 
++	while ((skb = __skb_dequeue(&sd->tofree_queue))) {
++		local_irq_enable();
++		kfree_skb(skb);
++		local_irq_disable();
++	}
++
+ 	while (!list_empty(&sd->poll_list)) {
+ 		struct napi_struct *n;
+ 		int work, weight;
+@@ -4599,7 +4687,7 @@ out:
+ 
+ softnet_break:
+ 	sd->time_squeeze++;
+-	__raise_softirq_irqoff(NET_RX_SOFTIRQ);
++	__raise_softirq_irqoff_ksoft(NET_RX_SOFTIRQ);
+ 	goto out;
+ }
+ 
+@@ -6774,7 +6862,7 @@ EXPORT_SYMBOL(free_netdev);
+ void synchronize_net(void)
+ {
+ 	might_sleep();
+-	if (rtnl_is_locked())
++	if (rtnl_is_locked() && !IS_ENABLED(CONFIG_PREEMPT_RT_FULL))
+ 		synchronize_rcu_expedited();
+ 	else
+ 		synchronize_rcu();
+@@ -7019,16 +7107,20 @@ static int dev_cpu_callback(struct notifier_block *nfb,
+ 
+ 	raise_softirq_irqoff(NET_TX_SOFTIRQ);
+ 	local_irq_enable();
++	preempt_check_resched_rt();
+ 
+ 	/* Process offline CPU's input_pkt_queue */
+ 	while ((skb = __skb_dequeue(&oldsd->process_queue))) {
+ 		netif_rx_internal(skb);
+ 		input_queue_head_incr(oldsd);
+ 	}
+-	while ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {
++	while ((skb = __skb_dequeue(&oldsd->input_pkt_queue))) {
+ 		netif_rx_internal(skb);
+ 		input_queue_head_incr(oldsd);
+ 	}
++	while ((skb = __skb_dequeue(&oldsd->tofree_queue))) {
++		kfree_skb(skb);
++	}
+ 
+ 	return NOTIFY_OK;
+ }
+@@ -7330,8 +7422,9 @@ static int __init net_dev_init(void)
+ 	for_each_possible_cpu(i) {
+ 		struct softnet_data *sd = &per_cpu(softnet_data, i);
+ 
+-		skb_queue_head_init(&sd->input_pkt_queue);
+-		skb_queue_head_init(&sd->process_queue);
++		skb_queue_head_init_raw(&sd->input_pkt_queue);
++		skb_queue_head_init_raw(&sd->process_queue);
++		skb_queue_head_init_raw(&sd->tofree_queue);
+ 		INIT_LIST_HEAD(&sd->poll_list);
+ 		sd->output_queue_tailp = &sd->output_queue;
+ #ifdef CONFIG_RPS
+diff --git a/net/core/skbuff.c b/net/core/skbuff.c
+index b2e2a53c2284..1c7477003e48 100644
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -63,6 +63,7 @@
+ #include <linux/errqueue.h>
+ #include <linux/prefetch.h>
+ #include <linux/if_vlan.h>
++#include <linux/locallock.h>
+ 
+ #include <net/protocol.h>
+ #include <net/dst.h>
+@@ -355,6 +356,7 @@ struct netdev_alloc_cache {
+ 	unsigned int		pagecnt_bias;
+ };
+ static DEFINE_PER_CPU(struct netdev_alloc_cache, netdev_alloc_cache);
++static DEFINE_LOCAL_IRQ_LOCK(netdev_alloc_lock);
+ 
+ static void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)
+ {
+@@ -363,7 +365,7 @@ static void *__netdev_alloc_frag(unsigned int fragsz, gfp_t gfp_mask)
+ 	int order;
+ 	unsigned long flags;
+ 
+-	local_irq_save(flags);
++	local_lock_irqsave(netdev_alloc_lock, flags);
+ 	nc = this_cpu_ptr(&netdev_alloc_cache);
+ 	if (unlikely(!nc->frag.page)) {
+ refill:
+@@ -409,7 +411,7 @@ refill:
+ 	nc->frag.offset += fragsz;
+ 	nc->pagecnt_bias--;
+ end:
+-	local_irq_restore(flags);
++	local_unlock_irqrestore(netdev_alloc_lock, flags);
+ 	return data;
+ }
+ 
+diff --git a/net/core/sock.c b/net/core/sock.c
+index 8c8fb6ab8a26..0300ab4c5ea3 100644
+--- a/net/core/sock.c
++++ b/net/core/sock.c
+@@ -2355,12 +2355,11 @@ void lock_sock_nested(struct sock *sk, int subclass)
+ 	if (sk->sk_lock.owned)
+ 		__lock_sock(sk);
+ 	sk->sk_lock.owned = 1;
+-	spin_unlock(&sk->sk_lock.slock);
++	spin_unlock_bh(&sk->sk_lock.slock);
+ 	/*
+ 	 * The sk_lock has mutex_lock() semantics here:
+ 	 */
+ 	mutex_acquire(&sk->sk_lock.dep_map, subclass, 0, _RET_IP_);
+-	local_bh_enable();
+ }
+ EXPORT_SYMBOL(lock_sock_nested);
+ 
+diff --git a/net/ipv4/icmp.c b/net/ipv4/icmp.c
+index 2d6f89070373..76b11e3e2b95 100644
+--- a/net/ipv4/icmp.c
++++ b/net/ipv4/icmp.c
+@@ -69,6 +69,7 @@
+ #include <linux/jiffies.h>
+ #include <linux/kernel.h>
+ #include <linux/fcntl.h>
++#include <linux/sysrq.h>
+ #include <linux/socket.h>
+ #include <linux/in.h>
+ #include <linux/inet.h>
+@@ -77,6 +78,7 @@
+ #include <linux/string.h>
+ #include <linux/netfilter_ipv4.h>
+ #include <linux/slab.h>
++#include <linux/locallock.h>
+ #include <net/snmp.h>
+ #include <net/ip.h>
+ #include <net/route.h>
+@@ -203,6 +205,8 @@ static const struct icmp_control icmp_pointers[NR_ICMP_TYPES+1];
+  *
+  *	On SMP we have one ICMP socket per-cpu.
+  */
++static DEFINE_LOCAL_IRQ_LOCK(icmp_sk_lock);
++
+ static struct sock *icmp_sk(struct net *net)
+ {
+ 	return net->ipv4.icmp_sk[smp_processor_id()];
+@@ -214,12 +218,14 @@ static inline struct sock *icmp_xmit_lock(struct net *net)
+ 
+ 	local_bh_disable();
+ 
++	local_lock(icmp_sk_lock);
+ 	sk = icmp_sk(net);
+ 
+ 	if (unlikely(!spin_trylock(&sk->sk_lock.slock))) {
+ 		/* This can happen if the output path signals a
+ 		 * dst_link_failure() for an outgoing ICMP packet.
+ 		 */
++		local_unlock(icmp_sk_lock);
+ 		local_bh_enable();
+ 		return NULL;
+ 	}
+@@ -229,6 +235,7 @@ static inline struct sock *icmp_xmit_lock(struct net *net)
+ static inline void icmp_xmit_unlock(struct sock *sk)
+ {
+ 	spin_unlock_bh(&sk->sk_lock.slock);
++	local_unlock(icmp_sk_lock);
+ }
+ 
+ int sysctl_icmp_msgs_per_sec __read_mostly = 1000;
+@@ -356,6 +363,7 @@ static void icmp_push_reply(struct icmp_bxm *icmp_param,
+ 	struct sock *sk;
+ 	struct sk_buff *skb;
+ 
++	local_lock(icmp_sk_lock);
+ 	sk = icmp_sk(dev_net((*rt)->dst.dev));
+ 	if (ip_append_data(sk, fl4, icmp_glue_bits, icmp_param,
+ 			   icmp_param->data_len+icmp_param->head_len,
+@@ -378,6 +386,7 @@ static void icmp_push_reply(struct icmp_bxm *icmp_param,
+ 		skb->ip_summed = CHECKSUM_NONE;
+ 		ip_push_pending_frames(sk, fl4);
+ 	}
++	local_unlock(icmp_sk_lock);
+ }
+ 
+ /*
+@@ -864,6 +873,30 @@ static void icmp_redirect(struct sk_buff *skb)
+ }
+ 
+ /*
++ * 32bit and 64bit have different timestamp length, so we check for
++ * the cookie at offset 20 and verify it is repeated at offset 50
++ */
++#define CO_POS0		20
++#define CO_POS1		50
++#define CO_SIZE		sizeof(int)
++#define ICMP_SYSRQ_SIZE	57
++
++/*
++ * We got a ICMP_SYSRQ_SIZE sized ping request. Check for the cookie
++ * pattern and if it matches send the next byte as a trigger to sysrq.
++ */
++static void icmp_check_sysrq(struct net *net, struct sk_buff *skb)
++{
++	int cookie = htonl(net->ipv4.sysctl_icmp_echo_sysrq);
++	char *p = skb->data;
++
++	if (!memcmp(&cookie, p + CO_POS0, CO_SIZE) &&
++	    !memcmp(&cookie, p + CO_POS1, CO_SIZE) &&
++	    p[CO_POS0 + CO_SIZE] == p[CO_POS1 + CO_SIZE])
++		handle_sysrq(p[CO_POS0 + CO_SIZE]);
++}
++
++/*
+  *	Handle ICMP_ECHO ("ping") requests.
+  *
+  *	RFC 1122: 3.2.2.6 MUST have an echo server that answers ICMP echo
+@@ -890,6 +923,11 @@ static void icmp_echo(struct sk_buff *skb)
+ 		icmp_param.data_len	   = skb->len;
+ 		icmp_param.head_len	   = sizeof(struct icmphdr);
+ 		icmp_reply(&icmp_param, skb);
++
++		if (skb->len == ICMP_SYSRQ_SIZE &&
++		    net->ipv4.sysctl_icmp_echo_sysrq) {
++			icmp_check_sysrq(net, skb);
++		}
+ 	}
+ }
+ 
+diff --git a/net/ipv4/sysctl_net_ipv4.c b/net/ipv4/sysctl_net_ipv4.c
+index 3d61bdbae920..4e220d6e3a7e 100644
+--- a/net/ipv4/sysctl_net_ipv4.c
++++ b/net/ipv4/sysctl_net_ipv4.c
+@@ -779,6 +779,13 @@ static struct ctl_table ipv4_net_table[] = {
+ 		.proc_handler	= proc_dointvec
+ 	},
+ 	{
++		.procname	= "icmp_echo_sysrq",
++		.data		= &init_net.ipv4.sysctl_icmp_echo_sysrq,
++		.maxlen		= sizeof(int),
++		.mode		= 0644,
++		.proc_handler	= proc_dointvec
++	},
++	{
+ 		.procname	= "icmp_ignore_bogus_error_responses",
+ 		.data		= &init_net.ipv4.sysctl_icmp_ignore_bogus_error_responses,
+ 		.maxlen		= sizeof(int),
+diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
+index d3d62be2376b..dee81eb14860 100644
+--- a/net/ipv4/tcp_ipv4.c
++++ b/net/ipv4/tcp_ipv4.c
+@@ -62,6 +62,7 @@
+ #include <linux/init.h>
+ #include <linux/times.h>
+ #include <linux/slab.h>
++#include <linux/locallock.h>
+ 
+ #include <net/net_namespace.h>
+ #include <net/icmp.h>
+@@ -566,6 +567,7 @@ void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb)
+ }
+ EXPORT_SYMBOL(tcp_v4_send_check);
+ 
++static DEFINE_LOCAL_IRQ_LOCK(tcp_sk_lock);
+ /*
+  *	This routine will send an RST to the other tcp.
+  *
+@@ -687,10 +689,13 @@ static void tcp_v4_send_reset(struct sock *sk, struct sk_buff *skb)
+ 		arg.bound_dev_if = sk->sk_bound_dev_if;
+ 
+ 	arg.tos = ip_hdr(skb)->tos;
++
++	local_lock(tcp_sk_lock);
+ 	ip_send_unicast_reply(*this_cpu_ptr(net->ipv4.tcp_sk),
+ 			      skb, &TCP_SKB_CB(skb)->header.h4.opt,
+ 			      ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,
+ 			      &arg, arg.iov[0].iov_len);
++	local_unlock(tcp_sk_lock);
+ 
+ 	TCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);
+ 	TCP_INC_STATS_BH(net, TCP_MIB_OUTRSTS);
+@@ -772,10 +777,12 @@ static void tcp_v4_send_ack(struct sk_buff *skb, u32 seq, u32 ack,
+ 	if (oif)
+ 		arg.bound_dev_if = oif;
+ 	arg.tos = tos;
++	local_lock(tcp_sk_lock);
+ 	ip_send_unicast_reply(*this_cpu_ptr(net->ipv4.tcp_sk),
+ 			      skb, &TCP_SKB_CB(skb)->header.h4.opt,
+ 			      ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,
+ 			      &arg, arg.iov[0].iov_len);
++	local_unlock(tcp_sk_lock);
+ 
+ 	TCP_INC_STATS_BH(net, TCP_MIB_OUTSEGS);
+ }
+diff --git a/net/mac80211/rx.c b/net/mac80211/rx.c
+index ea3b13987521..d206434a7a1f 100644
+--- a/net/mac80211/rx.c
++++ b/net/mac80211/rx.c
+@@ -3373,7 +3373,7 @@ void ieee80211_rx(struct ieee80211_hw *hw, struct sk_buff *skb)
+ 	struct ieee80211_supported_band *sband;
+ 	struct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);
+ 
+-	WARN_ON_ONCE(softirq_count() == 0);
++	WARN_ON_ONCE_NONRT(softirq_count() == 0);
+ 
+ 	if (WARN_ON(status->band >= IEEE80211_NUM_BANDS))
+ 		goto drop;
+diff --git a/net/netfilter/core.c b/net/netfilter/core.c
+index d047a6679ff2..942479a61b61 100644
+--- a/net/netfilter/core.c
++++ b/net/netfilter/core.c
+@@ -21,11 +21,17 @@
+ #include <linux/proc_fs.h>
+ #include <linux/mutex.h>
+ #include <linux/slab.h>
++#include <linux/locallock.h>
+ #include <net/net_namespace.h>
+ #include <net/sock.h>
+ 
+ #include "nf_internals.h"
+ 
++#ifdef CONFIG_PREEMPT_RT_BASE
++DEFINE_LOCAL_IRQ_LOCK(xt_write_lock);
++EXPORT_PER_CPU_SYMBOL(xt_write_lock);
++#endif
++
+ static DEFINE_MUTEX(afinfo_mutex);
+ 
+ const struct nf_afinfo __rcu *nf_afinfo[NFPROTO_NUMPROTO] __read_mostly;
+diff --git a/net/packet/af_packet.c b/net/packet/af_packet.c
+index 1c03d83edd7e..8cadf727b132 100644
+--- a/net/packet/af_packet.c
++++ b/net/packet/af_packet.c
+@@ -63,6 +63,7 @@
+ #include <linux/if_packet.h>
+ #include <linux/wireless.h>
+ #include <linux/kernel.h>
++#include <linux/delay.h>
+ #include <linux/kmod.h>
+ #include <linux/slab.h>
+ #include <linux/vmalloc.h>
+@@ -691,7 +692,7 @@ static void prb_retire_rx_blk_timer_expired(unsigned long data)
+ 	if (BLOCK_NUM_PKTS(pbd)) {
+ 		while (atomic_read(&pkc->blk_fill_in_prog)) {
+ 			/* Waiting for skb_copy_bits to finish... */
+-			cpu_relax();
++			cpu_chill();
+ 		}
+ 	}
+ 
+@@ -942,7 +943,7 @@ static void prb_retire_current_block(struct tpacket_kbdq_core *pkc,
+ 		if (!(status & TP_STATUS_BLK_TMO)) {
+ 			while (atomic_read(&pkc->blk_fill_in_prog)) {
+ 				/* Waiting for skb_copy_bits to finish... */
+-				cpu_relax();
++				cpu_chill();
+ 			}
+ 		}
+ 		prb_close_block(pkc, pbd, po, status);
+diff --git a/net/rds/ib_rdma.c b/net/rds/ib_rdma.c
+index 657ba9f5d308..c8faaf36423a 100644
+--- a/net/rds/ib_rdma.c
++++ b/net/rds/ib_rdma.c
+@@ -34,6 +34,7 @@
+ #include <linux/slab.h>
+ #include <linux/rculist.h>
+ #include <linux/llist.h>
++#include <linux/delay.h>
+ 
+ #include "rds.h"
+ #include "ib.h"
+@@ -286,7 +287,7 @@ static inline void wait_clean_list_grace(void)
+ 	for_each_online_cpu(cpu) {
+ 		flag = &per_cpu(clean_list_grace, cpu);
+ 		while (test_bit(CLEAN_LIST_BUSY_BIT, flag))
+-			cpu_relax();
++			cpu_chill();
+ 	}
+ }
+ 
+diff --git a/net/sched/sch_generic.c b/net/sched/sch_generic.c
+index 49003540186c..0fd98bd9ea8d 100644
+--- a/net/sched/sch_generic.c
++++ b/net/sched/sch_generic.c
+@@ -899,7 +899,7 @@ void dev_deactivate_many(struct list_head *head)
+ 	/* Wait for outstanding qdisc_run calls. */
+ 	list_for_each_entry(dev, head, close_list)
+ 		while (some_qdisc_is_busy(dev))
+-			yield();
++			msleep(1);
+ }
+ 
+ void dev_deactivate(struct net_device *dev)
+diff --git a/net/sunrpc/svc_xprt.c b/net/sunrpc/svc_xprt.c
+index c179ca2a5aa4..ac93e74d5515 100644
+--- a/net/sunrpc/svc_xprt.c
++++ b/net/sunrpc/svc_xprt.c
+@@ -357,7 +357,7 @@ static void svc_xprt_do_enqueue(struct svc_xprt *xprt)
+ 		return;
+ 	}
+ 
+-	cpu = get_cpu();
++	cpu = get_cpu_light();
+ 	pool = svc_pool_for_cpu(xprt->xpt_server, cpu);
+ 	spin_lock_bh(&pool->sp_lock);
+ 
+@@ -390,7 +390,7 @@ static void svc_xprt_do_enqueue(struct svc_xprt *xprt)
+ 	}
+ 
+ 	spin_unlock_bh(&pool->sp_lock);
+-	put_cpu();
++	put_cpu_light();
+ }
+ 
+ /*
+diff --git a/scripts/mkcompile_h b/scripts/mkcompile_h
+index 6fdc97ef6023..523e0420d7f0 100755
+--- a/scripts/mkcompile_h
++++ b/scripts/mkcompile_h
+@@ -4,7 +4,8 @@ TARGET=$1
+ ARCH=$2
+ SMP=$3
+ PREEMPT=$4
+-CC=$5
++RT=$5
++CC=$6
+ 
+ vecho() { [ "${quiet}" = "silent_" ] || echo "$@" ; }
+ 
+@@ -57,6 +58,7 @@ UTS_VERSION="#$VERSION"
+ CONFIG_FLAGS=""
+ if [ -n "$SMP" ] ; then CONFIG_FLAGS="SMP"; fi
+ if [ -n "$PREEMPT" ] ; then CONFIG_FLAGS="$CONFIG_FLAGS PREEMPT"; fi
++if [ -n "$RT" ] ; then CONFIG_FLAGS="$CONFIG_FLAGS RT"; fi
+ UTS_VERSION="$UTS_VERSION $CONFIG_FLAGS $TIMESTAMP"
+ 
+ # Truncate to maximum length
+diff --git a/sound/core/pcm_native.c b/sound/core/pcm_native.c
+index b04802c6ffb9..9dddcf59fe0c 100644
+--- a/sound/core/pcm_native.c
++++ b/sound/core/pcm_native.c
+@@ -138,7 +138,7 @@ EXPORT_SYMBOL_GPL(snd_pcm_stream_unlock);
+ void snd_pcm_stream_lock_irq(struct snd_pcm_substream *substream)
+ {
+ 	if (!substream->pcm->nonatomic)
+-		local_irq_disable();
++		local_irq_disable_nort();
+ 	snd_pcm_stream_lock(substream);
+ }
+ EXPORT_SYMBOL_GPL(snd_pcm_stream_lock_irq);
+@@ -153,7 +153,7 @@ void snd_pcm_stream_unlock_irq(struct snd_pcm_substream *substream)
+ {
+ 	snd_pcm_stream_unlock(substream);
+ 	if (!substream->pcm->nonatomic)
+-		local_irq_enable();
++		local_irq_enable_nort();
+ }
+ EXPORT_SYMBOL_GPL(snd_pcm_stream_unlock_irq);
+ 
+@@ -161,7 +161,7 @@ unsigned long _snd_pcm_stream_lock_irqsave(struct snd_pcm_substream *substream)
+ {
+ 	unsigned long flags = 0;
+ 	if (!substream->pcm->nonatomic)
+-		local_irq_save(flags);
++		local_irq_save_nort(flags);
+ 	snd_pcm_stream_lock(substream);
+ 	return flags;
+ }
+@@ -179,7 +179,7 @@ void snd_pcm_stream_unlock_irqrestore(struct snd_pcm_substream *substream,
+ {
+ 	snd_pcm_stream_unlock(substream);
+ 	if (!substream->pcm->nonatomic)
+-		local_irq_restore(flags);
++		local_irq_restore_nort(flags);
+ }
+ EXPORT_SYMBOL_GPL(snd_pcm_stream_unlock_irqrestore);
+ 
+diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
+index e06785470408..33fb81095507 100644
+--- a/virt/kvm/async_pf.c
++++ b/virt/kvm/async_pf.c
+@@ -94,8 +94,8 @@ static void async_pf_execute(struct work_struct *work)
+ 
+ 	trace_kvm_async_pf_completed(addr, gva);
+ 
+-	if (waitqueue_active(&vcpu->wq))
+-		wake_up_interruptible(&vcpu->wq);
++	if (swaitqueue_active(&vcpu->wq))
++		swait_wake_interruptible(&vcpu->wq);
+ 
+ 	mmput(mm);
+ 	kvm_put_kvm(vcpu->kvm);
+diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
+index 3d5ae6f655df..56e12853cb55 100644
+--- a/virt/kvm/kvm_main.c
++++ b/virt/kvm/kvm_main.c
+@@ -223,7 +223,7 @@ int kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
+ 	vcpu->kvm = kvm;
+ 	vcpu->vcpu_id = id;
+ 	vcpu->pid = NULL;
+-	init_waitqueue_head(&vcpu->wq);
++	init_swait_head(&vcpu->wq);
+ 	kvm_async_pf_vcpu_init(vcpu);
+ 
+ 	page = alloc_page(GFP_KERNEL | __GFP_ZERO);
+@@ -1747,10 +1747,10 @@ EXPORT_SYMBOL_GPL(mark_page_dirty);
+  */
+ void kvm_vcpu_block(struct kvm_vcpu *vcpu)
+ {
+-	DEFINE_WAIT(wait);
++	DEFINE_SWAITER(wait);
+ 
+ 	for (;;) {
+-		prepare_to_wait(&vcpu->wq, &wait, TASK_INTERRUPTIBLE);
++		swait_prepare(&vcpu->wq, &wait, TASK_INTERRUPTIBLE);
+ 
+ 		if (kvm_arch_vcpu_runnable(vcpu)) {
+ 			kvm_make_request(KVM_REQ_UNHALT, vcpu);
+@@ -1764,7 +1764,7 @@ void kvm_vcpu_block(struct kvm_vcpu *vcpu)
+ 		schedule();
+ 	}
+ 
+-	finish_wait(&vcpu->wq, &wait);
++	swait_finish(&vcpu->wq, &wait);
+ }
+ EXPORT_SYMBOL_GPL(kvm_vcpu_block);
+ 
+@@ -1776,11 +1776,11 @@ void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
+ {
+ 	int me;
+ 	int cpu = vcpu->cpu;
+-	wait_queue_head_t *wqp;
++	struct swait_head *wqp;
+ 
+ 	wqp = kvm_arch_vcpu_wq(vcpu);
+-	if (waitqueue_active(wqp)) {
+-		wake_up_interruptible(wqp);
++	if (swaitqueue_active(wqp)) {
++		swait_wake_interruptible(wqp);
+ 		++vcpu->stat.halt_wakeup;
+ 	}
+ 
+@@ -1885,7 +1885,7 @@ void kvm_vcpu_on_spin(struct kvm_vcpu *me)
+ 				continue;
+ 			if (vcpu == me)
+ 				continue;
+-			if (waitqueue_active(&vcpu->wq) && !kvm_arch_vcpu_runnable(vcpu))
++			if (swaitqueue_active(&vcpu->wq) && !kvm_arch_vcpu_runnable(vcpu))
+ 				continue;
+ 			if (!kvm_vcpu_eligible_for_directed_yield(vcpu))
+ 				continue;
-- 
2.49.0


From 3ca4967550606a0a5797e3a6a2f103259a80f1c2 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Wed, 9 Jan 2019 14:27:23 -0800
Subject: [PATCH 03/20] convert qcom code to raw_spin_lock

---
 kernel/msm-3.18/arch/arm/mach-msm/platsmp.c   |   8 +-
 .../arch/arm64/include/asm/dma-iommu.h        |   2 +-
 kernel/msm-3.18/arch/arm64/kernel/app_api.c   |  24 +-
 kernel/msm-3.18/block/test-iosched.c          |  34 +-
 kernel/msm-3.18/drivers/bluetooth/hci_ibs.c   |  38 +-
 kernel/msm-3.18/drivers/char/adsprpc.c        | 104 +++---
 kernel/msm-3.18/drivers/char/smd_ts.c         |  10 +-
 kernel/msm-3.18/drivers/char/smd_ts.h         |   2 +-
 .../drivers/cpufreq/cpufreq_interactive.c     |  96 ++---
 .../msm-3.18/drivers/cpuidle/lpm-levels-of.c  |   2 +-
 kernel/msm-3.18/drivers/cpuidle/lpm-levels.c  |  26 +-
 kernel/msm-3.18/drivers/cpuidle/lpm-levels.h  |   2 +-
 kernel/msm-3.18/drivers/devfreq/armbw-pm.c    |  12 +-
 kernel/msm-3.18/drivers/devfreq/bimc-bwmon.c  |   8 +-
 .../drivers/devfreq/governor_bw_hwmon.c       |  10 +-
 .../drivers/devfreq/governor_msm_adreno_tz.c  |  32 +-
 kernel/msm-3.18/drivers/devfreq/m4m-hwmon.c   |   4 +-
 .../msm-3.18/drivers/edac/cortex_arm64_edac.c |  20 +-
 kernel/msm-3.18/drivers/gpio/gpio-msm-smp2p.c |  46 +--
 .../gpu/drm/bridge/dw_hdmi-ahb-audio.c        |  16 +-
 .../msm-3.18/drivers/gpu/drm/bridge/dw_hdmi.c |  16 +-
 kernel/msm-3.18/drivers/gpu/drm/drm_atomic.c  |  14 +-
 .../gpu/drm/i915/i915_guc_submission.c        |  28 +-
 .../gpu/drm/i915/intel_fifo_underrun.c        |  12 +-
 .../msm-3.18/drivers/gpu/drm/i915/intel_guc.h |   4 +-
 .../drivers/gpu/drm/i915/intel_hotplug.c      |  28 +-
 .../drivers/gpu/drm/i915/intel_runtime_pm.c   |   8 +-
 .../drivers/gpu/drm/msm/dsi/dsi_host.c        |  12 +-
 .../drivers/gpu/drm/msm/hdmi/hdmi_hdcp.c      |  46 +--
 .../drivers/gpu/drm/msm/mdp/mdp5/mdp5_ctl.c   |  46 +--
 .../msm-3.18/drivers/gpu/drm/msm/msm_atomic.c |   8 +-
 .../msm-3.18/drivers/gpu/drm/sti/sti_crtc.c   |   4 +-
 .../drivers/gpu/drm/vmwgfx/vmwgfx_cmdbuf.c    |  36 +-
 kernel/msm-3.18/drivers/input/keycombo.c      |   8 +-
 .../msm-3.18/drivers/input/misc/gpio_input.c  |  24 +-
 kernel/msm-3.18/drivers/input/misc/keychord.c |  30 +-
 .../drivers/input/touchscreen/maxim_sti.c     |  24 +-
 .../msm-3.18/drivers/iommu/dma-mapping-fast.c |  26 +-
 kernel/msm-3.18/drivers/iommu/msm_iommu-v1.c  |  48 +--
 kernel/msm-3.18/drivers/irqchip/irq-gic-v2m.c |  12 +-
 .../msm-3.18/drivers/irqchip/irq-gic-v3-its.c |  16 +-
 .../msm-3.18/drivers/irqchip/irq-mtk-sysirq.c |   8 +-
 .../msm-3.18/drivers/media/radio/radio-iris.c |   4 +-
 kernel/msm-3.18/drivers/misc/qseecom.c        | 108 +++---
 kernel/msm-3.18/drivers/misc/uid_stat.c       |   6 +-
 .../drivers/mmc/card/mmc_block_test.c         |  26 +-
 .../msm-3.18/drivers/mmc/core/ring_buffer.c   |  10 +-
 kernel/msm-3.18/drivers/mtd/ubi/fastmap-wl.c  |  32 +-
 kernel/msm-3.18/drivers/nfc/nq-nci.c          |  16 +-
 kernel/msm-3.18/drivers/pci/host/pci-msm.c    |  68 ++--
 kernel/msm-3.18/drivers/power/pmic-voter.c    |  12 +-
 kernel/msm-3.18/drivers/power/qpnp-charger.c  |   8 +-
 kernel/msm-3.18/drivers/power/qpnp-fg.c       |  12 +-
 .../drivers/power/qpnp-linear-charger.c       |  60 ++--
 .../msm-3.18/drivers/power/qpnp-smbcharger.c  |  12 +-
 kernel/msm-3.18/drivers/power/qpnp-typec.c    |  12 +-
 .../msm-3.18/drivers/power/smb1351-charger.c  |  12 +-
 .../drivers/power/smb1360-charger-fg.c        |  12 +-
 .../msm-3.18/drivers/power/smb23x-charger.c   |  12 +-
 kernel/msm-3.18/drivers/pwm/pwm-qpnp.c        |  40 +--
 .../drivers/regulator/kryo-regulator.c        |  44 +--
 .../drivers/regulator/rpm-smd-regulator.c     |   8 +-
 kernel/msm-3.18/drivers/rtc/qpnp-rtc.c        |  28 +-
 .../msm-3.18/drivers/scsi/ufs/ufs-debugfs.c   |  28 +-
 .../drivers/scsi/ufs/ufs-qcom-debugfs.c       |   4 +-
 kernel/msm-3.18/drivers/scsi/ufs/ufs-qcom.c   |  34 +-
 kernel/msm-3.18/drivers/scsi/ufs/ufs_test.c   |   8 +-
 kernel/msm-3.18/drivers/soc/qcom/bam_dmux.c   | 104 +++---
 .../drivers/soc/qcom/cache_m4m_erp64.c        |   4 +-
 .../msm-3.18/drivers/soc/qcom/event_timer.c   |  34 +-
 kernel/msm-3.18/drivers/soc/qcom/glink.c      | 310 ++++++++--------
 .../drivers/soc/qcom/glink_bgcom_xprt.c       |  28 +-
 .../msm-3.18/drivers/soc/qcom/glink_debugfs.c |  20 +-
 .../msm-3.18/drivers/soc/qcom/glink_private.h |  30 +-
 .../drivers/soc/qcom/glink_smd_xprt.c         | 134 +++----
 .../drivers/soc/qcom/glink_smem_native_xprt.c |  98 +++---
 kernel/msm-3.18/drivers/soc/qcom/glink_ssr.c  |  22 +-
 kernel/msm-3.18/drivers/soc/qcom/icnss.c      |  16 +-
 .../drivers/soc/qcom/ipc_router_mhi_xprt.c    |  22 +-
 .../drivers/soc/qcom/ipc_router_smd_xprt.c    |  30 +-
 kernel/msm-3.18/drivers/soc/qcom/jtag-mm.c    |  16 +-
 kernel/msm-3.18/drivers/soc/qcom/jtagv8-etm.c |   8 +-
 kernel/msm-3.18/drivers/soc/qcom/mpm-of.c     |  38 +-
 .../msm-3.18/drivers/soc/qcom/msm_glink_pkt.c |  46 +--
 .../drivers/soc/qcom/msm_performance.c        |  94 ++---
 .../msm-3.18/drivers/soc/qcom/msm_rq_stats.c  |  14 +-
 .../msm-3.18/drivers/soc/qcom/perf_event_l2.c |   8 +-
 .../msm-3.18/drivers/soc/qcom/qmi_interface.c |  40 +--
 .../drivers/soc/qcom/qmi_interface_priv.h     |   2 +-
 kernel/msm-3.18/drivers/soc/qcom/rpm-smd.c    |  70 ++--
 kernel/msm-3.18/drivers/soc/qcom/smd.c        |  86 ++---
 .../msm-3.18/drivers/soc/qcom/smd_private.h   |   2 +-
 kernel/msm-3.18/drivers/soc/qcom/smem.c       |   8 +-
 kernel/msm-3.18/drivers/soc/qcom/smp2p.c      | 100 +++---
 .../drivers/soc/qcom/smp2p_spinlock_test.c    |   2 +-
 .../drivers/soc/qcom/smp2p_test_common.h      |   8 +-
 .../drivers/soc/qcom/subsystem_restart.c      |  24 +-
 kernel/msm-3.18/drivers/spi/spi_qsd.c         |  14 +-
 kernel/msm-3.18/drivers/spi/spi_qsd.h         |   2 +-
 .../drivers/staging/android/oneshot_sync.c    |  20 +-
 .../msm-3.18/drivers/staging/android/sync.c   |  70 ++--
 kernel/msm-3.18/drivers/thermal/msm-tsens.c   |  50 +--
 kernel/msm-3.18/drivers/thermal/qpnp-adc-tm.c |  16 +-
 .../drivers/tty/serial/msm_serial_hs_lite.c   |  24 +-
 kernel/msm-3.18/drivers/usb/dwc3/dwc3-msm.c   |  58 +--
 .../msm-3.18/drivers/usb/gadget/ci13xxx_msm.c |   6 +-
 .../msm-3.18/drivers/usb/gadget/ci13xxx_udc.c | 194 +++++-----
 .../msm-3.18/drivers/usb/gadget/ci13xxx_udc.h |   4 +-
 kernel/msm-3.18/drivers/usb/gadget/f_ccid.c   |  44 +--
 .../drivers/usb/gadget/function/f_accessory.c |  48 +--
 .../usb/gadget/function/f_audio_source.c      |  38 +-
 .../drivers/usb/gadget/function/f_diag.c      |  90 ++---
 .../drivers/usb/gadget/function/f_gps.c       |  44 +--
 .../drivers/usb/gadget/function/f_gsi.c       | 108 +++---
 .../drivers/usb/gadget/function/f_gsi.h       |   6 +-
 .../drivers/usb/gadget/function/f_mbim.c      |  72 ++--
 .../drivers/usb/gadget/function/f_mtp.c       |  66 ++--
 .../drivers/usb/gadget/function/f_qc_rndis.c  |  58 +--
 .../drivers/usb/gadget/function/f_qdss.c      |  92 ++---
 .../drivers/usb/gadget/function/f_qdss.h      |   2 +-
 .../drivers/usb/gadget/function/f_rmnet.c     |  54 +--
 .../drivers/usb/gadget/function/u_bam.c       | 330 +++++++++---------
 .../drivers/usb/gadget/function/u_bam_data.c  | 174 ++++-----
 .../drivers/usb/gadget/function/u_ctrl_hsic.c |  36 +-
 .../drivers/usb/gadget/function/u_ctrl_qti.c  |  60 ++--
 .../usb/gadget/function/u_data_bridge.c       | 114 +++---
 .../drivers/usb/gadget/function/u_data_hsic.c | 144 ++++----
 .../drivers/usb/gadget/function/u_data_ipa.c  |  56 +--
 .../drivers/usb/gadget/function/u_qc_ether.c  |  24 +-
 .../usb/gadget/function/u_rmnet_ctrl_smd.c    |  54 +--
 .../drivers/usb/gadget/function/u_smd.c       |  94 ++---
 .../msm-3.18/drivers/usb/host/ehci-msm-hsic.c |  36 +-
 kernel/msm-3.18/drivers/usb/host/hbm.c        |   4 +-
 kernel/msm-3.18/drivers/usb/misc/ks_bridge.c  |  48 +--
 .../msm-3.18/drivers/usb/phy/otg-wakelock.c   |   6 +-
 .../msm-3.18/drivers/usb/phy/phy-msm-qusb.c   |  12 +-
 kernel/msm-3.18/fs/ext4/crypto.c              |   8 +-
 kernel/msm-3.18/fs/f2fs/extent_cache.c        |  30 +-
 kernel/msm-3.18/fs/f2fs/shrinker.c            |  24 +-
 kernel/msm-3.18/fs/f2fs/sysfs.c               |   6 +-
 kernel/msm-3.18/fs/f2fs/trace.c               |  12 +-
 kernel/msm-3.18/fs/fuse/shortcircuit.c        |   4 +-
 .../include/linux/clk/msm-clk-provider.h      |   4 +-
 .../msm-3.18/include/linux/dma-mapping-fast.h |   2 +-
 kernel/msm-3.18/include/linux/fscrypt_supp.h  |   4 +-
 kernel/msm-3.18/include/linux/ipc_router.h    |   2 +-
 .../msm-3.18/include/linux/mmc/ring_buffer.h  |   2 +-
 .../msm-3.18/include/linux/remote_spinlock.h  |  16 +-
 kernel/msm-3.18/include/linux/rq_stats.h      |   2 +-
 kernel/msm-3.18/include/linux/sync.h          |   6 +-
 kernel/msm-3.18/include/linux/test-iosched.h  |   2 +-
 .../include/soc/qcom/msm_qmi_interface.h      |   2 +-
 kernel/msm-3.18/include/sound/q6afe-v2.h      |   2 +-
 kernel/msm-3.18/include/sound/q6asm-v2.h      |   4 +-
 kernel/msm-3.18/kernel/kcov.c                 |  20 +-
 kernel/msm-3.18/kernel/power/wakeup_reason.c  |  26 +-
 kernel/msm-3.18/kernel/sched/core_ctl.c       |  60 ++--
 kernel/msm-3.18/kernel/sched/qhmp_core.c      |  18 +-
 kernel/msm-3.18/kernel/sched/qhmp_fair.c      |  24 +-
 kernel/msm-3.18/kernel/sched/qhmp_sched.h     |  14 +-
 kernel/msm-3.18/kernel/sched/sched_avg.c      |   8 +-
 kernel/msm-3.18/kernel/time/tick-sched.c      |   2 +-
 kernel/msm-3.18/kernel/time/timer.c.rej       |   8 +-
 kernel/msm-3.18/kernel/trace/ipc_logging.c    |  22 +-
 .../kernel/trace/ipc_logging_private.h        |   2 +-
 .../kernel/trace/trace_cpu_freq_switch.c      |  26 +-
 kernel/msm-3.18/lib/ubsan.c                   |   4 +-
 kernel/msm-3.18/mm/cma.h                      |   2 +-
 kernel/msm-3.18/mm/cma_debug.c                |   8 +-
 kernel/msm-3.18/mm/swap_ratio.c               |  22 +-
 kernel/msm-3.18/mm/zcache.c                   |  40 +--
 kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6afe.c |   2 +-
 kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6asm.c |  40 +--
 173 files changed, 2920 insertions(+), 2920 deletions(-)

diff --git a/kernel/msm-3.18/arch/arm/mach-msm/platsmp.c b/kernel/msm-3.18/arch/arm/mach-msm/platsmp.c
index 649ba6cc3..79864cc39 100644
--- a/kernel/msm-3.18/arch/arm/mach-msm/platsmp.c
+++ b/kernel/msm-3.18/arch/arm/mach-msm/platsmp.c
@@ -58,8 +58,8 @@ void __cpuinit msm_secondary_init(unsigned int cpu)
 	/*
 	 * Synchronise with the boot thread.
 	 */
-	spin_lock(&boot_lock);
-	spin_unlock(&boot_lock);
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 }
 
 static int __cpuinit release_secondary_sim(unsigned long base, unsigned int cpu)
@@ -122,7 +122,7 @@ static int __cpuinit release_from_pen(unsigned int cpu)
 	 * set synchronisation state between this boot processor
 	 * and the secondary one
 	 */
-	spin_lock(&boot_lock);
+	raw_spin_lock(&boot_lock);
 
 	/*
 	 * The secondary processor is waiting to be released from
@@ -154,7 +154,7 @@ static int __cpuinit release_from_pen(unsigned int cpu)
 	 * now the secondary core is starting up let it run its
 	 * calibrations, then wait for it to finish
 	 */
-	spin_unlock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
 
 	return pen_release != -1 ? -ENOSYS : 0;
 }
diff --git a/kernel/msm-3.18/arch/arm64/include/asm/dma-iommu.h b/kernel/msm-3.18/arch/arm64/include/asm/dma-iommu.h
index c16cf151f..db427dbfc 100644
--- a/kernel/msm-3.18/arch/arm64/include/asm/dma-iommu.h
+++ b/kernel/msm-3.18/arch/arm64/include/asm/dma-iommu.h
@@ -18,7 +18,7 @@ struct dma_iommu_mapping {
 	size_t			bits;
 	dma_addr_t		base;
 
-	spinlock_t		lock;
+	raw_spinlock_t		lock;
 	struct kref		kref;
 #ifdef CONFIG_IOMMU_IO_PGTABLE_FAST
 	struct dma_fast_smmu_mapping *fast;
diff --git a/kernel/msm-3.18/arch/arm64/kernel/app_api.c b/kernel/msm-3.18/arch/arm64/kernel/app_api.c
index e995bbf3c..ba3c5d8b7 100644
--- a/kernel/msm-3.18/arch/arm64/kernel/app_api.c
+++ b/kernel/msm-3.18/arch/arm64/kernel/app_api.c
@@ -17,8 +17,8 @@
 
 #include <asm/app_api.h>
 
-static spinlock_t spinlock;
-static spinlock_t spinlock_32bit_app;
+static raw_spinlock_t spinlock;
+static raw_spinlock_t spinlock_32bit_app;
 static DEFINE_PER_CPU(int, app_config_applied);
 static unsigned long app_config_set[NR_CPUS];
 static unsigned long app_config_clear[NR_CPUS];
@@ -29,7 +29,7 @@ void set_app_setting_bit(uint32_t bit)
 	uint64_t reg;
 	int cpu;
 
-	spin_lock_irqsave(&spinlock, flags);
+	raw_spin_lock_irqsave(&spinlock, flags);
 	asm volatile("mrs %0, S3_1_C15_C15_0" : "=r" (reg));
 	reg = reg | BIT(bit);
 	isb();
@@ -41,7 +41,7 @@ void set_app_setting_bit(uint32_t bit)
 
 		this_cpu_write(app_config_applied, 1);
 	}
-	spin_unlock_irqrestore(&spinlock, flags);
+	raw_spin_unlock_irqrestore(&spinlock, flags);
 
 }
 EXPORT_SYMBOL(set_app_setting_bit);
@@ -52,7 +52,7 @@ void clear_app_setting_bit(uint32_t bit)
 	uint64_t reg;
 	int cpu;
 
-	spin_lock_irqsave(&spinlock, flags);
+	raw_spin_lock_irqsave(&spinlock, flags);
 	asm volatile("mrs %0, S3_1_C15_C15_0" : "=r" (reg));
 	reg = reg & ~BIT(bit);
 	isb();
@@ -64,7 +64,7 @@ void clear_app_setting_bit(uint32_t bit)
 
 		this_cpu_write(app_config_applied, 0);
 	}
-	spin_unlock_irqrestore(&spinlock, flags);
+	raw_spin_unlock_irqrestore(&spinlock, flags);
 }
 EXPORT_SYMBOL(clear_app_setting_bit);
 
@@ -73,7 +73,7 @@ void set_app_setting_bit_for_32bit_apps(void)
 	unsigned long flags;
 	uint64_t reg;
 
-	spin_lock_irqsave(&spinlock_32bit_app, flags);
+	raw_spin_lock_irqsave(&spinlock_32bit_app, flags);
 	if (use_32bit_app_setting) {
 		asm volatile("mrs %0, S3_0_c15_c15_0 " : "=r" (reg));
 		reg = reg | BIT(24);
@@ -92,7 +92,7 @@ void set_app_setting_bit_for_32bit_apps(void)
 		asm volatile("msr S3_0_c15_c15_1, %0" : : "r" (reg));
 		isb();
 	}
-	spin_unlock_irqrestore(&spinlock_32bit_app, flags);
+	raw_spin_unlock_irqrestore(&spinlock_32bit_app, flags);
 }
 EXPORT_SYMBOL(set_app_setting_bit_for_32bit_apps);
 
@@ -101,7 +101,7 @@ void clear_app_setting_bit_for_32bit_apps(void)
 	unsigned long flags;
 	uint64_t reg;
 
-	spin_lock_irqsave(&spinlock_32bit_app, flags);
+	raw_spin_lock_irqsave(&spinlock_32bit_app, flags);
 	if (use_32bit_app_setting) {
 		asm volatile("mrs %0, S3_0_c15_c15_0 " : "=r" (reg));
 		reg = reg & ~BIT(24);
@@ -122,14 +122,14 @@ void clear_app_setting_bit_for_32bit_apps(void)
 		asm volatile("msr S3_0_c15_c15_1, %0" : : "r" (reg));
 		isb();
 	}
-	spin_unlock_irqrestore(&spinlock_32bit_app, flags);
+	raw_spin_unlock_irqrestore(&spinlock_32bit_app, flags);
 }
 EXPORT_SYMBOL(clear_app_setting_bit_for_32bit_apps);
 
 static int __init init_app_api(void)
 {
-	spin_lock_init(&spinlock);
-	spin_lock_init(&spinlock_32bit_app);
+	raw_spin_lock_init(&spinlock);
+	raw_spin_lock_init(&spinlock_32bit_app);
 	return 0;
 }
 early_initcall(init_app_api);
diff --git a/kernel/msm-3.18/block/test-iosched.c b/kernel/msm-3.18/block/test-iosched.c
index 2a0be42dd..9e33d49e6 100644
--- a/kernel/msm-3.18/block/test-iosched.c
+++ b/kernel/msm-3.18/block/test-iosched.c
@@ -231,10 +231,10 @@ int test_iosched_add_unique_test_req(struct test_iosched *tios,
 		"%s: added request %d to the test requests list, type = %d",
 		__func__, test_rq->req_id, req_unique);
 
-	spin_lock_irqsave(tios->req_q->queue_lock, flags);
+	raw_spin_lock_irqsave(tios->req_q->queue_lock, flags);
 	list_add_tail(&test_rq->queuelist, &tios->test_queue);
 	tios->test_count++;
-	spin_unlock_irqrestore(tios->req_q->queue_lock, flags);
+	raw_spin_unlock_irqrestore(tios->req_q->queue_lock, flags);
 
 	return 0;
 }
@@ -414,10 +414,10 @@ int test_iosched_add_wr_rd_test_req(struct test_iosched *tios,
 	test_rq = test_iosched_create_test_req(tios, is_err_expcted, direction,
 		start_sec, num_bios, pattern, end_req_io);
 	if (test_rq) {
-		spin_lock_irqsave(tios->req_q->queue_lock, flags);
+		raw_spin_lock_irqsave(tios->req_q->queue_lock, flags);
 		list_add_tail(&test_rq->queuelist, &tios->test_queue);
 		tios->test_count++;
-		spin_unlock_irqrestore(tios->req_q->queue_lock, flags);
+		raw_spin_unlock_irqrestore(tios->req_q->queue_lock, flags);
 		return 0;
 	}
 	return -ENODEV;
@@ -692,20 +692,20 @@ int test_iosched_start_test(struct test_iosched *tios,
 			 */
 			msleep(2000);
 
-		spin_lock(&tios->lock);
+		raw_spin_lock(&tios->lock);
 
 		if (tios->test_state != TEST_IDLE) {
 			pr_info(
 				"%s: Another test is running, try again later",
 				__func__);
-			spin_unlock(&tios->lock);
+			raw_spin_unlock(&tios->lock);
 			return -EBUSY;
 		}
 
 		if (tios->start_sector == 0) {
 			pr_err("%s: Invalid start sector", __func__);
 			tios->test_result = TEST_FAILED;
-			spin_unlock(&tios->lock);
+			raw_spin_unlock(&tios->lock);
 			return -EINVAL;
 		}
 
@@ -722,7 +722,7 @@ int test_iosched_start_test(struct test_iosched *tios,
 
 		tios->test_state = TEST_RUNNING;
 
-		spin_unlock(&tios->lock);
+		raw_spin_unlock(&tios->lock);
 		/*
 		 * Give an already dispatch request from
 		 * FS a chanse to complete
@@ -981,21 +981,21 @@ static int test_dispatch_from(struct request_queue *q,
 	if (!tios)
 		goto err;
 
-	spin_lock_irqsave(&tios->lock, flags);
+	raw_spin_lock_irqsave(&tios->lock, flags);
 	if (!list_empty(queue)) {
 		test_rq = list_entry(queue->next, struct test_request,
 				queuelist);
 		rq = test_rq->rq;
 		if (!rq) {
 			pr_err("%s: null request,return", __func__);
-			spin_unlock_irqrestore(&tios->lock, flags);
+			raw_spin_unlock_irqrestore(&tios->lock, flags);
 			goto err;
 		}
 		list_move_tail(&test_rq->queuelist,
 			&tios->dispatched_queue);
 		tios->dispatched_count++;
 		(*count)--;
-		spin_unlock_irqrestore(&tios->lock, flags);
+		raw_spin_unlock_irqrestore(&tios->lock, flags);
 
 		print_req(rq);
 		elv_dispatch_sort(q, rq);
@@ -1003,7 +1003,7 @@ static int test_dispatch_from(struct request_queue *q,
 		ret = 1;
 		goto err;
 	}
-	spin_unlock_irqrestore(&tios->lock, flags);
+	raw_spin_unlock_irqrestore(&tios->lock, flags);
 
 err:
 	return ret;
@@ -1130,7 +1130,7 @@ static int test_init_queue(struct request_queue *q, struct elevator_type *e)
 	init_waitqueue_head(&tios->wait_q);
 	tios->req_q = q;
 
-	spin_lock_init(&tios->lock);
+	raw_spin_lock_init(&tios->lock);
 
 	ret = test_debugfs_init(tios);
 	if (ret) {
@@ -1171,9 +1171,9 @@ static int test_init_queue(struct request_queue *q, struct elevator_type *e)
 		}
 	}
 
-	spin_lock_irqsave(q->queue_lock, flags);
+	raw_spin_lock_irqsave(q->queue_lock, flags);
 	q->elevator = eq;
-	spin_unlock_irqrestore(q->queue_lock, flags);
+	raw_spin_unlock_irqrestore(q->queue_lock, flags);
 
 	return 0;
 
@@ -1217,11 +1217,11 @@ void test_iosched_add_urgent_req(struct test_iosched *tios,
 	if (!tios)
 		return;
 
-	spin_lock_irqsave(&tios->lock, flags);
+	raw_spin_lock_irqsave(&tios->lock, flags);
 	test_rq->rq->cmd_flags |= REQ_URGENT;
 	list_add_tail(&test_rq->queuelist, &tios->urgent_queue);
 	tios->urgent_count++;
-	spin_unlock_irqrestore(&tios->lock, flags);
+	raw_spin_unlock_irqrestore(&tios->lock, flags);
 }
 EXPORT_SYMBOL(test_iosched_add_urgent_req);
 
diff --git a/kernel/msm-3.18/drivers/bluetooth/hci_ibs.c b/kernel/msm-3.18/drivers/bluetooth/hci_ibs.c
index 40b60b018..cf03784d6 100644
--- a/kernel/msm-3.18/drivers/bluetooth/hci_ibs.c
+++ b/kernel/msm-3.18/drivers/bluetooth/hci_ibs.c
@@ -103,7 +103,7 @@ struct ibs_struct {
 	struct sk_buff *rx_skb;
 	struct sk_buff_head txq;
 	struct sk_buff_head tx_wait_q;	/* HCI_IBS wait queue	*/
-	spinlock_t hci_ibs_lock;	/* HCI_IBS state lock	*/
+	raw_spinlock_t hci_ibs_lock;	/* HCI_IBS state lock	*/
 	unsigned long tx_ibs_state;	/* HCI_IBS transmit side power state */
 	unsigned long rx_ibs_state;	/* HCI_IBS receive side power state */
 	unsigned long tx_vote;		/* clock must be on for TX */
@@ -259,7 +259,7 @@ static void ibs_wq_awake_device(struct work_struct *work)
 	/* Vote for serial clock */
 	ibs_msm_serial_clock_vote(HCI_IBS_TX_VOTE_CLOCK_ON, hu);
 
-	spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
+	raw_spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
 
 	/* send wake indication to device */
 	if (send_hci_ibs_cmd(HCI_IBS_WAKE_IND, hu) < 0)
@@ -270,7 +270,7 @@ static void ibs_wq_awake_device(struct work_struct *work)
 	/* start retransmit timer */
 	mod_timer(&ibs->wake_retrans_timer, jiffies + msecs_to_jiffies(10));
 
-	spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
+	raw_spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
 
 }
 
@@ -285,7 +285,7 @@ static void ibs_wq_awake_rx(struct work_struct *work)
 
 	ibs_msm_serial_clock_vote(HCI_IBS_RX_VOTE_CLOCK_ON, hu);
 
-	spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
+	raw_spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
 
 	ibs->rx_ibs_state = HCI_IBS_RX_AWAKE;
 	/* Always acknowledge device wake up,
@@ -296,7 +296,7 @@ static void ibs_wq_awake_rx(struct work_struct *work)
 
 	ibs->ibs_sent_wacks++; /* debug */
 
-	spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
+	raw_spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
 
 	/* actually send the packets */
 	hci_uart_tx_wakeup(hu);
@@ -339,7 +339,7 @@ static void hci_ibs_tx_idle_timeout(unsigned long arg)
 
 	BT_DBG("hu %pK idle timeout in %lu state", hu, ibs->tx_ibs_state);
 
-	spin_lock_irqsave_nested(&ibs->hci_ibs_lock,
+	raw_spin_lock_irqsave_nested(&ibs->hci_ibs_lock,
 					flags, SINGLE_DEPTH_NESTING);
 
 	switch (ibs->tx_ibs_state) {
@@ -361,7 +361,7 @@ static void hci_ibs_tx_idle_timeout(unsigned long arg)
 	queue_work(ibs->workqueue, &ibs->ws_tx_vote_off);
 
 out:
-	spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
+	raw_spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
 }
 
 static void hci_ibs_wake_retrans_timeout(unsigned long arg)
@@ -374,7 +374,7 @@ static void hci_ibs_wake_retrans_timeout(unsigned long arg)
 	BT_DBG("hu %pK wake retransmit timeout in %lu state",
 		hu, ibs->tx_ibs_state);
 
-	spin_lock_irqsave_nested(&ibs->hci_ibs_lock,
+	raw_spin_lock_irqsave_nested(&ibs->hci_ibs_lock,
 					flags, SINGLE_DEPTH_NESTING);
 
 	switch (ibs->tx_ibs_state) {
@@ -394,7 +394,7 @@ static void hci_ibs_wake_retrans_timeout(unsigned long arg)
 		break;
 	}
 out:
-	spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
+	raw_spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
 	if (retransmit)
 		hci_uart_tx_wakeup(hu);
 }
@@ -412,7 +412,7 @@ static int ibs_open(struct hci_uart *hu)
 
 	skb_queue_head_init(&ibs->txq);
 	skb_queue_head_init(&ibs->tx_wait_q);
-	spin_lock_init(&ibs->hci_ibs_lock);
+	raw_spin_lock_init(&ibs->hci_ibs_lock);
 	ibs->workqueue = create_singlethread_workqueue("ibs_wq");
 	if (!ibs->workqueue) {
 		BT_ERR("IBS Workqueue not initialized properly");
@@ -545,7 +545,7 @@ static void ibs_device_want_to_wakeup(struct hci_uart *hu)
 	BT_DBG("hu %pK", hu);
 
 	/* lock hci_ibs state */
-	spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
+	raw_spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
 
 	/* debug */
 	ibs->ibs_recv_wakes++;
@@ -557,7 +557,7 @@ static void ibs_device_want_to_wakeup(struct hci_uart *hu)
 		 */
 		/* awake rx clock */
 		queue_work(ibs->workqueue, &ibs->ws_awake_rx);
-		spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
+		raw_spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
 		return;
 	case HCI_IBS_RX_AWAKE:
 		/* Always acknowledge device wake up,
@@ -577,7 +577,7 @@ static void ibs_device_want_to_wakeup(struct hci_uart *hu)
 	}
 
 out:
-	spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
+	raw_spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
 
 	/* actually send the packets */
 	hci_uart_tx_wakeup(hu);
@@ -594,7 +594,7 @@ static void ibs_device_want_to_sleep(struct hci_uart *hu)
 	BT_DBG("hu %pK", hu);
 
 	/* lock hci_ibs state */
-	spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
+	raw_spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
 
 	/* debug */
 	ibs->ibs_recv_slps++;
@@ -615,7 +615,7 @@ static void ibs_device_want_to_sleep(struct hci_uart *hu)
 		break;
 	}
 
-	spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
+	raw_spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
 }
 
 /*
@@ -630,7 +630,7 @@ static void ibs_device_woke_up(struct hci_uart *hu)
 	BT_DBG("hu %pK", hu);
 
 	/* lock hci_ibs state */
-	spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
+	raw_spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
 
 	/* debug */
 	ibs->ibs_recv_wacks++;
@@ -659,7 +659,7 @@ static void ibs_device_woke_up(struct hci_uart *hu)
 		ibs->tx_ibs_state = HCI_IBS_TX_AWAKE;
 	}
 
-	spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
+	raw_spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
 
 	/* actually send the packets */
 	hci_uart_tx_wakeup(hu);
@@ -678,7 +678,7 @@ static int ibs_enqueue(struct hci_uart *hu, struct sk_buff *skb)
 	memcpy(skb_push(skb, 1), &bt_cb(skb)->pkt_type, 1);
 
 	/* lock hci_ibs state */
-	spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
+	raw_spin_lock_irqsave(&ibs->hci_ibs_lock, flags);
 
 	/* act according to current state */
 	switch (ibs->tx_ibs_state) {
@@ -712,7 +712,7 @@ static int ibs_enqueue(struct hci_uart *hu, struct sk_buff *skb)
 		break;
 	}
 
-	spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
+	raw_spin_unlock_irqrestore(&ibs->hci_ibs_lock, flags);
 
 	return 0;
 }
diff --git a/kernel/msm-3.18/drivers/char/adsprpc.c b/kernel/msm-3.18/drivers/char/adsprpc.c
index 2e36becd2..7e5b375be 100644
--- a/kernel/msm-3.18/drivers/char/adsprpc.c
+++ b/kernel/msm-3.18/drivers/char/adsprpc.c
@@ -210,7 +210,7 @@ struct fastrpc_apps {
 	dev_t dev_no;
 	int compat;
 	struct hlist_head drivers;
-	spinlock_t hlock;
+	raw_spinlock_t hlock;
 	struct ion_client *client;
 	struct device *dev;
 	struct device *modem_cma_dev;
@@ -238,7 +238,7 @@ struct fastrpc_mmap {
 
 struct fastrpc_file {
 	struct hlist_node hn;
-	spinlock_t hlock;
+	raw_spinlock_t hlock;
 	struct hlist_head maps;
 	struct hlist_head bufs;
 	struct fastrpc_ctx_lst clst;
@@ -281,9 +281,9 @@ static void fastrpc_buf_free(struct fastrpc_buf *buf, int cache)
 	if (!fl)
 		return;
 	if (cache) {
-		spin_lock(&fl->hlock);
+		raw_spin_lock(&fl->hlock);
 		hlist_add_head(&buf->hn, &fl->bufs);
-		spin_unlock(&fl->hlock);
+		raw_spin_unlock(&fl->hlock);
 		return;
 	}
 	if (!IS_ERR_OR_NULL(buf->virt)) {
@@ -311,13 +311,13 @@ static void fastrpc_buf_list_free(struct fastrpc_file *fl)
 	do {
 		struct hlist_node *n;
 		free = 0;
-		spin_lock(&fl->hlock);
+		raw_spin_lock(&fl->hlock);
 		hlist_for_each_entry_safe(buf, n, &fl->bufs, hn) {
 			hlist_del_init(&buf->hn);
 			free = buf;
 			break;
 		}
-		spin_unlock(&fl->hlock);
+		raw_spin_unlock(&fl->hlock);
 		if (free)
 			fastrpc_buf_free(free, 0);
 	} while (free);
@@ -328,15 +328,15 @@ static void fastrpc_mmap_add(struct fastrpc_mmap *map)
 	if (map->flags == ADSP_MMAP_HEAP_ADDR) {
 		struct fastrpc_apps *me = &gfa;
 
-		spin_lock(&me->hlock);
+		raw_spin_lock(&me->hlock);
 		hlist_add_head(&map->hn, &me->maps);
-		spin_unlock(&me->hlock);
+		raw_spin_unlock(&me->hlock);
 	} else {
 		struct fastrpc_file *fl = map->fl;
 
-		spin_lock(&fl->hlock);
+		raw_spin_lock(&fl->hlock);
 		hlist_add_head(&map->hn, &fl->maps);
-		spin_unlock(&fl->hlock);
+		raw_spin_unlock(&fl->hlock);
 	}
 }
 
@@ -347,7 +347,7 @@ static int fastrpc_mmap_find(struct fastrpc_file *fl, int fd, uintptr_t va,
 	struct fastrpc_mmap *match = 0, *map;
 	struct hlist_node *n;
 	if (mflags == ADSP_MMAP_HEAP_ADDR) {
-		spin_lock(&me->hlock);
+		raw_spin_lock(&me->hlock);
 		hlist_for_each_entry_safe(map, n, &me->maps, hn) {
 			if (va >= map->va &&
 				va + len <= map->va + map->len &&
@@ -357,9 +357,9 @@ static int fastrpc_mmap_find(struct fastrpc_file *fl, int fd, uintptr_t va,
 				break;
 			}
 		}
-		spin_unlock(&me->hlock);
+		raw_spin_unlock(&me->hlock);
 	} else {
-		spin_lock(&fl->hlock);
+		raw_spin_lock(&fl->hlock);
 		hlist_for_each_entry_safe(map, n, &fl->maps, hn) {
 			if (va >= map->va &&
 				va + len <= map->va + map->len &&
@@ -369,7 +369,7 @@ static int fastrpc_mmap_find(struct fastrpc_file *fl, int fd, uintptr_t va,
 				break;
 			}
 		}
-		spin_unlock(&fl->hlock);
+		raw_spin_unlock(&fl->hlock);
 	}
 	if (match) {
 		*ppmap = match;
@@ -407,7 +407,7 @@ static int fastrpc_mmap_remove(struct fastrpc_file *fl, uintptr_t va,
 	struct hlist_node *n;
 	struct fastrpc_apps *me = &gfa;
 
-	spin_lock(&me->hlock);
+	raw_spin_lock(&me->hlock);
 	hlist_for_each_entry_safe(map, n, &me->maps, hn) {
 		if (map->raddr == va &&
 			map->raddr + map->len == va + len &&
@@ -417,12 +417,12 @@ static int fastrpc_mmap_remove(struct fastrpc_file *fl, uintptr_t va,
 			break;
 		}
 	}
-	spin_unlock(&me->hlock);
+	raw_spin_unlock(&me->hlock);
 	if (match) {
 		*ppmap = match;
 		return 0;
 	}
-	spin_lock(&fl->hlock);
+	raw_spin_lock(&fl->hlock);
 	hlist_for_each_entry_safe(map, n, &fl->maps, hn) {
 		if (map->raddr == va &&
 			map->raddr + map->len == va + len &&
@@ -432,7 +432,7 @@ static int fastrpc_mmap_remove(struct fastrpc_file *fl, uintptr_t va,
 			break;
 		}
 	}
-	spin_unlock(&fl->hlock);
+	raw_spin_unlock(&fl->hlock);
 	if (match) {
 		*ppmap = match;
 		return 0;
@@ -450,17 +450,17 @@ static void fastrpc_mmap_free(struct fastrpc_mmap *map)
 		return;
 	fl = map->fl;
 	if (map->flags == ADSP_MMAP_HEAP_ADDR) {
-		spin_lock(&me->hlock);
+		raw_spin_lock(&me->hlock);
 		map->refs--;
 		if (!map->refs)
 			hlist_del_init(&map->hn);
-		spin_unlock(&me->hlock);
+		raw_spin_unlock(&me->hlock);
 	} else {
-		spin_lock(&fl->hlock);
+		raw_spin_lock(&fl->hlock);
 		map->refs--;
 		if (!map->refs)
 			hlist_del_init(&map->hn);
-		spin_unlock(&fl->hlock);
+		raw_spin_unlock(&fl->hlock);
 	}
 	if (map->refs > 0)
 		return;
@@ -620,14 +620,14 @@ static int fastrpc_buf_alloc(struct fastrpc_file *fl, ssize_t size,
 		goto bail;
 
 	/* find the smallest buffer that fits in the cache */
-	spin_lock(&fl->hlock);
+	raw_spin_lock(&fl->hlock);
 	hlist_for_each_entry_safe(buf, n, &fl->bufs, hn) {
 		if (buf->size >= size && (!fr || fr->size > buf->size))
 			fr = buf;
 	}
 	if (fr)
 		hlist_del_init(&fr->hn);
-	spin_unlock(&fl->hlock);
+	raw_spin_unlock(&fl->hlock);
 	if (fr) {
 		*obuf = fr;
 		return 0;
@@ -682,7 +682,7 @@ static int context_restore_interrupted(struct fastrpc_file *fl,
 	struct smq_invoke_ctx *ctx = 0, *ictx = 0;
 	struct hlist_node *n;
 	struct fastrpc_ioctl_invoke *invoke = &invokefd->inv;
-	spin_lock(&fl->hlock);
+	raw_spin_lock(&fl->hlock);
 	hlist_for_each_entry_safe(ictx, n, &fl->clst.interrupted, hn) {
 		if (ictx->pid == current->pid) {
 			if (invoke->sc != ictx->sc || ictx->fl != fl)
@@ -695,7 +695,7 @@ static int context_restore_interrupted(struct fastrpc_file *fl,
 			break;
 		}
 	}
-	spin_unlock(&fl->hlock);
+	raw_spin_unlock(&fl->hlock);
 	if (ctx)
 		*po = ctx;
 	return err;
@@ -829,9 +829,9 @@ static int context_alloc(struct fastrpc_file *fl, uint32_t kernel,
 	ctx->tgid = current->tgid;
 	init_completion(&ctx->work);
 
-	spin_lock(&fl->hlock);
+	raw_spin_lock(&fl->hlock);
 	hlist_add_head(&ctx->hn, &clst->pending);
-	spin_unlock(&fl->hlock);
+	raw_spin_unlock(&fl->hlock);
 
 	*po = ctx;
 bail:
@@ -843,10 +843,10 @@ bail:
 static void context_save_interrupted(struct smq_invoke_ctx *ctx)
 {
 	struct fastrpc_ctx_lst *clst = &ctx->fl->clst;
-	spin_lock(&ctx->fl->hlock);
+	raw_spin_lock(&ctx->fl->hlock);
 	hlist_del_init(&ctx->hn);
 	hlist_add_head(&ctx->hn, &clst->interrupted);
-	spin_unlock(&ctx->fl->hlock);
+	raw_spin_unlock(&ctx->fl->hlock);
 	/* free the cache on power collapse */
 	fastrpc_buf_list_free(ctx->fl);
 }
@@ -856,9 +856,9 @@ static void context_free(struct smq_invoke_ctx *ctx)
 	int i;
 	int nbufs = REMOTE_SCALARS_INBUFS(ctx->sc) +
 		    REMOTE_SCALARS_OUTBUFS(ctx->sc);
-	spin_lock(&ctx->fl->hlock);
+	raw_spin_lock(&ctx->fl->hlock);
 	hlist_del_init(&ctx->hn);
-	spin_unlock(&ctx->fl->hlock);
+	raw_spin_unlock(&ctx->fl->hlock);
 	for (i = 0; i < nbufs; ++i)
 		fastrpc_mmap_free(ctx->maps[i]);
 	fastrpc_buf_free(ctx->buf, 1);
@@ -876,14 +876,14 @@ static void fastrpc_notify_users(struct fastrpc_file *me)
 {
 	struct smq_invoke_ctx *ictx;
 	struct hlist_node *n;
-	spin_lock(&me->hlock);
+	raw_spin_lock(&me->hlock);
 	hlist_for_each_entry_safe(ictx, n, &me->clst.pending, hn) {
 		complete(&ictx->work);
 	}
 	hlist_for_each_entry_safe(ictx, n, &me->clst.interrupted, hn) {
 		complete(&ictx->work);
 	}
-	spin_unlock(&me->hlock);
+	raw_spin_unlock(&me->hlock);
 
 }
 
@@ -891,12 +891,12 @@ static void fastrpc_notify_drivers(struct fastrpc_apps *me, int cid)
 {
 	struct fastrpc_file *fl;
 	struct hlist_node *n;
-	spin_lock(&me->hlock);
+	raw_spin_lock(&me->hlock);
 	hlist_for_each_entry_safe(fl, n, &me->drivers, hn) {
 		if (fl->cid == cid)
 			fastrpc_notify_users(fl);
 	}
-	spin_unlock(&me->hlock);
+	raw_spin_unlock(&me->hlock);
 
 }
 static void context_list_ctor(struct fastrpc_ctx_lst *me)
@@ -912,25 +912,25 @@ static void fastrpc_context_list_dtor(struct fastrpc_file *fl)
 	struct hlist_node *n;
 	do {
 		ctxfree = 0;
-		spin_lock(&fl->hlock);
+		raw_spin_lock(&fl->hlock);
 		hlist_for_each_entry_safe(ictx, n, &clst->interrupted, hn) {
 			hlist_del_init(&ictx->hn);
 			ctxfree = ictx;
 			break;
 		}
-		spin_unlock(&fl->hlock);
+		raw_spin_unlock(&fl->hlock);
 		if (ctxfree)
 			context_free(ctxfree);
 	} while (ctxfree);
 	do {
 		ctxfree = 0;
-		spin_lock(&fl->hlock);
+		raw_spin_lock(&fl->hlock);
 		hlist_for_each_entry_safe(ictx, n, &clst->pending, hn) {
 			hlist_del_init(&ictx->hn);
 			ctxfree = ictx;
 			break;
 		}
-		spin_unlock(&fl->hlock);
+		raw_spin_unlock(&fl->hlock);
 		if (ctxfree)
 			context_free(ctxfree);
 	} while (ctxfree);
@@ -943,13 +943,13 @@ static void fastrpc_file_list_dtor(struct fastrpc_apps *me)
 	struct hlist_node *n;
 	do {
 		free = 0;
-		spin_lock(&me->hlock);
+		raw_spin_lock(&me->hlock);
 		hlist_for_each_entry_safe(fl, n, &me->drivers, hn) {
 			hlist_del_init(&fl->hn);
 			free = fl;
 			break;
 		}
-		spin_unlock(&me->hlock);
+		raw_spin_unlock(&me->hlock);
 		if (free)
 			fastrpc_file_free(free);
 	} while (free);
@@ -1269,11 +1269,11 @@ static int fastrpc_invoke_send(struct smq_invoke_ctx *ctx,
 			(void *)&fl->apps->channel[fl->cid], msg, sizeof(*msg),
 			GLINK_TX_REQ_INTENT);
 	} else {
-		spin_lock(&fl->apps->hlock);
+		raw_spin_lock(&fl->apps->hlock);
 		len = smd_write((smd_channel_t *)
 				fl->apps->channel[fl->cid].chan,
 				msg, sizeof(*msg));
-		spin_unlock(&fl->apps->hlock);
+		raw_spin_unlock(&fl->apps->hlock);
 		VERIFY(err, len == sizeof(*msg));
 	}
  bail:
@@ -1318,7 +1318,7 @@ static void fastrpc_init(struct fastrpc_apps *me)
 	int i;
 	INIT_HLIST_HEAD(&me->drivers);
 	INIT_HLIST_HEAD(&me->fls);
-	spin_lock_init(&me->hlock);
+	raw_spin_lock_init(&me->hlock);
 	mutex_init(&me->smd_mutex);
 	me->channel = &gcinfo[0];
 	for (i = 0; i < NUM_CHANNELS; i++) {
@@ -1646,13 +1646,13 @@ static int fastrpc_mmap_remove_ssr(struct fastrpc_file *fl)
 	struct fastrpc_apps *me = &gfa;
 	struct ramdump_segment *ramdump_segments_rh = NULL;
 
-	spin_lock(&me->hlock);
+	raw_spin_lock(&me->hlock);
 	hlist_for_each_entry_safe(map, n, &me->maps, hn) {
 			match = map;
 			hlist_del_init(&map->hn);
 			break;
 	}
-	spin_unlock(&me->hlock);
+	raw_spin_unlock(&me->hlock);
 
 	if (match) {
 		VERIFY(err, !fastrpc_munmap_on_dsp_rh(fl, match));
@@ -1759,9 +1759,9 @@ static int fastrpc_file_free(struct fastrpc_file *fl)
 		return 0;
 	cid = fl->cid;
 
-	spin_lock(&fl->apps->hlock);
+	raw_spin_lock(&fl->apps->hlock);
 	hlist_del_init(&fl->hn);
-	spin_unlock(&fl->apps->hlock);
+	raw_spin_unlock(&fl->apps->hlock);
 
 	if (!fl->sctx) {
 		kfree(fl);
@@ -2024,7 +2024,7 @@ static int fastrpc_device_open(struct inode *inode, struct file *filp)
 	mutex_lock(&me->smd_mutex);
 
 	context_list_ctor(&fl->clst);
-	spin_lock_init(&fl->hlock);
+	raw_spin_lock_init(&fl->hlock);
 	INIT_HLIST_HEAD(&fl->maps);
 	INIT_HLIST_HEAD(&fl->bufs);
 	INIT_HLIST_NODE(&fl->hn);
@@ -2068,9 +2068,9 @@ static int fastrpc_device_open(struct inode *inode, struct file *filp)
 						me->channel[cid].ssrcount;
 		}
 	}
-	spin_lock(&me->hlock);
+	raw_spin_lock(&me->hlock);
 	hlist_add_head(&fl->hn, &me->drivers);
-	spin_unlock(&me->hlock);
+	raw_spin_unlock(&me->hlock);
 
 bail:
 	mutex_unlock(&me->smd_mutex);
diff --git a/kernel/msm-3.18/drivers/char/smd_ts.c b/kernel/msm-3.18/drivers/char/smd_ts.c
index c05b7ce75..2d36103d1 100644
--- a/kernel/msm-3.18/drivers/char/smd_ts.c
+++ b/kernel/msm-3.18/drivers/char/smd_ts.c
@@ -133,15 +133,15 @@ static int ts_buf_read(struct smd_ts_apps *ts_app, char *user_buf, size_t count)
 	if (count == 0)
 		return 0;
 
-	spin_lock_irqsave(&ts_app->lock, flags);
+	raw_spin_lock_irqsave(&ts_app->lock, flags);
 
 	/* if none data ready, wait until at least one ready;
 	 * it should release the lock, and get the lock after completion. */
 	if (ts_app->ready_buf_len == 0) {
 		init_completion(&ts_app->work);
-		spin_unlock_irqrestore(&ts_app->lock, flags);
+		raw_spin_unlock_irqrestore(&ts_app->lock, flags);
 		wait_for_completion(&ts_app->work);
-		spin_lock_irqsave(&ts_app->lock, flags);
+		raw_spin_lock_irqsave(&ts_app->lock, flags);
 	}
 
 	if (count > (ts_app->ready_buf_len * sizeof(uint64_t)))
@@ -161,7 +161,7 @@ static int ts_buf_read(struct smd_ts_apps *ts_app, char *user_buf, size_t count)
 				(count - length) / sizeof(uint64_t) - 1;
 	}
 
-	spin_unlock_irqrestore(&ts_app->lock, flags);
+	raw_spin_unlock_irqrestore(&ts_app->lock, flags);
 
 	/* do memcpy outof spinlock_irq */
 	if (count <= length) {
@@ -221,7 +221,7 @@ static int smd_ts_apps_init(struct smd_ts_apps *ts_app)
 	ts_app->buf_ptr = ts_app->ts_buf;
 
 	init_completion(&ts_app->work);
-	spin_lock_init(&ts_app->lock);
+	raw_spin_lock_init(&ts_app->lock);
 
 	ret = alloc_chrdev_region(&ts_app->dev_no, 0, 1, ts_app->ch_name);
 	if (ret != 0)
diff --git a/kernel/msm-3.18/drivers/char/smd_ts.h b/kernel/msm-3.18/drivers/char/smd_ts.h
index 3e03dcc40..2932abcc4 100644
--- a/kernel/msm-3.18/drivers/char/smd_ts.h
+++ b/kernel/msm-3.18/drivers/char/smd_ts.h
@@ -39,7 +39,7 @@
 #define TS_DIFF_BUF_NUM		33
 
 struct smd_ts_apps {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct completion work;
 
 	/* for dev node */
diff --git a/kernel/msm-3.18/drivers/cpufreq/cpufreq_interactive.c b/kernel/msm-3.18/drivers/cpufreq/cpufreq_interactive.c
index d12713734..77a03ebc6 100644
--- a/kernel/msm-3.18/drivers/cpufreq/cpufreq_interactive.c
+++ b/kernel/msm-3.18/drivers/cpufreq/cpufreq_interactive.c
@@ -39,12 +39,12 @@ struct cpufreq_interactive_policyinfo {
 	struct timer_list policy_timer;
 	struct timer_list policy_slack_timer;
 	struct hrtimer notif_timer;
-	spinlock_t load_lock; /* protects load tracking stat */
+	raw_spinlock_t load_lock; /* protects load tracking stat */
 	u64 last_evaluated_jiffy;
 	struct cpufreq_policy *policy;
 	struct cpufreq_policy p_nolim; /* policy copy with no limits */
 	struct cpufreq_frequency_table *freq_table;
-	spinlock_t target_freq_lock; /*protects target freq */
+	raw_spinlock_t target_freq_lock; /*protects target freq */
 	unsigned int target_freq;
 	unsigned int floor_freq;
 	unsigned int min_freq;
@@ -75,7 +75,7 @@ static DEFINE_PER_CPU(struct cpufreq_interactive_cpuinfo, cpuinfo);
 /* realtime thread handles frequency scaling */
 static struct task_struct *speedchange_task;
 static cpumask_t speedchange_cpumask;
-static spinlock_t speedchange_cpumask_lock;
+static raw_spinlock_t speedchange_cpumask_lock;
 static struct mutex gov_lock;
 
 static int set_window_count;
@@ -100,7 +100,7 @@ struct cpufreq_interactive_tunables {
 #define DEFAULT_GO_HISPEED_LOAD 99
 	unsigned long go_hispeed_load;
 	/* Target load. Lower values result in higher CPU speeds. */
-	spinlock_t target_loads_lock;
+	raw_spinlock_t target_loads_lock;
 	unsigned int *target_loads;
 	int ntarget_loads;
 	/*
@@ -117,7 +117,7 @@ struct cpufreq_interactive_tunables {
 	 * Wait this long before raising speed above hispeed, by default a
 	 * single timer interval.
 	 */
-	spinlock_t above_hispeed_delay_lock;
+	raw_spinlock_t above_hispeed_delay_lock;
 	unsigned int *above_hispeed_delay;
 	int nabove_hispeed_delay;
 	/* Non-zero means indefinite speed boost active */
@@ -203,7 +203,7 @@ static void cpufreq_interactive_timer_resched(unsigned long cpu,
 	unsigned long flags;
 	int i;
 
-	spin_lock_irqsave(&ppol->load_lock, flags);
+	raw_spin_lock_irqsave(&ppol->load_lock, flags);
 	expires = round_to_nw_start(ppol->last_evaluated_jiffy, tunables);
 	if (!slack_only) {
 		for_each_cpu(i, ppol->policy->cpus) {
@@ -228,7 +228,7 @@ static void cpufreq_interactive_timer_resched(unsigned long cpu,
 		add_timer(&ppol->policy_slack_timer);
 	}
 
-	spin_unlock_irqrestore(&ppol->load_lock, flags);
+	raw_spin_unlock_irqrestore(&ppol->load_lock, flags);
 }
 
 /* The caller shall take enable_sem write semaphore to avoid any timer race.
@@ -244,7 +244,7 @@ static void cpufreq_interactive_timer_start(
 	unsigned long flags;
 	int i;
 
-	spin_lock_irqsave(&ppol->load_lock, flags);
+	raw_spin_lock_irqsave(&ppol->load_lock, flags);
 	ppol->policy_timer.expires = expires;
 	add_timer(&ppol->policy_timer);
 	if (tunables->timer_slack_val >= 0 &&
@@ -262,7 +262,7 @@ static void cpufreq_interactive_timer_start(
 		pcpu->cputime_speedadj = 0;
 		pcpu->cputime_speedadj_timestamp = pcpu->time_in_idle_timestamp;
 	}
-	spin_unlock_irqrestore(&ppol->load_lock, flags);
+	raw_spin_unlock_irqrestore(&ppol->load_lock, flags);
 }
 
 static unsigned int freq_to_above_hispeed_delay(
@@ -273,14 +273,14 @@ static unsigned int freq_to_above_hispeed_delay(
 	unsigned int ret;
 	unsigned long flags;
 
-	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+	raw_spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
 
 	for (i = 0; i < tunables->nabove_hispeed_delay - 1 &&
 			freq >= tunables->above_hispeed_delay[i+1]; i += 2)
 		;
 
 	ret = tunables->above_hispeed_delay[i];
-	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	raw_spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
 	return ret;
 }
 
@@ -291,14 +291,14 @@ static unsigned int freq_to_targetload(
 	unsigned int ret;
 	unsigned long flags;
 
-	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+	raw_spin_lock_irqsave(&tunables->target_loads_lock, flags);
 
 	for (i = 0; i < tunables->ntarget_loads - 1 &&
 		    freq >= tunables->target_loads[i+1]; i += 2)
 		;
 
 	ret = tunables->target_loads[i];
-	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	raw_spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
 	return ret;
 }
 
@@ -487,8 +487,8 @@ static void cpufreq_interactive_timer(unsigned long data)
 
 	now = ktime_to_us(ktime_get());
 
-	spin_lock_irqsave(&ppol->target_freq_lock, flags);
-	spin_lock(&ppol->load_lock);
+	raw_spin_lock_irqsave(&ppol->target_freq_lock, flags);
+	raw_spin_lock(&ppol->load_lock);
 
 	skip_hispeed_logic = tunables->enable_prediction ? true :
 		tunables->ignore_hispeed_on_notif && ppol->notif_pending;
@@ -551,7 +551,7 @@ static void cpufreq_interactive_timer(unsigned long data)
 		}
 		i++;
 	}
-	spin_unlock(&ppol->load_lock);
+	raw_spin_unlock(&ppol->load_lock);
 
 	tunables->boosted = tunables->boost_val || now < tunables->boostpulse_endtime;
 
@@ -599,7 +599,7 @@ static void cpufreq_interactive_timer(unsigned long data)
 		trace_cpufreq_interactive_notyet(
 			max_cpu, pol_load, ppol->target_freq,
 			ppol->policy->cur, new_freq);
-		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		raw_spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
 		goto rearm;
 	}
 
@@ -608,7 +608,7 @@ static void cpufreq_interactive_timer(unsigned long data)
 	if (cpufreq_frequency_table_target(&ppol->p_nolim, ppol->freq_table,
 					   new_freq, CPUFREQ_RELATION_L,
 					   &index)) {
-		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		raw_spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
 		goto rearm;
 	}
 
@@ -624,7 +624,7 @@ static void cpufreq_interactive_timer(unsigned long data)
 			trace_cpufreq_interactive_notyet(
 				max_cpu, pol_load, ppol->target_freq,
 				ppol->policy->cur, new_freq);
-			spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+			raw_spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
 			goto rearm;
 		}
 	}
@@ -654,7 +654,7 @@ static void cpufreq_interactive_timer(unsigned long data)
 		trace_cpufreq_interactive_already(
 			max_cpu, pol_load, ppol->target_freq,
 			ppol->policy->cur, new_freq);
-		spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+		raw_spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
 		goto rearm;
 	}
 
@@ -662,10 +662,10 @@ static void cpufreq_interactive_timer(unsigned long data)
 					 ppol->policy->cur, new_freq);
 
 	ppol->target_freq = new_freq;
-	spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
-	spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+	raw_spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+	raw_spin_lock_irqsave(&speedchange_cpumask_lock, flags);
 	cpumask_set_cpu(max_cpu, &speedchange_cpumask);
-	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+	raw_spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
 	wake_up_process_no_notif(speedchange_task);
 
 rearm:
@@ -702,23 +702,23 @@ static int cpufreq_interactive_speedchange_task(void *data)
 
 	while (1) {
 		set_current_state(TASK_INTERRUPTIBLE);
-		spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+		raw_spin_lock_irqsave(&speedchange_cpumask_lock, flags);
 
 		if (cpumask_empty(&speedchange_cpumask)) {
-			spin_unlock_irqrestore(&speedchange_cpumask_lock,
+			raw_spin_unlock_irqrestore(&speedchange_cpumask_lock,
 					       flags);
 			schedule();
 
 			if (kthread_should_stop())
 				break;
 
-			spin_lock_irqsave(&speedchange_cpumask_lock, flags);
+			raw_spin_lock_irqsave(&speedchange_cpumask_lock, flags);
 		}
 
 		set_current_state(TASK_RUNNING);
 		tmp_mask = speedchange_cpumask;
 		cpumask_clear(&speedchange_cpumask);
-		spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
+		raw_spin_unlock_irqrestore(&speedchange_cpumask_lock, flags);
 
 		for_each_cpu(cpu, &tmp_mask) {
 			ppol = per_cpu(polinfo, cpu);
@@ -752,14 +752,14 @@ static void cpufreq_interactive_boost(struct cpufreq_interactive_tunables *tunab
 
 	tunables->boosted = true;
 
-	spin_lock_irqsave(&speedchange_cpumask_lock, flags[0]);
+	raw_spin_lock_irqsave(&speedchange_cpumask_lock, flags[0]);
 
 	for_each_online_cpu(i) {
 		ppol = per_cpu(polinfo, i);
 		if (!ppol || tunables != ppol->policy->governor_data)
 			continue;
 
-		spin_lock_irqsave(&ppol->target_freq_lock, flags[1]);
+		raw_spin_lock_irqsave(&ppol->target_freq_lock, flags[1]);
 		if (ppol->target_freq < tunables->hispeed_freq) {
 			ppol->target_freq = tunables->hispeed_freq;
 			cpumask_set_cpu(i, &speedchange_cpumask);
@@ -775,11 +775,11 @@ static void cpufreq_interactive_boost(struct cpufreq_interactive_tunables *tunab
 
 		ppol->floor_freq = tunables->hispeed_freq;
 		ppol->floor_validate_time = ktime_to_us(ktime_get());
-		spin_unlock_irqrestore(&ppol->target_freq_lock, flags[1]);
+		raw_spin_unlock_irqrestore(&ppol->target_freq_lock, flags[1]);
 		break;
 	}
 
-	spin_unlock_irqrestore(&speedchange_cpumask_lock, flags[0]);
+	raw_spin_unlock_irqrestore(&speedchange_cpumask_lock, flags[0]);
 
 	if (anyboost)
 		wake_up_process_no_notif(speedchange_task);
@@ -805,10 +805,10 @@ static int load_change_callback(struct notifier_block *nb, unsigned long val,
 	if (!tunables->use_sched_load || !tunables->use_migration_notif)
 		goto exit;
 
-	spin_lock_irqsave(&ppol->target_freq_lock, flags);
+	raw_spin_lock_irqsave(&ppol->target_freq_lock, flags);
 	ppol->notif_pending = true;
 	ppol->notif_cpu = cpu;
-	spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
+	raw_spin_unlock_irqrestore(&ppol->target_freq_lock, flags);
 
 	if (!hrtimer_is_queued(&ppol->notif_timer))
 		__hrtimer_start_range_ns(&ppol->notif_timer, ms_to_ktime(1),
@@ -867,10 +867,10 @@ static int cpufreq_interactive_notifier(
 			up_read(&ppol->enable_sem);
 			return 0;
 		}
-		spin_lock_irqsave(&ppol->load_lock, flags);
+		raw_spin_lock_irqsave(&ppol->load_lock, flags);
 		for_each_cpu(cpu, ppol->policy->cpus)
 			update_load(cpu);
-		spin_unlock_irqrestore(&ppol->load_lock, flags);
+		raw_spin_unlock_irqrestore(&ppol->load_lock, flags);
 
 		up_read(&ppol->enable_sem);
 	}
@@ -934,14 +934,14 @@ static ssize_t show_target_loads(
 	ssize_t ret = 0;
 	unsigned long flags;
 
-	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+	raw_spin_lock_irqsave(&tunables->target_loads_lock, flags);
 
 	for (i = 0; i < tunables->ntarget_loads; i++)
 		ret += sprintf(buf + ret, "%u%s", tunables->target_loads[i],
 			       i & 0x1 ? ":" : " ");
 
 	sprintf(buf + ret - 1, "\n");
-	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	raw_spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
 	return ret;
 }
 
@@ -957,12 +957,12 @@ static ssize_t store_target_loads(
 	if (IS_ERR(new_target_loads))
 		return PTR_RET(new_target_loads);
 
-	spin_lock_irqsave(&tunables->target_loads_lock, flags);
+	raw_spin_lock_irqsave(&tunables->target_loads_lock, flags);
 	if (tunables->target_loads != default_target_loads)
 		kfree(tunables->target_loads);
 	tunables->target_loads = new_target_loads;
 	tunables->ntarget_loads = ntokens;
-	spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
+	raw_spin_unlock_irqrestore(&tunables->target_loads_lock, flags);
 
 	sched_update_freq_max_load(&controlled_cpus);
 
@@ -976,7 +976,7 @@ static ssize_t show_above_hispeed_delay(
 	ssize_t ret = 0;
 	unsigned long flags;
 
-	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+	raw_spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
 
 	for (i = 0; i < tunables->nabove_hispeed_delay; i++)
 		ret += sprintf(buf + ret, "%u%s",
@@ -984,7 +984,7 @@ static ssize_t show_above_hispeed_delay(
 			       i & 0x1 ? ":" : " ");
 
 	sprintf(buf + ret - 1, "\n");
-	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	raw_spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
 	return ret;
 }
 
@@ -1000,12 +1000,12 @@ static ssize_t store_above_hispeed_delay(
 	if (IS_ERR(new_above_hispeed_delay))
 		return PTR_RET(new_above_hispeed_delay);
 
-	spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
+	raw_spin_lock_irqsave(&tunables->above_hispeed_delay_lock, flags);
 	if (tunables->above_hispeed_delay != default_above_hispeed_delay)
 		kfree(tunables->above_hispeed_delay);
 	tunables->above_hispeed_delay = new_above_hispeed_delay;
 	tunables->nabove_hispeed_delay = ntokens;
-	spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
+	raw_spin_unlock_irqrestore(&tunables->above_hispeed_delay_lock, flags);
 	return count;
 
 }
@@ -1573,8 +1573,8 @@ static struct cpufreq_interactive_tunables *alloc_tunable(
 	tunables->boostpulse_duration_val = DEFAULT_MIN_SAMPLE_TIME;
 	tunables->timer_slack_val = DEFAULT_TIMER_SLACK;
 
-	spin_lock_init(&tunables->target_loads_lock);
-	spin_lock_init(&tunables->above_hispeed_delay_lock);
+	raw_spin_lock_init(&tunables->target_loads_lock);
+	raw_spin_lock_init(&tunables->above_hispeed_delay_lock);
 
 	return tunables;
 }
@@ -1609,8 +1609,8 @@ static struct cpufreq_interactive_policyinfo *get_policyinfo(
 	ppol->policy_slack_timer.function = cpufreq_interactive_nop_timer;
 	hrtimer_init(&ppol->notif_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	ppol->notif_timer.function = cpufreq_interactive_hrtimer;
-	spin_lock_init(&ppol->load_lock);
-	spin_lock_init(&ppol->target_freq_lock);
+	raw_spin_lock_init(&ppol->load_lock);
+	raw_spin_lock_init(&ppol->target_freq_lock);
 	init_rwsem(&ppol->enable_sem);
 
 	for_each_cpu(i, policy->related_cpus)
@@ -1827,7 +1827,7 @@ static int __init cpufreq_interactive_init(void)
 {
 	struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
 
-	spin_lock_init(&speedchange_cpumask_lock);
+	raw_spin_lock_init(&speedchange_cpumask_lock);
 	mutex_init(&gov_lock);
 	mutex_init(&sched_lock);
 	speedchange_task =
diff --git a/kernel/msm-3.18/drivers/cpuidle/lpm-levels-of.c b/kernel/msm-3.18/drivers/cpuidle/lpm-levels-of.c
index 3710c5258..24f6d34f5 100644
--- a/kernel/msm-3.18/drivers/cpuidle/lpm-levels-of.c
+++ b/kernel/msm-3.18/drivers/cpuidle/lpm-levels-of.c
@@ -875,7 +875,7 @@ struct lpm_cluster *parse_cluster(struct device_node *node,
 
 	INIT_LIST_HEAD(&c->child);
 	c->parent = parent;
-	spin_lock_init(&c->sync_lock);
+	raw_spin_lock_init(&c->sync_lock);
 	c->min_child_level = NR_LPM_LEVELS;
 
 	for_each_child_of_node(node, n) {
diff --git a/kernel/msm-3.18/drivers/cpuidle/lpm-levels.c b/kernel/msm-3.18/drivers/cpuidle/lpm-levels.c
index b0c65c761..8f38bcc17 100644
--- a/kernel/msm-3.18/drivers/cpuidle/lpm-levels.c
+++ b/kernel/msm-3.18/drivers/cpuidle/lpm-levels.c
@@ -291,7 +291,7 @@ static void update_debug_pc_event(enum debug_event event, uint32_t arg1,
 	if (!lpm_debug)
 		return;
 
-	spin_lock(&debug_lock);
+	raw_spin_lock(&debug_lock);
 	idx = pc_event_index++;
 	dbg = &lpm_debug[idx & (num_dbg_elements - 1)];
 
@@ -302,7 +302,7 @@ static void update_debug_pc_event(enum debug_event event, uint32_t arg1,
 	dbg->arg2 = arg2;
 	dbg->arg3 = arg3;
 	dbg->arg4 = arg4;
-	spin_unlock(&debug_lock);
+	raw_spin_unlock(&debug_lock);
 }
 
 static void setup_broadcast_timer(void *arg)
@@ -712,7 +712,7 @@ static void cluster_prepare(struct lpm_cluster *cluster,
 	if (cluster->min_child_level > child_idx)
 		return;
 
-	spin_lock(&cluster->sync_lock);
+	raw_spin_lock(&cluster->sync_lock);
 	cpumask_or(&cluster->num_children_in_sync, cpu,
 			&cluster->num_children_in_sync);
 
@@ -747,10 +747,10 @@ static void cluster_prepare(struct lpm_cluster *cluster,
 	cluster_prepare(cluster->parent, &cluster->num_children_in_sync, i,
 			from_idle, start_time);
 
-	spin_unlock(&cluster->sync_lock);
+	raw_spin_unlock(&cluster->sync_lock);
 	return;
 failed:
-	spin_unlock(&cluster->sync_lock);
+	raw_spin_unlock(&cluster->sync_lock);
 	cluster->stats->sleep_time = 0;
 	return;
 }
@@ -769,7 +769,7 @@ static void cluster_unprepare(struct lpm_cluster *cluster,
 	if (cluster->min_child_level > child_idx)
 		return;
 
-	spin_lock(&cluster->sync_lock);
+	raw_spin_lock(&cluster->sync_lock);
 	last_level = cluster->default_level;
 	first_cpu = cpumask_equal(&cluster->num_children_in_sync,
 				&cluster->child_cpus);
@@ -831,7 +831,7 @@ static void cluster_unprepare(struct lpm_cluster *cluster,
 	cluster_unprepare(cluster->parent, &cluster->child_cpus,
 			last_level, from_idle, end_time);
 unlock_return:
-	spin_unlock(&cluster->sync_lock);
+	raw_spin_unlock(&cluster->sync_lock);
 }
 
 static inline void cpu_prepare(struct lpm_cluster *cluster, int cpu_index,
@@ -901,7 +901,7 @@ int get_cluster_id(struct lpm_cluster *cluster, int *aff_lvl)
 	if (!cluster)
 		return 0;
 
-	spin_lock(&cluster->sync_lock);
+	raw_spin_lock(&cluster->sync_lock);
 
 	if (!cpumask_equal(&cluster->num_children_in_sync,
 				&cluster->child_cpus))
@@ -918,7 +918,7 @@ int get_cluster_id(struct lpm_cluster *cluster, int *aff_lvl)
 		(*aff_lvl)++;
 	}
 unlock_and_return:
-	spin_unlock(&cluster->sync_lock);
+	raw_spin_unlock(&cluster->sync_lock);
 	return state_id;
 }
 
@@ -1176,12 +1176,12 @@ static int cluster_cpuidle_register(struct lpm_cluster *cl)
 		p = per_cpu(cpu_cluster, cpu);
 		while (p) {
 			int j;
-			spin_lock(&p->sync_lock);
+			raw_spin_lock(&p->sync_lock);
 			cpumask_set_cpu(cpu, &p->num_children_in_sync);
 			for (j = 0; j < p->nlevels; j++)
 				cpumask_copy(&p->levels[j].num_cpu_votes,
 						&p->num_children_in_sync);
-			spin_unlock(&p->sync_lock);
+			raw_spin_unlock(&p->sync_lock);
 			p = p->parent;
 		}
 	}
@@ -1472,7 +1472,7 @@ enum msm_pm_l2_scm_flag lpm_cpu_pre_pc_cb(unsigned int cpu)
 	 * Assumes L2 only. What/How parameters gets passed into TZ will
 	 * determine how this function reports this info back in msm-pm.c
 	 */
-	spin_lock(&cluster->sync_lock);
+	raw_spin_lock(&cluster->sync_lock);
 
 	if (!cluster->lpm_dev) {
 		retflag = MSM_SCM_L2_OFF;
@@ -1499,7 +1499,7 @@ unlock_and_return:
 	trace_pre_pc_cb(retflag);
 	remote_spin_lock_rlock_id(&scm_handoff_lock,
 				  REMOTE_SPINLOCK_TID_START + cpu);
-	spin_unlock(&cluster->sync_lock);
+	raw_spin_unlock(&cluster->sync_lock);
 	return retflag;
 }
 
diff --git a/kernel/msm-3.18/drivers/cpuidle/lpm-levels.h b/kernel/msm-3.18/drivers/cpuidle/lpm-levels.h
index f6979c4d4..3f8cc8f2c 100644
--- a/kernel/msm-3.18/drivers/cpuidle/lpm-levels.h
+++ b/kernel/msm-3.18/drivers/cpuidle/lpm-levels.h
@@ -100,7 +100,7 @@ struct lpm_cluster {
 	int last_level;
 	struct lpm_cpu *cpu;
 	struct cpuidle_driver *drv;
-	spinlock_t sync_lock;
+	raw_spinlock_t sync_lock;
 	struct cpumask child_cpus;
 	struct cpumask num_children_in_sync;
 	struct lpm_cluster *parent;
diff --git a/kernel/msm-3.18/drivers/devfreq/armbw-pm.c b/kernel/msm-3.18/drivers/devfreq/armbw-pm.c
index b5c05953e..b4db4d7aa 100644
--- a/kernel/msm-3.18/drivers/devfreq/armbw-pm.c
+++ b/kernel/msm-3.18/drivers/devfreq/armbw-pm.c
@@ -197,7 +197,7 @@ static unsigned long measure_bw_and_set_irq(struct bw_hwmon *hw,
 	struct bwmon_data *data;
 	unsigned int sample_ms = hw->df->profile->polling_ms;
 
-	spin_lock(&bw_lock);
+	raw_spin_lock(&bw_lock);
 	on_each_cpu(get_beat_count, NULL, true);
 	for_each_possible_cpu(cpu) {
 		data = &per_cpu(gov_data, cpu);
@@ -212,7 +212,7 @@ static unsigned long measure_bw_and_set_irq(struct bw_hwmon *hw,
 		data->count = 0;
 	}
 	on_each_cpu(mon_enable, NULL, true);
-	spin_unlock(&bw_lock);
+	raw_spin_unlock(&bw_lock);
 	return bw;
 }
 
@@ -293,14 +293,14 @@ static int hotplug_notif(struct notifier_block *nb, unsigned long action,
 {
 	switch (action) {
 	case CPU_DYING:
-		spin_lock(&bw_lock);
+		raw_spin_lock(&bw_lock);
 		save_hotplugstate();
-		spin_unlock(&bw_lock);
+		raw_spin_unlock(&bw_lock);
 		break;
 	case CPU_STARTING:
-		spin_lock(&bw_lock);
+		raw_spin_lock(&bw_lock);
 		restore_hotplugstate();
-		spin_unlock(&bw_lock);
+		raw_spin_unlock(&bw_lock);
 		break;
 	}
 
diff --git a/kernel/msm-3.18/drivers/devfreq/bimc-bwmon.c b/kernel/msm-3.18/drivers/devfreq/bimc-bwmon.c
index 2b0bacdb5..405f729d7 100644
--- a/kernel/msm-3.18/drivers/devfreq/bimc-bwmon.c
+++ b/kernel/msm-3.18/drivers/devfreq/bimc-bwmon.c
@@ -95,11 +95,11 @@ static void mon_irq_enable(struct bwmon *m)
 {
 	u32 val;
 
-	spin_lock(&glb_lock);
+	raw_spin_lock(&glb_lock);
 	val = readl_relaxed(GLB_INT_EN(m));
 	val |= 1 << m->mport;
 	writel_relaxed(val, GLB_INT_EN(m));
-	spin_unlock(&glb_lock);
+	raw_spin_unlock(&glb_lock);
 
 	val = readl_relaxed(MON_INT_EN(m));
 	val |= 0x1;
@@ -115,11 +115,11 @@ static void mon_irq_disable(struct bwmon *m)
 {
 	u32 val;
 
-	spin_lock(&glb_lock);
+	raw_spin_lock(&glb_lock);
 	val = readl_relaxed(GLB_INT_EN(m));
 	val &= ~(1 << m->mport);
 	writel_relaxed(val, GLB_INT_EN(m));
-	spin_unlock(&glb_lock);
+	raw_spin_unlock(&glb_lock);
 
 	val = readl_relaxed(MON_INT_EN(m));
 	val &= ~0x1;
diff --git a/kernel/msm-3.18/drivers/devfreq/governor_bw_hwmon.c b/kernel/msm-3.18/drivers/devfreq/governor_bw_hwmon.c
index e0f75b002..b008a3fcb 100644
--- a/kernel/msm-3.18/drivers/devfreq/governor_bw_hwmon.c
+++ b/kernel/msm-3.18/drivers/devfreq/governor_bw_hwmon.c
@@ -84,7 +84,7 @@ struct hwmon_node {
 
 #define UP_WAKE 1
 #define DOWN_WAKE 2
-static DEFINE_SPINLOCK(irq_lock);
+static DEFINE_RAW_SPINLOCK(irq_lock);
 
 static LIST_HEAD(hwmon_list);
 static DEFINE_MUTEX(list_lock);
@@ -246,9 +246,9 @@ int bw_hwmon_sample_end(struct bw_hwmon *hwmon)
 	unsigned long flags;
 	int wake;
 
-	spin_lock_irqsave(&irq_lock, flags);
+	raw_spin_lock_irqsave(&irq_lock, flags);
 	wake = __bw_hwmon_sample_end(hwmon);
-	spin_unlock_irqrestore(&irq_lock, flags);
+	raw_spin_unlock_irqrestore(&irq_lock, flags);
 
 	return wake;
 }
@@ -277,7 +277,7 @@ static unsigned long get_bw_and_set_irq(struct hwmon_node *node,
 	ktime_t ts;
 	unsigned int ms;
 
-	spin_lock_irqsave(&irq_lock, flags);
+	raw_spin_lock_irqsave(&irq_lock, flags);
 
 	ts = ktime_get();
 	ms = ktime_to_ms(ktime_sub(ts, node->prev_ts));
@@ -412,7 +412,7 @@ static unsigned long get_bw_and_set_irq(struct hwmon_node *node,
 	node->wake = 0;
 	node->prev_req = req_mbps;
 
-	spin_unlock_irqrestore(&irq_lock, flags);
+	raw_spin_unlock_irqrestore(&irq_lock, flags);
 
 	adj_mbps = req_mbps + node->guard_band_mbps;
 
diff --git a/kernel/msm-3.18/drivers/devfreq/governor_msm_adreno_tz.c b/kernel/msm-3.18/drivers/devfreq/governor_msm_adreno_tz.c
index 38d5a3731..601a2161a 100644
--- a/kernel/msm-3.18/drivers/devfreq/governor_msm_adreno_tz.c
+++ b/kernel/msm-3.18/drivers/devfreq/governor_msm_adreno_tz.c
@@ -98,14 +98,14 @@ static ssize_t gpu_load_show(struct device *dev,
 	 * This will keep the average value in sync with
 	 * with the client sampling duration.
 	 */
-	spin_lock(&sample_lock);
+	raw_spin_lock(&sample_lock);
 	if (acc_total)
 		sysfs_busy_perc = (acc_relative_busy * 100) / acc_total;
 
 	/* Reset the parameters */
 	acc_total = 0;
 	acc_relative_busy = 0;
-	spin_unlock(&sample_lock);
+	raw_spin_unlock(&sample_lock);
 	return snprintf(buf, PAGE_SIZE, "%lu\n", sysfs_busy_perc);
 }
 
@@ -119,7 +119,7 @@ static ssize_t suspend_time_show(struct device *dev,
 {
 	u64 time_diff = 0;
 
-	spin_lock(&suspend_lock);
+	raw_spin_lock(&suspend_lock);
 	time_diff = suspend_time_ms();
 	/*
 	 * Adding the previous suspend time also as the gpu
@@ -129,7 +129,7 @@ static ssize_t suspend_time_show(struct device *dev,
 	 */
 	time_diff += suspend_time;
 	suspend_time = 0;
-	spin_unlock(&suspend_lock);
+	raw_spin_unlock(&suspend_lock);
 
 	return snprintf(buf, PAGE_SIZE, "%llu\n", time_diff);
 }
@@ -150,7 +150,7 @@ void compute_work_load(struct devfreq_dev_status *stats,
 		struct devfreq_msm_adreno_tz_data *priv,
 		struct devfreq *devfreq)
 {
-	spin_lock(&sample_lock);
+	raw_spin_lock(&sample_lock);
 	/*
 	 * Keep collecting the stats till the client
 	 * reads it. Average of all samples and reset
@@ -159,7 +159,7 @@ void compute_work_load(struct devfreq_dev_status *stats,
 	acc_total += stats->total_time;
 	acc_relative_busy += (stats->busy_time * stats->current_frequency) /
 				devfreq->profile->freq_table[0];
-	spin_unlock(&sample_lock);
+	raw_spin_unlock(&sample_lock);
 }
 
 /* Trap into the TrustZone, and call funcs there. */
@@ -171,10 +171,10 @@ static int __secure_tz_reset_entry2(unsigned int *scm_data, u32 size_scm_data,
 	__iowmb();
 
 	if (!is_64) {
-		spin_lock(&tz_lock);
+		raw_spin_lock(&tz_lock);
 		ret = scm_call_atomic2(SCM_SVC_IO, TZ_RESET_ID, scm_data[0],
 					scm_data[1]);
-		spin_unlock(&tz_lock);
+		raw_spin_unlock(&tz_lock);
 	} else {
 		if (is_scm_armv8()) {
 			struct scm_desc desc = {0};
@@ -197,10 +197,10 @@ static int __secure_tz_update_entry3(unsigned int *scm_data, u32 size_scm_data,
 	__iowmb();
 
 	if (!priv->is_64) {
-		spin_lock(&tz_lock);
+		raw_spin_lock(&tz_lock);
 		ret = scm_call_atomic3(SCM_SVC_IO, TZ_UPDATE_ID,
 					scm_data[0], scm_data[1], scm_data[2]);
-		spin_unlock(&tz_lock);
+		raw_spin_unlock(&tz_lock);
 		*val = ret;
 	} else {
 		if (is_scm_armv8()) {
@@ -546,28 +546,28 @@ static int tz_handler(struct devfreq *devfreq, unsigned int event, void *data)
 		if (partner_gpu_profile && partner_gpu_profile->bus_devfreq)
 			queue_work(workqueue,
 				&gpu_profile->partner_stop_event_ws);
-		spin_lock(&suspend_lock);
+		raw_spin_lock(&suspend_lock);
 		suspend_start = 0;
-		spin_unlock(&suspend_lock);
+		raw_spin_unlock(&suspend_lock);
 		result = tz_stop(devfreq);
 		break;
 
 	case DEVFREQ_GOV_SUSPEND:
 		result = tz_suspend(devfreq);
 		if (!result) {
-			spin_lock(&suspend_lock);
+			raw_spin_lock(&suspend_lock);
 			/* Collect the start sample for suspend time */
 			suspend_start = (u64)ktime_to_ms(ktime_get());
-			spin_unlock(&suspend_lock);
+			raw_spin_unlock(&suspend_lock);
 		}
 		break;
 
 	case DEVFREQ_GOV_RESUME:
-		spin_lock(&suspend_lock);
+		raw_spin_lock(&suspend_lock);
 		suspend_time += suspend_time_ms();
 		/* Reset the suspend_start when gpu resumes */
 		suspend_start = 0;
-		spin_unlock(&suspend_lock);
+		raw_spin_unlock(&suspend_lock);
 
 	case DEVFREQ_GOV_INTERVAL:
 		/* ignored, this governor doesn't use polling */
diff --git a/kernel/msm-3.18/drivers/devfreq/m4m-hwmon.c b/kernel/msm-3.18/drivers/devfreq/m4m-hwmon.c
index 51b077119..45faa3a18 100644
--- a/kernel/msm-3.18/drivers/devfreq/m4m-hwmon.c
+++ b/kernel/msm-3.18/drivers/devfreq/m4m-hwmon.c
@@ -200,10 +200,10 @@ static void mon_init(struct m4m_hwmon *m)
 	unsigned long flags;
 	int i;
 
-	spin_lock_irqsave(&init_lock, flags);
+	raw_spin_lock_irqsave(&init_lock, flags);
 	if (!mon_inited)
 		mon_global_init(m);
-	spin_unlock_irqrestore(&init_lock, flags);
+	raw_spin_unlock_irqrestore(&init_lock, flags);
 
 	/* configure counter events */
 	for (i = 0; i < m->num_cntr; i++)
diff --git a/kernel/msm-3.18/drivers/edac/cortex_arm64_edac.c b/kernel/msm-3.18/drivers/edac/cortex_arm64_edac.c
index 96dd58159..ec601798f 100644
--- a/kernel/msm-3.18/drivers/edac/cortex_arm64_edac.c
+++ b/kernel/msm-3.18/drivers/edac/cortex_arm64_edac.c
@@ -569,7 +569,7 @@ static void arm64_erp_local_handler(void *info)
 	unsigned long flags, flags2;
 	u32 l2ectlr;
 
-	spin_lock_irqsave(&local_handler_lock, flags);
+	raw_spin_lock_irqsave(&local_handler_lock, flags);
 	edac_printk(KERN_CRIT, EDAC_CPU, "%s error information from CPU %d, MIDR=%#08x:\n",
 		       err_name[errdata->err], raw_smp_processor_id(), cpuid);
 
@@ -596,7 +596,7 @@ static void arm64_erp_local_handler(void *info)
 	};
 
 	/* Acklowledge internal error in L2ECTLR */
-	spin_lock_irqsave(&l2ectlr_lock, flags2);
+	raw_spin_lock_irqsave(&l2ectlr_lock, flags2);
 
 	l2ectlr = read_l2ectlr_el1;
 
@@ -605,8 +605,8 @@ static void arm64_erp_local_handler(void *info)
 		write_l2ectlr_el1(l2ectlr);
 	}
 
-	spin_unlock_irqrestore(&l2ectlr_lock, flags2);
-	spin_unlock_irqrestore(&local_handler_lock, flags);
+	raw_spin_unlock_irqrestore(&l2ectlr_lock, flags2);
+	raw_spin_unlock_irqrestore(&local_handler_lock, flags);
 }
 
 static irqreturn_t arm64_dbe_handler(int irq, void *drvdata)
@@ -628,10 +628,10 @@ static void arm64_ext_local_handler(void *info)
 	unsigned long flags, flags2;
 	u32 l2ectlr;
 
-	spin_lock_irqsave(&local_handler_lock, flags);
+	raw_spin_lock_irqsave(&local_handler_lock, flags);
 
 	/* TODO: Shared locking for L2ECTLR access */
-	spin_lock_irqsave(&l2ectlr_lock, flags2);
+	raw_spin_lock_irqsave(&l2ectlr_lock, flags2);
 
 	l2ectlr = read_l2ectlr_el1;
 
@@ -647,8 +647,8 @@ static void arm64_ext_local_handler(void *info)
 		write_l2ectlr_el1(l2ectlr);
 	}
 
-	spin_unlock_irqrestore(&l2ectlr_lock, flags2);
-	spin_unlock_irqrestore(&local_handler_lock, flags);
+	raw_spin_unlock_irqrestore(&l2ectlr_lock, flags2);
+	raw_spin_unlock_irqrestore(&local_handler_lock, flags);
 }
 
 static irqreturn_t arm64_ext_handler(int irq, void *drvdata)
@@ -745,7 +745,7 @@ static void check_sbe_event(struct erp_drvdata *drv)
 	errdata.drv = drv;
 	errdata.err = SBE;
 
-	spin_lock_irqsave(&local_handler_lock, flags);
+	raw_spin_lock_irqsave(&local_handler_lock, flags);
 	switch (partnum) {
 	case ARM_CPU_PART_CORTEX_A53:
 	case ARM_CPU_PART_KRYO2XX_SILVER:
@@ -763,7 +763,7 @@ static void check_sbe_event(struct erp_drvdata *drv)
 		kryo2xx_gold_parse_l2merrsr(&errdata);
 	break;
 	};
-	spin_unlock_irqrestore(&local_handler_lock, flags);
+	raw_spin_unlock_irqrestore(&local_handler_lock, flags);
 }
 
 #ifdef CONFIG_EDAC_CORTEX_ARM64_DBE_IRQ_ONLY
diff --git a/kernel/msm-3.18/drivers/gpio/gpio-msm-smp2p.c b/kernel/msm-3.18/drivers/gpio/gpio-msm-smp2p.c
index ee7be47f1..f42ab20e9 100644
--- a/kernel/msm-3.18/drivers/gpio/gpio-msm-smp2p.c
+++ b/kernel/msm-3.18/drivers/gpio/gpio-msm-smp2p.c
@@ -35,7 +35,7 @@ struct smp2p_chip_dev {
 	bool in_shadow;
 	uint32_t shadow_value;
 	struct work_struct shadow_work;
-	spinlock_t shadow_lock;
+	raw_spinlock_t shadow_lock;
 	struct notifier_block out_notifier;
 	struct notifier_block in_notifier;
 	struct msm_smp2p_out *out_handle;
@@ -44,7 +44,7 @@ struct smp2p_chip_dev {
 	struct irq_domain *irq_domain;
 	int irq_base;
 
-	spinlock_t irq_lock;
+	raw_spinlock_t irq_lock;
 	DECLARE_BITMAP(irq_enabled, SMP2P_BITS_PER_ENTRY);
 	DECLARE_BITMAP(irq_rising_edge, SMP2P_BITS_PER_ENTRY);
 	DECLARE_BITMAP(irq_falling_edge, SMP2P_BITS_PER_ENTRY);
@@ -53,7 +53,7 @@ struct smp2p_chip_dev {
 static struct platform_driver smp2p_gpio_driver;
 static struct lock_class_key smp2p_gpio_lock_class;
 static struct irq_chip smp2p_gpio_irq_chip;
-static DEFINE_SPINLOCK(smp2p_entry_lock_lha1);
+static DEFINE_RAW_SPINLOCK(smp2p_entry_lock_lha1);
 static LIST_HEAD(smp2p_entry_list);
 
 /* Used for mapping edge to name for logging. */
@@ -158,12 +158,12 @@ static void smp2p_set_value(struct gpio_chip *cp, unsigned offset, int value)
 		data_clear = 1 << offset;
 	}
 
-	spin_lock_irqsave(&chip->shadow_lock, flags);
+	raw_spin_lock_irqsave(&chip->shadow_lock, flags);
 	if (!chip->is_open) {
 		chip->in_shadow = true;
 		chip->shadow_value &= ~data_clear;
 		chip->shadow_value |= data_set;
-		spin_unlock_irqrestore(&chip->shadow_lock, flags);
+		raw_spin_unlock_irqrestore(&chip->shadow_lock, flags);
 		return;
 	}
 
@@ -178,7 +178,7 @@ static void smp2p_set_value(struct gpio_chip *cp, unsigned offset, int value)
 		ret = msm_smp2p_out_modify(chip->out_handle,
 				data_set, data_clear, send_irq);
 	}
-	spin_unlock_irqrestore(&chip->shadow_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->shadow_lock, flags);
 
 	if (ret)
 		SMP2P_GPIO("'%s':%d gpio %d set to %d failed (%d)\n",
@@ -269,12 +269,12 @@ static void smp2p_gpio_irq_mask_helper(struct irq_data *d, bool mask)
 		return;
 
 	offset = d->irq - chip->irq_base;
-	spin_lock_irqsave(&chip->irq_lock, flags);
+	raw_spin_lock_irqsave(&chip->irq_lock, flags);
 	if (mask)
 		clear_bit(offset, chip->irq_enabled);
 	else
 		set_bit(offset, chip->irq_enabled);
-	spin_unlock_irqrestore(&chip->irq_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->irq_lock, flags);
 }
 
 /**
@@ -324,7 +324,7 @@ static int smp2p_gpio_irq_set_type(struct irq_data *d, unsigned int type)
 
 	offset = d->irq - chip->irq_base;
 
-	spin_lock_irqsave(&chip->irq_lock, flags);
+	raw_spin_lock_irqsave(&chip->irq_lock, flags);
 	clear_bit(offset, chip->irq_rising_edge);
 	clear_bit(offset, chip->irq_falling_edge);
 	switch (type) {
@@ -349,7 +349,7 @@ static int smp2p_gpio_irq_set_type(struct irq_data *d, unsigned int type)
 		ret = -EINVAL;
 		break;
 	}
-	spin_unlock_irqrestore(&chip->irq_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->irq_lock, flags);
 	return ret;
 }
 
@@ -426,7 +426,7 @@ static void msm_summary_irq_handler(struct smp2p_chip_dev *chip,
 			chip->name, chip->remote_pid, prev_val, cur_val);
 
 	for (i = 0; i < SMP2P_BITS_PER_ENTRY; ++i) {
-		spin_lock_irqsave(&chip->irq_lock, flags);
+		raw_spin_lock_irqsave(&chip->irq_lock, flags);
 		trigger_interrrupt = false;
 		edge = (prev_val & 0x1) << 1 | (cur_val & 0x1);
 		irq_rising = test_bit(i, chip->irq_rising_edge);
@@ -448,7 +448,7 @@ static void msm_summary_irq_handler(struct smp2p_chip_dev *chip,
 				edge_name_falling[irq_falling],
 				edge_names[edge]);
 		}
-		spin_unlock_irqrestore(&chip->irq_lock, flags);
+		raw_spin_unlock_irqrestore(&chip->irq_lock, flags);
 
 		if (trigger_interrrupt) {
 			SMP2P_INFO(
@@ -585,7 +585,7 @@ static void smp2p_gpio_shadow_worker(struct work_struct *work)
 	unsigned long flags;
 
 	chip = container_of(work, struct smp2p_chip_dev, shadow_work);
-	spin_lock_irqsave(&chip->shadow_lock, flags);
+	raw_spin_lock_irqsave(&chip->shadow_lock, flags);
 	if (chip->in_shadow) {
 		ret = msm_smp2p_out_modify(chip->out_handle,
 					chip->shadow_value, 0x0, true);
@@ -601,7 +601,7 @@ static void smp2p_gpio_shadow_worker(struct work_struct *work)
 		chip->shadow_value = 0;
 		chip->in_shadow = false;
 	}
-	spin_unlock_irqrestore(&chip->shadow_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->shadow_lock, flags);
 }
 
 /**
@@ -628,8 +628,8 @@ static int smp2p_gpio_probe(struct platform_device *pdev)
 		ret = -ENOMEM;
 		goto fail;
 	}
-	spin_lock_init(&chip->irq_lock);
-	spin_lock_init(&chip->shadow_lock);
+	raw_spin_lock_init(&chip->irq_lock);
+	raw_spin_lock_init(&chip->shadow_lock);
 	INIT_WORK(&chip->shadow_work, smp2p_gpio_shadow_worker);
 
 	/* parse device tree */
@@ -698,9 +698,9 @@ static int smp2p_gpio_probe(struct platform_device *pdev)
 		}
 	}
 
-	spin_lock_irqsave(&smp2p_entry_lock_lha1, flags);
+	raw_spin_lock_irqsave(&smp2p_entry_lock_lha1, flags);
 	list_add(&chip->entry_list, &smp2p_entry_list);
-	spin_unlock_irqrestore(&smp2p_entry_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&smp2p_entry_lock_lha1, flags);
 
 	/*
 	 * Create interrupt domain - note that chip can't be removed from the
@@ -780,9 +780,9 @@ void smp2p_gpio_open_test_entry(const char *name, int remote_pid, bool do_open)
 	struct smp2p_chip_dev *start_entry;
 	unsigned long flags;
 
-	spin_lock_irqsave(&smp2p_entry_lock_lha1, flags);
+	raw_spin_lock_irqsave(&smp2p_entry_lock_lha1, flags);
 	if (list_empty(&smp2p_entry_list)) {
-		spin_unlock_irqrestore(&smp2p_entry_lock_lha1, flags);
+		raw_spin_unlock_irqrestore(&smp2p_entry_lock_lha1, flags);
 		return;
 	}
 	start_entry = list_first_entry(&smp2p_entry_list,
@@ -793,16 +793,16 @@ void smp2p_gpio_open_test_entry(const char *name, int remote_pid, bool do_open)
 		if (!strncmp(entry->name, name, SMP2P_MAX_ENTRY_NAME)
 				&& entry->remote_pid == remote_pid) {
 			/* found entry to change */
-			spin_unlock_irqrestore(&smp2p_entry_lock_lha1, flags);
+			raw_spin_unlock_irqrestore(&smp2p_entry_lock_lha1, flags);
 			smp2p_gpio_open_close(entry, do_open);
-			spin_lock_irqsave(&smp2p_entry_lock_lha1, flags);
+			raw_spin_lock_irqsave(&smp2p_entry_lock_lha1, flags);
 		}
 		list_rotate_left(&smp2p_entry_list);
 		entry = list_first_entry(&smp2p_entry_list,
 						struct smp2p_chip_dev,
 						entry_list);
 	} while (entry != start_entry);
-	spin_unlock_irqrestore(&smp2p_entry_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&smp2p_entry_lock_lha1, flags);
 }
 
 static struct of_device_id msm_smp2p_match_table[] = {
diff --git a/kernel/msm-3.18/drivers/gpu/drm/bridge/dw_hdmi-ahb-audio.c b/kernel/msm-3.18/drivers/gpu/drm/bridge/dw_hdmi-ahb-audio.c
index 59f630f1c..0a75cc7db 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/bridge/dw_hdmi-ahb-audio.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/bridge/dw_hdmi-ahb-audio.c
@@ -123,7 +123,7 @@ static struct dw_hdmi_channel_conf default_hdmi_channel_config[7] = {
 struct snd_dw_hdmi {
 	struct snd_card *card;
 	struct snd_pcm *pcm;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct dw_hdmi_audio_data data;
 	struct snd_pcm_substream *substream;
 	void (*reformat)(struct snd_dw_hdmi *, size_t, size_t);
@@ -289,10 +289,10 @@ static irqreturn_t snd_dw_hdmi_irq(int irq, void *data)
 	if (stat & HDMI_IH_AHBDMAAUD_STAT0_DONE && substream) {
 		snd_pcm_period_elapsed(substream);
 
-		spin_lock(&dw->lock);
+		raw_spin_lock(&dw->lock);
 		if (dw->substream)
 			dw_hdmi_start_dma(dw);
-		spin_unlock(&dw->lock);
+		raw_spin_unlock(&dw->lock);
 	}
 
 	return IRQ_HANDLED;
@@ -480,21 +480,21 @@ static int dw_hdmi_trigger(struct snd_pcm_substream *substream, int cmd)
 
 	switch (cmd) {
 	case SNDRV_PCM_TRIGGER_START:
-		spin_lock_irqsave(&dw->lock, flags);
+		raw_spin_lock_irqsave(&dw->lock, flags);
 		dw->buf_offset = 0;
 		dw->substream = substream;
 		dw_hdmi_start_dma(dw);
 		dw_hdmi_audio_enable(dw->data.hdmi);
-		spin_unlock_irqrestore(&dw->lock, flags);
+		raw_spin_unlock_irqrestore(&dw->lock, flags);
 		substream->runtime->delay = substream->runtime->period_size;
 		break;
 
 	case SNDRV_PCM_TRIGGER_STOP:
-		spin_lock_irqsave(&dw->lock, flags);
+		raw_spin_lock_irqsave(&dw->lock, flags);
 		dw->substream = NULL;
 		dw_hdmi_stop_dma(dw);
 		dw_hdmi_audio_disable(dw->data.hdmi);
-		spin_unlock_irqrestore(&dw->lock, flags);
+		raw_spin_unlock_irqrestore(&dw->lock, flags);
 		break;
 
 	default:
@@ -564,7 +564,7 @@ static int snd_dw_hdmi_probe(struct platform_device *pdev)
 	dw->data = *data;
 	dw->revision = revision;
 
-	spin_lock_init(&dw->lock);
+	raw_spin_lock_init(&dw->lock);
 
 	ret = snd_pcm_new(card, "DW HDMI", 0, 1, 0, &pcm);
 	if (ret < 0)
diff --git a/kernel/msm-3.18/drivers/gpu/drm/bridge/dw_hdmi.c b/kernel/msm-3.18/drivers/gpu/drm/bridge/dw_hdmi.c
index 56de9f1c9..346ea5829 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/bridge/dw_hdmi.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/bridge/dw_hdmi.c
@@ -134,7 +134,7 @@ struct dw_hdmi {
 	bool rxsense;			/* rxsense state */
 	u8 phy_mask;			/* desired phy int mask settings */
 
-	spinlock_t audio_lock;
+	raw_spinlock_t audio_lock;
 	struct mutex audio_mutex;
 	unsigned int sample_rate;
 	unsigned int audio_cts;
@@ -296,11 +296,11 @@ static void hdmi_set_clk_regenerator(struct dw_hdmi *hdmi,
 		__func__, sample_rate, ftdms / 1000000, (ftdms / 1000) % 1000,
 		n, cts);
 
-	spin_lock_irq(&hdmi->audio_lock);
+	raw_spin_lock_irq(&hdmi->audio_lock);
 	hdmi->audio_n = n;
 	hdmi->audio_cts = cts;
 	hdmi_set_cts_n(hdmi, cts, hdmi->audio_enable ? n : 0);
-	spin_unlock_irq(&hdmi->audio_lock);
+	raw_spin_unlock_irq(&hdmi->audio_lock);
 }
 
 static void hdmi_init_clk_regenerator(struct dw_hdmi *hdmi)
@@ -332,10 +332,10 @@ void dw_hdmi_audio_enable(struct dw_hdmi *hdmi)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&hdmi->audio_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->audio_lock, flags);
 	hdmi->audio_enable = true;
 	hdmi_set_cts_n(hdmi, hdmi->audio_cts, hdmi->audio_n);
-	spin_unlock_irqrestore(&hdmi->audio_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->audio_lock, flags);
 }
 EXPORT_SYMBOL_GPL(dw_hdmi_audio_enable);
 
@@ -343,10 +343,10 @@ void dw_hdmi_audio_disable(struct dw_hdmi *hdmi)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&hdmi->audio_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->audio_lock, flags);
 	hdmi->audio_enable = false;
 	hdmi_set_cts_n(hdmi, hdmi->audio_cts, 0);
-	spin_unlock_irqrestore(&hdmi->audio_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->audio_lock, flags);
 }
 EXPORT_SYMBOL_GPL(dw_hdmi_audio_disable);
 
@@ -1686,7 +1686,7 @@ int dw_hdmi_bind(struct device *dev, struct device *master,
 
 	mutex_init(&hdmi->mutex);
 	mutex_init(&hdmi->audio_mutex);
-	spin_lock_init(&hdmi->audio_lock);
+	raw_spin_lock_init(&hdmi->audio_lock);
 
 	of_property_read_u32(np, "reg-io-width", &val);
 
diff --git a/kernel/msm-3.18/drivers/gpu/drm/drm_atomic.c b/kernel/msm-3.18/drivers/gpu/drm/drm_atomic.c
index 6253775b8..258c7c70f 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/drm_atomic.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/drm_atomic.c
@@ -1333,19 +1333,19 @@ static struct drm_pending_vblank_event *create_vblank_event(
 	struct drm_pending_vblank_event *e = NULL;
 	unsigned long flags;
 
-	spin_lock_irqsave(&dev->event_lock, flags);
+	raw_spin_lock_irqsave(&dev->event_lock, flags);
 	if (file_priv->event_space < sizeof e->event) {
-		spin_unlock_irqrestore(&dev->event_lock, flags);
+		raw_spin_unlock_irqrestore(&dev->event_lock, flags);
 		goto out;
 	}
 	file_priv->event_space -= sizeof e->event;
-	spin_unlock_irqrestore(&dev->event_lock, flags);
+	raw_spin_unlock_irqrestore(&dev->event_lock, flags);
 
 	e = kzalloc(sizeof *e, GFP_KERNEL);
 	if (e == NULL) {
-		spin_lock_irqsave(&dev->event_lock, flags);
+		raw_spin_lock_irqsave(&dev->event_lock, flags);
 		file_priv->event_space += sizeof e->event;
-		spin_unlock_irqrestore(&dev->event_lock, flags);
+		raw_spin_unlock_irqrestore(&dev->event_lock, flags);
 		goto out;
 	}
 
@@ -1365,9 +1365,9 @@ static void destroy_vblank_event(struct drm_device *dev,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&dev->event_lock, flags);
+	raw_spin_lock_irqsave(&dev->event_lock, flags);
 	file_priv->event_space += sizeof e->event;
-	spin_unlock_irqrestore(&dev->event_lock, flags);
+	raw_spin_unlock_irqrestore(&dev->event_lock, flags);
 	kfree(e);
 }
 
diff --git a/kernel/msm-3.18/drivers/gpu/drm/i915/i915_guc_submission.c b/kernel/msm-3.18/drivers/gpu/drm/i915/i915_guc_submission.c
index 036b42bae..cf4bdea3e 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/i915/i915_guc_submission.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/i915/i915_guc_submission.c
@@ -86,7 +86,7 @@ static int host2guc_action(struct intel_guc *guc, u32 *data, u32 len)
 		return -EINVAL;
 
 	intel_uncore_forcewake_get(dev_priv, FORCEWAKE_ALL);
-	spin_lock(&dev_priv->guc.host2guc_lock);
+	raw_spin_lock(&dev_priv->guc.host2guc_lock);
 
 	dev_priv->guc.action_count += 1;
 	dev_priv->guc.action_cmd = data[0];
@@ -119,7 +119,7 @@ static int host2guc_action(struct intel_guc *guc, u32 *data, u32 len)
 	}
 	dev_priv->guc.action_status = status;
 
-	spin_unlock(&dev_priv->guc.host2guc_lock);
+	raw_spin_unlock(&dev_priv->guc.host2guc_lock);
 	intel_uncore_forcewake_put(dev_priv, FORCEWAKE_ALL);
 
 	return ret;
@@ -292,7 +292,7 @@ static uint32_t select_doorbell_cacheline(struct intel_guc *guc)
 	const uint32_t cacheline_size = cache_line_size();
 	uint32_t offset;
 
-	spin_lock(&guc->host2guc_lock);
+	raw_spin_lock(&guc->host2guc_lock);
 
 	/* Doorbell uses a single cache line within a page */
 	offset = offset_in_page(guc->db_cacheline);
@@ -300,7 +300,7 @@ static uint32_t select_doorbell_cacheline(struct intel_guc *guc)
 	/* Moving to next cache line to reduce contention */
 	guc->db_cacheline += cacheline_size;
 
-	spin_unlock(&guc->host2guc_lock);
+	raw_spin_unlock(&guc->host2guc_lock);
 
 	DRM_DEBUG_DRIVER("selected doorbell cacheline 0x%x, next 0x%x, linesize %u\n",
 			offset, guc->db_cacheline, cacheline_size);
@@ -322,13 +322,13 @@ static uint16_t assign_doorbell(struct intel_guc *guc, uint32_t priority)
 	const uint16_t end = start + half;
 	uint16_t id;
 
-	spin_lock(&guc->host2guc_lock);
+	raw_spin_lock(&guc->host2guc_lock);
 	id = find_next_zero_bit(guc->doorbell_bitmap, end, start);
 	if (id == end)
 		id = GUC_INVALID_DOORBELL_ID;
 	else
 		bitmap_set(guc->doorbell_bitmap, id, 1);
-	spin_unlock(&guc->host2guc_lock);
+	raw_spin_unlock(&guc->host2guc_lock);
 
 	DRM_DEBUG_DRIVER("assigned %s priority doorbell id 0x%x\n",
 			hi_pri ? "high" : "normal", id);
@@ -338,9 +338,9 @@ static uint16_t assign_doorbell(struct intel_guc *guc, uint32_t priority)
 
 static void release_doorbell(struct intel_guc *guc, uint16_t id)
 {
-	spin_lock(&guc->host2guc_lock);
+	raw_spin_lock(&guc->host2guc_lock);
 	bitmap_clear(guc->doorbell_bitmap, id, 1);
-	spin_unlock(&guc->host2guc_lock);
+	raw_spin_unlock(&guc->host2guc_lock);
 }
 
 /*
@@ -605,7 +605,7 @@ int i915_guc_submit(struct i915_guc_client *client,
 	/* Shall we move this right after ring is pinned? */
 	lr_context_update(rq);
 
-	spin_lock_irqsave(&client->wq_lock, flags);
+	raw_spin_lock_irqsave(&client->wq_lock, flags);
 
 	q_ret = guc_add_workqueue_item(client, rq);
 	if (q_ret == 0)
@@ -621,12 +621,12 @@ int i915_guc_submit(struct i915_guc_client *client,
 	} else {
 		client->retcode = 0;
 	}
-	spin_unlock_irqrestore(&client->wq_lock, flags);
+	raw_spin_unlock_irqrestore(&client->wq_lock, flags);
 
-	spin_lock(&guc->host2guc_lock);
+	raw_spin_lock(&guc->host2guc_lock);
 	guc->submissions[ring_id] += 1;
 	guc->last_seqno[ring_id] = rq->seqno;
-	spin_unlock(&guc->host2guc_lock);
+	raw_spin_unlock(&guc->host2guc_lock);
 
 	return q_ret;
 }
@@ -768,7 +768,7 @@ static struct i915_guc_client *guc_client_alloc(struct drm_device *dev,
 	client->client_obj = obj;
 	client->wq_offset = GUC_DB_SIZE;
 	client->wq_size = GUC_WQ_SIZE;
-	spin_lock_init(&client->wq_lock);
+	raw_spin_lock_init(&client->wq_lock);
 
 	client->doorbell_offset = select_doorbell_cacheline(guc);
 
@@ -871,7 +871,7 @@ int i915_guc_submission_init(struct drm_device *dev)
 	if (!guc->ctx_pool_obj)
 		return -ENOMEM;
 
-	spin_lock_init(&dev_priv->guc.host2guc_lock);
+	raw_spin_lock_init(&dev_priv->guc.host2guc_lock);
 
 	ida_init(&guc->ctx_ids);
 
diff --git a/kernel/msm-3.18/drivers/gpu/drm/i915/intel_fifo_underrun.c b/kernel/msm-3.18/drivers/gpu/drm/i915/intel_fifo_underrun.c
index 54daa66c6..f2cee4e3b 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/i915/intel_fifo_underrun.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/i915/intel_fifo_underrun.c
@@ -96,7 +96,7 @@ void i9xx_check_fifo_underruns(struct drm_i915_private *dev_priv)
 {
 	struct intel_crtc *crtc;
 
-	spin_lock_irq(&dev_priv->irq_lock);
+	raw_spin_lock_irq(&dev_priv->irq_lock);
 
 	for_each_intel_crtc(dev_priv->dev, crtc) {
 		u32 reg = PIPESTAT(crtc->pipe);
@@ -115,7 +115,7 @@ void i9xx_check_fifo_underruns(struct drm_i915_private *dev_priv)
 		DRM_ERROR("pipe %c underrun\n", pipe_name(crtc->pipe));
 	}
 
-	spin_unlock_irq(&dev_priv->irq_lock);
+	raw_spin_unlock_irq(&dev_priv->irq_lock);
 }
 
 static void i9xx_set_fifo_underrun_reporting(struct drm_device *dev,
@@ -274,10 +274,10 @@ bool intel_set_cpu_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
 	unsigned long flags;
 	bool ret;
 
-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+	raw_spin_lock_irqsave(&dev_priv->irq_lock, flags);
 	ret = __intel_set_cpu_fifo_underrun_reporting(dev_priv->dev, pipe,
 						      enable);
-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+	raw_spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
 
 	return ret;
 }
@@ -314,7 +314,7 @@ bool intel_set_pch_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
 	 * crtc on LPT won't cause issues.
 	 */
 
-	spin_lock_irqsave(&dev_priv->irq_lock, flags);
+	raw_spin_lock_irqsave(&dev_priv->irq_lock, flags);
 
 	old = !intel_crtc->pch_fifo_underrun_disabled;
 	intel_crtc->pch_fifo_underrun_disabled = !enable;
@@ -326,7 +326,7 @@ bool intel_set_pch_fifo_underrun_reporting(struct drm_i915_private *dev_priv,
 		cpt_set_fifo_underrun_reporting(dev_priv->dev, pch_transcoder,
 						enable, old);
 
-	spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
+	raw_spin_unlock_irqrestore(&dev_priv->irq_lock, flags);
 	return old;
 }
 
diff --git a/kernel/msm-3.18/drivers/gpu/drm/i915/intel_guc.h b/kernel/msm-3.18/drivers/gpu/drm/i915/intel_guc.h
index 081d5f648..f7f57d0fa 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/i915/intel_guc.h
+++ b/kernel/msm-3.18/drivers/gpu/drm/i915/intel_guc.h
@@ -43,7 +43,7 @@ struct i915_guc_client {
 	uint32_t wq_offset;
 	uint32_t wq_size;
 
-	spinlock_t wq_lock;		/* Protects all data below	*/
+	raw_spinlock_t wq_lock;		/* Protects all data below	*/
 	uint32_t wq_tail;
 
 	/* GuC submission statistics & status */
@@ -89,7 +89,7 @@ struct intel_guc {
 
 	struct i915_guc_client *execbuf_client;
 
-	spinlock_t host2guc_lock;	/* Protects all data below	*/
+	raw_spinlock_t host2guc_lock;	/* Protects all data below	*/
 
 	DECLARE_BITMAP(doorbell_bitmap, GUC_MAX_DOORBELLS);
 	uint32_t db_cacheline;		/* Cyclic counter mod pagesize	*/
diff --git a/kernel/msm-3.18/drivers/gpu/drm/i915/intel_hotplug.c b/kernel/msm-3.18/drivers/gpu/drm/i915/intel_hotplug.c
index d7a6437d9..ee05884a5 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/i915/intel_hotplug.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/i915/intel_hotplug.c
@@ -197,7 +197,7 @@ static void intel_hpd_irq_storm_reenable_work(struct work_struct *work)
 
 	intel_runtime_pm_get(dev_priv);
 
-	spin_lock_irq(&dev_priv->irq_lock);
+	raw_spin_lock_irq(&dev_priv->irq_lock);
 	for_each_hpd_pin(i) {
 		struct drm_connector *connector;
 
@@ -221,7 +221,7 @@ static void intel_hpd_irq_storm_reenable_work(struct work_struct *work)
 	}
 	if (dev_priv->display.hpd_irq_setup)
 		dev_priv->display.hpd_irq_setup(dev);
-	spin_unlock_irq(&dev_priv->irq_lock);
+	raw_spin_unlock_irq(&dev_priv->irq_lock);
 
 	intel_runtime_pm_put(dev_priv);
 }
@@ -256,12 +256,12 @@ static void i915_digport_work_func(struct work_struct *work)
 	int i;
 	u32 old_bits = 0;
 
-	spin_lock_irq(&dev_priv->irq_lock);
+	raw_spin_lock_irq(&dev_priv->irq_lock);
 	long_port_mask = dev_priv->hotplug.long_port_mask;
 	dev_priv->hotplug.long_port_mask = 0;
 	short_port_mask = dev_priv->hotplug.short_port_mask;
 	dev_priv->hotplug.short_port_mask = 0;
-	spin_unlock_irq(&dev_priv->irq_lock);
+	raw_spin_unlock_irq(&dev_priv->irq_lock);
 
 	for (i = 0; i < I915_MAX_PORTS; i++) {
 		bool valid = false;
@@ -288,9 +288,9 @@ static void i915_digport_work_func(struct work_struct *work)
 	}
 
 	if (old_bits) {
-		spin_lock_irq(&dev_priv->irq_lock);
+		raw_spin_lock_irq(&dev_priv->irq_lock);
 		dev_priv->hotplug.event_bits |= old_bits;
-		spin_unlock_irq(&dev_priv->irq_lock);
+		raw_spin_unlock_irq(&dev_priv->irq_lock);
 		schedule_work(&dev_priv->hotplug.hotplug_work);
 	}
 }
@@ -313,7 +313,7 @@ static void i915_hotplug_work_func(struct work_struct *work)
 	mutex_lock(&mode_config->mutex);
 	DRM_DEBUG_KMS("running encoder hotplug functions\n");
 
-	spin_lock_irq(&dev_priv->irq_lock);
+	raw_spin_lock_irq(&dev_priv->irq_lock);
 
 	hpd_event_bits = dev_priv->hotplug.event_bits;
 	dev_priv->hotplug.event_bits = 0;
@@ -321,7 +321,7 @@ static void i915_hotplug_work_func(struct work_struct *work)
 	/* Disable hotplug on connectors that hit an irq storm. */
 	intel_hpd_irq_storm_disable(dev_priv);
 
-	spin_unlock_irq(&dev_priv->irq_lock);
+	raw_spin_unlock_irq(&dev_priv->irq_lock);
 
 	list_for_each_entry(connector, &mode_config->connector_list, head) {
 		intel_connector = to_intel_connector(connector);
@@ -373,7 +373,7 @@ void intel_hpd_irq_handler(struct drm_device *dev,
 	if (!pin_mask)
 		return;
 
-	spin_lock(&dev_priv->irq_lock);
+	raw_spin_lock(&dev_priv->irq_lock);
 	for_each_hpd_pin(i) {
 		if (!(BIT(i) & pin_mask))
 			continue;
@@ -428,7 +428,7 @@ void intel_hpd_irq_handler(struct drm_device *dev,
 
 	if (storm_detected)
 		dev_priv->display.hpd_irq_setup(dev);
-	spin_unlock(&dev_priv->irq_lock);
+	raw_spin_unlock(&dev_priv->irq_lock);
 
 	/*
 	 * Our hotplug handler can grab modeset locks (by calling down into the
@@ -483,10 +483,10 @@ void intel_hpd_init(struct drm_i915_private *dev_priv)
 	 * Interrupt setup is already guaranteed to be single-threaded, this is
 	 * just to make the assert_spin_locked checks happy.
 	 */
-	spin_lock_irq(&dev_priv->irq_lock);
+	raw_spin_lock_irq(&dev_priv->irq_lock);
 	if (dev_priv->display.hpd_irq_setup)
 		dev_priv->display.hpd_irq_setup(dev);
-	spin_unlock_irq(&dev_priv->irq_lock);
+	raw_spin_unlock_irq(&dev_priv->irq_lock);
 }
 
 void intel_hpd_init_work(struct drm_i915_private *dev_priv)
@@ -499,13 +499,13 @@ void intel_hpd_init_work(struct drm_i915_private *dev_priv)
 
 void intel_hpd_cancel_work(struct drm_i915_private *dev_priv)
 {
-	spin_lock_irq(&dev_priv->irq_lock);
+	raw_spin_lock_irq(&dev_priv->irq_lock);
 
 	dev_priv->hotplug.long_port_mask = 0;
 	dev_priv->hotplug.short_port_mask = 0;
 	dev_priv->hotplug.event_bits = 0;
 
-	spin_unlock_irq(&dev_priv->irq_lock);
+	raw_spin_unlock_irq(&dev_priv->irq_lock);
 
 	cancel_work_sync(&dev_priv->hotplug.dig_port_work);
 	cancel_work_sync(&dev_priv->hotplug.hotplug_work);
diff --git a/kernel/msm-3.18/drivers/gpu/drm/i915/intel_runtime_pm.c b/kernel/msm-3.18/drivers/gpu/drm/i915/intel_runtime_pm.c
index 7e23d65c9..c34951da9 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/i915/intel_runtime_pm.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/i915/intel_runtime_pm.c
@@ -883,9 +883,9 @@ static void vlv_display_power_well_init(struct drm_i915_private *dev_priv)
 		I915_WRITE(DPLL(pipe), val);
 	}
 
-	spin_lock_irq(&dev_priv->irq_lock);
+	raw_spin_lock_irq(&dev_priv->irq_lock);
 	valleyview_enable_display_irqs(dev_priv);
-	spin_unlock_irq(&dev_priv->irq_lock);
+	raw_spin_unlock_irq(&dev_priv->irq_lock);
 
 	/*
 	 * During driver initialization/resume we can avoid restoring the
@@ -901,9 +901,9 @@ static void vlv_display_power_well_init(struct drm_i915_private *dev_priv)
 
 static void vlv_display_power_well_deinit(struct drm_i915_private *dev_priv)
 {
-	spin_lock_irq(&dev_priv->irq_lock);
+	raw_spin_lock_irq(&dev_priv->irq_lock);
 	valleyview_disable_display_irqs(dev_priv);
-	spin_unlock_irq(&dev_priv->irq_lock);
+	raw_spin_unlock_irq(&dev_priv->irq_lock);
 
 	vlv_power_sequencer_reset(dev_priv);
 }
diff --git a/kernel/msm-3.18/drivers/gpu/drm/msm/dsi/dsi_host.c b/kernel/msm-3.18/drivers/gpu/drm/msm/dsi/dsi_host.c
index 4c49868ef..943806a50 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/msm/dsi/dsi_host.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/msm/dsi/dsi_host.c
@@ -113,7 +113,7 @@ struct msm_dsi_host {
 	struct mutex dev_mutex;
 	struct mutex cmd_mutex;
 	struct mutex clk_mutex;
-	spinlock_t intr_lock; /* Protect interrupt ctrl register */
+	raw_spinlock_t intr_lock; /* Protect interrupt ctrl register */
 
 	u32 err_work_state;
 	struct work_struct err_work;
@@ -553,7 +553,7 @@ static void dsi_intr_ctrl(struct msm_dsi_host *msm_host, u32 mask, int enable)
 	u32 intr;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_host->intr_lock, flags);
+	raw_spin_lock_irqsave(&msm_host->intr_lock, flags);
 	intr = dsi_read(msm_host, REG_DSI_INTR_CTRL);
 
 	if (enable)
@@ -564,7 +564,7 @@ static void dsi_intr_ctrl(struct msm_dsi_host *msm_host, u32 mask, int enable)
 	DBG("intr=%x enable=%d", intr, enable);
 
 	dsi_write(msm_host, REG_DSI_INTR_CTRL, intr);
-	spin_unlock_irqrestore(&msm_host->intr_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_host->intr_lock, flags);
 }
 
 static inline enum dsi_traffic_mode dsi_get_traffic_mode(const u32 mode_flags)
@@ -1242,10 +1242,10 @@ static irqreturn_t dsi_host_irq(int irq, void *ptr)
 	if (!msm_host->ctrl_base)
 		return IRQ_HANDLED;
 
-	spin_lock_irqsave(&msm_host->intr_lock, flags);
+	raw_spin_lock_irqsave(&msm_host->intr_lock, flags);
 	isr = dsi_read(msm_host, REG_DSI_INTR_CTRL);
 	dsi_write(msm_host, REG_DSI_INTR_CTRL, isr);
-	spin_unlock_irqrestore(&msm_host->intr_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_host->intr_lock, flags);
 
 	DBG("isr=0x%x, id=%d", isr, msm_host->id);
 
@@ -1448,7 +1448,7 @@ int msm_dsi_host_init(struct msm_dsi *msm_dsi)
 	mutex_init(&msm_host->dev_mutex);
 	mutex_init(&msm_host->cmd_mutex);
 	mutex_init(&msm_host->clk_mutex);
-	spin_lock_init(&msm_host->intr_lock);
+	raw_spin_lock_init(&msm_host->intr_lock);
 
 	/* setup workqueue */
 	msm_host->workqueue = alloc_ordered_workqueue("dsi_drm_work", 0);
diff --git a/kernel/msm-3.18/drivers/gpu/drm/msm/hdmi/hdmi_hdcp.c b/kernel/msm-3.18/drivers/gpu/drm/msm/hdmi/hdmi_hdcp.c
index e56a8675c..fa6197792 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/msm/hdmi/hdmi_hdcp.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/msm/hdmi/hdmi_hdcp.c
@@ -209,11 +209,11 @@ void hdmi_hdcp_ctrl_irq(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 	u32 reg_val, hdcp_int_status;
 	unsigned long flags;
 
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	reg_val = hdmi_read(hdmi, REG_HDMI_HDCP_INT_CTRL);
 	hdcp_int_status = reg_val & HDCP_INT_STATUS_MASK;
 	if (!hdcp_int_status) {
-		spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+		raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 		return;
 	}
 	/* Clear Interrupts */
@@ -222,7 +222,7 @@ void hdmi_hdcp_ctrl_irq(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 	if (hdcp_int_status & HDMI_HDCP_INT_CTRL_AUTH_FAIL_INT)
 		reg_val |= HDMI_HDCP_INT_CTRL_AUTH_FAIL_INFO_ACK;
 	hdmi_write(hdmi, REG_HDMI_HDCP_INT_CTRL, reg_val);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	DBG("hdcp irq %x", hdcp_int_status);
 
@@ -418,14 +418,14 @@ static void hdmi_hdcp_reauth_work(struct work_struct *work)
 	 * attempt a re-authentication, HW would clear the AN0_READY and
 	 * AN1_READY bits in HDMI_HDCP_LINK0_STATUS register
 	 */
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	reg_val = hdmi_read(hdmi, REG_HDMI_HPD_CTRL);
 	reg_val &= ~HDMI_HPD_CTRL_ENABLE;
 	hdmi_write(hdmi, REG_HDMI_HPD_CTRL, reg_val);
 
 	/* Disable HDCP interrupts */
 	hdmi_write(hdmi, REG_HDMI_HDCP_INT_CTRL, 0);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	hdmi_write(hdmi, REG_HDMI_HDCP_RESET,
 		HDMI_HDCP_RESET_LINK0_DEAUTHENTICATE);
@@ -440,11 +440,11 @@ static void hdmi_hdcp_reauth_work(struct work_struct *work)
 	hdmi_write(hdmi, REG_HDMI_HDCP_CTRL, 0);
 
 	/* Enable HPD circuitry */
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	reg_val = hdmi_read(hdmi, REG_HDMI_HPD_CTRL);
 	reg_val |= HDMI_HPD_CTRL_ENABLE;
 	hdmi_write(hdmi, REG_HDMI_HPD_CTRL, reg_val);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	/*
 	 * Only retry defined times then abort current authenticating process
@@ -480,7 +480,7 @@ static int hdmi_hdcp_auth_prepare(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 		hdcp_ctrl->aksv_valid = true;
 	}
 
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	/* disable HDMI Encrypt */
 	reg_val = hdmi_read(hdmi, REG_HDMI_CTRL);
 	reg_val &= ~HDMI_CTRL_ENCRYPTED;
@@ -490,7 +490,7 @@ static int hdmi_hdcp_auth_prepare(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 	reg_val = hdmi_read(hdmi, REG_HDMI_DDC_ARBITRATION);
 	reg_val &= ~HDMI_DDC_ARBITRATION_HW_ARBITRATION;
 	hdmi_write(hdmi, REG_HDMI_DDC_ARBITRATION, reg_val);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	/*
 	 * Write AKSV read from QFPROM to the HDCP registers.
@@ -552,11 +552,11 @@ static void hdmi_hdcp_auth_fail(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 
 	DBG("hdcp auth failed, queue reauth work");
 	/* clear HDMI Encrypt */
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	reg_val = hdmi_read(hdmi, REG_HDMI_CTRL);
 	reg_val &= ~HDMI_CTRL_ENCRYPTED;
 	hdmi_write(hdmi, REG_HDMI_CTRL, reg_val);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	hdcp_ctrl->hdcp_state = HDCP_STATE_AUTH_FAILED;
 	queue_work(hdmi->workq, &hdcp_ctrl->hdcp_reauth_work);
@@ -572,18 +572,18 @@ static void hdmi_hdcp_auth_done(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 	 * Disable software DDC before going into part3 to make sure
 	 * there is no Arbitration between software and hardware for DDC
 	 */
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	reg_val = hdmi_read(hdmi, REG_HDMI_DDC_ARBITRATION);
 	reg_val |= HDMI_DDC_ARBITRATION_HW_ARBITRATION;
 	hdmi_write(hdmi, REG_HDMI_DDC_ARBITRATION, reg_val);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	/* enable HDMI Encrypt */
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	reg_val = hdmi_read(hdmi, REG_HDMI_CTRL);
 	reg_val |= HDMI_CTRL_ENCRYPTED;
 	hdmi_write(hdmi, REG_HDMI_CTRL, reg_val);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	hdcp_ctrl->hdcp_state = HDCP_STATE_AUTHENTICATED;
 	hdcp_ctrl->auth_retries = 0;
@@ -800,14 +800,14 @@ static int hdmi_hdcp_auth_part1_key_exchange(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 	}
 
 	/* Enable HDCP interrupts and ack/clear any stale interrupts */
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	hdmi_write(hdmi, REG_HDMI_HDCP_INT_CTRL,
 		HDMI_HDCP_INT_CTRL_AUTH_SUCCESS_ACK |
 		HDMI_HDCP_INT_CTRL_AUTH_SUCCESS_MASK |
 		HDMI_HDCP_INT_CTRL_AUTH_FAIL_ACK |
 		HDMI_HDCP_INT_CTRL_AUTH_FAIL_MASK |
 		HDMI_HDCP_INT_CTRL_AUTH_FAIL_INFO_ACK);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	return 0;
 }
@@ -1324,11 +1324,11 @@ void hdmi_hdcp_ctrl_on(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 	}
 
 	/* clear HDMI Encrypt */
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	reg_val = hdmi_read(hdmi, REG_HDMI_CTRL);
 	reg_val &= ~HDMI_CTRL_ENCRYPTED;
 	hdmi_write(hdmi, REG_HDMI_CTRL, reg_val);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	hdcp_ctrl->auth_event = 0;
 	hdcp_ctrl->hdcp_state = HDCP_STATE_AUTHENTICATING;
@@ -1354,7 +1354,7 @@ void hdmi_hdcp_ctrl_off(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 	 * attempt a re-authentication, HW would clear the AN0_READY and
 	 * AN1_READY bits in HDMI_HDCP_LINK0_STATUS register
 	 */
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	reg_val = hdmi_read(hdmi, REG_HDMI_HPD_CTRL);
 	reg_val &= ~HDMI_HPD_CTRL_ENABLE;
 	hdmi_write(hdmi, REG_HDMI_HPD_CTRL, reg_val);
@@ -1365,7 +1365,7 @@ void hdmi_hdcp_ctrl_off(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 	 * reauth works will know that the HDCP session has been turned off.
 	 */
 	hdmi_write(hdmi, REG_HDMI_HDCP_INT_CTRL, 0);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	/*
 	 * Cancel any pending auth/reauth attempts.
@@ -1384,7 +1384,7 @@ void hdmi_hdcp_ctrl_off(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 	/* Disable encryption and disable the HDCP block */
 	hdmi_write(hdmi, REG_HDMI_HDCP_CTRL, 0);
 
-	spin_lock_irqsave(&hdmi->reg_lock, flags);
+	raw_spin_lock_irqsave(&hdmi->reg_lock, flags);
 	reg_val = hdmi_read(hdmi, REG_HDMI_CTRL);
 	reg_val &= ~HDMI_CTRL_ENCRYPTED;
 	hdmi_write(hdmi, REG_HDMI_CTRL, reg_val);
@@ -1393,7 +1393,7 @@ void hdmi_hdcp_ctrl_off(struct hdmi_hdcp_ctrl *hdcp_ctrl)
 	reg_val = hdmi_read(hdmi, REG_HDMI_HPD_CTRL);
 	reg_val |= HDMI_HPD_CTRL_ENABLE;
 	hdmi_write(hdmi, REG_HDMI_HPD_CTRL, reg_val);
-	spin_unlock_irqrestore(&hdmi->reg_lock, flags);
+	raw_spin_unlock_irqrestore(&hdmi->reg_lock, flags);
 
 	hdcp_ctrl->hdcp_state = HDCP_STATE_INACTIVE;
 
diff --git a/kernel/msm-3.18/drivers/gpu/drm/msm/mdp/mdp5/mdp5_ctl.c b/kernel/msm-3.18/drivers/gpu/drm/msm/mdp/mdp5/mdp5_ctl.c
index b575128b8..87cc0f601 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/msm/mdp/mdp5/mdp5_ctl.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/msm/mdp/mdp5/mdp5_ctl.c
@@ -52,7 +52,7 @@ struct mdp5_ctl {
 	struct op_mode pipeline;
 
 	/* REG_MDP5_CTL_*(<id>) registers access info + lock: */
-	spinlock_t hw_lock;
+	raw_spinlock_t hw_lock;
 	u32 reg_offset;
 
 	/* when do CTL registers need to be flushed? (mask of trigger bits) */
@@ -84,7 +84,7 @@ struct mdp5_ctl_manager {
 	u32 single_flush_pending_mask;
 
 	/* pool of CTLs + lock to protect resource allocation (ctls[i].busy) */
-	spinlock_t pool_lock;
+	raw_spinlock_t pool_lock;
 	struct mdp5_ctl ctls[MAX_CTL];
 };
 
@@ -120,7 +120,7 @@ static void set_display_intf(struct mdp5_kms *mdp5_kms,
 	unsigned long flags;
 	u32 intf_sel;
 
-	spin_lock_irqsave(&mdp5_kms->resource_lock, flags);
+	raw_spin_lock_irqsave(&mdp5_kms->resource_lock, flags);
 	intf_sel = mdp5_read(mdp5_kms, REG_MDP5_MDP_DISP_INTF_SEL(0));
 
 	switch (intf->num) {
@@ -146,7 +146,7 @@ static void set_display_intf(struct mdp5_kms *mdp5_kms,
 	}
 
 	mdp5_write(mdp5_kms, REG_MDP5_MDP_DISP_INTF_SEL(0), intf_sel);
-	spin_unlock_irqrestore(&mdp5_kms->resource_lock, flags);
+	raw_spin_unlock_irqrestore(&mdp5_kms->resource_lock, flags);
 }
 
 static void set_ctl_op(struct mdp5_ctl *ctl, struct mdp5_interface *intf)
@@ -172,9 +172,9 @@ static void set_ctl_op(struct mdp5_ctl *ctl, struct mdp5_interface *intf)
 		break;
 	}
 
-	spin_lock_irqsave(&ctl->hw_lock, flags);
+	raw_spin_lock_irqsave(&ctl->hw_lock, flags);
 	ctl_write(ctl, REG_MDP5_CTL_OP(ctl->id), ctl_op);
-	spin_unlock_irqrestore(&ctl->hw_lock, flags);
+	raw_spin_unlock_irqrestore(&ctl->hw_lock, flags);
 }
 
 int mdp5_ctl_set_pipeline(struct mdp5_ctl *ctl,
@@ -252,9 +252,9 @@ static void send_start_signal(struct mdp5_ctl *ctl)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctl->hw_lock, flags);
+	raw_spin_lock_irqsave(&ctl->hw_lock, flags);
 	ctl_write(ctl, REG_MDP5_CTL_START(ctl->id), 1);
-	spin_unlock_irqrestore(&ctl->hw_lock, flags);
+	raw_spin_unlock_irqrestore(&ctl->hw_lock, flags);
 }
 
 static void refill_start_mask(struct mdp5_ctl *ctl)
@@ -314,7 +314,7 @@ int mdp5_ctl_set_cursor(struct mdp5_ctl *ctl, int cursor_id, bool enable)
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&ctl->hw_lock, flags);
+	raw_spin_lock_irqsave(&ctl->hw_lock, flags);
 
 	blend_ext_cfg = ctl_read(ctl, REG_MDP5_CTL_LAYER_EXT_REG(ctl->id, lm));
 
@@ -332,7 +332,7 @@ int mdp5_ctl_set_cursor(struct mdp5_ctl *ctl, int cursor_id, bool enable)
 	ctl->cursor_on = enable;
 	ctl->cursor_id = cursor_id;
 
-	spin_unlock_irqrestore(&ctl->hw_lock, flags);
+	raw_spin_unlock_irqrestore(&ctl->hw_lock, flags);
 
 	ctl->pending_ctl_trigger = (mdp_ctl_flush_mask_cursor(cursor_id)|
 					mdp_ctl_flush_mask_lm(ctl->lm));
@@ -400,13 +400,13 @@ int mdp5_ctl_blend(struct mdp5_ctl *ctl, u8 *stage, u32 stage_cnt,
 		blend_ext_cfg |= mdp_ctl_blend_ext_mask(stage[i], i);
 	}
 
-	spin_lock_irqsave(&ctl->hw_lock, flags);
+	raw_spin_lock_irqsave(&ctl->hw_lock, flags);
 	if (ctl->cursor_on)
 		blend_ext_cfg |= cursor_blend_value(ctl->cursor_id, STAGE6);
 
 	ctl_write(ctl, REG_MDP5_CTL_LAYER_REG(ctl->id, ctl->lm), blend_cfg);
 	ctl_write(ctl, REG_MDP5_CTL_LAYER_EXT_REG(ctl->id, ctl->lm), blend_ext_cfg);
-	spin_unlock_irqrestore(&ctl->hw_lock, flags);
+	raw_spin_unlock_irqrestore(&ctl->hw_lock, flags);
 
 	ctl->pending_ctl_trigger = mdp_ctl_flush_mask_lm(ctl->lm);
 
@@ -549,9 +549,9 @@ u32 mdp5_ctl_commit(struct mdp5_ctl *ctl, u32 flush_mask)
 	fix_for_single_flush(ctl, &flush_mask, &flush_id);
 
 	if (flush_mask) {
-		spin_lock_irqsave(&ctl->hw_lock, flags);
+		raw_spin_lock_irqsave(&ctl->hw_lock, flags);
 		ctl_write(ctl, REG_MDP5_CTL_FLUSH(flush_id), flush_mask);
-		spin_unlock_irqrestore(&ctl->hw_lock, flags);
+		raw_spin_unlock_irqrestore(&ctl->hw_lock, flags);
 	}
 
 	if (start_signal_needed(ctl)) {
@@ -623,7 +623,7 @@ struct mdp5_ctl *mdp5_ctlm_request(struct mdp5_ctl_manager *ctl_mgr,
 	unsigned long flags;
 	int c;
 
-	spin_lock_irqsave(&ctl_mgr->pool_lock, flags);
+	raw_spin_lock_irqsave(&ctl_mgr->pool_lock, flags);
 
 	/* search the preferred */
 	for (c = 0; c < ctl_mgr->nctl; c++)
@@ -650,7 +650,7 @@ found:
 	DBG("CTL %d allocated", ctl->id);
 
 unlock:
-	spin_unlock_irqrestore(&ctl_mgr->pool_lock, flags);
+	raw_spin_unlock_irqrestore(&ctl_mgr->pool_lock, flags);
 	return ctl;
 }
 
@@ -662,9 +662,9 @@ void mdp5_ctlm_hw_reset(struct mdp5_ctl_manager *ctl_mgr)
 	for (c = 0; c < ctl_mgr->nctl; c++) {
 		struct mdp5_ctl *ctl = &ctl_mgr->ctls[c];
 
-		spin_lock_irqsave(&ctl->hw_lock, flags);
+		raw_spin_lock_irqsave(&ctl->hw_lock, flags);
 		ctl_write(ctl, REG_MDP5_CTL_OP(ctl->id), 0);
-		spin_unlock_irqrestore(&ctl->hw_lock, flags);
+		raw_spin_unlock_irqrestore(&ctl->hw_lock, flags);
 	}
 }
 
@@ -702,24 +702,24 @@ struct mdp5_ctl_manager *mdp5_ctlm_init(struct drm_device *dev,
 	ctl_mgr->nlm = hw_cfg->lm.count;
 	ctl_mgr->nctl = ctl_cfg->count;
 	ctl_mgr->flush_hw_mask = ctl_cfg->flush_hw_mask;
-	spin_lock_init(&ctl_mgr->pool_lock);
+	raw_spin_lock_init(&ctl_mgr->pool_lock);
 
 	/* initialize each CTL of the pool: */
-	spin_lock_irqsave(&ctl_mgr->pool_lock, flags);
+	raw_spin_lock_irqsave(&ctl_mgr->pool_lock, flags);
 	for (c = 0; c < ctl_mgr->nctl; c++) {
 		struct mdp5_ctl *ctl = &ctl_mgr->ctls[c];
 
 		if (WARN_ON(!ctl_cfg->base[c])) {
 			dev_err(dev->dev, "CTL_%d: base is null!\n", c);
 			ret = -EINVAL;
-			spin_unlock_irqrestore(&ctl_mgr->pool_lock, flags);
+			raw_spin_unlock_irqrestore(&ctl_mgr->pool_lock, flags);
 			goto fail;
 		}
 		ctl->ctlm = ctl_mgr;
 		ctl->id = c;
 		ctl->reg_offset = ctl_cfg->base[c];
 		ctl->status = 0;
-		spin_lock_init(&ctl->hw_lock);
+		raw_spin_lock_init(&ctl->hw_lock);
 	}
 
 	/*
@@ -734,7 +734,7 @@ struct mdp5_ctl_manager *mdp5_ctlm_init(struct drm_device *dev,
 		ctl_mgr->ctls[0].status |= CTL_STAT_BOOKED;
 		ctl_mgr->ctls[1].status |= CTL_STAT_BOOKED;
 	}
-	spin_unlock_irqrestore(&ctl_mgr->pool_lock, flags);
+	raw_spin_unlock_irqrestore(&ctl_mgr->pool_lock, flags);
 	DBG("Pool of %d CTLs created.", ctl_mgr->nctl);
 
 	return ctl_mgr;
diff --git a/kernel/msm-3.18/drivers/gpu/drm/msm/msm_atomic.c b/kernel/msm-3.18/drivers/gpu/drm/msm/msm_atomic.c
index fa746d71c..6871dbf53 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/msm/msm_atomic.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/msm/msm_atomic.c
@@ -36,14 +36,14 @@ static int start_atomic(struct msm_drm_private *priv, uint32_t crtc_mask)
 {
 	int ret;
 
-	spin_lock(&priv->pending_crtcs_event.lock);
+	raw_spin_lock(&priv->pending_crtcs_event.lock);
 	ret = wait_event_interruptible_locked(priv->pending_crtcs_event,
 			!(priv->pending_crtcs & crtc_mask));
 	if (ret == 0) {
 		DBG("start: %08x", crtc_mask);
 		priv->pending_crtcs |= crtc_mask;
 	}
-	spin_unlock(&priv->pending_crtcs_event.lock);
+	raw_spin_unlock(&priv->pending_crtcs_event.lock);
 
 	return ret;
 }
@@ -52,11 +52,11 @@ static int start_atomic(struct msm_drm_private *priv, uint32_t crtc_mask)
  */
 static void end_atomic(struct msm_drm_private *priv, uint32_t crtc_mask)
 {
-	spin_lock(&priv->pending_crtcs_event.lock);
+	raw_spin_lock(&priv->pending_crtcs_event.lock);
 	DBG("end: %08x", crtc_mask);
 	priv->pending_crtcs &= ~crtc_mask;
 	wake_up_all_locked(&priv->pending_crtcs_event);
-	spin_unlock(&priv->pending_crtcs_event.lock);
+	raw_spin_unlock(&priv->pending_crtcs_event.lock);
 }
 
 static void commit_destroy(struct msm_commit *commit)
diff --git a/kernel/msm-3.18/drivers/gpu/drm/sti/sti_crtc.c b/kernel/msm-3.18/drivers/gpu/drm/sti/sti_crtc.c
index 3ae09dcd4..a6760948b 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/sti/sti_crtc.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/sti/sti_crtc.c
@@ -274,13 +274,13 @@ int sti_crtc_vblank_cb(struct notifier_block *nb,
 
 	drm_crtc_handle_vblank(crtc);
 
-	spin_lock_irqsave(&crtc->dev->event_lock, flags);
+	raw_spin_lock_irqsave(&crtc->dev->event_lock, flags);
 	if (mixer->pending_event) {
 		drm_crtc_send_vblank_event(crtc, mixer->pending_event);
 		drm_crtc_vblank_put(crtc);
 		mixer->pending_event = NULL;
 	}
-	spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
+	raw_spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
 
 	if (mixer->status == STI_MIXER_DISABLING) {
 		struct drm_plane *p;
diff --git a/kernel/msm-3.18/drivers/gpu/drm/vmwgfx/vmwgfx_cmdbuf.c b/kernel/msm-3.18/drivers/gpu/drm/vmwgfx/vmwgfx_cmdbuf.c
index 67cebb23c..eafe8655a 100644
--- a/kernel/msm-3.18/drivers/gpu/drm/vmwgfx/vmwgfx_cmdbuf.c
+++ b/kernel/msm-3.18/drivers/gpu/drm/vmwgfx/vmwgfx_cmdbuf.c
@@ -113,7 +113,7 @@ struct vmw_cmdbuf_man {
 	size_t cur_pos;
 	size_t default_size;
 	unsigned max_hw_submitted;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct dma_pool *headers;
 	struct dma_pool *dheaders;
 	struct tasklet_struct tasklet;
@@ -277,9 +277,9 @@ void vmw_cmdbuf_header_free(struct vmw_cmdbuf_header *header)
 		vmw_cmdbuf_header_inline_free(header);
 		return;
 	}
-	spin_lock_bh(&man->lock);
+	raw_spin_lock_bh(&man->lock);
 	__vmw_cmdbuf_header_free(header);
-	spin_unlock_bh(&man->lock);
+	raw_spin_unlock_bh(&man->lock);
 }
 
 
@@ -484,9 +484,9 @@ static void vmw_cmdbuf_man_tasklet(unsigned long data)
 {
 	struct vmw_cmdbuf_man *man = (struct vmw_cmdbuf_man *) data;
 
-	spin_lock(&man->lock);
+	raw_spin_lock(&man->lock);
 	vmw_cmdbuf_man_process(man);
-	spin_unlock(&man->lock);
+	raw_spin_unlock(&man->lock);
 }
 
 /**
@@ -506,7 +506,7 @@ static void vmw_cmdbuf_work_func(struct work_struct *work)
 	uint32_t dummy;
 	bool restart = false;
 
-	spin_lock_bh(&man->lock);
+	raw_spin_lock_bh(&man->lock);
 	list_for_each_entry_safe(entry, next, &man->error, list) {
 		restart = true;
 		DRM_ERROR("Command buffer error.\n");
@@ -515,7 +515,7 @@ static void vmw_cmdbuf_work_func(struct work_struct *work)
 		__vmw_cmdbuf_header_free(entry);
 		wake_up_all(&man->idle_queue);
 	}
-	spin_unlock_bh(&man->lock);
+	raw_spin_unlock_bh(&man->lock);
 
 	if (restart && vmw_cmdbuf_startstop(man, true))
 		DRM_ERROR("Failed restarting command buffer context 0.\n");
@@ -538,7 +538,7 @@ static bool vmw_cmdbuf_man_idle(struct vmw_cmdbuf_man *man,
 	bool idle = false;
 	int i;
 
-	spin_lock_bh(&man->lock);
+	raw_spin_lock_bh(&man->lock);
 	vmw_cmdbuf_man_process(man);
 	for_each_cmdbuf_ctx(man, i, ctx) {
 		if (!list_empty(&ctx->submitted) ||
@@ -550,7 +550,7 @@ static bool vmw_cmdbuf_man_idle(struct vmw_cmdbuf_man *man,
 	idle = list_empty(&man->error);
 
 out_unlock:
-	spin_unlock_bh(&man->lock);
+	raw_spin_unlock_bh(&man->lock);
 
 	return idle;
 }
@@ -573,7 +573,7 @@ static void __vmw_cmdbuf_cur_flush(struct vmw_cmdbuf_man *man)
 	if (!cur)
 		return;
 
-	spin_lock_bh(&man->lock);
+	raw_spin_lock_bh(&man->lock);
 	if (man->cur_pos == 0) {
 		__vmw_cmdbuf_header_free(cur);
 		goto out_unlock;
@@ -582,7 +582,7 @@ static void __vmw_cmdbuf_cur_flush(struct vmw_cmdbuf_man *man)
 	man->cur->cb_header->length = man->cur_pos;
 	vmw_cmdbuf_ctx_add(man, man->cur, SVGA_CB_CONTEXT_0);
 out_unlock:
-	spin_unlock_bh(&man->lock);
+	raw_spin_unlock_bh(&man->lock);
 	man->cur = NULL;
 	man->cur_pos = 0;
 }
@@ -675,7 +675,7 @@ static bool vmw_cmdbuf_try_alloc(struct vmw_cmdbuf_man *man,
 		return true;
  
 	memset(info->node, 0, sizeof(*info->node));
-	spin_lock_bh(&man->lock);
+	raw_spin_lock_bh(&man->lock);
 	ret = drm_mm_insert_node_generic(&man->mm, info->node, info->page_size,
 					 0, 0,
 					 DRM_MM_SEARCH_DEFAULT,
@@ -688,7 +688,7 @@ static bool vmw_cmdbuf_try_alloc(struct vmw_cmdbuf_man *man,
 						 DRM_MM_CREATE_DEFAULT);
 	}
 
-	spin_unlock_bh(&man->lock);
+	raw_spin_unlock_bh(&man->lock);
 	info->done = !ret;
 
 	return info->done;
@@ -810,9 +810,9 @@ static int vmw_cmdbuf_space_pool(struct vmw_cmdbuf_man *man,
 	return 0;
 
 out_no_cb_header:
-	spin_lock_bh(&man->lock);
+	raw_spin_lock_bh(&man->lock);
 	drm_mm_remove_node(&header->node);
-	spin_unlock_bh(&man->lock);
+	raw_spin_unlock_bh(&man->lock);
 
 	return ret;
 }
@@ -1069,9 +1069,9 @@ static int vmw_cmdbuf_send_device_command(struct vmw_cmdbuf_man *man,
 	memcpy(cmd, command, size);
 	header->cb_header->length = size;
 	header->cb_context = SVGA_CB_CONTEXT_DEVICE;
-	spin_lock_bh(&man->lock);
+	raw_spin_lock_bh(&man->lock);
 	status = vmw_cmdbuf_header_submit(header);
-	spin_unlock_bh(&man->lock);
+	raw_spin_unlock_bh(&man->lock);
 	vmw_cmdbuf_header_free(header);
 
 	if (status != SVGA_CB_STATUS_COMPLETED) {
@@ -1233,7 +1233,7 @@ struct vmw_cmdbuf_man *vmw_cmdbuf_man_create(struct vmw_private *dev_priv)
 		vmw_cmdbuf_ctx_init(ctx);
 
 	INIT_LIST_HEAD(&man->error);
-	spin_lock_init(&man->lock);
+	raw_spin_lock_init(&man->lock);
 	mutex_init(&man->cur_mutex);
 	mutex_init(&man->space_mutex);
 	tasklet_init(&man->tasklet, vmw_cmdbuf_man_tasklet,
diff --git a/kernel/msm-3.18/drivers/input/keycombo.c b/kernel/msm-3.18/drivers/input/keycombo.c
index 2fba451b9..1dae12a10 100644
--- a/kernel/msm-3.18/drivers/input/keycombo.c
+++ b/kernel/msm-3.18/drivers/input/keycombo.c
@@ -26,7 +26,7 @@ struct keycombo_state {
 	unsigned long keybit[BITS_TO_LONGS(KEY_CNT)];
 	unsigned long upbit[BITS_TO_LONGS(KEY_CNT)];
 	unsigned long key[BITS_TO_LONGS(KEY_CNT)];
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct  workqueue_struct *wq;
 	int key_down_target;
 	int key_down;
@@ -76,7 +76,7 @@ static void keycombo_event(struct input_handle *handle, unsigned int type,
 	if (!test_bit(code, state->keybit))
 		return;
 
-	spin_lock_irqsave(&state->lock, flags);
+	raw_spin_lock_irqsave(&state->lock, flags);
 	if (!test_bit(code, state->key) == !value)
 		goto done;
 	__change_bit(code, state->key);
@@ -106,7 +106,7 @@ static void keycombo_event(struct input_handle *handle, unsigned int type,
 		state->key_is_down = 0;
 	}
 done:
-	spin_unlock_irqrestore(&state->lock, flags);
+	raw_spin_unlock_irqrestore(&state->lock, flags);
 }
 
 static int keycombo_connect(struct input_handler *handler,
@@ -181,7 +181,7 @@ static int keycombo_probe(struct platform_device *pdev)
 	if (!state)
 		return -ENOMEM;
 
-	spin_lock_init(&state->lock);
+	raw_spin_lock_init(&state->lock);
 	keyp = pdata->keys_down;
 	while ((key = *keyp++)) {
 		if (key >= KEY_MAX)
diff --git a/kernel/msm-3.18/drivers/input/misc/gpio_input.c b/kernel/msm-3.18/drivers/input/misc/gpio_input.c
index eefd02725..8ee7be52d 100644
--- a/kernel/msm-3.18/drivers/input/misc/gpio_input.c
+++ b/kernel/msm-3.18/drivers/input/misc/gpio_input.c
@@ -44,7 +44,7 @@ struct gpio_input_state {
 	struct hrtimer timer;
 	int use_irq;
 	int debounce_count;
-	spinlock_t irq_lock;
+	raw_spinlock_t irq_lock;
 	struct wakeup_source *ws;
 	struct gpio_key_state key_state[0];
 };
@@ -74,7 +74,7 @@ static enum hrtimer_restart gpio_event_input_timer_func(struct hrtimer *timer)
 	key_entry = ds->info->keymap;
 	key_state = ds->key_state;
 	sync_needed = false;
-	spin_lock_irqsave(&ds->irq_lock, irqflags);
+	raw_spin_lock_irqsave(&ds->irq_lock, irqflags);
 	for (i = 0; i < nkeys; i++, key_entry++, key_state++) {
 		debounce = key_state->debounce;
 		if (debounce & DEBOUNCE_WAIT_IRQ)
@@ -155,7 +155,7 @@ static enum hrtimer_restart gpio_event_input_timer_func(struct hrtimer *timer)
 	else
 		__pm_relax(ds->ws);
 
-	spin_unlock_irqrestore(&ds->irq_lock, irqflags);
+	raw_spin_unlock_irqrestore(&ds->irq_lock, irqflags);
 
 	return HRTIMER_NORESTART;
 }
@@ -175,7 +175,7 @@ static irqreturn_t gpio_event_input_irq_handler(int irq, void *dev_id)
 	key_entry = &ds->info->keymap[keymap_index];
 
 	if (ds->info->debounce_time.tv64) {
-		spin_lock_irqsave(&ds->irq_lock, irqflags);
+		raw_spin_lock_irqsave(&ds->irq_lock, irqflags);
 		if (ks->debounce & DEBOUNCE_WAIT_IRQ) {
 			ks->debounce = DEBOUNCE_UNKNOWN;
 			if (ds->debounce_count++ == 0) {
@@ -193,7 +193,7 @@ static irqreturn_t gpio_event_input_irq_handler(int irq, void *dev_id)
 			disable_irq_nosync(irq);
 			ks->debounce = DEBOUNCE_UNSTABLE;
 		}
-		spin_unlock_irqrestore(&ds->irq_lock, irqflags);
+		raw_spin_unlock_irqrestore(&ds->irq_lock, irqflags);
 	} else {
 		pressed = gpio_get_value(key_entry->gpio) ^
 			!(ds->info->flags & GPIOEDF_ACTIVE_HIGH);
@@ -274,12 +274,12 @@ int gpio_event_input_func(struct gpio_event_input_devs *input_devs,
 		return 0;
 	}
 	if (func == GPIO_EVENT_FUNC_RESUME) {
-		spin_lock_irqsave(&ds->irq_lock, irqflags);
+		raw_spin_lock_irqsave(&ds->irq_lock, irqflags);
 		if (ds->use_irq)
 			for (i = 0; i < di->keymap_size; i++)
 				enable_irq(gpio_to_irq(di->keymap[i].gpio));
 		hrtimer_start(&ds->timer, ktime_set(0, 0), HRTIMER_MODE_REL);
-		spin_unlock_irqrestore(&ds->irq_lock, irqflags);
+		raw_spin_unlock_irqrestore(&ds->irq_lock, irqflags);
 		return 0;
 	}
 
@@ -311,7 +311,7 @@ int gpio_event_input_func(struct gpio_event_input_devs *input_devs,
 			goto err_ws_failed;
 		}
 
-		spin_lock_init(&ds->irq_lock);
+		raw_spin_lock_init(&ds->irq_lock);
 
 		for (i = 0; i < di->keymap_size; i++) {
 			int dev = di->keymap[i].dev;
@@ -347,7 +347,7 @@ int gpio_event_input_func(struct gpio_event_input_devs *input_devs,
 
 		ret = gpio_event_input_request_irqs(ds);
 
-		spin_lock_irqsave(&ds->irq_lock, irqflags);
+		raw_spin_lock_irqsave(&ds->irq_lock, irqflags);
 		ds->use_irq = ret == 0;
 
 		pr_info("GPIO Input Driver: Start gpio inputs for %s%s in %s "
@@ -358,12 +358,12 @@ int gpio_event_input_func(struct gpio_event_input_devs *input_devs,
 		hrtimer_init(&ds->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 		ds->timer.function = gpio_event_input_timer_func;
 		hrtimer_start(&ds->timer, ktime_set(0, 0), HRTIMER_MODE_REL);
-		spin_unlock_irqrestore(&ds->irq_lock, irqflags);
+		raw_spin_unlock_irqrestore(&ds->irq_lock, irqflags);
 		return 0;
 	}
 
 	ret = 0;
-	spin_lock_irqsave(&ds->irq_lock, irqflags);
+	raw_spin_lock_irqsave(&ds->irq_lock, irqflags);
 	hrtimer_cancel(&ds->timer);
 	if (ds->use_irq) {
 		for (i = di->keymap_size - 1; i >= 0; i--) {
@@ -373,7 +373,7 @@ int gpio_event_input_func(struct gpio_event_input_devs *input_devs,
 			free_irq(irq, &ds->key_state[i]);
 		}
 	}
-	spin_unlock_irqrestore(&ds->irq_lock, irqflags);
+	raw_spin_unlock_irqrestore(&ds->irq_lock, irqflags);
 
 	for (i = di->keymap_size - 1; i >= 0; i--) {
 err_gpio_configure_failed:
diff --git a/kernel/msm-3.18/drivers/input/misc/keychord.c b/kernel/msm-3.18/drivers/input/misc/keychord.c
index fdcc14653..127874f43 100644
--- a/kernel/msm-3.18/drivers/input/misc/keychord.c
+++ b/kernel/msm-3.18/drivers/input/misc/keychord.c
@@ -55,7 +55,7 @@ struct keychord_device {
 	/* second input_device_id is needed for null termination */
 	struct input_device_id  device_ids[2];
 
-	spinlock_t		lock;
+	raw_spinlock_t		lock;
 	wait_queue_head_t	waitq;
 	unsigned char		head;
 	unsigned char		tail;
@@ -94,7 +94,7 @@ static void keychord_event(struct input_handle *handle, unsigned int type,
 	if (type != EV_KEY || code >= KEY_MAX)
 		return;
 
-	spin_lock_irqsave(&kdev->lock, flags);
+	raw_spin_lock_irqsave(&kdev->lock, flags);
 	/* do nothing if key state did not change */
 	if (!test_bit(code, kdev->keystate) == !value)
 		goto done;
@@ -128,7 +128,7 @@ static void keychord_event(struct input_handle *handle, unsigned int type,
 	}
 
 done:
-	spin_unlock_irqrestore(&kdev->lock, flags);
+	raw_spin_unlock_irqrestore(&kdev->lock, flags);
 
 	if (got_chord) {
 		pr_info("keychord: got keychord id %d. Any tasks: %d\n",
@@ -215,11 +215,11 @@ static ssize_t keychord_read(struct file *file, char __user *buffer,
 	if (retval)
 		return retval;
 
-	spin_lock_irqsave(&kdev->lock, flags);
+	raw_spin_lock_irqsave(&kdev->lock, flags);
 	/* pop a keychord ID off the queue */
 	id = kdev->buff[kdev->tail];
 	kdev->tail = (kdev->tail + 1) % BUFFER_SIZE;
-	spin_unlock_irqrestore(&kdev->lock, flags);
+	raw_spin_unlock_irqrestore(&kdev->lock, flags);
 
 	if (copy_to_user(buffer, &id, count))
 		return -EFAULT;
@@ -237,17 +237,17 @@ keychord_write_lock(struct keychord_device *kdev)
 	int ret;
 	unsigned long flags;
 
-	spin_lock_irqsave(&kdev->lock, flags);
+	raw_spin_lock_irqsave(&kdev->lock, flags);
 	while (kdev->flags & KEYCHORD_BUSY) {
-		spin_unlock_irqrestore(&kdev->lock, flags);
+		raw_spin_unlock_irqrestore(&kdev->lock, flags);
 		ret = wait_event_interruptible(kdev->write_waitq,
 			       ((kdev->flags & KEYCHORD_BUSY) == 0));
 		if (ret)
 			return ret;
-		spin_lock_irqsave(&kdev->lock, flags);
+		raw_spin_lock_irqsave(&kdev->lock, flags);
 	}
 	kdev->flags |= KEYCHORD_BUSY;
-	spin_unlock_irqrestore(&kdev->lock, flags);
+	raw_spin_unlock_irqrestore(&kdev->lock, flags);
 	return 0;
 }
 
@@ -256,9 +256,9 @@ keychord_write_unlock(struct keychord_device *kdev)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&kdev->lock, flags);
+	raw_spin_lock_irqsave(&kdev->lock, flags);
 	kdev->flags &= ~KEYCHORD_BUSY;
-	spin_unlock_irqrestore(&kdev->lock, flags);
+	raw_spin_unlock_irqrestore(&kdev->lock, flags);
 	wake_up_interruptible(&kdev->write_waitq);
 }
 
@@ -310,7 +310,7 @@ static ssize_t keychord_write(struct file *file, const char __user *buffer,
 		kdev->registered = 0;
 	}
 
-	spin_lock_irqsave(&kdev->lock, flags);
+	raw_spin_lock_irqsave(&kdev->lock, flags);
 	/* clear any existing configuration */
 	kfree(kdev->keychords);
 	kdev->keychords = 0;
@@ -366,7 +366,7 @@ static ssize_t keychord_write(struct file *file, const char __user *buffer,
 	}
 
 	kdev->keychords = keychords;
-	spin_unlock_irqrestore(&kdev->lock, flags);
+	raw_spin_unlock_irqrestore(&kdev->lock, flags);
 
 	ret = input_register_handler(&kdev->input_handler);
 	if (ret) {
@@ -382,7 +382,7 @@ static ssize_t keychord_write(struct file *file, const char __user *buffer,
 	return count;
 
 err_unlock_return:
-	spin_unlock_irqrestore(&kdev->lock, flags);
+	raw_spin_unlock_irqrestore(&kdev->lock, flags);
 	kfree(keychords);
 	keychord_write_unlock(kdev);
 	return -EINVAL;
@@ -408,7 +408,7 @@ static int keychord_open(struct inode *inode, struct file *file)
 	if (!kdev)
 		return -ENOMEM;
 
-	spin_lock_init(&kdev->lock);
+	raw_spin_lock_init(&kdev->lock);
 	init_waitqueue_head(&kdev->waitq);
 	init_waitqueue_head(&kdev->write_waitq);
 
diff --git a/kernel/msm-3.18/drivers/input/touchscreen/maxim_sti.c b/kernel/msm-3.18/drivers/input/touchscreen/maxim_sti.c
index d6633d382..13febfa1a 100644
--- a/kernel/msm-3.18/drivers/input/touchscreen/maxim_sti.c
+++ b/kernel/msm-3.18/drivers/input/touchscreen/maxim_sti.c
@@ -152,7 +152,7 @@ struct dev_data {
 
 static unsigned short    panel_id;
 static struct list_head  dev_list;
-static spinlock_t        dev_lock;
+static raw_spinlock_t        dev_lock;
 
 static irqreturn_t irq_handler(int irq, void *context);
 static void service_irq(struct dev_data *dd);
@@ -1630,11 +1630,11 @@ nl_callback_driver(struct sk_buff *skb, struct genl_info *info)
 	unsigned long    flags;
 
 	/* locate device structure */
-	spin_lock_irqsave(&dev_lock, flags);
+	raw_spin_lock_irqsave(&dev_lock, flags);
 	list_for_each_entry(dd, &dev_list, dev_list)
 		if (dd->nl_family.id == NL_TYPE(skb->data))
 			break;
-	spin_unlock_irqrestore(&dev_lock, flags);
+	raw_spin_unlock_irqrestore(&dev_lock, flags);
 	if (&dd->dev_list == &dev_list)
 		return -ENODEV;
 	if (!dd->nl_enabled)
@@ -1659,11 +1659,11 @@ nl_callback_fusion(struct sk_buff *skb, struct genl_info *info)
 	unsigned long    flags;
 
 	/* locate device structure */
-	spin_lock_irqsave(&dev_lock, flags);
+	raw_spin_lock_irqsave(&dev_lock, flags);
 	list_for_each_entry(dd, &dev_list, dev_list)
 		if (dd->nl_family.id == NL_TYPE(skb->data))
 			break;
-	spin_unlock_irqrestore(&dev_lock, flags);
+	raw_spin_unlock_irqrestore(&dev_lock, flags);
 	if (&dd->dev_list == &dev_list)
 		return -ENODEV;
 	if (!dd->nl_enabled)
@@ -2651,9 +2651,9 @@ static int probe(struct spi_device *spi)
 	mutex_init(&dd->sysfs_update_mutex);
 
 	/* add us to the devices list */
-	spin_lock_irqsave(&dev_lock, flags);
+	raw_spin_lock_irqsave(&dev_lock, flags);
 	list_add_tail(&dd->dev_list, &dev_list);
-	spin_unlock_irqrestore(&dev_lock, flags);
+	raw_spin_unlock_irqrestore(&dev_lock, flags);
 
 	ret = create_sysfs_entries(dd);
 	if (ret) {
@@ -2673,9 +2673,9 @@ static int probe(struct spi_device *spi)
 
 sysfs_failure:
 	dd->nl_enabled = false;
-	spin_lock_irqsave(&dev_lock, flags);
+	raw_spin_lock_irqsave(&dev_lock, flags);
 	list_del(&dd->dev_list);
-	spin_unlock_irqrestore(&dev_lock, flags);
+	raw_spin_unlock_irqrestore(&dev_lock, flags);
 	(void)kthread_stop(dd->thread);
 kthread_failure:
 	if (dd->outgoing_skb)
@@ -2748,9 +2748,9 @@ static int remove(struct spi_device *spi)
 		free_irq(dd->spi->irq, dd);
 	}
 
-	spin_lock_irqsave(&dev_lock, flags);
+	raw_spin_lock_irqsave(&dev_lock, flags);
 	list_del(&dd->dev_list);
-	spin_unlock_irqrestore(&dev_lock, flags);
+	raw_spin_unlock_irqrestore(&dev_lock, flags);
 
 	pdata->reset(pdata, 0);
 	usleep_range(100, 120);
@@ -2821,7 +2821,7 @@ static struct spi_driver driver = {
 static int maxim_sti_init(void)
 {
 	INIT_LIST_HEAD(&dev_list);
-	spin_lock_init(&dev_lock);
+	raw_spin_lock_init(&dev_lock);
 	return spi_register_driver(&driver);
 }
 
diff --git a/kernel/msm-3.18/drivers/iommu/dma-mapping-fast.c b/kernel/msm-3.18/drivers/iommu/dma-mapping-fast.c
index ff22d71be..be6161c97 100644
--- a/kernel/msm-3.18/drivers/iommu/dma-mapping-fast.c
+++ b/kernel/msm-3.18/drivers/iommu/dma-mapping-fast.c
@@ -288,7 +288,7 @@ static dma_addr_t fast_smmu_map_page(struct device *dev, struct page *page,
 		__fast_dma_page_cpu_to_dev(phys_to_page(phys_to_map),
 					   offset_from_phys_to_map, size, dir);
 
-	spin_lock_irqsave(&mapping->lock, flags);
+	raw_spin_lock_irqsave(&mapping->lock, flags);
 
 	iova = __fast_smmu_alloc_iova(mapping, attrs, len);
 
@@ -303,13 +303,13 @@ static dma_addr_t fast_smmu_map_page(struct device *dev, struct page *page,
 	if (!skip_sync)		/* TODO: should ask SMMU if coherent */
 		dmac_clean_range(pmd, pmd + nptes);
 
-	spin_unlock_irqrestore(&mapping->lock, flags);
+	raw_spin_unlock_irqrestore(&mapping->lock, flags);
 	return iova + offset_from_phys_to_map;
 
 fail_free_iova:
 	__fast_smmu_free_iova(mapping, iova, size);
 fail:
-	spin_unlock_irqrestore(&mapping->lock, flags);
+	raw_spin_unlock_irqrestore(&mapping->lock, flags);
 	return DMA_ERROR_CODE;
 }
 
@@ -329,12 +329,12 @@ static void fast_smmu_unmap_page(struct device *dev, dma_addr_t iova,
 	if (!skip_sync)
 		__fast_dma_page_dev_to_cpu(page, offset, size, dir);
 
-	spin_lock_irqsave(&mapping->lock, flags);
+	raw_spin_lock_irqsave(&mapping->lock, flags);
 	av8l_fast_unmap_public(pmd, len);
 	if (!skip_sync)		/* TODO: should ask SMMU if coherent */
 		dmac_clean_range(pmd, pmd + nptes);
 	__fast_smmu_free_iova(mapping, iova, len);
-	spin_unlock_irqrestore(&mapping->lock, flags);
+	raw_spin_unlock_irqrestore(&mapping->lock, flags);
 }
 
 static int fast_smmu_map_sg(struct device *dev, struct scatterlist *sg,
@@ -429,11 +429,11 @@ static void *fast_smmu_alloc(struct device *dev, size_t size,
 		sg_miter_stop(&miter);
 	}
 
-	spin_lock_irqsave(&mapping->lock, flags);
+	raw_spin_lock_irqsave(&mapping->lock, flags);
 	dma_addr = __fast_smmu_alloc_iova(mapping, attrs, size);
 	if (dma_addr == DMA_ERROR_CODE) {
 		dev_err(dev, "no iova\n");
-		spin_unlock_irqrestore(&mapping->lock, flags);
+		raw_spin_unlock_irqrestore(&mapping->lock, flags);
 		goto out_free_sg;
 	}
 	iova_iter = dma_addr;
@@ -454,7 +454,7 @@ static void *fast_smmu_alloc(struct device *dev, size_t size,
 		iova_iter += miter.length;
 	}
 	sg_miter_stop(&miter);
-	spin_unlock_irqrestore(&mapping->lock, flags);
+	raw_spin_unlock_irqrestore(&mapping->lock, flags);
 
 	addr = dma_common_pages_remap(pages, size, VM_USERMAP, remap_prot,
 				      __builtin_return_address(0));
@@ -469,13 +469,13 @@ static void *fast_smmu_alloc(struct device *dev, size_t size,
 
 out_unmap:
 	/* need to take the lock again for page tables and iova */
-	spin_lock_irqsave(&mapping->lock, flags);
+	raw_spin_lock_irqsave(&mapping->lock, flags);
 	ptep = iopte_pmd_offset(mapping->pgtbl_pmds, dma_addr);
 	av8l_fast_unmap_public(ptep, size);
 	dmac_clean_range(ptep, ptep + count);
 out_free_iova:
 	__fast_smmu_free_iova(mapping, dma_addr, size);
-	spin_unlock_irqrestore(&mapping->lock, flags);
+	raw_spin_unlock_irqrestore(&mapping->lock, flags);
 out_free_sg:
 	sg_free_table(&sgt);
 out_free_pages:
@@ -503,11 +503,11 @@ static void fast_smmu_free(struct device *dev, size_t size,
 	pages = area->pages;
 	dma_common_free_remap(vaddr, size, VM_USERMAP, false);
 	ptep = iopte_pmd_offset(mapping->pgtbl_pmds, dma_handle);
-	spin_lock_irqsave(&mapping->lock, flags);
+	raw_spin_lock_irqsave(&mapping->lock, flags);
 	av8l_fast_unmap_public(ptep, size);
 	dmac_clean_range(ptep, ptep + count);
 	__fast_smmu_free_iova(mapping, dma_handle, size);
-	spin_unlock_irqrestore(&mapping->lock, flags);
+	raw_spin_unlock_irqrestore(&mapping->lock, flags);
 	__fast_smmu_free_pages(pages, count);
 }
 
@@ -596,7 +596,7 @@ static struct dma_fast_smmu_mapping *__fast_smmu_create_mapping_sized(
 	if (!fast->bitmap)
 		goto err2;
 
-	spin_lock_init(&fast->lock);
+	raw_spin_lock_init(&fast->lock);
 
 	return fast;
 err2:
diff --git a/kernel/msm-3.18/drivers/iommu/msm_iommu-v1.c b/kernel/msm-3.18/drivers/iommu/msm_iommu-v1.c
index 8380c87c4..650d019c0 100644
--- a/kernel/msm-3.18/drivers/iommu/msm_iommu-v1.c
+++ b/kernel/msm-3.18/drivers/iommu/msm_iommu-v1.c
@@ -974,7 +974,7 @@ static void msm_iommu_domain_destroy(struct iommu_domain *domain)
 	unsigned long flags;
 
 	mutex_lock(&msm_iommu_lock);
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	priv = domain->priv;
 	domain->priv = NULL;
 
@@ -982,7 +982,7 @@ static void msm_iommu_domain_destroy(struct iommu_domain *domain)
 		msm_iommu_pagetable_free(&priv->pt);
 
 	kfree(priv);
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	mutex_unlock(&msm_iommu_lock);
 }
 
@@ -1055,21 +1055,21 @@ static int msm_iommu_attach_dev(struct iommu_domain *domain, struct device *dev)
 	if (ctx_drvdata->attach_count > 1)
 		goto already_attached;
 
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	if (!list_empty(&ctx_drvdata->attached_elm)) {
 		ret = -EBUSY;
-		spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+		raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 		goto unlock;
 	}
 
 	list_for_each_entry(tmp_drvdata, &priv->list_attached, attached_elm)
 		if (tmp_drvdata == ctx_drvdata) {
 			ret = -EBUSY;
-			spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+			raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 			goto unlock;
 		}
 
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 
 	is_secure = iommu_drvdata->sec_id != -1;
 
@@ -1121,9 +1121,9 @@ static int msm_iommu_attach_dev(struct iommu_domain *domain, struct device *dev)
 
 	__disable_clocks(iommu_drvdata);
 
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	list_add(&(ctx_drvdata->attached_elm), &priv->list_attached);
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 
 	ctx_drvdata->attached_domain = domain;
 	++iommu_drvdata->ctx_attach_count;
@@ -1235,9 +1235,9 @@ static void msm_iommu_detach_dev(struct iommu_domain *domain,
 
 	__disable_regulators(iommu_drvdata);
 
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	list_del_init(&ctx_drvdata->attached_elm);
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 
 	ctx_drvdata->attached_domain = NULL;
 	BUG_ON(iommu_drvdata->ctx_attach_count == 0);
@@ -1253,7 +1253,7 @@ static int msm_iommu_map(struct iommu_domain *domain, unsigned long va,
 	int ret = 0;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	priv = domain->priv;
 	if (!priv) {
 		ret = -EINVAL;
@@ -1266,7 +1266,7 @@ static int msm_iommu_map(struct iommu_domain *domain, unsigned long va,
 
 	msm_iommu_flush_pagetable(&priv->pt, va, len);
 fail:
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	return ret;
 }
 
@@ -1277,7 +1277,7 @@ static size_t msm_iommu_unmap(struct iommu_domain *domain, unsigned long va,
 	int ret = -ENODEV;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	priv = domain->priv;
 	if (!priv)
 		goto fail;
@@ -1294,7 +1294,7 @@ static size_t msm_iommu_unmap(struct iommu_domain *domain, unsigned long va,
 
 	msm_iommu_pagetable_free_tables(&priv->pt, va, len);
 fail:
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	/* the IOMMU API requires us to return how many bytes were unmapped */
 	len = ret ? 0 : len;
 	return len;
@@ -1308,7 +1308,7 @@ static int msm_iommu_map_range(struct iommu_domain *domain, unsigned long va,
 	struct msm_iommu_priv *priv;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	priv = domain->priv;
 	if (!priv) {
 		ret = -EINVAL;
@@ -1319,7 +1319,7 @@ static int msm_iommu_map_range(struct iommu_domain *domain, unsigned long va,
 	msm_iommu_flush_pagetable(&priv->pt, va, len);
 
 fail:
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	return ret;
 }
 
@@ -1330,7 +1330,7 @@ static int msm_iommu_unmap_range(struct iommu_domain *domain, unsigned long va,
 	struct msm_iommu_priv *priv;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	priv = domain->priv;
 	msm_iommu_pagetable_unmap_range(&priv->pt, va, len);
 
@@ -1338,7 +1338,7 @@ static int msm_iommu_unmap_range(struct iommu_domain *domain, unsigned long va,
 	__flush_iotlb(domain);
 
 	msm_iommu_pagetable_free_tables(&priv->pt, va, len);
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	return 0;
 }
 
@@ -1370,9 +1370,9 @@ static phys_addr_t msm_iommu_iova_to_phys(struct iommu_domain *domain,
 	phys_addr_t ret;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	ret = msm_iommu_iova_to_phys_soft(domain, va);
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	return ret;
 }
 
@@ -1396,10 +1396,10 @@ static phys_addr_t msm_iommu_iova_to_phys_hard(struct iommu_domain *domain,
 		goto fail;
 
 
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	ctx_drvdata = list_entry(priv->list_attached.next,
 				 struct msm_iommu_ctx_drvdata, attached_elm);
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 
 	if (is_domain_dynamic(priv) || ctx_drvdata->dynamic)
 		goto fail;
@@ -1421,7 +1421,7 @@ static phys_addr_t msm_iommu_iova_to_phys_hard(struct iommu_domain *domain,
 		goto fail;
 	}
 
-	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	SET_ATS1PR(base, ctx, va & CB_ATS1PR_ADDR);
 	/* make sure ATS1PR is visible */
 	mb();
@@ -1439,7 +1439,7 @@ static phys_addr_t msm_iommu_iova_to_phys_hard(struct iommu_domain *domain,
 	}
 
 	par = GET_PAR(base, ctx);
-	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 
 	__disable_clocks(iommu_drvdata);
 
diff --git a/kernel/msm-3.18/drivers/irqchip/irq-gic-v2m.c b/kernel/msm-3.18/drivers/irqchip/irq-gic-v2m.c
index ec9c37674..a7cf47e01 100644
--- a/kernel/msm-3.18/drivers/irqchip/irq-gic-v2m.c
+++ b/kernel/msm-3.18/drivers/irqchip/irq-gic-v2m.c
@@ -44,7 +44,7 @@
 #define V2M_MSI_TYPER_NUM_SPI(x)       ((x) & V2M_MSI_TYPER_NUM_MASK)
 
 struct v2m_data {
-	spinlock_t msi_cnt_lock;
+	raw_spinlock_t msi_cnt_lock;
 	struct resource res;	/* GICv2m resource */
 	void __iomem *base;	/* GICv2m virt address */
 	u32 spi_start;		/* The SPI number that MSIs start */
@@ -144,9 +144,9 @@ static void gicv2m_unalloc_msi(struct v2m_data *v2m, unsigned int hwirq)
 		return;
 	}
 
-	spin_lock(&v2m->msi_cnt_lock);
+	raw_spin_lock(&v2m->msi_cnt_lock);
 	__clear_bit(pos, v2m->bm);
-	spin_unlock(&v2m->msi_cnt_lock);
+	raw_spin_unlock(&v2m->msi_cnt_lock);
 }
 
 static int gicv2m_irq_domain_alloc(struct irq_domain *domain, unsigned int virq,
@@ -155,13 +155,13 @@ static int gicv2m_irq_domain_alloc(struct irq_domain *domain, unsigned int virq,
 	struct v2m_data *v2m = domain->host_data;
 	int hwirq, offset, err = 0;
 
-	spin_lock(&v2m->msi_cnt_lock);
+	raw_spin_lock(&v2m->msi_cnt_lock);
 	offset = find_first_zero_bit(v2m->bm, v2m->nr_spis);
 	if (offset < v2m->nr_spis)
 		__set_bit(offset, v2m->bm);
 	else
 		err = -ENOSPC;
-	spin_unlock(&v2m->msi_cnt_lock);
+	raw_spin_unlock(&v2m->msi_cnt_lock);
 
 	if (err)
 		return err;
@@ -278,7 +278,7 @@ static int __init gicv2m_init_one(struct device_node *node,
 		goto err_free_domains;
 	}
 
-	spin_lock_init(&v2m->msi_cnt_lock);
+	raw_spin_lock_init(&v2m->msi_cnt_lock);
 
 	pr_info("Node %s: range[%#lx:%#lx], SPI[%d:%d]\n", node->name,
 		(unsigned long)v2m->res.start, (unsigned long)v2m->res.end,
diff --git a/kernel/msm-3.18/drivers/irqchip/irq-gic-v3-its.c b/kernel/msm-3.18/drivers/irqchip/irq-gic-v3-its.c
index 78db25293..8b99995ac 100644
--- a/kernel/msm-3.18/drivers/irqchip/irq-gic-v3-its.c
+++ b/kernel/msm-3.18/drivers/irqchip/irq-gic-v3-its.c
@@ -692,7 +692,7 @@ static unsigned long *its_lpi_alloc_chunks(int nr_irqs, int *base, int *nr_ids)
 
 	nr_chunks = DIV_ROUND_UP(nr_irqs, IRQS_PER_CHUNK);
 
-	spin_lock(&lpi_lock);
+	raw_spin_lock(&lpi_lock);
 
 	do {
 		chunk_id = bitmap_find_next_zero_area(lpi_bitmap, lpi_chunks,
@@ -718,7 +718,7 @@ static unsigned long *its_lpi_alloc_chunks(int nr_irqs, int *base, int *nr_ids)
 	*nr_ids = nr_chunks * IRQS_PER_CHUNK;
 
 out:
-	spin_unlock(&lpi_lock);
+	raw_spin_unlock(&lpi_lock);
 
 	if (!bitmap)
 		*base = *nr_ids = 0;
@@ -732,7 +732,7 @@ static void its_lpi_free(struct event_lpi_map *map)
 	int nr_ids = map->nr_lpis;
 	int lpi;
 
-	spin_lock(&lpi_lock);
+	raw_spin_lock(&lpi_lock);
 
 	for (lpi = base; lpi < (base + nr_ids); lpi += IRQS_PER_CHUNK) {
 		int chunk = its_lpi_to_chunk(lpi);
@@ -744,7 +744,7 @@ static void its_lpi_free(struct event_lpi_map *map)
 		}
 	}
 
-	spin_unlock(&lpi_lock);
+	raw_spin_unlock(&lpi_lock);
 
 	kfree(map->lpi_map);
 	kfree(map->col_map);
@@ -1057,7 +1057,7 @@ static void its_cpu_init_collection(void)
 	struct its_node *its;
 	int cpu;
 
-	spin_lock(&its_lock);
+	raw_spin_lock(&its_lock);
 	cpu = smp_processor_id();
 
 	list_for_each_entry(its, &its_nodes, entry) {
@@ -1089,7 +1089,7 @@ static void its_cpu_init_collection(void)
 		its_send_invall(its, &its->collections[cpu]);
 	}
 
-	spin_unlock(&its_lock);
+	raw_spin_unlock(&its_lock);
 }
 
 static struct its_device *its_find_device(struct its_node *its, u32 dev_id)
@@ -1488,9 +1488,9 @@ static int its_probe(struct device_node *node, struct irq_domain *parent)
 		inner_domain->host_data = info;
 	}
 
-	spin_lock(&its_lock);
+	raw_spin_lock(&its_lock);
 	list_add(&its->entry, &its_nodes);
-	spin_unlock(&its_lock);
+	raw_spin_unlock(&its_lock);
 
 	return 0;
 
diff --git a/kernel/msm-3.18/drivers/irqchip/irq-mtk-sysirq.c b/kernel/msm-3.18/drivers/irqchip/irq-mtk-sysirq.c
index 7e342df6a..54abda89c 100644
--- a/kernel/msm-3.18/drivers/irqchip/irq-mtk-sysirq.c
+++ b/kernel/msm-3.18/drivers/irqchip/irq-mtk-sysirq.c
@@ -26,7 +26,7 @@
 #define MT6577_SYS_INTPOL_NUM	(224)
 
 struct mtk_sysirq_chip_data {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	void __iomem *intpol_base;
 };
 
@@ -41,7 +41,7 @@ static int mtk_sysirq_set_type(struct irq_data *data, unsigned int type)
 	offset = hwirq & 0x1f;
 	reg_index = hwirq >> 5;
 
-	spin_lock_irqsave(&chip_data->lock, flags);
+	raw_spin_lock_irqsave(&chip_data->lock, flags);
 	value = readl_relaxed(chip_data->intpol_base + reg_index * 4);
 	if (type == IRQ_TYPE_LEVEL_LOW || type == IRQ_TYPE_EDGE_FALLING) {
 		if (type == IRQ_TYPE_LEVEL_LOW)
@@ -56,7 +56,7 @@ static int mtk_sysirq_set_type(struct irq_data *data, unsigned int type)
 
 	data = data->parent_data;
 	ret = data->chip->irq_set_type(data, type);
-	spin_unlock_irqrestore(&chip_data->lock, flags);
+	raw_spin_unlock_irqrestore(&chip_data->lock, flags);
 	return ret;
 }
 
@@ -150,7 +150,7 @@ static int __init mtk_sysirq_of_init(struct device_node *node,
 		ret = -ENOMEM;
 		goto out_unmap;
 	}
-	spin_lock_init(&chip_data->lock);
+	raw_spin_lock_init(&chip_data->lock);
 
 	return 0;
 
diff --git a/kernel/msm-3.18/drivers/media/radio/radio-iris.c b/kernel/msm-3.18/drivers/media/radio/radio-iris.c
index c0d602711..54a3b2acd 100644
--- a/kernel/msm-3.18/drivers/media/radio/radio-iris.c
+++ b/kernel/msm-3.18/drivers/media/radio/radio-iris.c
@@ -88,7 +88,7 @@ struct iris_device {
 	struct v4l2_device v4l2_dev;
 
 	struct mutex lock;
-	spinlock_t buf_lock[IRIS_BUF_MAX];
+	raw_spinlock_t buf_lock[IRIS_BUF_MAX];
 	wait_queue_head_t event_queue;
 	wait_queue_head_t read_queue;
 
@@ -5538,7 +5538,7 @@ static int iris_probe(struct platform_device *pdev)
 	for (i = 0; i < IRIS_BUF_MAX; i++) {
 		int kfifo_alloc_rc = 0;
 
-		spin_lock_init(&radio->buf_lock[i]);
+		raw_spin_lock_init(&radio->buf_lock[i]);
 
 		if ((i == IRIS_BUF_RAW_RDS) || (i == IRIS_BUF_PEEK))
 			kfifo_alloc_rc = kfifo_alloc(&radio->data_buf[i],
diff --git a/kernel/msm-3.18/drivers/misc/qseecom.c b/kernel/msm-3.18/drivers/misc/qseecom.c
index fee385ef0..e03ab2cc4 100644
--- a/kernel/msm-3.18/drivers/misc/qseecom.c
+++ b/kernel/msm-3.18/drivers/misc/qseecom.c
@@ -232,13 +232,13 @@ struct qseecom_clk {
 struct qseecom_control {
 	struct ion_client *ion_clnt;		/* Ion client */
 	struct list_head  registered_listener_list_head;
-	spinlock_t        registered_listener_list_lock;
+	raw_spinlock_t        registered_listener_list_lock;
 
 	struct list_head  registered_app_list_head;
-	spinlock_t        registered_app_list_lock;
+	raw_spinlock_t        registered_app_list_lock;
 
 	struct list_head   registered_kclient_list_head;
-	spinlock_t        registered_kclient_list_lock;
+	raw_spinlock_t        registered_kclient_list_lock;
 
 	wait_queue_head_t send_resp_wq;
 	int               send_resp_flag;
@@ -1054,7 +1054,7 @@ static int __qseecom_is_svc_unique(struct qseecom_dev_handle *data,
 	int unique = 1;
 	unsigned long flags;
 
-	spin_lock_irqsave(&qseecom.registered_listener_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_listener_list_lock, flags);
 	list_for_each_entry(ptr, &qseecom.registered_listener_list_head, list) {
 		if (ptr->svc.listener_id == svc->listener_id) {
 			pr_err("Service id: %u is already registered\n",
@@ -1063,7 +1063,7 @@ static int __qseecom_is_svc_unique(struct qseecom_dev_handle *data,
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&qseecom.registered_listener_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_listener_list_lock, flags);
 	return unique;
 }
 
@@ -1073,13 +1073,13 @@ static struct qseecom_registered_listener_list *__qseecom_find_svc(
 	struct qseecom_registered_listener_list *entry = NULL;
 	unsigned long flags;
 
-	spin_lock_irqsave(&qseecom.registered_listener_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_listener_list_lock, flags);
 	list_for_each_entry(entry, &qseecom.registered_listener_list_head, list)
 	{
 		if (entry->svc.listener_id == listener_id)
 			break;
 	}
-	spin_unlock_irqrestore(&qseecom.registered_listener_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_listener_list_lock, flags);
 
 	if ((entry != NULL) && (entry->svc.listener_id != listener_id)) {
 		pr_err("Service id: %u is not found\n", listener_id);
@@ -1203,9 +1203,9 @@ static int qseecom_register_listener(struct qseecom_dev_handle *data,
 	init_waitqueue_head(&new_entry->listener_block_app_wq);
 	new_entry->send_resp_flag = 0;
 	new_entry->listener_in_use = false;
-	spin_lock_irqsave(&qseecom.registered_listener_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_listener_list_lock, flags);
 	list_add_tail(&new_entry->list, &qseecom.registered_listener_list_head);
-	spin_unlock_irqrestore(&qseecom.registered_listener_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_listener_list_lock, flags);
 
 	return ret;
 }
@@ -1239,7 +1239,7 @@ static int qseecom_unregister_listener(struct qseecom_dev_handle *data)
 	}
 
 	data->abort = 1;
-	spin_lock_irqsave(&qseecom.registered_listener_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_listener_list_lock, flags);
 	list_for_each_entry(ptr_svc, &qseecom.registered_listener_list_head,
 			list) {
 		if (ptr_svc->svc.listener_id == data->listener.id) {
@@ -1247,7 +1247,7 @@ static int qseecom_unregister_listener(struct qseecom_dev_handle *data)
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&qseecom.registered_listener_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_listener_list_lock, flags);
 
 	while (atomic_read(&data->ioctl_count) > 1) {
 		if (wait_event_freezable(data->abort_wq,
@@ -1258,7 +1258,7 @@ static int qseecom_unregister_listener(struct qseecom_dev_handle *data)
 		}
 	}
 
-	spin_lock_irqsave(&qseecom.registered_listener_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_listener_list_lock, flags);
 	list_for_each_entry(ptr_svc,
 			&qseecom.registered_listener_list_head,
 			list)
@@ -1273,7 +1273,7 @@ static int qseecom_unregister_listener(struct qseecom_dev_handle *data)
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&qseecom.registered_listener_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_listener_list_lock, flags);
 
 	/* Unmap the memory */
 	if (unmap_mem) {
@@ -1689,7 +1689,7 @@ static int __qseecom_process_incomplete_cmd(struct qseecom_dev_handle *data,
 		/*
 		 * Wake up blocking lsitener service with the lstnr id
 		 */
-		spin_lock_irqsave(&qseecom.registered_listener_list_lock,
+		raw_spin_lock_irqsave(&qseecom.registered_listener_list_lock,
 					flags);
 		list_for_each_entry(ptr_svc,
 				&qseecom.registered_listener_list_head, list) {
@@ -1700,7 +1700,7 @@ static int __qseecom_process_incomplete_cmd(struct qseecom_dev_handle *data,
 				break;
 			}
 		}
-		spin_unlock_irqrestore(&qseecom.registered_listener_list_lock,
+		raw_spin_unlock_irqrestore(&qseecom.registered_listener_list_lock,
 				flags);
 
 		if (ptr_svc == NULL) {
@@ -1857,7 +1857,7 @@ int __qseecom_process_reentrancy_blocked_on_listener(
 
 	/* find app_id & img_name from list */
 	if (!ptr_app) {
-		spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+		raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 		list_for_each_entry(ptr_app, &qseecom.registered_app_list_head,
 							list) {
 			if ((ptr_app->app_id == data->client.app_id) &&
@@ -1867,7 +1867,7 @@ int __qseecom_process_reentrancy_blocked_on_listener(
 				break;
 			}
 		}
-		spin_unlock_irqrestore(&qseecom.registered_app_list_lock,
+		raw_spin_unlock_irqrestore(&qseecom.registered_app_list_lock,
 					flags);
 		if (!found_app) {
 			pr_err("app_id %d (%s) is not found\n",
@@ -1954,7 +1954,7 @@ static int __qseecom_reentrancy_process_incomplete_cmd(
 		/*
 		 * Wake up blocking lsitener service with the lstnr id
 		 */
-		spin_lock_irqsave(&qseecom.registered_listener_list_lock,
+		raw_spin_lock_irqsave(&qseecom.registered_listener_list_lock,
 					flags);
 		list_for_each_entry(ptr_svc,
 				&qseecom.registered_listener_list_head, list) {
@@ -1965,7 +1965,7 @@ static int __qseecom_reentrancy_process_incomplete_cmd(
 				break;
 			}
 		}
-		spin_unlock_irqrestore(&qseecom.registered_listener_list_lock,
+		raw_spin_unlock_irqrestore(&qseecom.registered_listener_list_lock,
 				flags);
 
 		if (ptr_svc == NULL) {
@@ -2179,7 +2179,7 @@ static int __qseecom_check_app_exists(struct qseecom_check_app_ireq req,
 	*app_id = 0;
 
 	/* check if app exists and has been registered locally */
-	spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 	list_for_each_entry(entry,
 			&qseecom.registered_app_list_head, list) {
 		if (!strcmp(entry->app_name, req.app_name)) {
@@ -2187,7 +2187,7 @@ static int __qseecom_check_app_exists(struct qseecom_check_app_ireq req,
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&qseecom.registered_app_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_app_list_lock, flags);
 	if (found_app) {
 		pr_debug("Found app with id %d\n", entry->app_id);
 		*app_id = entry->app_id;
@@ -2298,7 +2298,7 @@ static int qseecom_load_app(struct qseecom_dev_handle *data, void __user *argp)
 	if (app_id) {
 		pr_debug("App id %d (%s) already exists\n", app_id,
 			(char *)(req.app_name));
-		spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+		raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 		list_for_each_entry(entry,
 		&qseecom.registered_app_list_head, list){
 			if (entry->app_id == app_id) {
@@ -2306,7 +2306,7 @@ static int qseecom_load_app(struct qseecom_dev_handle *data, void __user *argp)
 				break;
 			}
 		}
-		spin_unlock_irqrestore(
+		raw_spin_unlock_irqrestore(
 		&qseecom.registered_app_list_lock, flags);
 		ret = 0;
 	} else {
@@ -2434,9 +2434,9 @@ static int qseecom_load_app(struct qseecom_dev_handle *data, void __user *argp)
 		if (!IS_ERR_OR_NULL(ihandle))
 			ion_free(qseecom.ion_clnt, ihandle);
 
-		spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+		raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 		list_add_tail(&entry->list, &qseecom.registered_app_list_head);
-		spin_unlock_irqrestore(&qseecom.registered_app_list_lock,
+		raw_spin_unlock_irqrestore(&qseecom.registered_app_list_lock,
 									flags);
 
 		pr_warn("App with id %d (%s) now loaded\n", app_id,
@@ -2454,10 +2454,10 @@ static int qseecom_load_app(struct qseecom_dev_handle *data, void __user *argp)
 		pr_err("copy_to_user failed\n");
 		ret = -EFAULT;
 		if (first_time == true) {
-			spin_lock_irqsave(
+			raw_spin_lock_irqsave(
 				&qseecom.registered_app_list_lock, flags);
 			list_del(&entry->list);
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 				&qseecom.registered_app_list_lock, flags);
 			kzfree(entry);
 		}
@@ -2530,7 +2530,7 @@ static int qseecom_unload_app(struct qseecom_dev_handle *data,
 	__qseecom_reentrancy_check_if_no_app_blocked(TZ_OS_APP_SHUTDOWN_ID);
 
 	if (data->client.app_id > 0) {
-		spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+		raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 		list_for_each_entry(ptr_app, &qseecom.registered_app_list_head,
 									list) {
 			if (ptr_app->app_id == data->client.app_id) {
@@ -2546,7 +2546,7 @@ static int qseecom_unload_app(struct qseecom_dev_handle *data,
 				}
 			}
 		}
-		spin_unlock_irqrestore(&qseecom.registered_app_list_lock,
+		raw_spin_unlock_irqrestore(&qseecom.registered_app_list_lock,
 								flags);
 		if (found_app == false && found_dead_app == false) {
 			pr_err("Cannot find app with id = %d (%s)\n",
@@ -2599,7 +2599,7 @@ static int qseecom_unload_app(struct qseecom_dev_handle *data,
 	}
 
 	if (found_app) {
-		spin_lock_irqsave(&qseecom.registered_app_list_lock, flags1);
+		raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags1);
 		if (app_crash) {
 			ptr_app->ref_cnt = 0;
 			pr_debug("app_crash: ref_count = 0\n");
@@ -2617,7 +2617,7 @@ static int qseecom_unload_app(struct qseecom_dev_handle *data,
 			list_del(&ptr_app->list);
 			kzfree(ptr_app);
 		}
-		spin_unlock_irqrestore(&qseecom.registered_app_list_lock,
+		raw_spin_unlock_irqrestore(&qseecom.registered_app_list_lock,
 								flags1);
 	}
 unload_exit:
@@ -3060,7 +3060,7 @@ static int __qseecom_send_cmd(struct qseecom_dev_handle *data,
 
 	reqd_len_sb_in = req->cmd_req_len + req->resp_len;
 	/* find app_id & img_name from list */
-	spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 	list_for_each_entry(ptr_app, &qseecom.registered_app_list_head,
 							list) {
 		if ((ptr_app->app_id == data->client.app_id) &&
@@ -3069,7 +3069,7 @@ static int __qseecom_send_cmd(struct qseecom_dev_handle *data,
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&qseecom.registered_app_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_app_list_lock, flags);
 
 	if (!found_app) {
 		pr_err("app_id %d (%s) is not found\n", data->client.app_id,
@@ -4426,7 +4426,7 @@ int qseecom_start_app(struct qseecom_handle **handle,
 	if (app_id) {
 		pr_warn("App id %d for [%s] app exists\n", app_id,
 			(char *)app_ireq.app_name);
-		spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+		raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 		list_for_each_entry(entry,
 				&qseecom.registered_app_list_head, list){
 			if (entry->app_id == app_id) {
@@ -4435,7 +4435,7 @@ int qseecom_start_app(struct qseecom_handle **handle,
 				break;
 			}
 		}
-		spin_unlock_irqrestore(
+		raw_spin_unlock_irqrestore(
 				&qseecom.registered_app_list_lock, flags);
 		if (!found_app)
 			pr_warn("App_id %d [%s] was loaded but not registered\n",
@@ -4467,9 +4467,9 @@ int qseecom_start_app(struct qseecom_handle **handle,
 		entry->app_arch = app_arch;
 		entry->app_blocked = false;
 		entry->blocked_on_listener_id = 0;
-		spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+		raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 		list_add_tail(&entry->list, &qseecom.registered_app_list_head);
-		spin_unlock_irqrestore(&qseecom.registered_app_list_lock,
+		raw_spin_unlock_irqrestore(&qseecom.registered_app_list_lock,
 									flags);
 	}
 
@@ -4503,10 +4503,10 @@ int qseecom_start_app(struct qseecom_handle **handle,
 	}
 	kclient_entry->handle = *handle;
 
-	spin_lock_irqsave(&qseecom.registered_kclient_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_kclient_list_lock, flags);
 	list_add_tail(&kclient_entry->list,
 			&qseecom.registered_kclient_list_head);
-	spin_unlock_irqrestore(&qseecom.registered_kclient_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_kclient_list_lock, flags);
 
 	mutex_unlock(&app_access_lock);
 	return 0;
@@ -4542,7 +4542,7 @@ int qseecom_shutdown_app(struct qseecom_handle **handle)
 	data =	(struct qseecom_dev_handle *) ((*handle)->dev);
 	mutex_lock(&app_access_lock);
 
-	spin_lock_irqsave(&qseecom.registered_kclient_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_kclient_list_lock, flags);
 	list_for_each_entry(kclient, &qseecom.registered_kclient_list_head,
 				list) {
 		if (kclient->handle == (*handle)) {
@@ -4551,7 +4551,7 @@ int qseecom_shutdown_app(struct qseecom_handle **handle)
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&qseecom.registered_kclient_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_kclient_list_lock, flags);
 	if (!found_handle)
 		pr_err("Unable to find the handle, exiting\n");
 	else
@@ -5288,7 +5288,7 @@ static int qseecom_query_app_loaded(struct qseecom_dev_handle *data,
 	if (app_id) {
 		pr_debug("App id %d (%s) already exists\n", app_id,
 			(char *)(req.app_name));
-		spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+		raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 		list_for_each_entry(entry,
 				&qseecom.registered_app_list_head, list){
 			if (entry->app_id == app_id) {
@@ -5298,7 +5298,7 @@ static int qseecom_query_app_loaded(struct qseecom_dev_handle *data,
 				break;
 			}
 		}
-		spin_unlock_irqrestore(
+		raw_spin_unlock_irqrestore(
 				&qseecom.registered_app_list_lock, flags);
 		data->client.app_id = app_id;
 		query_req.app_id = app_id;
@@ -5330,11 +5330,11 @@ static int qseecom_query_app_loaded(struct qseecom_dev_handle *data,
 				MAX_APP_NAME_SIZE);
 			entry->app_blocked = false;
 			entry->blocked_on_listener_id = 0;
-			spin_lock_irqsave(&qseecom.registered_app_list_lock,
+			raw_spin_lock_irqsave(&qseecom.registered_app_list_lock,
 				flags);
 			list_add_tail(&entry->list,
 				&qseecom.registered_app_list_head);
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 				&qseecom.registered_app_list_lock, flags);
 		}
 		if (copy_to_user(argp, &query_req, sizeof(query_req))) {
@@ -6471,7 +6471,7 @@ static int __qseecom_qteec_issue_cmd(struct qseecom_dev_handle *data,
 	resp_ptr = req->resp_ptr;
 
 	/* find app_id & img_name from list */
-	spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 	list_for_each_entry(ptr_app, &qseecom.registered_app_list_head,
 							list) {
 		if ((ptr_app->app_id == data->client.app_id) &&
@@ -6480,7 +6480,7 @@ static int __qseecom_qteec_issue_cmd(struct qseecom_dev_handle *data,
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&qseecom.registered_app_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_app_list_lock, flags);
 	if (!found_app) {
 		pr_err("app_id %d (%s) is not found\n", data->client.app_id,
 			(char *)data->client.app_name);
@@ -6670,7 +6670,7 @@ static int qseecom_qteec_invoke_modfd_cmd(struct qseecom_dev_handle *data,
 	resp_ptr = req.resp_ptr;
 
 	/* find app_id & img_name from list */
-	spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_app_list_lock, flags);
 	list_for_each_entry(ptr_app, &qseecom.registered_app_list_head,
 							list) {
 		if ((ptr_app->app_id == data->client.app_id) &&
@@ -6679,7 +6679,7 @@ static int qseecom_qteec_invoke_modfd_cmd(struct qseecom_dev_handle *data,
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&qseecom.registered_app_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_app_list_lock, flags);
 	if (!found_app) {
 		pr_err("app_id %d (%s) is not found\n", data->client.app_id,
 			(char *)data->client.app_name);
@@ -8395,11 +8395,11 @@ static int qseecom_probe(struct platform_device *pdev)
 	}
 
 	INIT_LIST_HEAD(&qseecom.registered_listener_list_head);
-	spin_lock_init(&qseecom.registered_listener_list_lock);
+	raw_spin_lock_init(&qseecom.registered_listener_list_lock);
 	INIT_LIST_HEAD(&qseecom.registered_app_list_head);
-	spin_lock_init(&qseecom.registered_app_list_lock);
+	raw_spin_lock_init(&qseecom.registered_app_list_lock);
 	INIT_LIST_HEAD(&qseecom.registered_kclient_list_head);
-	spin_lock_init(&qseecom.registered_kclient_list_lock);
+	raw_spin_lock_init(&qseecom.registered_kclient_list_lock);
 	init_waitqueue_head(&qseecom.send_resp_wq);
 	qseecom.send_resp_flag = 0;
 
@@ -8640,7 +8640,7 @@ static int qseecom_remove(struct platform_device *pdev)
 	struct qseecom_ce_info_use *pce_info_use;
 
 	atomic_set(&qseecom.qseecom_state, QSEECOM_STATE_NOT_READY);
-	spin_lock_irqsave(&qseecom.registered_kclient_list_lock, flags);
+	raw_spin_lock_irqsave(&qseecom.registered_kclient_list_lock, flags);
 
 	list_for_each_entry(kclient, &qseecom.registered_kclient_list_head,
 								list) {
@@ -8670,7 +8670,7 @@ exit_free_kc_handle:
 exit_free_kclient:
 	kzfree(kclient);
 exit_irqrestore:
-	spin_unlock_irqrestore(&qseecom.registered_kclient_list_lock, flags);
+	raw_spin_unlock_irqrestore(&qseecom.registered_kclient_list_lock, flags);
 
 	if (qseecom.qseos_version > QSEEE_VERSION_00)
 		qseecom_unload_commonlib_image();
diff --git a/kernel/msm-3.18/drivers/misc/uid_stat.c b/kernel/msm-3.18/drivers/misc/uid_stat.c
index 27b516b8a..048a2d45c 100644
--- a/kernel/msm-3.18/drivers/misc/uid_stat.c
+++ b/kernel/msm-3.18/drivers/misc/uid_stat.c
@@ -106,14 +106,14 @@ static struct uid_stat *find_or_create_uid_stat(uid_t uid)
 {
 	struct uid_stat *entry;
 	unsigned long flags;
-	spin_lock_irqsave(&uid_lock, flags);
+	raw_spin_lock_irqsave(&uid_lock, flags);
 	entry = find_uid_stat(uid);
 	if (entry) {
-		spin_unlock_irqrestore(&uid_lock, flags);
+		raw_spin_unlock_irqrestore(&uid_lock, flags);
 		return entry;
 	}
 	entry = create_stat(uid);
-	spin_unlock_irqrestore(&uid_lock, flags);
+	raw_spin_unlock_irqrestore(&uid_lock, flags);
 	if (entry)
 		create_stat_proc(entry);
 	return entry;
diff --git a/kernel/msm-3.18/drivers/mmc/card/mmc_block_test.c b/kernel/msm-3.18/drivers/mmc/card/mmc_block_test.c
index 5df077d85..52327e2fd 100644
--- a/kernel/msm-3.18/drivers/mmc/card/mmc_block_test.c
+++ b/kernel/msm-3.18/drivers/mmc/card/mmc_block_test.c
@@ -202,7 +202,7 @@ void print_mmc_packing_stats(struct mmc_card *card)
 
 	max_num_of_packed_reqs = card->ext_csd.max_packed_writes;
 
-	spin_lock(&card->wr_pack_stats.lock);
+	raw_spin_lock(&card->wr_pack_stats.lock);
 
 	pr_info("%s: write packing statistics:",
 		mmc_hostname(card->host));
@@ -246,7 +246,7 @@ void print_mmc_packing_stats(struct mmc_card *card)
 			mmc_hostname(card->host),
 			card->wr_pack_stats.pack_stop_reason[THRESHOLD]);
 
-	spin_unlock(&card->wr_pack_stats.lock);
+	raw_spin_unlock(&card->wr_pack_stats.lock);
 }
 
 /*
@@ -638,7 +638,7 @@ static int check_wr_packing_statistics(struct test_iosched *tios)
 		return -EINVAL;
 	}
 
-	spin_lock(&mmc_packed_stats->lock);
+	raw_spin_lock(&mmc_packed_stats->lock);
 
 	if (!mmc_packed_stats->enabled) {
 		pr_err("%s write packing statistics are not enabled",
@@ -736,12 +736,12 @@ static int check_wr_packing_statistics(struct test_iosched *tios)
 	}
 
 exit_err:
-	spin_unlock(&mmc_packed_stats->lock);
+	raw_spin_unlock(&mmc_packed_stats->lock);
 	if (ret && mmc_packed_stats->enabled)
 		print_mmc_packing_stats(card);
 	return ret;
 cancel_round:
-	spin_unlock(&mmc_packed_stats->lock);
+	raw_spin_unlock(&mmc_packed_stats->lock);
 	test_iosched_set_ignore_round(tios, true);
 	return 0;
 }
@@ -1655,10 +1655,10 @@ static void new_req_free_end_io_fn(struct request *rq, int err)
 
 	BUG_ON(!test_rq);
 
-	spin_lock_irqsave(&tios->lock, flags);
+	raw_spin_lock_irqsave(&tios->lock, flags);
 	list_del_init(&test_rq->queuelist);
 	tios->dispatched_count--;
-	spin_unlock_irqrestore(&tios->lock, flags);
+	raw_spin_unlock_irqrestore(&tios->lock, flags);
 
 	__blk_put_request(tios->req_q, test_rq->rq);
 	test_iosched_free_test_req_data_buffer(test_rq);
@@ -1695,12 +1695,12 @@ static int run_new_req(struct test_iosched *tios)
 				READ, tios->start_sector, bio_num,
 				TEST_PATTERN_5A, new_req_free_end_io_fn);
 			if (test_rq) {
-				spin_lock_irqsave(tios->req_q->queue_lock,
+				raw_spin_lock_irqsave(tios->req_q->queue_lock,
 					flags);
 				list_add_tail(&test_rq->queuelist,
 					      &tios->test_queue);
 				tios->test_count++;
-				spin_unlock_irqrestore(tios->req_q->queue_lock,
+				raw_spin_unlock_irqrestore(tios->req_q->queue_lock,
 					flags);
 			} else {
 				pr_err("%s: failed to create read request",
@@ -1729,12 +1729,12 @@ static int run_new_req(struct test_iosched *tios)
 				READ, tios->start_sector, bio_num,
 				TEST_PATTERN_5A, new_req_free_end_io_fn);
 			if (test_rq) {
-				spin_lock_irqsave(tios->req_q->queue_lock,
+				raw_spin_lock_irqsave(tios->req_q->queue_lock,
 					flags);
 				list_add_tail(&test_rq->queuelist,
 					      &tios->test_queue);
 				tios->test_count++;
-				spin_unlock_irqrestore(tios->req_q->queue_lock,
+				raw_spin_unlock_irqrestore(tios->req_q->queue_lock,
 					flags);
 			} else {
 				pr_err("%s: failed to create read request",
@@ -2330,11 +2330,11 @@ static void long_seq_write_free_end_io_fn(struct request *rq, int err)
 
 	BUG_ON(!test_rq);
 
-	spin_lock_irqsave(&tios->lock, flags);
+	raw_spin_lock_irqsave(&tios->lock, flags);
 	list_del_init(&test_rq->queuelist);
 	tios->dispatched_count--;
 	__blk_put_request(tios->req_q, test_rq->rq);
-	spin_unlock_irqrestore(&tios->lock, flags);
+	raw_spin_unlock_irqrestore(&tios->lock, flags);
 
 	test_iosched_free_test_req_data_buffer(test_rq);
 	kfree(test_rq);
diff --git a/kernel/msm-3.18/drivers/mmc/core/ring_buffer.c b/kernel/msm-3.18/drivers/mmc/core/ring_buffer.c
index 55b820de2..2b339c09e 100644
--- a/kernel/msm-3.18/drivers/mmc/core/ring_buffer.c
+++ b/kernel/msm-3.18/drivers/mmc/core/ring_buffer.c
@@ -37,11 +37,11 @@ void mmc_trace_write(struct mmc_host *mmc,
 	 * index within array bounds. The cast to unsigned is
 	 * necessary so increment and rolover wraps to 0 correctly
 	 */
-	spin_lock_irqsave(&mmc->trace_buf.trace_lock, flags);
+	raw_spin_lock_irqsave(&mmc->trace_buf.trace_lock, flags);
 	mmc->trace_buf.wr_idx += 1;
 	idx = ((unsigned int)mmc->trace_buf.wr_idx) &
 			(MMC_TRACE_RBUF_NUM_EVENTS - 1);
-	spin_unlock_irqrestore(&mmc->trace_buf.trace_lock, flags);
+	raw_spin_unlock_irqrestore(&mmc->trace_buf.trace_lock, flags);
 
 	/* Catch some unlikely machine specific wrap-around bug */
 	if (unlikely(idx > (MMC_TRACE_RBUF_NUM_EVENTS - 1))) {
@@ -76,7 +76,7 @@ void mmc_trace_init(struct mmc_host *mmc)
 		return;
 	}
 
-	spin_lock_init(&mmc->trace_buf.trace_lock);
+	raw_spin_lock_init(&mmc->trace_buf.trace_lock);
 	mmc->trace_buf.wr_idx = -1;
 }
 
@@ -97,7 +97,7 @@ void mmc_dump_trace_buffer(struct mmc_host *mmc, struct seq_file *s)
 	if (!mmc->trace_buf.data)
 		return;
 
-	spin_lock_irqsave(&mmc->trace_buf.trace_lock, flags);
+	raw_spin_lock_irqsave(&mmc->trace_buf.trace_lock, flags);
 	idx = ((unsigned int)mmc->trace_buf.wr_idx) & N;
 	cur_idx = (idx + 1) & N;
 
@@ -119,5 +119,5 @@ void mmc_dump_trace_buffer(struct mmc_host *mmc, struct seq_file *s)
 			break;
 		}
 	} while (1);
-	spin_unlock_irqrestore(&mmc->trace_buf.trace_lock, flags);
+	raw_spin_unlock_irqrestore(&mmc->trace_buf.trace_lock, flags);
 }
diff --git a/kernel/msm-3.18/drivers/mtd/ubi/fastmap-wl.c b/kernel/msm-3.18/drivers/mtd/ubi/fastmap-wl.c
index 30d3999dd..ece898265 100644
--- a/kernel/msm-3.18/drivers/mtd/ubi/fastmap-wl.c
+++ b/kernel/msm-3.18/drivers/mtd/ubi/fastmap-wl.c
@@ -23,9 +23,9 @@ static void update_fastmap_work_fn(struct work_struct *wrk)
 	struct ubi_device *ubi = container_of(wrk, struct ubi_device, fm_work);
 
 	ubi_update_fastmap(ubi);
-	spin_lock(&ubi->wl_lock);
+	raw_spin_lock(&ubi->wl_lock);
 	ubi->fm_work_scheduled = 0;
-	spin_unlock(&ubi->wl_lock);
+	raw_spin_unlock(&ubi->wl_lock);
 }
 
 /**
@@ -123,7 +123,7 @@ void ubi_refill_pools(struct ubi_device *ubi)
 	struct ubi_wl_entry *e;
 	int enough;
 
-	spin_lock(&ubi->wl_lock);
+	raw_spin_lock(&ubi->wl_lock);
 
 	return_unused_pool_pebs(ubi, wl_pool);
 	return_unused_pool_pebs(ubi, pool);
@@ -168,7 +168,7 @@ void ubi_refill_pools(struct ubi_device *ubi)
 	wl_pool->used = 0;
 	pool->used = 0;
 
-	spin_unlock(&ubi->wl_lock);
+	raw_spin_unlock(&ubi->wl_lock);
 }
 
 /**
@@ -211,12 +211,12 @@ int ubi_wl_get_peb(struct ubi_device *ubi)
 
 again:
 	down_read(&ubi->fm_eba_sem);
-	spin_lock(&ubi->wl_lock);
+	raw_spin_lock(&ubi->wl_lock);
 
 	/* We check here also for the WL pool because at this point we can
 	 * refill the WL pool synchronous. */
 	if (pool->used == pool->size || wl_pool->used == wl_pool->size) {
-		spin_unlock(&ubi->wl_lock);
+		raw_spin_unlock(&ubi->wl_lock);
 		up_read(&ubi->fm_eba_sem);
 		ret = ubi_update_fastmap(ubi);
 		if (ret) {
@@ -225,11 +225,11 @@ again:
 			return -ENOSPC;
 		}
 		down_read(&ubi->fm_eba_sem);
-		spin_lock(&ubi->wl_lock);
+		raw_spin_lock(&ubi->wl_lock);
 	}
 
 	if (pool->used == pool->size) {
-		spin_unlock(&ubi->wl_lock);
+		raw_spin_unlock(&ubi->wl_lock);
 		if (retried) {
 			ubi_err(ubi, "Unable to get a free PEB from user WL pool");
 			ret = -ENOSPC;
@@ -248,7 +248,7 @@ again:
 	ubi_assert(pool->used < pool->size);
 	ret = pool->pebs[pool->used++];
 	prot_queue_add(ubi, ubi->lookuptbl[ret]);
-	spin_unlock(&ubi->wl_lock);
+	raw_spin_unlock(&ubi->wl_lock);
 out:
 	return ret;
 }
@@ -285,19 +285,19 @@ int ubi_ensure_anchor_pebs(struct ubi_device *ubi)
 {
 	struct ubi_work *wrk;
 
-	spin_lock(&ubi->wl_lock);
+	raw_spin_lock(&ubi->wl_lock);
 	if (ubi->wl_scheduled) {
-		spin_unlock(&ubi->wl_lock);
+		raw_spin_unlock(&ubi->wl_lock);
 		return 0;
 	}
 	ubi->wl_scheduled = 1;
-	spin_unlock(&ubi->wl_lock);
+	raw_spin_unlock(&ubi->wl_lock);
 
 	wrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);
 	if (!wrk) {
-		spin_lock(&ubi->wl_lock);
+		raw_spin_lock(&ubi->wl_lock);
 		ubi->wl_scheduled = 0;
-		spin_unlock(&ubi->wl_lock);
+		raw_spin_unlock(&ubi->wl_lock);
 		return -ENOMEM;
 	}
 
@@ -328,7 +328,7 @@ int ubi_wl_put_fm_peb(struct ubi_device *ubi, struct ubi_wl_entry *fm_e,
 	ubi_assert(pnum >= 0);
 	ubi_assert(pnum < ubi->peb_count);
 
-	spin_lock(&ubi->wl_lock);
+	raw_spin_lock(&ubi->wl_lock);
 	e = ubi->lookuptbl[pnum];
 
 	/* This can happen if we recovered from a fastmap the very
@@ -341,7 +341,7 @@ int ubi_wl_put_fm_peb(struct ubi_device *ubi, struct ubi_wl_entry *fm_e,
 		ubi->lookuptbl[pnum] = e;
 	}
 
-	spin_unlock(&ubi->wl_lock);
+	raw_spin_unlock(&ubi->wl_lock);
 
 	vol_id = lnum ? UBI_FM_DATA_VOLUME_ID : UBI_FM_SB_VOLUME_ID;
 	return schedule_erase(ubi, e, vol_id, lnum, torture);
diff --git a/kernel/msm-3.18/drivers/nfc/nq-nci.c b/kernel/msm-3.18/drivers/nfc/nq-nci.c
index ced2d00bd..e46eb662a 100644
--- a/kernel/msm-3.18/drivers/nfc/nq-nci.c
+++ b/kernel/msm-3.18/drivers/nfc/nq-nci.c
@@ -67,7 +67,7 @@ struct nqx_dev {
 	bool			irq_enabled;
 	/* NFC_IRQ wake-up state */
 	bool			irq_wake_up;
-	spinlock_t		irq_enabled_lock;
+	raw_spinlock_t		irq_enabled_lock;
 	unsigned int		count_irq;
 	/* Initial CORE RESET notification */
 	unsigned int		core_reset_ntf;
@@ -103,12 +103,12 @@ static void nqx_disable_irq(struct nqx_dev *nqx_dev)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&nqx_dev->irq_enabled_lock, flags);
+	raw_spin_lock_irqsave(&nqx_dev->irq_enabled_lock, flags);
 	if (nqx_dev->irq_enabled) {
 		disable_irq_nosync(nqx_dev->client->irq);
 		nqx_dev->irq_enabled = false;
 	}
-	spin_unlock_irqrestore(&nqx_dev->irq_enabled_lock, flags);
+	raw_spin_unlock_irqrestore(&nqx_dev->irq_enabled_lock, flags);
 }
 
 /**
@@ -123,12 +123,12 @@ static void nqx_enable_irq(struct nqx_dev *nqx_dev)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&nqx_dev->irq_enabled_lock, flags);
+	raw_spin_lock_irqsave(&nqx_dev->irq_enabled_lock, flags);
 	if (!nqx_dev->irq_enabled) {
 		nqx_dev->irq_enabled = true;
 		enable_irq(nqx_dev->client->irq);
 	}
-	spin_unlock_irqrestore(&nqx_dev->irq_enabled_lock, flags);
+	raw_spin_unlock_irqrestore(&nqx_dev->irq_enabled_lock, flags);
 }
 
 static irqreturn_t nqx_dev_irq_handler(int irq, void *dev_id)
@@ -140,9 +140,9 @@ static irqreturn_t nqx_dev_irq_handler(int irq, void *dev_id)
 		pm_wakeup_event(&nqx_dev->client->dev, WAKEUP_SRC_TIMEOUT);
 
 	nqx_disable_irq(nqx_dev);
-	spin_lock_irqsave(&nqx_dev->irq_enabled_lock, flags);
+	raw_spin_lock_irqsave(&nqx_dev->irq_enabled_lock, flags);
 	nqx_dev->count_irq++;
-	spin_unlock_irqrestore(&nqx_dev->irq_enabled_lock, flags);
+	raw_spin_unlock_irqrestore(&nqx_dev->irq_enabled_lock, flags);
 	wake_up(&nqx_dev->read_wq);
 
 	return IRQ_HANDLED;
@@ -1025,7 +1025,7 @@ static int nqx_probe(struct i2c_client *client,
 	/* init mutex and queues */
 	init_waitqueue_head(&nqx_dev->read_wq);
 	mutex_init(&nqx_dev->read_mutex);
-	spin_lock_init(&nqx_dev->irq_enabled_lock);
+	raw_spin_lock_init(&nqx_dev->irq_enabled_lock);
 
 	nqx_dev->nqx_device.minor = MISC_DYNAMIC_MINOR;
 	nqx_dev->nqx_device.name = "nq-nci";
diff --git a/kernel/msm-3.18/drivers/pci/host/pci-msm.c b/kernel/msm-3.18/drivers/pci/host/pci-msm.c
index 435c50207..4b81eee8c 100644
--- a/kernel/msm-3.18/drivers/pci/host/pci-msm.c
+++ b/kernel/msm-3.18/drivers/pci/host/pci-msm.c
@@ -594,7 +594,7 @@ struct msm_pcie_dev_t {
 	uint32_t			    parf_swing;
 
 	bool				 cfg_access;
-	spinlock_t			 cfg_lock;
+	raw_spinlock_t			 cfg_lock;
 	unsigned long		    irqsave_flags;
 	struct mutex		     setup_lock;
 
@@ -642,10 +642,10 @@ struct msm_pcie_dev_t {
 	bool				 enumerated;
 	struct work_struct	     handle_wake_work;
 	struct mutex		     recovery_lock;
-	spinlock_t                   linkdown_lock;
-	spinlock_t                   wakeup_lock;
-	spinlock_t			global_irq_lock;
-	spinlock_t			aer_lock;
+	raw_spinlock_t                   linkdown_lock;
+	raw_spinlock_t                   wakeup_lock;
+	raw_spinlock_t			global_irq_lock;
+	raw_spinlock_t			aer_lock;
 	ulong				linkdown_counter;
 	ulong				link_turned_on_counter;
 	ulong				link_turned_off_counter;
@@ -3226,7 +3226,7 @@ static inline int msm_pcie_oper_conf(struct pci_bus *bus, u32 devfn, int oper,
 	rc_idx = dev->rc_idx;
 	rc = (bus->number == 0);
 
-	spin_lock_irqsave(&dev->cfg_lock, dev->irqsave_flags);
+	raw_spin_lock_irqsave(&dev->cfg_lock, dev->irqsave_flags);
 
 	if (!dev->cfg_access) {
 		PCIE_DBG3(dev,
@@ -3324,7 +3324,7 @@ static inline int msm_pcie_oper_conf(struct pci_bus *bus, u32 devfn, int oper,
 	}
 
 unlock:
-	spin_unlock_irqrestore(&dev->cfg_lock, dev->irqsave_flags);
+	raw_spin_unlock_irqrestore(&dev->cfg_lock, dev->irqsave_flags);
 out:
 	return rv;
 }
@@ -5166,13 +5166,13 @@ static irqreturn_t handle_aer_irq(int irq, void *data)
 		dev->ep_corr_counter, dev->ep_non_fatal_counter,
 		dev->ep_fatal_counter);
 
-	spin_lock_irqsave(&dev->aer_lock, irqsave_flags);
+	raw_spin_lock_irqsave(&dev->aer_lock, irqsave_flags);
 
 	if (dev->suspending) {
 		PCIE_DBG2(dev,
 			"PCIe: RC%d is currently suspending.\n",
 			dev->rc_idx);
-		spin_unlock_irqrestore(&dev->aer_lock, irqsave_flags);
+		raw_spin_unlock_irqrestore(&dev->aer_lock, irqsave_flags);
 		return IRQ_HANDLED;
 	}
 
@@ -5289,7 +5289,7 @@ out:
 			PCIE20_AER_ROOT_ERR_STATUS_REG,
 			0x7f, 0x7f);
 
-	spin_unlock_irqrestore(&dev->aer_lock, irqsave_flags);
+	raw_spin_unlock_irqrestore(&dev->aer_lock, irqsave_flags);
 	return IRQ_HANDLED;
 }
 
@@ -5299,7 +5299,7 @@ static irqreturn_t handle_wake_irq(int irq, void *data)
 	unsigned long irqsave_flags;
 	int i;
 
-	spin_lock_irqsave(&dev->wakeup_lock, irqsave_flags);
+	raw_spin_lock_irqsave(&dev->wakeup_lock, irqsave_flags);
 
 	dev->wake_counter++;
 	PCIE_DBG(dev, "PCIe: No. %ld wake IRQ for RC%d\n",
@@ -5329,7 +5329,7 @@ static irqreturn_t handle_wake_irq(int irq, void *data)
 		}
 	}
 
-	spin_unlock_irqrestore(&dev->wakeup_lock, irqsave_flags);
+	raw_spin_unlock_irqrestore(&dev->wakeup_lock, irqsave_flags);
 
 	return IRQ_HANDLED;
 }
@@ -5340,7 +5340,7 @@ static irqreturn_t handle_linkdown_irq(int irq, void *data)
 	unsigned long irqsave_flags;
 	int i;
 
-	spin_lock_irqsave(&dev->linkdown_lock, irqsave_flags);
+	raw_spin_lock_irqsave(&dev->linkdown_lock, irqsave_flags);
 
 	dev->linkdown_counter++;
 
@@ -5380,7 +5380,7 @@ static irqreturn_t handle_linkdown_irq(int irq, void *data)
 		}
 	}
 
-	spin_unlock_irqrestore(&dev->linkdown_lock, irqsave_flags);
+	raw_spin_unlock_irqrestore(&dev->linkdown_lock, irqsave_flags);
 
 	return IRQ_HANDLED;
 }
@@ -5423,7 +5423,7 @@ static irqreturn_t handle_global_irq(int irq, void *data)
 	unsigned long irqsave_flags;
 	u32 status;
 
-	spin_lock_irqsave(&dev->global_irq_lock, irqsave_flags);
+	raw_spin_lock_irqsave(&dev->global_irq_lock, irqsave_flags);
 
 	status = readl_relaxed(dev->parf + PCIE20_PARF_INT_ALL_STATUS) &
 			readl_relaxed(dev->parf + PCIE20_PARF_INT_ALL_MASK);
@@ -5462,7 +5462,7 @@ static irqreturn_t handle_global_irq(int irq, void *data)
 		}
 	}
 
-	spin_unlock_irqrestore(&dev->global_irq_lock, irqsave_flags);
+	raw_spin_unlock_irqrestore(&dev->global_irq_lock, irqsave_flags);
 
 	return IRQ_HANDLED;
 }
@@ -6397,14 +6397,14 @@ int __init pcie_init(void)
 			PCIE_DBG(&msm_pcie_dev[i],
 				"PCIe IPC logging %s is enable for RC%d\n",
 				rc_name, i);
-		spin_lock_init(&msm_pcie_dev[i].cfg_lock);
+		raw_spin_lock_init(&msm_pcie_dev[i].cfg_lock);
 		msm_pcie_dev[i].cfg_access = true;
 		mutex_init(&msm_pcie_dev[i].setup_lock);
 		mutex_init(&msm_pcie_dev[i].recovery_lock);
-		spin_lock_init(&msm_pcie_dev[i].linkdown_lock);
-		spin_lock_init(&msm_pcie_dev[i].wakeup_lock);
-		spin_lock_init(&msm_pcie_dev[i].global_irq_lock);
-		spin_lock_init(&msm_pcie_dev[i].aer_lock);
+		raw_spin_lock_init(&msm_pcie_dev[i].linkdown_lock);
+		raw_spin_lock_init(&msm_pcie_dev[i].wakeup_lock);
+		raw_spin_lock_init(&msm_pcie_dev[i].global_irq_lock);
+		raw_spin_lock_init(&msm_pcie_dev[i].aer_lock);
 		msm_pcie_dev[i].drv_ready = false;
 	}
 	for (i = 0; i < MAX_RC_NUM * MAX_DEVICE_NUM; i++) {
@@ -6468,9 +6468,9 @@ static int msm_pcie_pm_suspend(struct pci_dev *dev,
 
 	PCIE_DBG(pcie_dev, "RC%d: entry\n", pcie_dev->rc_idx);
 
-	spin_lock_irqsave(&pcie_dev->aer_lock, irqsave_flags);
+	raw_spin_lock_irqsave(&pcie_dev->aer_lock, irqsave_flags);
 	pcie_dev->suspending = true;
-	spin_unlock_irqrestore(&pcie_dev->aer_lock, irqsave_flags);
+	raw_spin_unlock_irqrestore(&pcie_dev->aer_lock, irqsave_flags);
 
 	if (!pcie_dev->power_on) {
 		PCIE_DBG(pcie_dev,
@@ -6492,10 +6492,10 @@ static int msm_pcie_pm_suspend(struct pci_dev *dev,
 		return ret;
 	}
 
-	spin_lock_irqsave(&pcie_dev->cfg_lock,
+	raw_spin_lock_irqsave(&pcie_dev->cfg_lock,
 				pcie_dev->irqsave_flags);
 	pcie_dev->cfg_access = false;
-	spin_unlock_irqrestore(&pcie_dev->cfg_lock,
+	raw_spin_unlock_irqrestore(&pcie_dev->cfg_lock,
 				pcie_dev->irqsave_flags);
 
 	msm_pcie_write_mask(pcie_dev->elbi + PCIE20_ELBI_SYS_CTRL, 0,
@@ -6539,17 +6539,17 @@ static void msm_pcie_fixup_suspend(struct pci_dev *dev)
 	if (pcie_dev->link_status != MSM_PCIE_LINK_ENABLED)
 		return;
 
-	spin_lock_irqsave(&pcie_dev->cfg_lock,
+	raw_spin_lock_irqsave(&pcie_dev->cfg_lock,
 				pcie_dev->irqsave_flags);
 	if (pcie_dev->disable_pc) {
 		PCIE_DBG(pcie_dev,
 			"RC%d: Skip suspend because of user request\n",
 			pcie_dev->rc_idx);
-		spin_unlock_irqrestore(&pcie_dev->cfg_lock,
+		raw_spin_unlock_irqrestore(&pcie_dev->cfg_lock,
 				pcie_dev->irqsave_flags);
 		return;
 	}
-	spin_unlock_irqrestore(&pcie_dev->cfg_lock,
+	raw_spin_unlock_irqrestore(&pcie_dev->cfg_lock,
 				pcie_dev->irqsave_flags);
 
 	mutex_lock(&pcie_dev->recovery_lock);
@@ -6577,10 +6577,10 @@ static int msm_pcie_pm_resume(struct pci_dev *dev,
 		pinctrl_select_state(pcie_dev->pinctrl,
 					pcie_dev->pins_default);
 
-	spin_lock_irqsave(&pcie_dev->cfg_lock,
+	raw_spin_lock_irqsave(&pcie_dev->cfg_lock,
 				pcie_dev->irqsave_flags);
 	pcie_dev->cfg_access = true;
-	spin_unlock_irqrestore(&pcie_dev->cfg_lock,
+	raw_spin_unlock_irqrestore(&pcie_dev->cfg_lock,
 				pcie_dev->irqsave_flags);
 
 	ret = msm_pcie_enable(pcie_dev, PM_PIPE_CLK | PM_CLK | PM_VREG);
@@ -6816,7 +6816,7 @@ int msm_pcie_pm_control(enum msm_pcie_pm_opt pm_opt, u32 busnr, void *user,
 		PCIE_DBG(&msm_pcie_dev[rc_idx],
 			"User of RC%d requests to keep the link always alive.\n",
 			rc_idx);
-		spin_lock_irqsave(&msm_pcie_dev[rc_idx].cfg_lock,
+		raw_spin_lock_irqsave(&msm_pcie_dev[rc_idx].cfg_lock,
 				msm_pcie_dev[rc_idx].irqsave_flags);
 		if (msm_pcie_dev[rc_idx].suspending) {
 			PCIE_ERR(&msm_pcie_dev[rc_idx],
@@ -6826,17 +6826,17 @@ int msm_pcie_pm_control(enum msm_pcie_pm_opt pm_opt, u32 busnr, void *user,
 		} else {
 			msm_pcie_dev[rc_idx].disable_pc = true;
 		}
-		spin_unlock_irqrestore(&msm_pcie_dev[rc_idx].cfg_lock,
+		raw_spin_unlock_irqrestore(&msm_pcie_dev[rc_idx].cfg_lock,
 				msm_pcie_dev[rc_idx].irqsave_flags);
 		break;
 	case MSM_PCIE_ENABLE_PC:
 		PCIE_DBG(&msm_pcie_dev[rc_idx],
 			"User of RC%d cancels the request of alive link.\n",
 			rc_idx);
-		spin_lock_irqsave(&msm_pcie_dev[rc_idx].cfg_lock,
+		raw_spin_lock_irqsave(&msm_pcie_dev[rc_idx].cfg_lock,
 				msm_pcie_dev[rc_idx].irqsave_flags);
 		msm_pcie_dev[rc_idx].disable_pc = false;
-		spin_unlock_irqrestore(&msm_pcie_dev[rc_idx].cfg_lock,
+		raw_spin_unlock_irqrestore(&msm_pcie_dev[rc_idx].cfg_lock,
 				msm_pcie_dev[rc_idx].irqsave_flags);
 		break;
 	default:
diff --git a/kernel/msm-3.18/drivers/power/pmic-voter.c b/kernel/msm-3.18/drivers/power/pmic-voter.c
index b99558ed2..18c98e75d 100644
--- a/kernel/msm-3.18/drivers/power/pmic-voter.c
+++ b/kernel/msm-3.18/drivers/power/pmic-voter.c
@@ -455,7 +455,7 @@ struct votable *find_votable(const char *name)
 	struct votable *v;
 	bool found = false;
 
-	spin_lock_irqsave(&votable_list_slock, flags);
+	raw_spin_lock_irqsave(&votable_list_slock, flags);
 	if (list_empty(&votable_list))
 		goto out;
 
@@ -466,7 +466,7 @@ struct votable *find_votable(const char *name)
 		}
 	}
 out:
-	spin_unlock_irqrestore(&votable_list_slock, flags);
+	raw_spin_unlock_irqrestore(&votable_list_slock, flags);
 
 	if (found)
 		return v;
@@ -621,9 +621,9 @@ struct votable *create_votable(const char *name,
 		votable->effective_result = 0;
 	votable->effective_client_id = -EINVAL;
 
-	spin_lock_irqsave(&votable_list_slock, flags);
+	raw_spin_lock_irqsave(&votable_list_slock, flags);
 	list_add(&votable->list, &votable_list);
-	spin_unlock_irqrestore(&votable_list_slock, flags);
+	raw_spin_unlock_irqrestore(&votable_list_slock, flags);
 
 	votable->root = debugfs_create_dir(name, debug_root);
 	if (!votable->root) {
@@ -680,9 +680,9 @@ void destroy_votable(struct votable *votable)
 	if (!votable)
 		return;
 
-	spin_lock_irqsave(&votable_list_slock, flags);
+	raw_spin_lock_irqsave(&votable_list_slock, flags);
 	list_del(&votable->list);
-	spin_unlock_irqrestore(&votable_list_slock, flags);
+	raw_spin_unlock_irqrestore(&votable_list_slock, flags);
 
 	debugfs_remove_recursive(votable->root);
 
diff --git a/kernel/msm-3.18/drivers/power/qpnp-charger.c b/kernel/msm-3.18/drivers/power/qpnp-charger.c
index d7137e390..874c8e232 100644
--- a/kernel/msm-3.18/drivers/power/qpnp-charger.c
+++ b/kernel/msm-3.18/drivers/power/qpnp-charger.c
@@ -398,7 +398,7 @@ struct qpnp_chg_chip {
 	struct qpnp_adc_tm_chip		*adc_tm_dev;
 	struct mutex			jeita_configure_lock;
 	struct mutex			batfet_vreg_lock;
-	spinlock_t			usbin_health_monitor_lock;
+	raw_spinlock_t			usbin_health_monitor_lock;
 	struct alarm			reduce_power_stage_alarm;
 	struct work_struct		reduce_power_stage_work;
 	bool				power_stage_workaround_running;
@@ -1630,7 +1630,7 @@ qpnp_usbin_health_check_work(struct work_struct *work)
 				struct qpnp_chg_chip, usbin_health_check);
 
 	usbin_health = qpnp_chg_check_usbin_health(chip);
-	spin_lock(&chip->usbin_health_monitor_lock);
+	raw_spin_lock(&chip->usbin_health_monitor_lock);
 	if (chip->usbin_health != usbin_health) {
 		pr_debug("health_check_work: pr_usbin_health = %d, usbin_health = %d",
 			chip->usbin_health, usbin_health);
@@ -1644,7 +1644,7 @@ qpnp_usbin_health_check_work(struct work_struct *work)
 	}
 	/* enable OVP monitor in usb valid after coarse-det complete */
 	chip->usb_valid_check_ovp = true;
-	spin_unlock(&chip->usbin_health_monitor_lock);
+	raw_spin_unlock(&chip->usbin_health_monitor_lock);
 	return;
 }
 
@@ -5383,7 +5383,7 @@ qpnp_charger_probe(struct spmi_device *spmi)
 
 	mutex_init(&chip->jeita_configure_lock);
 	mutex_init(&chip->batfet_vreg_lock);
-	spin_lock_init(&chip->usbin_health_monitor_lock);
+	raw_spin_lock_init(&chip->usbin_health_monitor_lock);
 	alarm_init(&chip->reduce_power_stage_alarm, ALARM_REALTIME,
 			qpnp_chg_reduce_power_stage_callback);
 	INIT_WORK(&chip->reduce_power_stage_work,
diff --git a/kernel/msm-3.18/drivers/power/qpnp-fg.c b/kernel/msm-3.18/drivers/power/qpnp-fg.c
index 7b1105986..17990361a 100644
--- a/kernel/msm-3.18/drivers/power/qpnp-fg.c
+++ b/kernel/msm-3.18/drivers/power/qpnp-fg.c
@@ -488,7 +488,7 @@ struct fg_chip {
 	struct completion	batt_id_avail;
 	struct completion	first_soc_done;
 	struct power_supply	bms_psy;
-	spinlock_t		sec_access_lock;
+	raw_spinlock_t		sec_access_lock;
 	struct mutex		rw_lock;
 	struct mutex		sysfs_restart_lock;
 	struct delayed_work	batt_profile_init;
@@ -810,9 +810,9 @@ static int fg_masked_write(struct fg_chip *chip, u16 addr,
 	int rc;
 	unsigned long flags;
 
-	spin_lock_irqsave(&chip->sec_access_lock, flags);
+	raw_spin_lock_irqsave(&chip->sec_access_lock, flags);
 	rc = fg_masked_write_raw(chip, addr, mask, val, len);
-	spin_unlock_irqrestore(&chip->sec_access_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->sec_access_lock, flags);
 
 	return rc;
 }
@@ -828,7 +828,7 @@ static int fg_sec_masked_write(struct fg_chip *chip, u16 addr, u8 mask, u8 val,
 	u8 temp;
 	u16 base = addr & (~PERIPHERAL_MASK);
 
-	spin_lock_irqsave(&chip->sec_access_lock, flags);
+	raw_spin_lock_irqsave(&chip->sec_access_lock, flags);
 	temp = SEC_ACCESS_VALUE;
 	rc = fg_write(chip, &temp, base + SEC_ACCESS_OFFSET, 1);
 	if (rc) {
@@ -841,7 +841,7 @@ static int fg_sec_masked_write(struct fg_chip *chip, u16 addr, u8 mask, u8 val,
 		pr_err("Unable to write securely to address 0x%x: %d", addr,
 			rc);
 out:
-	spin_unlock_irqrestore(&chip->sec_access_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->sec_access_lock, flags);
 	return rc;
 }
 
@@ -8737,7 +8737,7 @@ static int fg_probe(struct spmi_device *spmi)
 			"qpnp_fg_cc_soc");
 	wakeup_source_init(&chip->sanity_wakeup_source.source,
 			"qpnp_fg_sanity_check");
-	spin_lock_init(&chip->sec_access_lock);
+	raw_spin_lock_init(&chip->sec_access_lock);
 	mutex_init(&chip->rw_lock);
 	mutex_init(&chip->cyc_ctr.lock);
 	mutex_init(&chip->learning_data.learning_lock);
diff --git a/kernel/msm-3.18/drivers/power/qpnp-linear-charger.c b/kernel/msm-3.18/drivers/power/qpnp-linear-charger.c
index 5c0f69afb..db73fd491 100644
--- a/kernel/msm-3.18/drivers/power/qpnp-linear-charger.c
+++ b/kernel/msm-3.18/drivers/power/qpnp-linear-charger.c
@@ -379,9 +379,9 @@ struct qpnp_lbc_chip {
 	struct qpnp_lbc_irq		irqs[MAX_IRQS];
 	struct mutex			jeita_configure_lock;
 	struct mutex			chg_enable_lock;
-	spinlock_t			ibat_change_lock;
-	spinlock_t			hw_access_lock;
-	spinlock_t			irq_lock;
+	raw_spinlock_t			ibat_change_lock;
+	raw_spinlock_t			hw_access_lock;
+	raw_spinlock_t			irq_lock;
 	struct power_supply		*usb_psy;
 	struct power_supply		*bms_psy;
 	struct power_supply		batt_psy;
@@ -401,14 +401,14 @@ static void qpnp_lbc_enable_irq(struct qpnp_lbc_chip *chip,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&chip->irq_lock, flags);
+	raw_spin_lock_irqsave(&chip->irq_lock, flags);
 	if (__test_and_clear_bit(0, &irq->disabled)) {
 		pr_debug("number = %d\n", irq->irq);
 		enable_irq(irq->irq);
 		if (irq->is_wake)
 			enable_irq_wake(irq->irq);
 	}
-	spin_unlock_irqrestore(&chip->irq_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->irq_lock, flags);
 }
 
 static void qpnp_lbc_disable_irq(struct qpnp_lbc_chip *chip,
@@ -416,14 +416,14 @@ static void qpnp_lbc_disable_irq(struct qpnp_lbc_chip *chip,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&chip->irq_lock, flags);
+	raw_spin_lock_irqsave(&chip->irq_lock, flags);
 	if (!__test_and_set_bit(0, &irq->disabled)) {
 		pr_debug("number = %d\n", irq->irq);
 		disable_irq_nosync(irq->irq);
 		if (irq->is_wake)
 			disable_irq_wake(irq->irq);
 	}
-	spin_unlock_irqrestore(&chip->irq_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->irq_lock, flags);
 }
 
 static int __qpnp_lbc_read(struct spmi_device *spmi, u16 base,
@@ -488,9 +488,9 @@ static int qpnp_lbc_read(struct qpnp_lbc_chip *chip, u16 base,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&chip->hw_access_lock, flags);
+	raw_spin_lock_irqsave(&chip->hw_access_lock, flags);
 	rc = __qpnp_lbc_read(spmi, base, val, count);
-	spin_unlock_irqrestore(&chip->hw_access_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->hw_access_lock, flags);
 
 	return rc;
 }
@@ -508,9 +508,9 @@ static int qpnp_lbc_write(struct qpnp_lbc_chip *chip, u16 base,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&chip->hw_access_lock, flags);
+	raw_spin_lock_irqsave(&chip->hw_access_lock, flags);
 	rc = __qpnp_lbc_write(spmi, base, val, count);
-	spin_unlock_irqrestore(&chip->hw_access_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->hw_access_lock, flags);
 
 	return rc;
 }
@@ -523,7 +523,7 @@ static int qpnp_lbc_masked_write(struct qpnp_lbc_chip *chip, u16 base,
 	struct spmi_device *spmi = chip->spmi;
 	unsigned long flags;
 
-	spin_lock_irqsave(&chip->hw_access_lock, flags);
+	raw_spin_lock_irqsave(&chip->hw_access_lock, flags);
 	rc = __qpnp_lbc_read(spmi, base, &reg_val, 1);
 	if (rc) {
 		pr_err("spmi read failed: addr=%03X, rc=%d\n", base, rc);
@@ -541,7 +541,7 @@ static int qpnp_lbc_masked_write(struct qpnp_lbc_chip *chip, u16 base,
 		pr_err("spmi write failed: addr=%03X, rc=%d\n", base, rc);
 
 out:
-	spin_unlock_irqrestore(&chip->hw_access_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->hw_access_lock, flags);
 	return rc;
 }
 
@@ -877,7 +877,7 @@ static int qpnp_lbc_vddmax_set(struct qpnp_lbc_chip *chip, int voltage)
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&chip->hw_access_lock, flags);
+	raw_spin_lock_irqsave(&chip->hw_access_lock, flags);
 	reg_val = (voltage - QPNP_LBC_VBAT_MIN_MV) / QPNP_LBC_VBAT_STEP_MV;
 	pr_debug("voltage=%d setting %02x\n", voltage, reg_val);
 	rc = __qpnp_lbc_write(chip->spmi, chip->chgr_base + CHG_VDD_MAX_REG,
@@ -921,7 +921,7 @@ static int qpnp_lbc_vddmax_set(struct qpnp_lbc_chip *chip, int voltage)
 	}
 
 out:
-	spin_unlock_irqrestore(&chip->hw_access_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->hw_access_lock, flags);
 	return rc;
 }
 
@@ -1177,7 +1177,7 @@ static int qpnp_lbc_vbatdet_override(struct qpnp_lbc_chip *chip, int ovr_val)
 	struct spmi_device *spmi = chip->spmi;
 	unsigned long flags;
 
-	spin_lock_irqsave(&chip->hw_access_lock, flags);
+	raw_spin_lock_irqsave(&chip->hw_access_lock, flags);
 
 	rc = __qpnp_lbc_read(spmi, chip->chgr_base + CHG_COMP_OVR1,
 				&reg_val, 1);
@@ -1200,7 +1200,7 @@ static int qpnp_lbc_vbatdet_override(struct qpnp_lbc_chip *chip, int ovr_val)
 						chip->chgr_base, rc);
 
 out:
-	spin_unlock_irqrestore(&chip->hw_access_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->hw_access_lock, flags);
 	return rc;
 }
 
@@ -1392,7 +1392,7 @@ static void qpnp_batt_external_power_changed(struct power_supply *psy)
 	int current_ma;
 	unsigned long flags;
 
-	spin_lock_irqsave(&chip->ibat_change_lock, flags);
+	raw_spin_lock_irqsave(&chip->ibat_change_lock, flags);
 	if (!chip->bms_psy)
 		chip->bms_psy = power_supply_get_by_name("bms");
 
@@ -1418,7 +1418,7 @@ static void qpnp_batt_external_power_changed(struct power_supply *psy)
 	}
 
 skip_current_config:
-	spin_unlock_irqrestore(&chip->ibat_change_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->ibat_change_lock, flags);
 	pr_debug("power supply changed batt_psy\n");
 	power_supply_changed(&chip->batt_psy);
 }
@@ -1449,7 +1449,7 @@ static int qpnp_lbc_system_temp_level_set(struct qpnp_lbc_chip *chip,
 	if (lvl_sel == chip->therm_lvl_sel)
 		return 0;
 
-	spin_lock_irqsave(&chip->ibat_change_lock, flags);
+	raw_spin_lock_irqsave(&chip->ibat_change_lock, flags);
 	prev_therm_lvl = chip->therm_lvl_sel;
 	chip->therm_lvl_sel = lvl_sel;
 	if (chip->therm_lvl_sel == (chip->cfg_thermal_levels - 1)) {
@@ -1475,7 +1475,7 @@ static int qpnp_lbc_system_temp_level_set(struct qpnp_lbc_chip *chip,
 		}
 	}
 out:
-	spin_unlock_irqrestore(&chip->ibat_change_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->ibat_change_lock, flags);
 	return rc;
 }
 
@@ -1939,12 +1939,12 @@ static void qpnp_lbc_jeita_adc_notification(enum qpnp_tm_state state, void *ctx)
 	}
 
 	if (chip->bat_is_cool ^ bat_cool || chip->bat_is_warm ^ bat_warm) {
-		spin_lock_irqsave(&chip->ibat_change_lock, flags);
+		raw_spin_lock_irqsave(&chip->ibat_change_lock, flags);
 		chip->bat_is_cool = bat_cool;
 		chip->bat_is_warm = bat_warm;
 		qpnp_lbc_set_appropriate_vddmax(chip);
 		qpnp_lbc_set_appropriate_current(chip);
-		spin_unlock_irqrestore(&chip->ibat_change_lock, flags);
+		raw_spin_unlock_irqrestore(&chip->ibat_change_lock, flags);
 	}
 
 	pr_debug("warm %d, cool %d, low = %d deciDegC, high = %d deciDegC\n",
@@ -2469,10 +2469,10 @@ static irqreturn_t qpnp_lbc_usbin_valid_irq_handler(int irq, void *_chip)
 		chip->usb_present = usb_present;
 		if (!usb_present) {
 			qpnp_lbc_charger_enable(chip, CURRENT, 0);
-			spin_lock_irqsave(&chip->ibat_change_lock, flags);
+			raw_spin_lock_irqsave(&chip->ibat_change_lock, flags);
 			chip->usb_psy_ma = QPNP_CHG_I_MAX_MIN_90;
 			qpnp_lbc_set_appropriate_current(chip);
-			spin_unlock_irqrestore(&chip->ibat_change_lock,
+			raw_spin_unlock_irqrestore(&chip->ibat_change_lock,
 								flags);
 			if (chip->cfg_collapsible_chgr_support)
 				chip->non_collapsible_chgr_detected = false;
@@ -3111,8 +3111,8 @@ static int qpnp_lbc_parallel_probe(struct spmi_device *spmi)
 	chip->spmi = spmi;
 	dev_set_drvdata(&spmi->dev, chip);
 	device_init_wakeup(&spmi->dev, 1);
-	spin_lock_init(&chip->hw_access_lock);
-	spin_lock_init(&chip->ibat_change_lock);
+	raw_spin_lock_init(&chip->hw_access_lock);
+	raw_spin_lock_init(&chip->ibat_change_lock);
 	INIT_DELAYED_WORK(&chip->parallel_work, qpnp_lbc_parallel_work);
 
 	OF_PROP_READ(chip, cfg_max_voltage_mv, "vddmax-mv", rc, 0);
@@ -3183,9 +3183,9 @@ static int qpnp_lbc_main_probe(struct spmi_device *spmi)
 	device_init_wakeup(&spmi->dev, 1);
 	mutex_init(&chip->jeita_configure_lock);
 	mutex_init(&chip->chg_enable_lock);
-	spin_lock_init(&chip->hw_access_lock);
-	spin_lock_init(&chip->ibat_change_lock);
-	spin_lock_init(&chip->irq_lock);
+	raw_spin_lock_init(&chip->hw_access_lock);
+	raw_spin_lock_init(&chip->ibat_change_lock);
+	raw_spin_lock_init(&chip->irq_lock);
 	INIT_WORK(&chip->vddtrim_work, qpnp_lbc_vddtrim_work_fn);
 	alarm_init(&chip->vddtrim_alarm, ALARM_REALTIME, vddtrim_callback);
 	INIT_DELAYED_WORK(&chip->collapsible_detection_work,
diff --git a/kernel/msm-3.18/drivers/power/qpnp-smbcharger.c b/kernel/msm-3.18/drivers/power/qpnp-smbcharger.c
index 328877a87..61573e657 100644
--- a/kernel/msm-3.18/drivers/power/qpnp-smbcharger.c
+++ b/kernel/msm-3.18/drivers/power/qpnp-smbcharger.c
@@ -254,7 +254,7 @@ struct smbchg_chip {
 	struct work_struct		usb_set_online_work;
 	struct delayed_work		vfloat_adjust_work;
 	struct delayed_work		hvdcp_det_work;
-	spinlock_t			sec_access_lock;
+	raw_spinlock_t			sec_access_lock;
 	struct mutex			therm_lvl_lock;
 	struct mutex			usb_set_online_lock;
 	struct mutex			pm_lock;
@@ -599,9 +599,9 @@ static int smbchg_masked_write(struct smbchg_chip *chip, u16 base, u8 mask,
 	unsigned long flags;
 	int rc;
 
-	spin_lock_irqsave(&chip->sec_access_lock, flags);
+	raw_spin_lock_irqsave(&chip->sec_access_lock, flags);
 	rc = smbchg_masked_write_raw(chip, base, mask, val);
-	spin_unlock_irqrestore(&chip->sec_access_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->sec_access_lock, flags);
 
 	return rc;
 }
@@ -622,7 +622,7 @@ static int smbchg_sec_masked_write(struct smbchg_chip *chip, u16 base, u8 mask,
 	int rc;
 	u16 peripheral_base = base & (~PERIPHERAL_MASK);
 
-	spin_lock_irqsave(&chip->sec_access_lock, flags);
+	raw_spin_lock_irqsave(&chip->sec_access_lock, flags);
 
 	rc = smbchg_masked_write_raw(chip, peripheral_base + SEC_ACCESS_OFFSET,
 				SEC_ACCESS_VALUE, SEC_ACCESS_VALUE);
@@ -634,7 +634,7 @@ static int smbchg_sec_masked_write(struct smbchg_chip *chip, u16 base, u8 mask,
 	rc = smbchg_masked_write_raw(chip, base, mask, val);
 
 out:
-	spin_unlock_irqrestore(&chip->sec_access_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->sec_access_lock, flags);
 	return rc;
 }
 
@@ -8576,7 +8576,7 @@ static int smbchg_probe(struct spmi_device *spmi)
 	chip->usb_online = -EINVAL;
 	dev_set_drvdata(&spmi->dev, chip);
 
-	spin_lock_init(&chip->sec_access_lock);
+	raw_spin_lock_init(&chip->sec_access_lock);
 	mutex_init(&chip->therm_lvl_lock);
 	mutex_init(&chip->usb_set_online_lock);
 	mutex_init(&chip->parallel.lock);
diff --git a/kernel/msm-3.18/drivers/power/qpnp-typec.c b/kernel/msm-3.18/drivers/power/qpnp-typec.c
index 8fc721fc8..cac9e7e9e 100644
--- a/kernel/msm-3.18/drivers/power/qpnp-typec.c
+++ b/kernel/msm-3.18/drivers/power/qpnp-typec.c
@@ -93,7 +93,7 @@ struct qpnp_typec_chip {
 	struct power_supply	type_c_psy;
 	struct regulator	*ss_mux_vreg;
 	struct mutex		typec_lock;
-	spinlock_t		rw_lock;
+	raw_spinlock_t		rw_lock;
 
 	u16			base;
 
@@ -167,9 +167,9 @@ static int qpnp_typec_read(struct qpnp_typec_chip *chip, u8 *val, u16 addr,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&chip->rw_lock, flags);
+	raw_spin_lock_irqsave(&chip->rw_lock, flags);
 	rc = __qpnp_typec_read(spmi, val, addr, count);
-	spin_unlock_irqrestore(&chip->rw_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->rw_lock, flags);
 
 	return rc;
 }
@@ -182,7 +182,7 @@ static int qpnp_typec_masked_write(struct qpnp_typec_chip *chip, u16 base,
 	unsigned long flags;
 	struct spmi_device *spmi = chip->spmi;
 
-	spin_lock_irqsave(&chip->rw_lock, flags);
+	raw_spin_lock_irqsave(&chip->rw_lock, flags);
 	rc = __qpnp_typec_read(spmi, &reg, base, 1);
 	if (rc) {
 		pr_err("spmi read failed: addr=%03X, rc=%d\n", base, rc);
@@ -201,7 +201,7 @@ static int qpnp_typec_masked_write(struct qpnp_typec_chip *chip, u16 base,
 	}
 
 out:
-	spin_unlock_irqrestore(&chip->rw_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->rw_lock, flags);
 	return rc;
 }
 
@@ -900,7 +900,7 @@ static int qpnp_typec_probe(struct spmi_device *spmi)
 	dev_set_drvdata(&spmi->dev, chip);
 	device_init_wakeup(&spmi->dev, 1);
 	mutex_init(&chip->typec_lock);
-	spin_lock_init(&chip->rw_lock);
+	raw_spin_lock_init(&chip->rw_lock);
 
 	/* determine initial status */
 	rc = qpnp_typec_determine_initial_status(chip);
diff --git a/kernel/msm-3.18/drivers/power/smb1351-charger.c b/kernel/msm-3.18/drivers/power/smb1351-charger.c
index d45c4dc5e..bdabf1692 100644
--- a/kernel/msm-3.18/drivers/power/smb1351-charger.c
+++ b/kernel/msm-3.18/drivers/power/smb1351-charger.c
@@ -463,7 +463,7 @@ enum wakeup_src {
 struct smb1351_wakeup_source {
 	struct wakeup_source	source;
 	unsigned long		enabled_bitmap;
-	spinlock_t		ws_lock;
+	raw_spinlock_t		ws_lock;
 };
 
 /* parallel primary charger */
@@ -646,10 +646,10 @@ static void smb1351_stay_awake(struct smb1351_wakeup_source *source,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&source->ws_lock, flags);
+	raw_spin_lock_irqsave(&source->ws_lock, flags);
 	if (!__test_and_set_bit(wk_src, &source->enabled_bitmap))
 		__pm_stay_awake(&source->source);
-	spin_unlock_irqrestore(&source->ws_lock, flags);
+	raw_spin_unlock_irqrestore(&source->ws_lock, flags);
 }
 
 static void smb1351_relax(struct smb1351_wakeup_source *source,
@@ -657,18 +657,18 @@ static void smb1351_relax(struct smb1351_wakeup_source *source,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&source->ws_lock, flags);
+	raw_spin_lock_irqsave(&source->ws_lock, flags);
 	if (__test_and_clear_bit(wk_src, &source->enabled_bitmap) &&
 		!(source->enabled_bitmap & WAKEUP_SRC_MASK)) {
 		__pm_relax(&source->source);
 	}
-	spin_unlock_irqrestore(&source->ws_lock, flags);
+	raw_spin_unlock_irqrestore(&source->ws_lock, flags);
 
 }
 
 static void smb1351_wakeup_src_init(struct smb1351_charger *chip)
 {
-	spin_lock_init(&chip->smb1351_ws.ws_lock);
+	raw_spin_lock_init(&chip->smb1351_ws.ws_lock);
 	wakeup_source_init(&chip->smb1351_ws.source, "smb1351");
 }
 
diff --git a/kernel/msm-3.18/drivers/power/smb1360-charger-fg.c b/kernel/msm-3.18/drivers/power/smb1360-charger-fg.c
index 748a180ec..13e65894a 100644
--- a/kernel/msm-3.18/drivers/power/smb1360-charger-fg.c
+++ b/kernel/msm-3.18/drivers/power/smb1360-charger-fg.c
@@ -320,7 +320,7 @@ enum wakeup_src {
 struct smb1360_wakeup_source {
 	struct wakeup_source source;
 	unsigned long enabled_bitmap;
-	spinlock_t ws_lock;
+	raw_spinlock_t ws_lock;
 };
 
 struct smb1360_chip {
@@ -477,14 +477,14 @@ static void smb1360_stay_awake(struct smb1360_wakeup_source *source,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&source->ws_lock, flags);
+	raw_spin_lock_irqsave(&source->ws_lock, flags);
 
 	if (!__test_and_set_bit(wk_src, &source->enabled_bitmap)) {
 		__pm_stay_awake(&source->source);
 		pr_debug("enabled source %s, wakeup_src %d\n",
 			source->source.name, wk_src);
 	}
-	spin_unlock_irqrestore(&source->ws_lock, flags);
+	raw_spin_unlock_irqrestore(&source->ws_lock, flags);
 }
 
 static void smb1360_relax(struct smb1360_wakeup_source *source,
@@ -492,13 +492,13 @@ static void smb1360_relax(struct smb1360_wakeup_source *source,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&source->ws_lock, flags);
+	raw_spin_lock_irqsave(&source->ws_lock, flags);
 	if (__test_and_clear_bit(wk_src, &source->enabled_bitmap) &&
 		!(source->enabled_bitmap & WAKEUP_SRC_MASK)) {
 		__pm_relax(&source->source);
 		pr_debug("disabled source %s\n", source->source.name);
 	}
-	spin_unlock_irqrestore(&source->ws_lock, flags);
+	raw_spin_unlock_irqrestore(&source->ws_lock, flags);
 
 	pr_debug("relax source %s, wakeup_src %d\n",
 		source->source.name, wk_src);
@@ -506,7 +506,7 @@ static void smb1360_relax(struct smb1360_wakeup_source *source,
 
 static void smb1360_wakeup_src_init(struct smb1360_chip *chip)
 {
-	spin_lock_init(&chip->smb1360_ws.ws_lock);
+	raw_spin_lock_init(&chip->smb1360_ws.ws_lock);
 	wakeup_source_init(&chip->smb1360_ws.source, "smb1360");
 }
 
diff --git a/kernel/msm-3.18/drivers/power/smb23x-charger.c b/kernel/msm-3.18/drivers/power/smb23x-charger.c
index 393415783..c3b0d934b 100644
--- a/kernel/msm-3.18/drivers/power/smb23x-charger.c
+++ b/kernel/msm-3.18/drivers/power/smb23x-charger.c
@@ -27,7 +27,7 @@
 struct smb23x_wakeup_source {
 	struct wakeup_source source;
 	unsigned long enabled_bitmap;
-	spinlock_t ws_lock;
+	raw_spinlock_t ws_lock;
 };
 
 enum wakeup_src {
@@ -454,7 +454,7 @@ i2c_error:
 
 static void smb23x_wakeup_src_init(struct smb23x_chip *chip)
 {
-	spin_lock_init(&chip->smb23x_ws.ws_lock);
+	raw_spin_lock_init(&chip->smb23x_ws.ws_lock);
 	wakeup_source_init(&chip->smb23x_ws.source, "smb23x");
 }
 
@@ -463,14 +463,14 @@ static void smb23x_stay_awake(struct smb23x_wakeup_source *source,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&source->ws_lock, flags);
+	raw_spin_lock_irqsave(&source->ws_lock, flags);
 
 	if (!__test_and_set_bit(wk_src, &source->enabled_bitmap)) {
 		__pm_stay_awake(&source->source);
 		pr_debug("enabled source %s, wakeup_src %d\n",
 			source->source.name, wk_src);
 	}
-	spin_unlock_irqrestore(&source->ws_lock, flags);
+	raw_spin_unlock_irqrestore(&source->ws_lock, flags);
 }
 
 static void smb23x_relax(struct smb23x_wakeup_source *source,
@@ -478,13 +478,13 @@ static void smb23x_relax(struct smb23x_wakeup_source *source,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&source->ws_lock, flags);
+	raw_spin_lock_irqsave(&source->ws_lock, flags);
 	if (__test_and_clear_bit(wk_src, &source->enabled_bitmap) &&
 		!(source->enabled_bitmap & WAKEUP_SRC_MASK)) {
 		__pm_relax(&source->source);
 		pr_debug("disabled source %s\n", source->source.name);
 	}
-	spin_unlock_irqrestore(&source->ws_lock, flags);
+	raw_spin_unlock_irqrestore(&source->ws_lock, flags);
 
 	pr_debug("relax source %s, wakeup_src %d\n",
 		source->source.name, wk_src);
diff --git a/kernel/msm-3.18/drivers/pwm/pwm-qpnp.c b/kernel/msm-3.18/drivers/pwm/pwm-qpnp.c
index 3bfdcd87b..16a259368 100644
--- a/kernel/msm-3.18/drivers/pwm/pwm-qpnp.c
+++ b/kernel/msm-3.18/drivers/pwm/pwm-qpnp.c
@@ -323,7 +323,7 @@ struct qpnp_pwm_chip {
 	bool			enabled;
 	struct _qpnp_pwm_config	pwm_config;
 	struct	qpnp_lpg_config	lpg_config;
-	spinlock_t		lpg_lock;
+	raw_spinlock_t		lpg_lock;
 	enum qpnp_lpg_revision	revision;
 	u8			sub_type;
 	u32			flags;
@@ -1311,7 +1311,7 @@ static int _pwm_enable(struct qpnp_pwm_chip *chip)
 	int rc = 0;
 	unsigned long flags;
 
-	spin_lock_irqsave(&chip->lpg_lock, flags);
+	raw_spin_lock_irqsave(&chip->lpg_lock, flags);
 
 	if (QPNP_IS_PWM_CONFIG_SELECTED(
 		chip->qpnp_lpg_registers[QPNP_ENABLE_CONTROL]) ||
@@ -1325,7 +1325,7 @@ static int _pwm_enable(struct qpnp_pwm_chip *chip)
 	if (!rc)
 		chip->enabled = true;
 
-	spin_unlock_irqrestore(&chip->lpg_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->lpg_lock, flags);
 
 	return rc;
 }
@@ -1342,14 +1342,14 @@ static void qpnp_pwm_free(struct pwm_chip *pwm_chip,
 	struct qpnp_pwm_chip	*chip = qpnp_pwm_from_pwm_chip(pwm_chip);
 	unsigned long		flags;
 
-	spin_lock_irqsave(&chip->lpg_lock, flags);
+	raw_spin_lock_irqsave(&chip->lpg_lock, flags);
 
 	qpnp_lpg_configure_pwm_state(chip, QPNP_PWM_DISABLE);
 	if (!(chip->flags & QPNP_PWM_LUT_NOT_SUPPORTED))
 		qpnp_lpg_configure_lut_state(chip, QPNP_LUT_DISABLE);
 
 	chip->enabled = false;
-	spin_unlock_irqrestore(&chip->lpg_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->lpg_lock, flags);
 }
 
 /**
@@ -1371,7 +1371,7 @@ static int qpnp_pwm_config(struct pwm_chip *pwm_chip,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&chip->lpg_lock, flags);
+	raw_spin_lock_irqsave(&chip->lpg_lock, flags);
 
 	if (prev_period_us > INT_MAX / NSEC_PER_USEC ||
 			prev_period_us * NSEC_PER_USEC != period_ns) {
@@ -1383,7 +1383,7 @@ static int qpnp_pwm_config(struct pwm_chip *pwm_chip,
 
 	rc = _pwm_config(chip, LVL_NSEC, duty_ns, period_ns);
 
-	spin_unlock_irqrestore(&chip->lpg_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->lpg_lock, flags);
 
 	if (rc)
 		pr_err("Failed to configure PWM mode\n");
@@ -1421,7 +1421,7 @@ static void qpnp_pwm_disable(struct pwm_chip *pwm_chip,
 	unsigned long		flags;
 	int rc = 0;
 
-	spin_lock_irqsave(&chip->lpg_lock, flags);
+	raw_spin_lock_irqsave(&chip->lpg_lock, flags);
 
 	if (QPNP_IS_PWM_CONFIG_SELECTED(
 		chip->qpnp_lpg_registers[QPNP_ENABLE_CONTROL]) ||
@@ -1435,7 +1435,7 @@ static void qpnp_pwm_disable(struct pwm_chip *pwm_chip,
 	if (!rc)
 		chip->enabled = false;
 
-	spin_unlock_irqrestore(&chip->lpg_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->lpg_lock, flags);
 
 	if (rc)
 		pr_err("Failed to disable PWM channel: %d\n",
@@ -1479,9 +1479,9 @@ int pwm_change_mode(struct pwm_device *pwm, enum pm_pwm_mode mode)
 
 	chip = qpnp_pwm_from_pwm_dev(pwm);
 
-	spin_lock_irqsave(&chip->lpg_lock, flags);
+	raw_spin_lock_irqsave(&chip->lpg_lock, flags);
 	rc = _pwm_change_mode(chip, mode);
-	spin_unlock_irqrestore(&chip->lpg_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->lpg_lock, flags);
 
 	return rc;
 }
@@ -1511,7 +1511,7 @@ int pwm_config_period(struct pwm_device *pwm,
 	pwm_config = &chip->pwm_config;
 	lpg_config = &chip->lpg_config;
 
-	spin_lock_irqsave(&chip->lpg_lock, flags);
+	raw_spin_lock_irqsave(&chip->lpg_lock, flags);
 
 	pwm_config->period.pwm_size = period->pwm_size;
 	pwm_config->period.clk = period->clk;
@@ -1541,7 +1541,7 @@ int pwm_config_period(struct pwm_device *pwm,
 	}
 
 out_unlock:
-	spin_unlock_irqrestore(&chip->lpg_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->lpg_lock, flags);
 	return rc;
 }
 EXPORT_SYMBOL(pwm_config_period);
@@ -1573,7 +1573,7 @@ int pwm_config_pwm_value(struct pwm_device *pwm, int pwm_value)
 	lpg_config = &chip->lpg_config;
 	pwm_config = &chip->pwm_config;
 
-	spin_lock_irqsave(&chip->lpg_lock, flags);
+	raw_spin_lock_irqsave(&chip->lpg_lock, flags);
 
 	if (pwm_config->pwm_value == pwm_value)
 		goto out_unlock;
@@ -1587,7 +1587,7 @@ int pwm_config_pwm_value(struct pwm_device *pwm, int pwm_value)
 						chip->channel_id, rc);
 
 out_unlock:
-	spin_unlock_irqrestore(&chip->lpg_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->lpg_lock, flags);
 	return rc;
 }
 EXPORT_SYMBOL_GPL(pwm_config_pwm_value);
@@ -1614,7 +1614,7 @@ int pwm_config_us(struct pwm_device *pwm, int duty_us, int period_us)
 
 	chip = qpnp_pwm_from_pwm_dev(pwm);
 
-	spin_lock_irqsave(&chip->lpg_lock, flags);
+	raw_spin_lock_irqsave(&chip->lpg_lock, flags);
 
 	if (chip->pwm_config.pwm_period != period_us) {
 		qpnp_lpg_calc_period(LVL_USEC, period_us, chip);
@@ -1628,7 +1628,7 @@ int pwm_config_us(struct pwm_device *pwm, int duty_us, int period_us)
 
 	rc = _pwm_config(chip, LVL_USEC, duty_us, period_us);
 
-	spin_unlock_irqrestore(&chip->lpg_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->lpg_lock, flags);
 
 	if (rc)
 		pr_err("Failed to configure PWM mode\n");
@@ -1683,7 +1683,7 @@ int pwm_lut_config(struct pwm_device *pwm, int period_us,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&chip->lpg_lock, flags);
+	raw_spin_lock_irqsave(&chip->lpg_lock, flags);
 
 	if (chip->pwm_config.pwm_period != period_us) {
 		qpnp_lpg_calc_period(LVL_USEC, period_us, chip);
@@ -1693,7 +1693,7 @@ int pwm_lut_config(struct pwm_device *pwm, int period_us,
 
 	rc = _pwm_lut_config(chip, period_us, duty_pct, lut_params);
 
-	spin_unlock_irqrestore(&chip->lpg_lock, flags);
+	raw_spin_unlock_irqrestore(&chip->lpg_lock, flags);
 
 	if (rc)
 		pr_err("Failed to configure LUT\n");
@@ -2093,7 +2093,7 @@ static int qpnp_pwm_probe(struct spmi_device *spmi)
 		return -ENOMEM;
 	}
 
-	spin_lock_init(&pwm_chip->lpg_lock);
+	raw_spin_lock_init(&pwm_chip->lpg_lock);
 
 	pwm_chip->spmi_dev = spmi;
 	dev_set_drvdata(&spmi->dev, pwm_chip);
diff --git a/kernel/msm-3.18/drivers/regulator/kryo-regulator.c b/kernel/msm-3.18/drivers/regulator/kryo-regulator.c
index fe2a62db1..e50d9c359 100644
--- a/kernel/msm-3.18/drivers/regulator/kryo-regulator.c
+++ b/kernel/msm-3.18/drivers/regulator/kryo-regulator.c
@@ -83,7 +83,7 @@
 
 struct kryo_regulator {
 	struct list_head		link;
-	spinlock_t			slock;
+	raw_spinlock_t			slock;
 	struct regulator_desc		desc;
 	struct regulator_dev		*rdev;
 	struct regulator_dev		*retention_rdev;
@@ -292,7 +292,7 @@ static int kryo_regulator_enable(struct regulator_dev *rdev)
 	if (kvreg->vreg_en == true)
 		return 0;
 
-	spin_lock_irqsave(&kvreg->slock, flags);
+	raw_spin_lock_irqsave(&kvreg->slock, flags);
 	rc = kryo_set_ldo_volt(kvreg, kvreg->volt);
 	if (rc) {
 		kvreg_err(kvreg, "set voltage failed, rc=%d\n", rc);
@@ -303,7 +303,7 @@ static int kryo_regulator_enable(struct regulator_dev *rdev)
 	kvreg_debug(kvreg, "enabled\n");
 
 done:
-	spin_unlock_irqrestore(&kvreg->slock, flags);
+	raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 
 	return rc;
 }
@@ -317,10 +317,10 @@ static int kryo_regulator_disable(struct regulator_dev *rdev)
 	if (kvreg->vreg_en == false)
 		return 0;
 
-	spin_lock_irqsave(&kvreg->slock, flags);
+	raw_spin_lock_irqsave(&kvreg->slock, flags);
 	kvreg->vreg_en = false;
 	kvreg_debug(kvreg, "disabled\n");
-	spin_unlock_irqrestore(&kvreg->slock, flags);
+	raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 
 	return rc;
 }
@@ -339,11 +339,11 @@ static int kryo_regulator_set_voltage(struct regulator_dev *rdev,
 	int rc;
 	unsigned long flags;
 
-	spin_lock_irqsave(&kvreg->slock, flags);
+	raw_spin_lock_irqsave(&kvreg->slock, flags);
 
 	if (!kvreg->vreg_en) {
 		kvreg->volt = min_volt;
-		spin_unlock_irqrestore(&kvreg->slock, flags);
+		raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 		return 0;
 	}
 
@@ -351,7 +351,7 @@ static int kryo_regulator_set_voltage(struct regulator_dev *rdev,
 	if (rc)
 		kvreg_err(kvreg, "set voltage failed, rc=%d\n", rc);
 
-	spin_unlock_irqrestore(&kvreg->slock, flags);
+	raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 
 	return rc;
 }
@@ -370,7 +370,7 @@ static int kryo_regulator_set_bypass(struct regulator_dev *rdev,
 	int rc;
 	unsigned long flags;
 
-	spin_lock_irqsave(&kvreg->slock, flags);
+	raw_spin_lock_irqsave(&kvreg->slock, flags);
 
 	/*
 	 * LDO Vref voltage must be programmed before switching
@@ -384,7 +384,7 @@ static int kryo_regulator_set_bypass(struct regulator_dev *rdev,
 	if (rc)
 		kvreg_err(kvreg, "could not configure to %s mode\n",
 			  enable == LDO_MODE ? "LDO" : "BHS");
-	spin_unlock_irqrestore(&kvreg->slock, flags);
+	raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 
 	return rc;
 }
@@ -417,12 +417,12 @@ static int kryo_regulator_retention_set_voltage(struct regulator_dev *rdev,
 	int rc;
 	unsigned long flags;
 
-	spin_lock_irqsave(&kvreg->slock, flags);
+	raw_spin_lock_irqsave(&kvreg->slock, flags);
 	rc = kryo_set_retention_volt(kvreg, min_volt);
 	if (rc)
 		kvreg_err(kvreg, "set voltage failed, rc=%d\n", rc);
 
-	spin_unlock_irqrestore(&kvreg->slock, flags);
+	raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 
 	return rc;
 }
@@ -443,7 +443,7 @@ static int kryo_regulator_retention_set_bypass(struct regulator_dev *rdev,
 	u32 reg_val;
 	unsigned long flags;
 
-	spin_lock_irqsave(&kvreg->slock, flags);
+	raw_spin_lock_irqsave(&kvreg->slock, flags);
 
 	kryo_pm_apcc_masked_write(kvreg,
 				  APCC_PWR_CTL_OVERRIDE,
@@ -483,7 +483,7 @@ static int kryo_regulator_retention_set_bypass(struct regulator_dev *rdev,
 		: BHS_MODE;
 
 done:
-	spin_unlock_irqrestore(&kvreg->slock, flags);
+	raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 
 	return rc;
 }
@@ -575,7 +575,7 @@ static ssize_t kryo_dbg_mode_read(struct file *file, char __user *buff,
 		return -ENODEV;
 
 	/* Confirm HW state matches Kryo regulator device state */
-	spin_lock_irqsave(&kvreg->slock, flags);
+	raw_spin_lock_irqsave(&kvreg->slock, flags);
 	reg_val = readl_relaxed(kvreg->reg_base + APC_PWR_GATE_MODE);
 	if (((reg_val & PWR_GATE_SWITCH_MODE_MASK) == PWR_GATE_SWITCH_MODE_LDO
 	     && kvreg->mode != LDO_MODE) ||
@@ -589,7 +589,7 @@ static ssize_t kryo_dbg_mode_read(struct file *file, char __user *buff,
 			       kvreg->mode == LDO_MODE ?
 			       "LDO" : "BHS");
 	}
-	spin_unlock_irqrestore(&kvreg->slock, flags);
+	raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 
 	return simple_read_from_buffer(buff, count, ppos, buf, len);
 }
@@ -854,7 +854,7 @@ static int kryo_regulator_lpm_prepare(struct kryo_regulator *kvreg)
 	int vdd_volt_uv, bhs_volt, vdd_vlvl = 0;
 	unsigned long flags;
 
-	spin_lock_irqsave(&kvreg->slock, flags);
+	raw_spin_lock_irqsave(&kvreg->slock, flags);
 
 	kvreg->pre_lpm_state_mode = kvreg->mode;
 	kvreg->pre_lpm_state_volt = kvreg->volt;
@@ -865,7 +865,7 @@ static int kryo_regulator_lpm_prepare(struct kryo_regulator *kvreg)
 			if (vdd_vlvl < 0) {
 				kvreg_err(kvreg, "could not get vdd supply voltage level, rc=%d\n",
 					  vdd_vlvl);
-				spin_unlock_irqrestore(&kvreg->slock, flags);
+				raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 				return NOTIFY_BAD;
 			}
 
@@ -894,7 +894,7 @@ static int kryo_regulator_lpm_prepare(struct kryo_regulator *kvreg)
 	}
 
 	kvreg->lpm_enter_count++;
-	spin_unlock_irqrestore(&kvreg->slock, flags);
+	raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 
 	return NOTIFY_OK;
 }
@@ -903,7 +903,7 @@ static int kryo_regulator_lpm_resume(struct kryo_regulator *kvreg)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&kvreg->slock, flags);
+	raw_spin_lock_irqsave(&kvreg->slock, flags);
 
 	if (kvreg->mode == BHS_MODE &&
 	    kvreg->pre_lpm_state_mode == LDO_MODE) {
@@ -924,7 +924,7 @@ static int kryo_regulator_lpm_resume(struct kryo_regulator *kvreg)
 	}
 
 	kvreg->lpm_exit_count++;
-	spin_unlock_irqrestore(&kvreg->slock, flags);
+	raw_spin_unlock_irqrestore(&kvreg->slock, flags);
 
 	if (kvreg->lpm_exit_count != kvreg->lpm_enter_count) {
 		kvreg_err(kvreg, "LPM entry/exit counter mismatch, this is not expected: enter=%lx exit=%lx\n",
@@ -999,7 +999,7 @@ static int kryo_regulator_probe(struct platform_device *pdev)
 		return rc;
 	}
 
-	spin_lock_init(&kvreg->slock);
+	raw_spin_lock_init(&kvreg->slock);
 	kvreg->name		= init_data->constraints.name;
 	kvreg->desc.name	= kvreg->name;
 	kvreg->desc.n_voltages	= LDO_N_VOLTAGES;
diff --git a/kernel/msm-3.18/drivers/regulator/rpm-smd-regulator.c b/kernel/msm-3.18/drivers/regulator/rpm-smd-regulator.c
index 042884f3d..6f467bc98 100644
--- a/kernel/msm-3.18/drivers/regulator/rpm-smd-regulator.c
+++ b/kernel/msm-3.18/drivers/regulator/rpm-smd-regulator.c
@@ -184,7 +184,7 @@ struct rpm_vreg {
 	int			regulator_type;
 	int			hpm_min_load;
 	int			enable_time;
-	spinlock_t		slock;
+	raw_spinlock_t		slock;
 	struct mutex		mlock;
 	unsigned long		flags;
 	bool			sleep_request_sent;
@@ -251,7 +251,7 @@ static u32 rpm_vreg_string_to_int(const u8 *str)
 static inline void rpm_vreg_lock(struct rpm_vreg *rpm_vreg)
 {
 	if (rpm_vreg->allow_atomic)
-		spin_lock_irqsave(&rpm_vreg->slock, rpm_vreg->flags);
+		raw_spin_lock_irqsave(&rpm_vreg->slock, rpm_vreg->flags);
 	else
 		mutex_lock(&rpm_vreg->mlock);
 }
@@ -259,7 +259,7 @@ static inline void rpm_vreg_lock(struct rpm_vreg *rpm_vreg)
 static inline void rpm_vreg_unlock(struct rpm_vreg *rpm_vreg)
 {
 	if (rpm_vreg->allow_atomic)
-		spin_unlock_irqrestore(&rpm_vreg->slock, rpm_vreg->flags);
+		raw_spin_unlock_irqrestore(&rpm_vreg->slock, rpm_vreg->flags);
 	else
 		mutex_unlock(&rpm_vreg->mlock);
 }
@@ -1842,7 +1842,7 @@ static int rpm_vreg_resource_probe(struct platform_device *pdev)
 	INIT_LIST_HEAD(&rpm_vreg->reg_list);
 
 	if (rpm_vreg->allow_atomic)
-		spin_lock_init(&rpm_vreg->slock);
+		raw_spin_lock_init(&rpm_vreg->slock);
 	else
 		mutex_init(&rpm_vreg->mlock);
 
diff --git a/kernel/msm-3.18/drivers/rtc/qpnp-rtc.c b/kernel/msm-3.18/drivers/rtc/qpnp-rtc.c
index a9a0800d9..fc34e6ef8 100644
--- a/kernel/msm-3.18/drivers/rtc/qpnp-rtc.c
+++ b/kernel/msm-3.18/drivers/rtc/qpnp-rtc.c
@@ -63,7 +63,7 @@ struct qpnp_rtc {
 	struct device *rtc_dev;
 	struct rtc_device *rtc;
 	struct spmi_device *spmi;
-	spinlock_t alarm_ctrl_lock;
+	raw_spinlock_t alarm_ctrl_lock;
 };
 
 static int qpnp_read_wrapper(struct qpnp_rtc *rtc_dd, u8 *rtc_val,
@@ -115,7 +115,7 @@ qpnp_rtc_set_time(struct device *dev, struct rtc_time *tm)
 
 	dev_dbg(dev, "Seconds value to be written to RTC = %lu\n", secs);
 
-	spin_lock_irqsave(&rtc_dd->alarm_ctrl_lock, irq_flags);
+	raw_spin_lock_irqsave(&rtc_dd->alarm_ctrl_lock, irq_flags);
 	ctrl_reg = rtc_dd->alarm_ctrl_reg1;
 
 	if (ctrl_reg & BIT_RTC_ALARM_ENABLE) {
@@ -128,7 +128,7 @@ qpnp_rtc_set_time(struct device *dev, struct rtc_time *tm)
 			goto rtc_rw_fail;
 		}
 	} else
-		spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
+		raw_spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
 
 	/*
 	 * 32 bit seconds value is coverted to four 8 bit values
@@ -223,7 +223,7 @@ qpnp_rtc_set_time(struct device *dev, struct rtc_time *tm)
 
 rtc_rw_fail:
 	if (alarm_enabled)
-		spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
+		raw_spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
 
 	return rc;
 }
@@ -314,7 +314,7 @@ qpnp_rtc_set_alarm(struct device *dev, struct rtc_wkalrm *alarm)
 	value[2] = (secs >> 16) & 0xFF;
 	value[3] = (secs >> 24) & 0xFF;
 
-	spin_lock_irqsave(&rtc_dd->alarm_ctrl_lock, irq_flags);
+	raw_spin_lock_irqsave(&rtc_dd->alarm_ctrl_lock, irq_flags);
 
 	rc = qpnp_write_wrapper(rtc_dd, value,
 				rtc_dd->alarm_base + REG_OFFSET_ALARM_RW,
@@ -342,7 +342,7 @@ qpnp_rtc_set_alarm(struct device *dev, struct rtc_wkalrm *alarm)
 			alarm->time.tm_sec, alarm->time.tm_mday,
 			alarm->time.tm_mon, alarm->time.tm_year);
 rtc_rw_fail:
-	spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
+	raw_spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
 	return rc;
 }
 
@@ -398,7 +398,7 @@ qpnp_rtc_alarm_irq_enable(struct device *dev, unsigned int enabled)
 	u8 ctrl_reg;
 	u8 value[4] = {0};
 
-	spin_lock_irqsave(&rtc_dd->alarm_ctrl_lock, irq_flags);
+	raw_spin_lock_irqsave(&rtc_dd->alarm_ctrl_lock, irq_flags);
 	ctrl_reg = rtc_dd->alarm_ctrl_reg1;
 	ctrl_reg = enabled ? (ctrl_reg | BIT_RTC_ALARM_ENABLE) :
 				(ctrl_reg & ~BIT_RTC_ALARM_ENABLE);
@@ -422,7 +422,7 @@ qpnp_rtc_alarm_irq_enable(struct device *dev, unsigned int enabled)
 	}
 
 rtc_rw_fail:
-	spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
+	raw_spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
 	return rc;
 }
 
@@ -442,7 +442,7 @@ static irqreturn_t qpnp_alarm_trigger(int irq, void *dev_id)
 
 	rtc_update_irq(rtc_dd->rtc, 1, RTC_IRQF | RTC_AF);
 
-	spin_lock_irqsave(&rtc_dd->alarm_ctrl_lock, irq_flags);
+	raw_spin_lock_irqsave(&rtc_dd->alarm_ctrl_lock, irq_flags);
 
 	/* Clear the alarm enable bit */
 	ctrl_reg = rtc_dd->alarm_ctrl_reg1;
@@ -451,14 +451,14 @@ static irqreturn_t qpnp_alarm_trigger(int irq, void *dev_id)
 	rc = qpnp_write_wrapper(rtc_dd, &ctrl_reg,
 			rtc_dd->alarm_base + REG_OFFSET_ALARM_CTRL1, 1);
 	if (rc) {
-		spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
+		raw_spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
 		dev_err(rtc_dd->rtc_dev,
 				"Write to ALARM control reg failed\n");
 		goto rtc_alarm_handled;
 	}
 
 	rtc_dd->alarm_ctrl_reg1 = ctrl_reg;
-	spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
+	raw_spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
 
 	/* Set ALARM_CLR bit */
 	ctrl_reg = 0x1;
@@ -505,7 +505,7 @@ static int qpnp_rtc_probe(struct spmi_device *spmi)
 	}
 
 	/* Initialise spinlock to protect RTC control register */
-	spin_lock_init(&rtc_dd->alarm_ctrl_lock);
+	raw_spin_lock_init(&rtc_dd->alarm_ctrl_lock);
 
 	rtc_dd->rtc_dev = &(spmi->dev);
 	rtc_dd->spmi = spmi;
@@ -660,7 +660,7 @@ static void qpnp_rtc_shutdown(struct spmi_device *spmi)
 	}
 	rtc_alarm_powerup = rtc_dd->rtc_alarm_powerup;
 	if (!rtc_alarm_powerup && !poweron_alarm) {
-		spin_lock_irqsave(&rtc_dd->alarm_ctrl_lock, irq_flags);
+		raw_spin_lock_irqsave(&rtc_dd->alarm_ctrl_lock, irq_flags);
 		dev_dbg(&spmi->dev, "Disabling alarm interrupts\n");
 
 		/* Disable RTC alarms */
@@ -681,7 +681,7 @@ static void qpnp_rtc_shutdown(struct spmi_device *spmi)
 			dev_err(rtc_dd->rtc_dev, "SPMI write failed\n");
 
 fail_alarm_disable:
-		spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
+		raw_spin_unlock_irqrestore(&rtc_dd->alarm_ctrl_lock, irq_flags);
 	}
 }
 
diff --git a/kernel/msm-3.18/drivers/scsi/ufs/ufs-debugfs.c b/kernel/msm-3.18/drivers/scsi/ufs/ufs-debugfs.c
index 0547853c4..c49f0987b 100644
--- a/kernel/msm-3.18/drivers/scsi/ufs/ufs-debugfs.c
+++ b/kernel/msm-3.18/drivers/scsi/ufs/ufs-debugfs.c
@@ -494,7 +494,7 @@ static int ufsdbg_tag_stats_show(struct seq_file *file, void *data)
 
 	max_depth = hba->nutrs;
 
-	spin_lock_irqsave(hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(hba->host->host_lock, flags);
 	/* Header */
 	seq_printf(file, " Tag Stat\t\t%s Number of pending reqs upon issue (Q fullness)\n",
 		sep);
@@ -532,7 +532,7 @@ static int ufsdbg_tag_stats_show(struct seq_file *file, void *data)
 		}
 		seq_puts(file, "\n");
 	}
-	spin_unlock_irqrestore(hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 
 	if (is_tag_empty)
 		pr_debug("%s: All tags statistics are empty", __func__);
@@ -563,7 +563,7 @@ static ssize_t ufsdbg_tag_stats_write(struct file *filp,
 	}
 
 	ufs_stats = &hba->ufs_stats;
-	spin_lock_irqsave(hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(hba->host->host_lock, flags);
 
 	if (!val) {
 		ufs_stats->enabled = false;
@@ -583,7 +583,7 @@ static ssize_t ufsdbg_tag_stats_write(struct file *filp,
 		pr_debug("%s: Enabled UFS tag statistics", __func__);
 	}
 
-	spin_unlock_irqrestore(hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 	return cnt;
 }
 
@@ -676,7 +676,7 @@ static int ufsdbg_err_stats_show(struct seq_file *file, void *data)
 
 	err_stats = hba->ufs_stats.err_stats;
 
-	spin_lock_irqsave(hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(hba->host->host_lock, flags);
 
 	seq_puts(file, "\n==UFS errors that caused controller reset==\n");
 
@@ -734,7 +734,7 @@ static int ufsdbg_err_stats_show(struct seq_file *file, void *data)
 		seq_puts(file,
 		"so far, no other UFS related errors\n\n");
 
-	spin_unlock_irqrestore(hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 exit:
 	return 0;
 }
@@ -753,12 +753,12 @@ static ssize_t ufsdbg_err_stats_write(struct file *filp,
 	unsigned long flags;
 
 	ufs_stats = &hba->ufs_stats;
-	spin_lock_irqsave(hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(hba->host->host_lock, flags);
 
 	pr_debug("%s: Resetting UFS error statistics", __func__);
 	memset(ufs_stats->err_stats, 0, sizeof(hba->ufs_stats.err_stats));
 
-	spin_unlock_irqrestore(hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 	return cnt;
 }
 
@@ -1341,9 +1341,9 @@ static ssize_t ufsdbg_req_stats_write(struct file *filp,
 		return ret;
 	}
 
-	spin_lock_irqsave(hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(hba->host->host_lock, flags);
 	ufshcd_init_req_stats(hba);
-	spin_unlock_irqrestore(hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 
 	return cnt;
 }
@@ -1358,7 +1358,7 @@ static int ufsdbg_req_stats_show(struct seq_file *file, void *data)
 	seq_printf(file, "\t%-10s %-10s %-10s %-10s %-10s %-10s",
 		"All", "Write", "Read", "Read(urg)", "Write(urg)", "Flush");
 
-	spin_lock_irqsave(hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(hba->host->host_lock, flags);
 
 	seq_printf(file, "\n%s:\t", "Min");
 	for (i = 0; i < TS_NUM_STATS; i++)
@@ -1375,7 +1375,7 @@ static int ufsdbg_req_stats_show(struct seq_file *file, void *data)
 	for (i = 0; i < TS_NUM_STATS; i++)
 		seq_printf(file, "%-10llu ", hba->ufs_stats.req_stats[i].count);
 	seq_puts(file, "\n");
-	spin_unlock_irqrestore(hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 
 	return 0;
 }
@@ -1415,7 +1415,7 @@ static ssize_t ufsdbg_reset_controller_write(struct file *filp,
 	pm_runtime_get_sync(hba->dev);
 	ufshcd_hold(hba, false);
 
-	spin_lock_irqsave(hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(hba->host->host_lock, flags);
 	/*
 	 * simulating a dummy error in order to "convince"
 	 * eh_work to actually reset the controller
@@ -1423,7 +1423,7 @@ static ssize_t ufsdbg_reset_controller_write(struct file *filp,
 	hba->saved_err |= INT_FATAL_ERRORS;
 	hba->silence_err_logs = true;
 	schedule_work(&hba->eh_work);
-	spin_unlock_irqrestore(hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 
 	flush_work(&hba->eh_work);
 
diff --git a/kernel/msm-3.18/drivers/scsi/ufs/ufs-qcom-debugfs.c b/kernel/msm-3.18/drivers/scsi/ufs/ufs-qcom-debugfs.c
index 8532439c3..8cf146e80 100644
--- a/kernel/msm-3.18/drivers/scsi/ufs/ufs-qcom-debugfs.c
+++ b/kernel/msm-3.18/drivers/scsi/ufs/ufs-qcom-debugfs.c
@@ -225,7 +225,7 @@ static int ufs_qcom_dbg_pm_qos_show(struct seq_file *file, void *data)
 	unsigned long flags;
 	int i;
 
-	spin_lock_irqsave(host->hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(host->hba->host->host_lock, flags);
 
 	seq_printf(file, "enabled: %d\n", host->pm_qos.is_enabled);
 	for (i = 0; i < host->pm_qos.num_groups && host->pm_qos.groups; i++)
@@ -236,7 +236,7 @@ static int ufs_qcom_dbg_pm_qos_show(struct seq_file *file, void *data)
 			host->pm_qos.groups[i].state,
 			host->pm_qos.groups[i].latency_us);
 
-	spin_unlock_irqrestore(host->hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(host->hba->host->host_lock, flags);
 
 	return 0;
 }
diff --git a/kernel/msm-3.18/drivers/scsi/ufs/ufs-qcom.c b/kernel/msm-3.18/drivers/scsi/ufs/ufs-qcom.c
index 634e7ea98..5654d599f 100644
--- a/kernel/msm-3.18/drivers/scsi/ufs/ufs-qcom.c
+++ b/kernel/msm-3.18/drivers/scsi/ufs/ufs-qcom.c
@@ -1381,7 +1381,7 @@ static void ufs_qcom_pm_qos_req_start(struct ufs_hba *hba, struct request *req)
 
 	group = &host->pm_qos.groups[ufs_qcom_cpu_to_group(host, req->cpu)];
 
-	spin_lock_irqsave(hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(hba->host->host_lock, flags);
 	if (!host->pm_qos.is_enabled)
 		goto out;
 
@@ -1392,7 +1392,7 @@ static void ufs_qcom_pm_qos_req_start(struct ufs_hba *hba, struct request *req)
 		queue_work(host->pm_qos.workq, &group->vote_work);
 	}
 out:
-	spin_unlock_irqrestore(hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 }
 
 /* hba->host->host_lock is assumed to be held by caller */
@@ -1420,10 +1420,10 @@ static void ufs_qcom_pm_qos_req_end(struct ufs_hba *hba, struct request *req,
 		return;
 
 	if (should_lock)
-		spin_lock_irqsave(hba->host->host_lock, flags);
+		raw_spin_lock_irqsave(hba->host->host_lock, flags);
 	__ufs_qcom_pm_qos_req_end(ufshcd_get_variant(hba), req->cpu);
 	if (should_lock)
-		spin_unlock_irqrestore(hba->host->host_lock, flags);
+		raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 }
 
 static void ufs_qcom_pm_qos_vote_work(struct work_struct *work)
@@ -1433,15 +1433,15 @@ static void ufs_qcom_pm_qos_vote_work(struct work_struct *work)
 	struct ufs_qcom_host *host = group->host;
 	unsigned long flags;
 
-	spin_lock_irqsave(host->hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(host->hba->host->host_lock, flags);
 
 	if (!host->pm_qos.is_enabled || !group->active_reqs) {
-		spin_unlock_irqrestore(host->hba->host->host_lock, flags);
+		raw_spin_unlock_irqrestore(host->hba->host->host_lock, flags);
 		return;
 	}
 
 	group->state = PM_QOS_VOTED;
-	spin_unlock_irqrestore(host->hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(host->hba->host->host_lock, flags);
 
 	pm_qos_update_request(&group->req, group->latency_us);
 }
@@ -1457,15 +1457,15 @@ static void ufs_qcom_pm_qos_unvote_work(struct work_struct *work)
 	 * Check if new requests were submitted in the meantime and do not
 	 * unvote if so.
 	 */
-	spin_lock_irqsave(host->hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(host->hba->host->host_lock, flags);
 
 	if (!host->pm_qos.is_enabled || group->active_reqs) {
-		spin_unlock_irqrestore(host->hba->host->host_lock, flags);
+		raw_spin_unlock_irqrestore(host->hba->host->host_lock, flags);
 		return;
 	}
 
 	group->state = PM_QOS_UNVOTED;
-	spin_unlock_irqrestore(host->hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(host->hba->host->host_lock, flags);
 
 	pm_qos_update_request_timeout(&group->req,
 		group->latency_us, UFS_QCOM_PM_QOS_UNVOTE_TIMEOUT_US);
@@ -1499,22 +1499,22 @@ static ssize_t ufs_qcom_pm_qos_enable_store(struct device *dev,
 	 * Must take the spinlock and save irqs before changing the enabled
 	 * flag in order to keep correctness of PM QoS release.
 	 */
-	spin_lock_irqsave(hba->host->host_lock, flags);
+	raw_spin_lock_irqsave(hba->host->host_lock, flags);
 	if (enable == host->pm_qos.is_enabled) {
-		spin_unlock_irqrestore(hba->host->host_lock, flags);
+		raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 		return count;
 	}
 	host->pm_qos.is_enabled = enable;
-	spin_unlock_irqrestore(hba->host->host_lock, flags);
+	raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 
 	if (!enable)
 		for (i = 0; i < host->pm_qos.num_groups; i++) {
 			cancel_work_sync(&host->pm_qos.groups[i].vote_work);
 			cancel_work_sync(&host->pm_qos.groups[i].unvote_work);
-			spin_lock_irqsave(hba->host->host_lock, flags);
+			raw_spin_lock_irqsave(hba->host->host_lock, flags);
 			host->pm_qos.groups[i].state = PM_QOS_UNVOTED;
 			host->pm_qos.groups[i].active_reqs = 0;
-			spin_unlock_irqrestore(hba->host->host_lock, flags);
+			raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 			pm_qos_update_request(&host->pm_qos.groups[i].req,
 				PM_QOS_DEFAULT_VALUE);
 		}
@@ -1574,9 +1574,9 @@ static ssize_t ufs_qcom_pm_qos_latency_store(struct device *dev,
 		if (ret)
 			break;
 
-		spin_lock_irqsave(hba->host->host_lock, flags);
+		raw_spin_lock_irqsave(hba->host->host_lock, flags);
 		host->pm_qos.groups[i].latency_us = value;
-		spin_unlock_irqrestore(hba->host->host_lock, flags);
+		raw_spin_unlock_irqrestore(hba->host->host_lock, flags);
 	}
 
 	kfree(strbuf_copy);
diff --git a/kernel/msm-3.18/drivers/scsi/ufs/ufs_test.c b/kernel/msm-3.18/drivers/scsi/ufs/ufs_test.c
index 8953722e8..c53717320 100644
--- a/kernel/msm-3.18/drivers/scsi/ufs/ufs_test.c
+++ b/kernel/msm-3.18/drivers/scsi/ufs/ufs_test.c
@@ -683,11 +683,11 @@ static void scenario_free_end_io_fn(struct request *rq, int err)
 	test_rq = (struct test_request *)rq->elv.priv[0];
 	BUG_ON(!test_rq);
 
-	spin_lock_irqsave(&test_iosched->lock, flags);
+	raw_spin_lock_irqsave(&test_iosched->lock, flags);
 	test_iosched->dispatched_count--;
 	list_del_init(&test_rq->queuelist);
 	__blk_put_request(test_iosched->req_q, test_rq->rq);
-	spin_unlock_irqrestore(&test_iosched->lock, flags);
+	raw_spin_unlock_irqrestore(&test_iosched->lock, flags);
 
 	test_iosched_free_test_req_data_buffer(test_rq);
 	kfree(test_rq);
@@ -968,11 +968,11 @@ static void long_test_free_end_io_fn(struct request *rq, int err)
 
 	BUG_ON(!test_rq);
 
-	spin_lock_irqsave(&test_iosched->lock, flags);
+	raw_spin_lock_irqsave(&test_iosched->lock, flags);
 	test_iosched->dispatched_count--;
 	list_del_init(&test_rq->queuelist);
 	__blk_put_request(test_iosched->req_q, test_rq->rq);
-	spin_unlock_irqrestore(&test_iosched->lock, flags);
+	raw_spin_unlock_irqrestore(&test_iosched->lock, flags);
 
 	if (utd->test_stage == UFS_TEST_LONG_SEQUENTIAL_MIXED_STAGE2 &&
 			rq_data_dir(rq) == READ &&
diff --git a/kernel/msm-3.18/drivers/soc/qcom/bam_dmux.c b/kernel/msm-3.18/drivers/soc/qcom/bam_dmux.c
index a52feefad..e64a63f07 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/bam_dmux.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/bam_dmux.c
@@ -160,7 +160,7 @@ struct bam_ch_info {
 	uint32_t status;
 	void (*notify)(void *, int, unsigned long);
 	void *priv;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct platform_device *pdev;
 	char name[BAM_DMUX_CH_NAME_MAX_LEN];
 	int num_tx_pkts;
@@ -210,7 +210,7 @@ static LIST_HEAD(bam_rx_pool);
 static DEFINE_MUTEX(bam_rx_pool_mutexlock);
 static int bam_rx_pool_len;
 static LIST_HEAD(bam_tx_pool);
-static DEFINE_SPINLOCK(bam_tx_pool_spinlock);
+static DEFINE_RAW_SPINLOCK(bam_tx_pool_spinlock);
 static DEFINE_MUTEX(bam_pdev_mutexlock);
 
 static void notify_all(int event, unsigned long data);
@@ -263,7 +263,7 @@ static DEFINE_MUTEX(dfab_status_lock);
 static int dfab_is_on;
 static int wait_for_dfab;
 static struct completion dfab_unvote_completion;
-static DEFINE_SPINLOCK(wakelock_reference_lock);
+static DEFINE_RAW_SPINLOCK(wakelock_reference_lock);
 static int wakelock_reference_count;
 static int a2_pc_disabled_wakelock_skipped;
 static LIST_HEAD(bam_other_notify_funcs);
@@ -367,7 +367,7 @@ static inline void verify_tx_queue_is_empty(const char *func)
 	struct tx_pkt_info *info;
 	int reported = 0;
 
-	spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
+	raw_spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
 	list_for_each_entry(info, &bam_tx_pool, list_node) {
 		if (!reported) {
 			BAM_DMUX_LOG("%s: tx pool not empty\n", func);
@@ -381,7 +381,7 @@ static inline void verify_tx_queue_is_empty(const char *func)
 			pr_err("%s: node=%p ts=%u.%09lu\n", __func__,
 			&info->list_node, info->ts_sec, info->ts_nsec);
 	}
-	spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
+	raw_spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
 }
 
 static void __queue_rx(gfp_t alloc_flags)
@@ -542,12 +542,12 @@ static void bam_mux_process_data(struct sk_buff *rx_skb)
 	notify = NULL;
 	priv = NULL;
 
-	spin_lock_irqsave(&bam_ch[ch_id].lock, flags);
+	raw_spin_lock_irqsave(&bam_ch[ch_id].lock, flags);
 	if (bam_ch[ch_id].notify) {
 		notify = bam_ch[ch_id].notify;
 		priv = bam_ch[ch_id].priv;
 	}
-	spin_unlock_irqrestore(&bam_ch[ch_id].lock, flags);
+	raw_spin_unlock_irqrestore(&bam_ch[ch_id].lock, flags);
 	if (notify)
 		notify(priv, BAM_DMUX_RECEIVE, event_data);
 	else
@@ -634,7 +634,7 @@ static inline void handle_bam_mux_cmd_open(struct bam_mux_hdr *rx_hdr)
 	} else {
 		set_ul_mtu(0, false);
 	}
-	spin_lock_irqsave(&bam_ch[rx_hdr->ch_id].lock, flags);
+	raw_spin_lock_irqsave(&bam_ch[rx_hdr->ch_id].lock, flags);
 	if (bam_ch_is_remote_open(rx_hdr->ch_id)) {
 		/*
 		 * Receiving an open command for a channel that is already open
@@ -647,7 +647,7 @@ static inline void handle_bam_mux_cmd_open(struct bam_mux_hdr *rx_hdr)
 	}
 	bam_ch[rx_hdr->ch_id].status |= BAM_CH_REMOTE_OPEN;
 	bam_ch[rx_hdr->ch_id].num_tx_pkts = 0;
-	spin_unlock_irqrestore(&bam_ch[rx_hdr->ch_id].lock, flags);
+	raw_spin_unlock_irqrestore(&bam_ch[rx_hdr->ch_id].lock, flags);
 	ret = platform_device_add(bam_ch[rx_hdr->ch_id].pdev);
 	if (ret)
 		pr_err("%s: platform_device_add() error: %d\n",
@@ -735,9 +735,9 @@ static void handle_bam_mux_cmd(struct work_struct *work)
 			mutex_unlock(&bam_pdev_mutexlock);
 			break;
 		}
-		spin_lock_irqsave(&bam_ch[rx_hdr->ch_id].lock, flags);
+		raw_spin_lock_irqsave(&bam_ch[rx_hdr->ch_id].lock, flags);
 		bam_ch[rx_hdr->ch_id].status &= ~BAM_CH_REMOTE_OPEN;
-		spin_unlock_irqrestore(&bam_ch[rx_hdr->ch_id].lock, flags);
+		raw_spin_unlock_irqrestore(&bam_ch[rx_hdr->ch_id].lock, flags);
 		platform_device_unregister(bam_ch[rx_hdr->ch_id].pdev);
 		bam_ch[rx_hdr->ch_id].pdev =
 			platform_device_alloc(bam_ch[rx_hdr->ch_id].name, 2);
@@ -787,7 +787,7 @@ static int bam_mux_write_cmd(void *data, uint32_t len)
 	pkt->is_cmd = 1;
 	set_tx_timestamp(pkt);
 	INIT_WORK(&pkt->work, bam_mux_write_done);
-	spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
+	raw_spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
 	list_add_tail(&pkt->list_node, &bam_tx_pool);
 	rc = bam_ops->sps_transfer_one_ptr(bam_tx_pipe, dma_address, len,
 				pkt, SPS_IOVEC_FLAG_EOT);
@@ -796,13 +796,13 @@ static int bam_mux_write_cmd(void *data, uint32_t len)
 			__func__, rc);
 		list_del(&pkt->list_node);
 		DBG_INC_TX_SPS_FAILURE_CNT();
-		spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
+		raw_spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
 		dma_unmap_single(dma_dev, pkt->dma_address,
 					pkt->len,
 					bam_ops->dma_to);
 		kfree(pkt);
 	} else {
-		spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
+		raw_spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
 	}
 
 	ul_packet_written = 1;
@@ -823,7 +823,7 @@ static void bam_mux_write_done(struct work_struct *work)
 
 	info = container_of(work, struct tx_pkt_info, work);
 
-	spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
+	raw_spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
 	info_expected = list_first_entry(&bam_tx_pool,
 			struct tx_pkt_info, list_node);
 	if (unlikely(info != info_expected)) {
@@ -841,11 +841,11 @@ static void bam_mux_write_done(struct work_struct *work)
 			errant_pkt->ts_nsec);
 
 		}
-		spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
+		raw_spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
 		BUG();
 	}
 	list_del(&info->list_node);
-	spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
+	raw_spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
 
 	if (info->is_cmd) {
 		kfree(info->skb);
@@ -862,9 +862,9 @@ static void bam_mux_write_done(struct work_struct *work)
 		skb_trim(skb, skb->len - hdr->pad_len);
 
 	event_data = (unsigned long)(skb);
-	spin_lock_irqsave(&bam_ch[hdr->ch_id].lock, flags);
+	raw_spin_lock_irqsave(&bam_ch[hdr->ch_id].lock, flags);
 	bam_ch[hdr->ch_id].num_tx_pkts--;
-	spin_unlock_irqrestore(&bam_ch[hdr->ch_id].lock, flags);
+	raw_spin_unlock_irqrestore(&bam_ch[hdr->ch_id].lock, flags);
 	if (bam_ch[hdr->ch_id].notify)
 		bam_ch[hdr->ch_id].notify(
 			bam_ch[hdr->ch_id].priv, BAM_DMUX_WRITE_DONE,
@@ -898,9 +898,9 @@ int msm_bam_dmux_write(uint32_t id, struct sk_buff *skb)
 	}
 
 	DBG("%s: writing to ch %d len %d\n", __func__, id, skb->len);
-	spin_lock_irqsave(&bam_ch[id].lock, flags);
+	raw_spin_lock_irqsave(&bam_ch[id].lock, flags);
 	if (!bam_ch_is_open(id)) {
-		spin_unlock_irqrestore(&bam_ch[id].lock, flags);
+		raw_spin_unlock_irqrestore(&bam_ch[id].lock, flags);
 		pr_err("%s: port not open: %d\n", __func__, bam_ch[id].status);
 		srcu_read_unlock(&bam_dmux_srcu, rcu_id);
 		return -ENODEV;
@@ -908,12 +908,12 @@ int msm_bam_dmux_write(uint32_t id, struct sk_buff *skb)
 
 	if (bam_ch[id].use_wm &&
 	    (bam_ch[id].num_tx_pkts >= HIGH_WATERMARK)) {
-		spin_unlock_irqrestore(&bam_ch[id].lock, flags);
+		raw_spin_unlock_irqrestore(&bam_ch[id].lock, flags);
 		pr_err("%s: watermark exceeded: %d\n", __func__, id);
 		srcu_read_unlock(&bam_dmux_srcu, rcu_id);
 		return -EAGAIN;
 	}
-	spin_unlock_irqrestore(&bam_ch[id].lock, flags);
+	raw_spin_unlock_irqrestore(&bam_ch[id].lock, flags);
 
 	read_lock(&ul_wakeup_lock);
 	if (!bam_is_connected) {
@@ -980,7 +980,7 @@ int msm_bam_dmux_write(uint32_t id, struct sk_buff *skb)
 	pkt->is_cmd = 0;
 	set_tx_timestamp(pkt);
 	INIT_WORK(&pkt->work, bam_mux_write_done);
-	spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
+	raw_spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
 	list_add_tail(&pkt->list_node, &bam_tx_pool);
 	rc = bam_ops->sps_transfer_one_ptr(bam_tx_pipe, dma_address, skb->len,
 				pkt, SPS_IOVEC_FLAG_EOT);
@@ -989,17 +989,17 @@ int msm_bam_dmux_write(uint32_t id, struct sk_buff *skb)
 			__func__, rc);
 		list_del(&pkt->list_node);
 		DBG_INC_TX_SPS_FAILURE_CNT();
-		spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
+		raw_spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
 		dma_unmap_single(dma_dev, pkt->dma_address,
 					pkt->skb->len,	bam_ops->dma_to);
 		kfree(pkt);
 		if (new_skb)
 			dev_kfree_skb_any(new_skb);
 	} else {
-		spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
-		spin_lock_irqsave(&bam_ch[id].lock, flags);
+		raw_spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
+		raw_spin_lock_irqsave(&bam_ch[id].lock, flags);
 		bam_ch[id].num_tx_pkts++;
-		spin_unlock_irqrestore(&bam_ch[id].lock, flags);
+		raw_spin_unlock_irqrestore(&bam_ch[id].lock, flags);
 	}
 	ul_packet_written = 1;
 	read_unlock(&ul_wakeup_lock);
@@ -1102,16 +1102,16 @@ int msm_bam_dmux_open(uint32_t id, void *priv,
 		pr_err("%s: hdr kmalloc failed. ch: %d\n", __func__, id);
 		return -ENOMEM;
 	}
-	spin_lock_irqsave(&bam_ch[id].lock, flags);
+	raw_spin_lock_irqsave(&bam_ch[id].lock, flags);
 	if (bam_ch_is_open(id)) {
 		DBG("%s: Already opened %d\n", __func__, id);
-		spin_unlock_irqrestore(&bam_ch[id].lock, flags);
+		raw_spin_unlock_irqrestore(&bam_ch[id].lock, flags);
 		kfree(hdr);
 		goto open_done;
 	}
 	if (!bam_ch_is_remote_open(id)) {
 		DBG("%s: Remote not open; ch: %d\n", __func__, id);
-		spin_unlock_irqrestore(&bam_ch[id].lock, flags);
+		raw_spin_unlock_irqrestore(&bam_ch[id].lock, flags);
 		kfree(hdr);
 		return -ENODEV;
 	}
@@ -1121,7 +1121,7 @@ int msm_bam_dmux_open(uint32_t id, void *priv,
 	bam_ch[id].status |= BAM_CH_LOCAL_OPEN;
 	bam_ch[id].num_tx_pkts = 0;
 	bam_ch[id].use_wm = 0;
-	spin_unlock_irqrestore(&bam_ch[id].lock, flags);
+	raw_spin_unlock_irqrestore(&bam_ch[id].lock, flags);
 
 	notify(priv, BAM_DMUX_TRANSMIT_SIZE, ul_mtu);
 
@@ -1181,11 +1181,11 @@ int msm_bam_dmux_close(uint32_t id)
 		atomic_dec(&ul_ondemand_vote);
 	}
 
-	spin_lock_irqsave(&bam_ch[id].lock, flags);
+	raw_spin_lock_irqsave(&bam_ch[id].lock, flags);
 	bam_ch[id].notify = NULL;
 	bam_ch[id].priv = NULL;
 	bam_ch[id].status &= ~BAM_CH_LOCAL_OPEN;
-	spin_unlock_irqrestore(&bam_ch[id].lock, flags);
+	raw_spin_unlock_irqrestore(&bam_ch[id].lock, flags);
 
 	if (bam_ch_is_in_reset(id)) {
 		read_unlock(&ul_wakeup_lock);
@@ -1221,7 +1221,7 @@ int msm_bam_dmux_is_ch_full(uint32_t id)
 	if (id >= BAM_DMUX_NUM_CHANNELS)
 		return -EINVAL;
 
-	spin_lock_irqsave(&bam_ch[id].lock, flags);
+	raw_spin_lock_irqsave(&bam_ch[id].lock, flags);
 	bam_ch[id].use_wm = 1;
 	ret = bam_ch[id].num_tx_pkts >= HIGH_WATERMARK;
 	DBG("%s: ch %d num tx pkts=%d, HWM=%d\n", __func__,
@@ -1230,7 +1230,7 @@ int msm_bam_dmux_is_ch_full(uint32_t id)
 		ret = -ENODEV;
 		pr_err("%s: port not open: %d\n", __func__, bam_ch[id].status);
 	}
-	spin_unlock_irqrestore(&bam_ch[id].lock, flags);
+	raw_spin_unlock_irqrestore(&bam_ch[id].lock, flags);
 
 	return ret;
 }
@@ -1243,7 +1243,7 @@ int msm_bam_dmux_is_ch_low(uint32_t id)
 	if (id >= BAM_DMUX_NUM_CHANNELS)
 		return -EINVAL;
 
-	spin_lock_irqsave(&bam_ch[id].lock, flags);
+	raw_spin_lock_irqsave(&bam_ch[id].lock, flags);
 	bam_ch[id].use_wm = 1;
 	ret = bam_ch[id].num_tx_pkts <= LOW_WATERMARK;
 	DBG("%s: ch %d num tx pkts=%d, LWM=%d\n", __func__,
@@ -1252,7 +1252,7 @@ int msm_bam_dmux_is_ch_low(uint32_t id)
 		ret = -ENODEV;
 		pr_err("%s: port not open: %d\n", __func__, bam_ch[id].status);
 	}
-	spin_unlock_irqrestore(&bam_ch[id].lock, flags);
+	raw_spin_unlock_irqrestore(&bam_ch[id].lock, flags);
 
 	return ret;
 }
@@ -1573,11 +1573,11 @@ static int debug_ul_pkt_cnt(char *buf, int max)
 	unsigned long flags;
 	int n = 0;
 
-	spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
+	raw_spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
 	list_for_each(p, &bam_tx_pool) {
 		++n;
 	}
-	spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
+	raw_spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
 
 	return scnprintf(buf, max, "Number of UL packets in flight: %d\n", n);
 }
@@ -1664,12 +1664,12 @@ static void notify_all(int event, unsigned long data)
 	for (i = 0; i < BAM_DMUX_NUM_CHANNELS; ++i) {
 		notify = NULL;
 		priv = NULL;
-		spin_lock_irqsave(&bam_ch[i].lock, flags);
+		raw_spin_lock_irqsave(&bam_ch[i].lock, flags);
 		if (bam_ch_is_open(i)) {
 			notify = bam_ch[i].notify;
 			priv = bam_ch[i].priv;
 		}
-		spin_unlock_irqrestore(&bam_ch[i].lock, flags);
+		raw_spin_unlock_irqrestore(&bam_ch[i].lock, flags);
 		if (notify)
 			notify(priv, event, data);
 	}
@@ -1833,7 +1833,7 @@ static void ul_timeout(struct work_struct *work)
 	}
 	if (bam_is_connected) {
 		if (!ul_packet_written) {
-			spin_lock(&bam_tx_pool_spinlock);
+			raw_spin_lock(&bam_tx_pool_spinlock);
 			if (!list_empty(&bam_tx_pool)) {
 				struct tx_pkt_info *info;
 
@@ -1844,7 +1844,7 @@ static void ul_timeout(struct work_struct *work)
 				DBG_INC_TX_STALL_CNT();
 				ul_packet_written = 1;
 			}
-			spin_unlock(&bam_tx_pool_spinlock);
+			raw_spin_unlock(&bam_tx_pool_spinlock);
 		}
 
 		if (ul_packet_written || atomic_read(&ul_ondemand_vote)) {
@@ -2156,24 +2156,24 @@ static void grab_wakelock(void)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&wakelock_reference_lock, flags);
+	raw_spin_lock_irqsave(&wakelock_reference_lock, flags);
 	BAM_DMUX_LOG("%s: ref count = %d\n", __func__,
 						wakelock_reference_count);
 	if (wakelock_reference_count == 0)
 		__pm_stay_awake(&bam_wakelock);
 	++wakelock_reference_count;
-	spin_unlock_irqrestore(&wakelock_reference_lock, flags);
+	raw_spin_unlock_irqrestore(&wakelock_reference_lock, flags);
 }
 
 static void release_wakelock(void)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&wakelock_reference_lock, flags);
+	raw_spin_lock_irqsave(&wakelock_reference_lock, flags);
 	if (wakelock_reference_count == 0) {
 		DMUX_LOG_KERR("%s: bam_dmux wakelock not locked\n", __func__);
 		dump_stack();
-		spin_unlock_irqrestore(&wakelock_reference_lock, flags);
+		raw_spin_unlock_irqrestore(&wakelock_reference_lock, flags);
 		return;
 	}
 	BAM_DMUX_LOG("%s: ref count = %d\n", __func__,
@@ -2181,7 +2181,7 @@ static void release_wakelock(void)
 	--wakelock_reference_count;
 	if (wakelock_reference_count == 0)
 		__pm_relax(&bam_wakelock);
-	spin_unlock_irqrestore(&wakelock_reference_lock, flags);
+	raw_spin_unlock_irqrestore(&wakelock_reference_lock, flags);
 }
 
 static int restart_notifier_cb(struct notifier_block *this,
@@ -2259,7 +2259,7 @@ static int restart_notifier_cb(struct notifier_block *this,
 	mutex_unlock(&bam_pdev_mutexlock);
 
 	/* Cleanup pending UL data */
-	spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
+	raw_spin_lock_irqsave(&bam_tx_pool_spinlock, flags);
 	while (!list_empty(&bam_tx_pool)) {
 		node = bam_tx_pool.next;
 		list_del(node);
@@ -2278,7 +2278,7 @@ static int restart_notifier_cb(struct notifier_block *this,
 		}
 		kfree(info);
 	}
-	spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
+	raw_spin_unlock_irqrestore(&bam_tx_pool_spinlock, flags);
 
 	BAM_DMUX_LOG("%s: complete\n", __func__);
 	return NOTIFY_DONE;
@@ -2763,7 +2763,7 @@ static int bam_dmux_probe(struct platform_device *pdev)
 	}
 
 	for (rc = 0; rc < BAM_DMUX_NUM_CHANNELS; ++rc) {
-		spin_lock_init(&bam_ch[rc].lock);
+		raw_spin_lock_init(&bam_ch[rc].lock);
 		scnprintf(bam_ch[rc].name, BAM_DMUX_CH_NAME_MAX_LEN,
 					"bam_dmux_ch_%d", rc);
 		/* bus 2, ie a2 stream 2 */
diff --git a/kernel/msm-3.18/drivers/soc/qcom/cache_m4m_erp64.c b/kernel/msm-3.18/drivers/soc/qcom/cache_m4m_erp64.c
index b9a7d98d4..2a77f8fa2 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/cache_m4m_erp64.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/cache_m4m_erp64.c
@@ -242,7 +242,7 @@ static void msm_l2_erp_local_handler(void *force)
 	bool parity_ue, parity_ce, misc_ue;
 	int cpu;
 
-	spin_lock_irqsave(&local_handler_lock, flags);
+	raw_spin_lock_irqsave(&local_handler_lock, flags);
 
 	esr0 = get_l2_indirect_reg(L2ESR0_IA);
 	esr1 = get_l2_indirect_reg(L2ESR1_IA);
@@ -286,7 +286,7 @@ static void msm_l2_erp_local_handler(void *force)
 	else
 		WARN_ON(parity_ce);
 
-	spin_unlock_irqrestore(&local_handler_lock, flags);
+	raw_spin_unlock_irqrestore(&local_handler_lock, flags);
 }
 
 static irqreturn_t msm_l2_erp_irq(int irq, void *dev_id)
diff --git a/kernel/msm-3.18/drivers/soc/qcom/event_timer.c b/kernel/msm-3.18/drivers/soc/qcom/event_timer.c
index 374fa56b0..5fc93ffb2 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/event_timer.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/event_timer.c
@@ -202,7 +202,7 @@ static enum hrtimer_restart event_hrtimer_cb(struct hrtimer *hrtimer)
 	unsigned long flags;
 	int cpu;
 
-	spin_lock_irqsave(&event_timer_lock, flags);
+	raw_spin_lock_irqsave(&event_timer_lock, flags);
 	cpu = smp_processor_id();
 	next = timerqueue_getnext(&per_cpu(timer_head, cpu));
 
@@ -231,7 +231,7 @@ static enum hrtimer_restart event_hrtimer_cb(struct hrtimer *hrtimer)
 		create_hrtimer(event);
 	}
 hrtimer_cb_exit:
-	spin_unlock_irqrestore(&event_timer_lock, flags);
+	raw_spin_unlock_irqrestore(&event_timer_lock, flags);
 	return HRTIMER_NORESTART;
 }
 
@@ -245,7 +245,7 @@ static void create_timer_smp(void *data)
 		(struct event_timer_info *)data;
 	struct timerqueue_node *next;
 
-	spin_lock_irqsave(&event_timer_lock, flags);
+	raw_spin_lock_irqsave(&event_timer_lock, flags);
 
 	if (is_event_active(event))
 		timerqueue_del(&per_cpu(timer_head, event->cpu), &event->node);
@@ -267,7 +267,7 @@ static void create_timer_smp(void *data)
 
 		create_hrtimer(event);
 	}
-	spin_unlock_irqrestore(&event_timer_lock, flags);
+	raw_spin_unlock_irqrestore(&event_timer_lock, flags);
 }
 
 /**
@@ -323,7 +323,7 @@ static void irq_affinity_change_notifier(struct irq_affinity_notify *notify,
 	if (old_cpu == new_cpu)
 		return;
 
-	spin_lock_irqsave(&event_timer_lock, flags);
+	raw_spin_lock_irqsave(&event_timer_lock, flags);
 
 	/* If the event is not active OR
 	 * If it is the next event
@@ -335,7 +335,7 @@ static void irq_affinity_change_notifier(struct irq_affinity_notify *notify,
 		(hrtimer_try_to_cancel(&per_cpu(per_cpu_hrtimer.
 				event_hrtimer, old_cpu)) < 0))) {
 		event->cpu = new_cpu;
-		spin_unlock_irqrestore(&event_timer_lock, flags);
+		raw_spin_unlock_irqrestore(&event_timer_lock, flags);
 		if (msm_event_debug_mask && MSM_EVENT_TIMER_DEBUG)
 			pr_debug("Event:%p is not active or in callback\n",
 					event);
@@ -357,7 +357,7 @@ static void irq_affinity_change_notifier(struct irq_affinity_notify *notify,
 	if (msm_event_debug_mask && MSM_EVENT_TIMER_DEBUG)
 		pr_debug("Event:%p is in the list\n", event);
 
-	spin_unlock_irqrestore(&event_timer_lock, flags);
+	raw_spin_unlock_irqrestore(&event_timer_lock, flags);
 
 	/*
 	 * Migrating event timer to a new CPU is automatically
@@ -387,7 +387,7 @@ static void irq_affinity_change_notifier(struct irq_affinity_notify *notify,
 	 * Here after moving the E1 to C1. Need to start
 	 * E2 on C0.
 	 */
-	spin_lock(&event_setup_lock);
+	raw_spin_lock(&event_setup_lock);
 	/* Setup event timer on new cpu*/
 	setup_event_hrtimer(event);
 
@@ -402,7 +402,7 @@ static void irq_affinity_change_notifier(struct irq_affinity_notify *notify,
 			setup_event_hrtimer(event);
 		}
 	}
-	spin_unlock(&event_setup_lock);
+	raw_spin_unlock(&event_setup_lock);
 }
 
 /**
@@ -422,11 +422,11 @@ void activate_event_timer(struct event_timer_info *event, ktime_t event_time)
 				(unsigned long)ktime_to_us(event_time),
 				event->cpu);
 
-	spin_lock(&event_setup_lock);
+	raw_spin_lock(&event_setup_lock);
 	event->node.expires = event_time;
 	/* Start hrtimer and add event to rb tree */
 	setup_event_hrtimer(event);
-	spin_unlock(&event_setup_lock);
+	raw_spin_unlock(&event_setup_lock);
 }
 EXPORT_SYMBOL(activate_event_timer);
 
@@ -442,7 +442,7 @@ void deactivate_event_timer(struct event_timer_info *event)
 	if (msm_event_debug_mask && MSM_EVENT_TIMER_DEBUG)
 		pr_debug("Deactivate timer\n");
 
-	spin_lock_irqsave(&event_timer_lock, flags);
+	raw_spin_lock_irqsave(&event_timer_lock, flags);
 	if (is_event_active(event)) {
 		if (is_event_next(event))
 			hrtimer_try_to_cancel(&per_cpu(
@@ -450,7 +450,7 @@ void deactivate_event_timer(struct event_timer_info *event)
 
 		timerqueue_del(&per_cpu(timer_head, event->cpu), &event->node);
 	}
-	spin_unlock_irqrestore(&event_timer_lock, flags);
+	raw_spin_unlock_irqrestore(&event_timer_lock, flags);
 }
 
 /**
@@ -462,7 +462,7 @@ void destroy_event_timer(struct event_timer_info *event)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&event_timer_lock, flags);
+	raw_spin_lock_irqsave(&event_timer_lock, flags);
 	if (is_event_active(event)) {
 		if (is_event_next(event))
 			hrtimer_try_to_cancel(&per_cpu(
@@ -470,7 +470,7 @@ void destroy_event_timer(struct event_timer_info *event)
 
 		timerqueue_del(&per_cpu(timer_head, event->cpu), &event->node);
 	}
-	spin_unlock_irqrestore(&event_timer_lock, flags);
+	raw_spin_unlock_irqrestore(&event_timer_lock, flags);
 	kfree(event);
 }
 EXPORT_SYMBOL(destroy_event_timer);
@@ -486,10 +486,10 @@ ktime_t get_next_event_time(int cpu)
 	struct event_timer_info *event;
 	ktime_t next_event = ns_to_ktime(0);
 
-	spin_lock_irqsave(&event_timer_lock, flags);
+	raw_spin_lock_irqsave(&event_timer_lock, flags);
 	next = timerqueue_getnext(&per_cpu(timer_head, cpu));
 	event = container_of(next, struct event_timer_info, node);
-	spin_unlock_irqrestore(&event_timer_lock, flags);
+	raw_spin_unlock_irqrestore(&event_timer_lock, flags);
 
 	if (!next || event->cpu != cpu)
 		return next_event;
diff --git a/kernel/msm-3.18/drivers/soc/qcom/glink.c b/kernel/msm-3.18/drivers/soc/qcom/glink.c
index bf33fe5c0..2dcaf9156 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/glink.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/glink.c
@@ -118,7 +118,7 @@ struct glink_core_xprt_ctx {
 	enum transport_state_e local_state;
 	bool remote_neg_completed;
 
-	spinlock_t xprt_ctx_lock_lhb1;
+	raw_spinlock_t xprt_ctx_lock_lhb1;
 	struct list_head channels;
 	uint32_t next_lcid;
 	struct list_head free_lcid_list;
@@ -135,7 +135,7 @@ struct glink_core_xprt_ctx {
 	unsigned long curr_qos_rate_kBps;
 	unsigned long threshold_rate_kBps;
 	uint32_t num_priority;
-	spinlock_t tx_ready_lock_lhb3;
+	raw_spinlock_t tx_ready_lock_lhb3;
 	uint32_t active_high_prio;
 	struct glink_qos_priority_bin *prio_bin;
 
@@ -273,21 +273,21 @@ struct channel_ctx {
 	struct completion int_req_complete;
 	unsigned long rx_intent_req_timeout_jiffies;
 
-	spinlock_t local_rx_intent_lst_lock_lhc1;
+	raw_spinlock_t local_rx_intent_lst_lock_lhc1;
 	struct list_head local_rx_intent_list;
 	struct list_head local_rx_intent_ntfy_list;
 	struct list_head local_rx_intent_free_list;
 
-	spinlock_t rmt_rx_intent_lst_lock_lhc2;
+	raw_spinlock_t rmt_rx_intent_lst_lock_lhc2;
 	struct list_head rmt_rx_intent_list;
 
 	uint32_t max_used_liid;
 	uint32_t dummy_riid;
 
-	spinlock_t tx_lists_lock_lhc3;
+	raw_spinlock_t tx_lists_lock_lhc3;
 	struct list_head tx_active;
 
-	spinlock_t tx_pending_rmt_done_lock_lhc4;
+	raw_spinlock_t tx_pending_rmt_done_lock_lhc4;
 	struct list_head tx_pending_remote_done;
 
 	uint32_t lsigs;
@@ -473,7 +473,7 @@ int glink_ssr(const char *subsystem)
 		if (!strcmp(subsystem, xprt_ctx->edge) &&
 				xprt_is_fully_opened(xprt_ctx)) {
 			GLINK_INFO_XPRT(xprt_ctx, "%s: SSR\n", __func__);
-			spin_lock_irqsave(&xprt_ctx->tx_ready_lock_lhb3,
+			raw_spin_lock_irqsave(&xprt_ctx->tx_ready_lock_lhb3,
 					  flags);
 			for (i = 0; i < xprt_ctx->num_priority; i++)
 				list_for_each_entry_safe(ch_ctx, temp_ch_ctx,
@@ -481,7 +481,7 @@ int glink_ssr(const char *subsystem)
 						tx_ready_list_node)
 					list_del_init(
 						&ch_ctx->tx_ready_list_node);
-			spin_unlock_irqrestore(&xprt_ctx->tx_ready_lock_lhb3,
+			raw_spin_unlock_irqrestore(&xprt_ctx->tx_ready_lock_lhb3,
 						flags);
 
 			xprt_ctx->ops->ssr(xprt_ctx->ops);
@@ -678,9 +678,9 @@ static int glink_qos_assign_priority(struct channel_ctx *ctx,
 	uint32_t i;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
+	raw_spin_lock_irqsave(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
 	if (ctx->req_rate_kBps) {
-		spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3,
+		raw_spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3,
 					flags);
 		GLINK_ERR_CH(ctx, "%s: QoS Request already exists\n", __func__);
 		return -EINVAL;
@@ -688,12 +688,12 @@ static int glink_qos_assign_priority(struct channel_ctx *ctx,
 
 	ret = glink_qos_check_feasibility(ctx->transport_ptr, req_rate_kBps);
 	if (ret < 0) {
-		spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3,
+		raw_spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3,
 					flags);
 		return ret;
 	}
 
-	spin_lock(&ctx->tx_lists_lock_lhc3);
+	raw_spin_lock(&ctx->tx_lists_lock_lhc3);
 	i = ctx->transport_ptr->num_priority - 1;
 	while (i > 0 &&
 	       ctx->transport_ptr->prio_bin[i-1].max_rate_kBps >= req_rate_kBps)
@@ -708,8 +708,8 @@ static int glink_qos_assign_priority(struct channel_ctx *ctx,
 		ctx->txd_len = 0;
 		ctx->token_start_time = arch_counter_get_cntpct();
 	}
-	spin_unlock(&ctx->tx_lists_lock_lhc3);
-	spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
+	raw_spin_unlock(&ctx->tx_lists_lock_lhc3);
+	raw_spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
 	return 0;
 }
 
@@ -726,8 +726,8 @@ static int glink_qos_reset_priority(struct channel_ctx *ctx)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
-	spin_lock(&ctx->tx_lists_lock_lhc3);
+	raw_spin_lock_irqsave(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
+	raw_spin_lock(&ctx->tx_lists_lock_lhc3);
 	if (ctx->initial_priority > 0) {
 		ctx->initial_priority = 0;
 		glink_qos_update_ch_prio(ctx, 0);
@@ -735,8 +735,8 @@ static int glink_qos_reset_priority(struct channel_ctx *ctx)
 		ctx->txd_len = 0;
 		ctx->req_rate_kBps = 0;
 	}
-	spin_unlock(&ctx->tx_lists_lock_lhc3);
-	spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
+	raw_spin_unlock(&ctx->tx_lists_lock_lhc3);
+	raw_spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
 	return 0;
 }
 
@@ -1084,15 +1084,15 @@ static struct channel_ctx *xprt_lcid_to_ch_ctx_get(
 	struct channel_ctx *entry;
 	unsigned long flags;
 
-	spin_lock_irqsave(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
 	list_for_each_entry(entry, &xprt_ctx->channels, port_list_node)
 		if (entry->lcid == lcid) {
 			rwref_get(&entry->ch_state_lhb2);
-			spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1,
+			raw_spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1,
 					flags);
 			return entry;
 		}
-	spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
+	raw_spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
 
 	return NULL;
 }
@@ -1115,15 +1115,15 @@ static struct channel_ctx *xprt_rcid_to_ch_ctx_get(
 	struct channel_ctx *entry;
 	unsigned long flags;
 
-	spin_lock_irqsave(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
 	list_for_each_entry(entry, &xprt_ctx->channels, port_list_node)
 		if (entry->rcid == rcid) {
 			rwref_get(&entry->ch_state_lhb2);
-			spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1,
+			raw_spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1,
 					flags);
 			return entry;
 		}
-	spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
+	raw_spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
 
 	return NULL;
 }
@@ -1140,15 +1140,15 @@ bool ch_check_duplicate_riid(struct channel_ctx *ctx, int riid)
 	struct glink_core_rx_intent *intent;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
+	raw_spin_lock_irqsave(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
 	list_for_each_entry(intent, &ctx->rmt_rx_intent_list, list) {
 		if (riid == intent->id) {
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 				&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
 			return true;
 		}
 	}
-	spin_unlock_irqrestore(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
+	raw_spin_unlock_irqrestore(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
 	return false;
 }
 
@@ -1179,10 +1179,10 @@ int ch_pop_remote_rx_intent(struct channel_ctx *ctx, size_t size,
 		return -EINVAL;
 
 	*riid_ptr = 0;
-	spin_lock_irqsave(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
+	raw_spin_lock_irqsave(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
 	if (ctx->transport_ptr->capabilities & GCAP_INTENTLESS) {
 		*riid_ptr = ++ctx->dummy_riid;
-		spin_unlock_irqrestore(&ctx->rmt_rx_intent_lst_lock_lhc2,
+		raw_spin_unlock_irqrestore(&ctx->rmt_rx_intent_lst_lock_lhc2,
 					flags);
 		return 0;
 	}
@@ -1208,11 +1208,11 @@ int ch_pop_remote_rx_intent(struct channel_ctx *ctx, size_t size,
 		*intent_size = best_intent->intent_size;
 		*cookie = best_intent->cookie;
 		kfree(best_intent);
-		spin_unlock_irqrestore(
+		raw_spin_unlock_irqrestore(
 			&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
 		return 0;
 	}
-	spin_unlock_irqrestore(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
+	raw_spin_unlock_irqrestore(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
 	return -EAGAIN;
 }
 
@@ -1257,13 +1257,13 @@ void ch_push_remote_rx_intent(struct channel_ctx *ctx, size_t size,
 	intent->intent_size = size;
 	intent->cookie = cookie;
 
-	spin_lock_irqsave(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
+	raw_spin_lock_irqsave(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
 	list_add_tail(&intent->list, &ctx->rmt_rx_intent_list);
 
 	complete_all(&ctx->int_req_complete);
 	if (ctx->notify_remote_rx_intent)
 		ctx->notify_remote_rx_intent(ctx, ctx->user_priv, size);
-	spin_unlock_irqrestore(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
+	raw_spin_unlock_irqrestore(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
 
 	GLINK_DBG_CH(ctx, "%s: R[%u]:%zu Pushed remote intent\n", __func__,
 			intent->id,
@@ -1319,10 +1319,10 @@ struct glink_core_rx_intent *ch_push_local_rx_intent(struct channel_ctx *ctx,
 		/* intent data allocation failure */
 		GLINK_ERR_CH(ctx, "%s: unable to allocate intent sz[%zu] %d",
 			__func__, size, ret);
-		spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+		raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 		list_add_tail(&intent->list,
 				&ctx->local_rx_intent_free_list);
-		spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1,
+		raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1,
 				flags);
 		return NULL;
 	}
@@ -1333,9 +1333,9 @@ struct glink_core_rx_intent *ch_push_local_rx_intent(struct channel_ctx *ctx,
 	intent->pkt_size = 0;
 	intent->bounce_buf = NULL;
 
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	list_add_tail(&intent->list, &ctx->local_rx_intent_list);
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	GLINK_DBG_CH(ctx, "%s: L[%u]:%zu Pushed intent\n", __func__,
 			intent->id,
 			intent->intent_size);
@@ -1362,14 +1362,14 @@ void ch_remove_local_rx_intent(struct channel_ctx *ctx, uint32_t liid)
 		return;
 	}
 
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	list_for_each_entry_safe(intent, tmp_intent, &ctx->local_rx_intent_list,
 									list) {
 		if (liid == intent->id) {
 			list_del(&intent->list);
 			list_add_tail(&intent->list,
 					&ctx->local_rx_intent_free_list);
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 					&ctx->local_rx_intent_lst_lock_lhc1,
 					flags);
 			GLINK_DBG_CH(ctx,
@@ -1380,7 +1380,7 @@ void ch_remove_local_rx_intent(struct channel_ctx *ctx, uint32_t liid)
 			return;
 		}
 	}
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	GLINK_ERR_CH(ctx, "%s: L[%u] Intent not found.\n", __func__,
 			liid);
 }
@@ -1402,15 +1402,15 @@ struct glink_core_rx_intent *ch_get_dummy_rx_intent(struct channel_ctx *ctx,
 	struct glink_core_rx_intent *intent;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	if (!list_empty(&ctx->local_rx_intent_list)) {
 		intent = list_first_entry(&ctx->local_rx_intent_list,
 					  struct glink_core_rx_intent, list);
-		spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1,
+		raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1,
 					flags);
 		return intent;
 	}
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 
 	intent = ch_get_free_local_rx_intent(ctx);
 	if (!intent) {
@@ -1430,9 +1430,9 @@ struct glink_core_rx_intent *ch_get_dummy_rx_intent(struct channel_ctx *ctx,
 	intent->bounce_buf = NULL;
 	intent->pkt_priv = NULL;
 
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	list_add_tail(&intent->list, &ctx->local_rx_intent_list);
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	GLINK_DBG_CH(ctx, "%s: L[%u]:%zu Pushed intent\n", __func__,
 			intent->id,
 			intent->intent_size);
@@ -1465,15 +1465,15 @@ struct glink_core_rx_intent *ch_get_local_rx_intent(struct channel_ctx *ctx,
 	if (ctx->transport_ptr->capabilities & GCAP_INTENTLESS)
 		return ch_get_dummy_rx_intent(ctx, liid);
 
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	list_for_each_entry(intent, &ctx->local_rx_intent_list, list) {
 		if (liid == intent->id) {
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 				&ctx->local_rx_intent_lst_lock_lhc1, flags);
 			return intent;
 		}
 	}
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	GLINK_ERR_CH(ctx, "%s: L[%u] Intent not found.\n", __func__,
 			liid);
 	return NULL;
@@ -1495,7 +1495,7 @@ void ch_set_local_rx_intent_notified(struct channel_ctx *ctx,
 	struct glink_core_rx_intent *tmp_intent, *intent;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	list_for_each_entry_safe(intent, tmp_intent, &ctx->local_rx_intent_list,
 									list) {
 		if (intent == intent_ptr) {
@@ -1508,13 +1508,13 @@ void ch_set_local_rx_intent_notified(struct channel_ctx *ctx,
 				intent_ptr->id,
 				intent_ptr->intent_size,
 				"from local to notify list\n");
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 					&ctx->local_rx_intent_lst_lock_lhc1,
 					flags);
 			return;
 		}
 	}
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	GLINK_ERR_CH(ctx, "%s: L[%u] Intent not found.\n", __func__,
 			intent_ptr->id);
 }
@@ -1535,18 +1535,18 @@ struct glink_core_rx_intent *ch_get_local_rx_intent_notified(
 	struct glink_core_rx_intent *ptr_intent;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	list_for_each_entry(ptr_intent, &ctx->local_rx_intent_ntfy_list,
 								list) {
 		if (ptr_intent->data == ptr || ptr_intent->iovec == ptr ||
 		    ptr_intent->bounce_buf == ptr) {
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 					&ctx->local_rx_intent_lst_lock_lhc1,
 					flags);
 			return ptr_intent;
 		}
 	}
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	GLINK_ERR_CH(ctx, "%s: Local intent not found\n", __func__);
 	return NULL;
 }
@@ -1568,7 +1568,7 @@ void ch_remove_local_rx_intent_notified(struct channel_ctx *ctx,
 	struct glink_core_rx_intent *ptr_intent, *tmp_intent;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	list_for_each_entry_safe(ptr_intent, tmp_intent,
 				&ctx->local_rx_intent_ntfy_list, list) {
 		if (ptr_intent == liid_ptr) {
@@ -1588,13 +1588,13 @@ void ch_remove_local_rx_intent_notified(struct channel_ctx *ctx,
 			else
 				list_add_tail(&ptr_intent->list,
 					&ctx->local_rx_intent_free_list);
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 					&ctx->local_rx_intent_lst_lock_lhc1,
 					flags);
 			return;
 		}
 	}
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	GLINK_ERR_CH(ctx, "%s: L[%u] Intent not found.\n", __func__,
 			liid_ptr->id);
 }
@@ -1614,14 +1614,14 @@ struct glink_core_rx_intent *ch_get_free_local_rx_intent(
 	struct glink_core_rx_intent *ptr_intent = NULL;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	if (!list_empty(&ctx->local_rx_intent_free_list)) {
 		ptr_intent = list_first_entry(&ctx->local_rx_intent_free_list,
 				struct glink_core_rx_intent,
 				list);
 		list_del(&ptr_intent->list);
 	}
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	return ptr_intent;
 }
 
@@ -1639,16 +1639,16 @@ void ch_purge_intent_lists(struct channel_ctx *ctx)
 	struct glink_core_tx_pkt *tx_info, *tx_info_temp;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ctx->tx_lists_lock_lhc3, flags);
+	raw_spin_lock_irqsave(&ctx->tx_lists_lock_lhc3, flags);
 	list_for_each_entry_safe(tx_info, tx_info_temp, &ctx->tx_active,
 			list_node) {
 		ctx->notify_tx_abort(ctx, ctx->user_priv,
 				tx_info->pkt_priv);
 		rwref_put(&tx_info->pkt_ref);
 	}
-	spin_unlock_irqrestore(&ctx->tx_lists_lock_lhc3, flags);
+	raw_spin_unlock_irqrestore(&ctx->tx_lists_lock_lhc3, flags);
 
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	list_for_each_entry_safe(ptr_intent, tmp_intent,
 				&ctx->local_rx_intent_list, list) {
 		ctx->notify_rx_abort(ctx, ctx->user_priv,
@@ -1674,15 +1674,15 @@ void ch_purge_intent_lists(struct channel_ctx *ctx)
 		kfree(ptr_intent);
 	}
 	ctx->max_used_liid = 0;
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 
-	spin_lock_irqsave(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
+	raw_spin_lock_irqsave(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
 	list_for_each_entry_safe(ptr_intent, tmp_intent,
 			&ctx->rmt_rx_intent_list, list) {
 		list_del(&ptr_intent->list);
 		kfree(ptr_intent);
 	}
-	spin_unlock_irqrestore(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
+	raw_spin_unlock_irqrestore(&ctx->rmt_rx_intent_lst_lock_lhc2, flags);
 }
 
 /**
@@ -1708,7 +1708,7 @@ struct glink_core_tx_pkt *ch_get_tx_pending_remote_done(
 		return NULL;
 	}
 
-	spin_lock_irqsave(&ctx->tx_pending_rmt_done_lock_lhc4, flags);
+	raw_spin_lock_irqsave(&ctx->tx_pending_rmt_done_lock_lhc4, flags);
 	list_for_each_entry(tx_pkt, &ctx->tx_pending_remote_done, list_done) {
 		if (tx_pkt->riid == riid) {
 			if (tx_pkt->size_remaining) {
@@ -1716,12 +1716,12 @@ struct glink_core_tx_pkt *ch_get_tx_pending_remote_done(
 						__func__, riid);
 				tx_pkt = NULL;
 			}
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 				&ctx->tx_pending_rmt_done_lock_lhc4, flags);
 			return tx_pkt;
 		}
 	}
-	spin_unlock_irqrestore(&ctx->tx_pending_rmt_done_lock_lhc4, flags);
+	raw_spin_unlock_irqrestore(&ctx->tx_pending_rmt_done_lock_lhc4, flags);
 
 	GLINK_ERR_CH(ctx, "%s: R[%u] Tx packet for intent not found.\n",
 			__func__, riid);
@@ -1748,7 +1748,7 @@ void ch_remove_tx_pending_remote_done(struct channel_ctx *ctx,
 		return;
 	}
 
-	spin_lock_irqsave(&ctx->tx_pending_rmt_done_lock_lhc4, flags);
+	raw_spin_lock_irqsave(&ctx->tx_pending_rmt_done_lock_lhc4, flags);
 	list_for_each_entry_safe(local_tx_pkt, tmp_tx_pkt,
 			&ctx->tx_pending_remote_done, list_done) {
 		if (tx_pkt == local_tx_pkt) {
@@ -1758,12 +1758,12 @@ void ch_remove_tx_pending_remote_done(struct channel_ctx *ctx,
 				__func__,
 				tx_pkt->riid);
 			rwref_put(&tx_pkt->pkt_ref);
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 				&ctx->tx_pending_rmt_done_lock_lhc4, flags);
 			return;
 		}
 	}
-	spin_unlock_irqrestore(&ctx->tx_pending_rmt_done_lock_lhc4, flags);
+	raw_spin_unlock_irqrestore(&ctx->tx_pending_rmt_done_lock_lhc4, flags);
 
 	GLINK_ERR_CH(ctx, "%s: R[%u] Tx packet for intent not found", __func__,
 			tx_pkt->riid);
@@ -1788,10 +1788,10 @@ static void glink_add_free_lcid_list(struct channel_ctx *ctx)
 		return;
 	}
 	free_lcid->lcid = ctx->lcid;
-	spin_lock_irqsave(&ctx->transport_ptr->xprt_ctx_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&ctx->transport_ptr->xprt_ctx_lock_lhb1, flags);
 	list_add_tail(&free_lcid->list_node,
 			&ctx->transport_ptr->free_lcid_list);
-	spin_unlock_irqrestore(&ctx->transport_ptr->xprt_ctx_lock_lhb1,
+	raw_spin_unlock_irqrestore(&ctx->transport_ptr->xprt_ctx_lock_lhb1,
 					flags);
 }
 
@@ -1849,13 +1849,13 @@ static struct channel_ctx *ch_name_to_ch_ctx_create(
 	INIT_LIST_HEAD(&ctx->local_rx_intent_list);
 	INIT_LIST_HEAD(&ctx->local_rx_intent_ntfy_list);
 	INIT_LIST_HEAD(&ctx->local_rx_intent_free_list);
-	spin_lock_init(&ctx->local_rx_intent_lst_lock_lhc1);
+	raw_spin_lock_init(&ctx->local_rx_intent_lst_lock_lhc1);
 	INIT_LIST_HEAD(&ctx->rmt_rx_intent_list);
-	spin_lock_init(&ctx->rmt_rx_intent_lst_lock_lhc2);
+	raw_spin_lock_init(&ctx->rmt_rx_intent_lst_lock_lhc2);
 	INIT_LIST_HEAD(&ctx->tx_active);
-	spin_lock_init(&ctx->tx_pending_rmt_done_lock_lhc4);
+	raw_spin_lock_init(&ctx->tx_pending_rmt_done_lock_lhc4);
 	INIT_LIST_HEAD(&ctx->tx_pending_remote_done);
-	spin_lock_init(&ctx->tx_lists_lock_lhc3);
+	raw_spin_lock_init(&ctx->tx_lists_lock_lhc3);
 
 check_ctx:
 	rwref_write_get(&xprt_ctx->xprt_state_lhb0);
@@ -1864,11 +1864,11 @@ check_ctx:
 		rwref_write_put(&xprt_ctx->xprt_state_lhb0);
 		return NULL;
 	}
-	spin_lock_irqsave(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
 	list_for_each_entry_safe(entry, temp, &xprt_ctx->channels,
 		    port_list_node)
 		if (!strcmp(entry->name, name) && !entry->pending_delete) {
-			spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1,
+			raw_spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1,
 					flags);
 			kfree(ctx);
 			rwref_write_put(&xprt_ctx->xprt_state_lhb0);
@@ -1882,7 +1882,7 @@ check_ctx:
 				GLINK_ERR_XPRT(xprt_ctx,
 					"%s: unable to exceed %u channels\n",
 					__func__, xprt_ctx->max_cid);
-				spin_unlock_irqrestore(
+				raw_spin_unlock_irqrestore(
 						&xprt_ctx->xprt_ctx_lock_lhb1,
 						flags);
 				kfree(ctx);
@@ -1905,7 +1905,7 @@ check_ctx:
 			"%s: local:GLINK_CHANNEL_CLOSED\n",
 			__func__);
 	}
-	spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
+	raw_spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
 	rwref_write_put(&xprt_ctx->xprt_state_lhb0);
 	mutex_lock(&xprt_ctx->xprt_dbgfs_lock_lhb4);
 	if (ctx != NULL)
@@ -2689,14 +2689,14 @@ static bool glink_delete_ch_from_list(struct channel_ctx *ctx, bool add_flcid)
 	unsigned long flags;
 	bool ret = false;
 
-	spin_lock_irqsave(&ctx->transport_ptr->xprt_ctx_lock_lhb1,
+	raw_spin_lock_irqsave(&ctx->transport_ptr->xprt_ctx_lock_lhb1,
 				flags);
 	if (!list_empty(&ctx->port_list_node))
 		list_del_init(&ctx->port_list_node);
 	if (list_empty(&ctx->transport_ptr->channels) &&
 			list_empty(&ctx->transport_ptr->notified))
 		ret = true;
-	spin_unlock_irqrestore(
+	raw_spin_unlock_irqrestore(
 			&ctx->transport_ptr->xprt_ctx_lock_lhb1,
 			flags);
 	if (add_flcid)
@@ -2762,10 +2762,10 @@ relock: xprt_ctx = ctx->transport_ptr;
 	ctx->pending_delete = true;
 	ctx->int_req_ack = false;
 
-	spin_lock_irqsave(&xprt_ctx->tx_ready_lock_lhb3, flags);
+	raw_spin_lock_irqsave(&xprt_ctx->tx_ready_lock_lhb3, flags);
 	if (!list_empty(&ctx->tx_ready_list_node))
 		list_del_init(&ctx->tx_ready_list_node);
-	spin_unlock_irqrestore(&xprt_ctx->tx_ready_lock_lhb3, flags);
+	raw_spin_unlock_irqrestore(&xprt_ctx->tx_ready_lock_lhb3, flags);
 
 	if (xprt_ctx->local_state != GLINK_XPRT_DOWN) {
 		glink_qos_reset_priority(ctx);
@@ -2977,10 +2977,10 @@ static int glink_tx_common(void *handle, void *pkt_priv,
 	}
 
 	if (!is_atomic) {
-		spin_lock_irqsave(&ctx->transport_ptr->tx_ready_lock_lhb3,
+		raw_spin_lock_irqsave(&ctx->transport_ptr->tx_ready_lock_lhb3,
 				  flags);
 		glink_pm_qos_vote(ctx->transport_ptr);
-		spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3,
+		raw_spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3,
 					flags);
 	}
 
@@ -3124,16 +3124,16 @@ bool glink_rx_intent_exists(void *handle, size_t size)
 	ret = glink_get_ch_ctx(ctx);
 	if (ret)
 		return false;
-	spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_lock_irqsave(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	list_for_each_entry(intent, &ctx->local_rx_intent_list, list) {
 		if (size <= intent->intent_size) {
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 				&ctx->local_rx_intent_lst_lock_lhc1, flags);
 			glink_put_ch_ctx(ctx);
 			return true;
 		}
 	}
-	spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
+	raw_spin_unlock_irqrestore(&ctx->local_rx_intent_lst_lock_lhc1, flags);
 	glink_put_ch_ctx(ctx);
 	return false;
 }
@@ -3491,11 +3491,11 @@ int glink_qos_start(void *handle)
 		return -EBUSY;
 	}
 
-	spin_lock_irqsave(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
-	spin_lock(&ctx->tx_lists_lock_lhc3);
+	raw_spin_lock_irqsave(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
+	raw_spin_lock(&ctx->tx_lists_lock_lhc3);
 	ret = glink_qos_add_ch_tx_intent(ctx);
-	spin_unlock(&ctx->tx_lists_lock_lhc3);
-	spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
+	raw_spin_unlock(&ctx->tx_lists_lock_lhc3);
+	raw_spin_unlock_irqrestore(&ctx->transport_ptr->tx_ready_lock_lhb3, flags);
 	glink_put_ch_ctx(ctx);
 	return ret;
 }
@@ -3945,7 +3945,7 @@ int glink_core_register_transport(struct glink_transport_if *if_ptr,
 		if_ptr->power_unvote = dummy_power_unvote;
 	xprt_ptr->capabilities = 0;
 	xprt_ptr->ops = if_ptr;
-	spin_lock_init(&xprt_ptr->xprt_ctx_lock_lhb1);
+	raw_spin_lock_init(&xprt_ptr->xprt_ctx_lock_lhb1);
 	xprt_ptr->next_lcid = 1; /* 0 reserved for default unconfigured */
 	INIT_LIST_HEAD(&xprt_ptr->free_lcid_list);
 	xprt_ptr->max_cid = cfg->max_cid;
@@ -3955,7 +3955,7 @@ int glink_core_register_transport(struct glink_transport_if *if_ptr,
 	INIT_LIST_HEAD(&xprt_ptr->channels);
 	INIT_LIST_HEAD(&xprt_ptr->notified);
 
-	spin_lock_init(&xprt_ptr->tx_ready_lock_lhb3);
+	raw_spin_lock_init(&xprt_ptr->tx_ready_lock_lhb3);
 	mutex_init(&xprt_ptr->xprt_dbgfs_lock_lhb4);
 	init_kthread_work(&xprt_ptr->tx_kwork, tx_func);
 	init_kthread_worker(&xprt_ptr->tx_wq);
@@ -4123,14 +4123,14 @@ static struct glink_core_xprt_ctx *glink_create_dummy_xprt_ctx(
 
 	xprt_ptr->ops = if_ptr;
 	xprt_ptr->log_ctx = log_ctx;
-	spin_lock_init(&xprt_ptr->xprt_ctx_lock_lhb1);
+	raw_spin_lock_init(&xprt_ptr->xprt_ctx_lock_lhb1);
 	INIT_LIST_HEAD(&xprt_ptr->free_lcid_list);
 	xprt_ptr->local_state = GLINK_XPRT_DOWN;
 	xprt_ptr->remote_neg_completed = false;
 	INIT_LIST_HEAD(&xprt_ptr->channels);
 	xprt_ptr->dummy_in_use = true;
 	INIT_LIST_HEAD(&xprt_ptr->notified);
-	spin_lock_init(&xprt_ptr->tx_ready_lock_lhb3);
+	raw_spin_lock_init(&xprt_ptr->tx_ready_lock_lhb3);
 	mutex_init(&xprt_ptr->xprt_dbgfs_lock_lhb4);
 	return xprt_ptr;
 }
@@ -4141,7 +4141,7 @@ static struct channel_ctx *get_first_ch_ctx(
 	unsigned long flags;
 	struct channel_ctx *ctx;
 
-	spin_lock_irqsave(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
 	if (!list_empty(&xprt_ctx->channels)) {
 		ctx = list_first_entry(&xprt_ctx->channels,
 					struct channel_ctx, port_list_node);
@@ -4149,7 +4149,7 @@ static struct channel_ctx *get_first_ch_ctx(
 	} else {
 		ctx = NULL;
 	}
-	spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
+	raw_spin_unlock_irqrestore(&xprt_ctx->xprt_ctx_lock_lhb1, flags);
 	return ctx;
 }
 
@@ -4158,12 +4158,12 @@ static void glink_core_move_ch_node(struct glink_core_xprt_ctx *xprt_ptr,
 {
 	unsigned long flags, d_flags;
 
-	spin_lock_irqsave(&dummy_xprt_ctx->xprt_ctx_lock_lhb1, d_flags);
-	spin_lock_irqsave(&xprt_ptr->xprt_ctx_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&dummy_xprt_ctx->xprt_ctx_lock_lhb1, d_flags);
+	raw_spin_lock_irqsave(&xprt_ptr->xprt_ctx_lock_lhb1, flags);
 	rwref_get(&dummy_xprt_ctx->xprt_state_lhb0);
 	list_move_tail(&ctx->port_list_node, &dummy_xprt_ctx->channels);
-	spin_unlock_irqrestore(&xprt_ptr->xprt_ctx_lock_lhb1, flags);
-	spin_unlock_irqrestore(&dummy_xprt_ctx->xprt_ctx_lock_lhb1, d_flags);
+	raw_spin_unlock_irqrestore(&xprt_ptr->xprt_ctx_lock_lhb1, flags);
+	raw_spin_unlock_irqrestore(&dummy_xprt_ctx->xprt_ctx_lock_lhb1, d_flags);
 }
 
 /**
@@ -4207,16 +4207,16 @@ static void glink_core_channel_cleanup(struct glink_core_xprt_ctx *xprt_ptr)
 		rwref_write_put(&ctx->ch_state_lhb2);
 		ctx = get_first_ch_ctx(xprt_ptr);
 	}
-	spin_lock_irqsave(&xprt_ptr->xprt_ctx_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&xprt_ptr->xprt_ctx_lock_lhb1, flags);
 	list_for_each_entry_safe(temp_lcid, temp_lcid1,
 			&xprt_ptr->free_lcid_list, list_node) {
 		list_del(&temp_lcid->list_node);
 		kfree(&temp_lcid->list_node);
 	}
-	spin_unlock_irqrestore(&xprt_ptr->xprt_ctx_lock_lhb1, flags);
+	raw_spin_unlock_irqrestore(&xprt_ptr->xprt_ctx_lock_lhb1, flags);
 	rwref_read_put(&xprt_ptr->xprt_state_lhb0);
 
-	spin_lock_irqsave(&dummy_xprt_ctx->xprt_ctx_lock_lhb1, d_flags);
+	raw_spin_lock_irqsave(&dummy_xprt_ctx->xprt_ctx_lock_lhb1, d_flags);
 	dummy_xprt_ctx->dummy_in_use = false;
 	while (!list_empty(&dummy_xprt_ctx->channels)) {
 		ctx = list_first_entry(&dummy_xprt_ctx->channels,
@@ -4225,14 +4225,14 @@ static void glink_core_channel_cleanup(struct glink_core_xprt_ctx *xprt_ptr)
 					&dummy_xprt_ctx->notified);
 
 		rwref_get(&ctx->ch_state_lhb2);
-		spin_unlock_irqrestore(&dummy_xprt_ctx->xprt_ctx_lock_lhb1,
+		raw_spin_unlock_irqrestore(&dummy_xprt_ctx->xprt_ctx_lock_lhb1,
 				d_flags);
 		glink_core_remote_close_common(ctx, false);
-		spin_lock_irqsave(&dummy_xprt_ctx->xprt_ctx_lock_lhb1,
+		raw_spin_lock_irqsave(&dummy_xprt_ctx->xprt_ctx_lock_lhb1,
 				d_flags);
 		rwref_put(&ctx->ch_state_lhb2);
 	}
-	spin_unlock_irqrestore(&dummy_xprt_ctx->xprt_ctx_lock_lhb1, d_flags);
+	raw_spin_unlock_irqrestore(&dummy_xprt_ctx->xprt_ctx_lock_lhb1, d_flags);
 	rwref_read_put(&dummy_xprt_ctx->xprt_state_lhb0);
 }
 /**
@@ -4462,7 +4462,7 @@ static struct channel_ctx *find_l_ctx_get(struct channel_ctx *r_ctx)
 				rwref_write_put(&xprt->xprt_state_lhb0);
 				continue;
 			}
-			spin_lock_irqsave(&xprt->xprt_ctx_lock_lhb1, flags);
+			raw_spin_lock_irqsave(&xprt->xprt_ctx_lock_lhb1, flags);
 			list_for_each_entry(ctx, &xprt->channels,
 							port_list_node)
 				if (!strcmp(ctx->name, r_ctx->name) &&
@@ -4471,7 +4471,7 @@ static struct channel_ctx *find_l_ctx_get(struct channel_ctx *r_ctx)
 					l_ctx = ctx;
 					rwref_get(&l_ctx->ch_state_lhb2);
 				}
-			spin_unlock_irqrestore(&xprt->xprt_ctx_lock_lhb1,
+			raw_spin_unlock_irqrestore(&xprt->xprt_ctx_lock_lhb1,
 									flags);
 			rwref_write_put(&xprt->xprt_state_lhb0);
 		}
@@ -4504,7 +4504,7 @@ static struct channel_ctx *find_r_ctx_get(struct channel_ctx *l_ctx)
 				rwref_write_put(&xprt->xprt_state_lhb0);
 				continue;
 			}
-			spin_lock_irqsave(&xprt->xprt_ctx_lock_lhb1, flags);
+			raw_spin_lock_irqsave(&xprt->xprt_ctx_lock_lhb1, flags);
 			list_for_each_entry(ctx, &xprt->channels,
 							port_list_node)
 				if (!strcmp(ctx->name, l_ctx->name) &&
@@ -4513,7 +4513,7 @@ static struct channel_ctx *find_r_ctx_get(struct channel_ctx *l_ctx)
 					r_ctx = ctx;
 					rwref_get(&r_ctx->ch_state_lhb2);
 				}
-			spin_unlock_irqrestore(&xprt->xprt_ctx_lock_lhb1,
+			raw_spin_unlock_irqrestore(&xprt->xprt_ctx_lock_lhb1,
 									flags);
 			rwref_write_put(&xprt->xprt_state_lhb0);
 		}
@@ -4646,9 +4646,9 @@ static bool ch_migrate(struct channel_ctx *l_ctx, struct channel_ctx *r_ctx)
 				break;
 	mutex_unlock(&transport_list_lock_lha0);
 
-	spin_lock_irqsave(&l_ctx->transport_ptr->xprt_ctx_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&l_ctx->transport_ptr->xprt_ctx_lock_lhb1, flags);
 	list_del_init(&l_ctx->port_list_node);
-	spin_unlock_irqrestore(&l_ctx->transport_ptr->xprt_ctx_lock_lhb1,
+	raw_spin_unlock_irqrestore(&l_ctx->transport_ptr->xprt_ctx_lock_lhb1,
 									flags);
 	mutex_lock(&l_ctx->transport_ptr->xprt_dbgfs_lock_lhb4);
 	glink_debugfs_remove_channel(l_ctx, l_ctx->transport_ptr);
@@ -4664,21 +4664,21 @@ static bool ch_migrate(struct channel_ctx *l_ctx, struct channel_ctx *r_ctx)
 	rwref_lock_init(&ctx_clone->ch_state_lhb2, glink_ch_ctx_release);
 	init_completion(&ctx_clone->int_req_ack_complete);
 	init_completion(&ctx_clone->int_req_complete);
-	spin_lock_init(&ctx_clone->local_rx_intent_lst_lock_lhc1);
-	spin_lock_init(&ctx_clone->rmt_rx_intent_lst_lock_lhc2);
+	raw_spin_lock_init(&ctx_clone->local_rx_intent_lst_lock_lhc1);
+	raw_spin_lock_init(&ctx_clone->rmt_rx_intent_lst_lock_lhc2);
 	INIT_LIST_HEAD(&ctx_clone->tx_ready_list_node);
 	INIT_LIST_HEAD(&ctx_clone->local_rx_intent_list);
 	INIT_LIST_HEAD(&ctx_clone->local_rx_intent_ntfy_list);
 	INIT_LIST_HEAD(&ctx_clone->local_rx_intent_free_list);
 	INIT_LIST_HEAD(&ctx_clone->rmt_rx_intent_list);
 	INIT_LIST_HEAD(&ctx_clone->tx_active);
-	spin_lock_init(&ctx_clone->tx_pending_rmt_done_lock_lhc4);
+	raw_spin_lock_init(&ctx_clone->tx_pending_rmt_done_lock_lhc4);
 	INIT_LIST_HEAD(&ctx_clone->tx_pending_remote_done);
-	spin_lock_init(&ctx_clone->tx_lists_lock_lhc3);
-	spin_lock_irqsave(&l_ctx->transport_ptr->xprt_ctx_lock_lhb1, flags);
+	raw_spin_lock_init(&ctx_clone->tx_lists_lock_lhc3);
+	raw_spin_lock_irqsave(&l_ctx->transport_ptr->xprt_ctx_lock_lhb1, flags);
 	list_add_tail(&ctx_clone->port_list_node,
 					&l_ctx->transport_ptr->channels);
-	spin_unlock_irqrestore(&l_ctx->transport_ptr->xprt_ctx_lock_lhb1,
+	raw_spin_unlock_irqrestore(&l_ctx->transport_ptr->xprt_ctx_lock_lhb1,
 									flags);
 
 	l_ctx->transport_ptr->ops->tx_cmd_ch_close(l_ctx->transport_ptr->ops,
@@ -4700,7 +4700,7 @@ static bool ch_migrate(struct channel_ctx *l_ctx, struct channel_ctx *r_ctx)
 		l_ctx->remote_opened = false;
 
 		rwref_write_get(&xprt->xprt_state_lhb0);
-		spin_lock_irqsave(&xprt->xprt_ctx_lock_lhb1, flags);
+		raw_spin_lock_irqsave(&xprt->xprt_ctx_lock_lhb1, flags);
 		if (list_empty(&xprt->free_lcid_list)) {
 			l_ctx->lcid = xprt->next_lcid++;
 		} else {
@@ -4711,7 +4711,7 @@ static bool ch_migrate(struct channel_ctx *l_ctx, struct channel_ctx *r_ctx)
 			kfree(flcid);
 		}
 		list_add_tail(&l_ctx->port_list_node, &xprt->channels);
-		spin_unlock_irqrestore(&xprt->xprt_ctx_lock_lhb1, flags);
+		raw_spin_unlock_irqrestore(&xprt->xprt_ctx_lock_lhb1, flags);
 		rwref_write_put(&xprt->xprt_state_lhb0);
 	} else {
 		l_ctx->lcid = r_ctx->lcid;
@@ -4721,9 +4721,9 @@ static bool ch_migrate(struct channel_ctx *l_ctx, struct channel_ctx *r_ctx)
 		l_ctx->remote_xprt_resp = r_ctx->remote_xprt_resp;
 		glink_delete_ch_from_list(r_ctx, false);
 
-		spin_lock_irqsave(&xprt->xprt_ctx_lock_lhb1, flags);
+		raw_spin_lock_irqsave(&xprt->xprt_ctx_lock_lhb1, flags);
 		list_add_tail(&l_ctx->port_list_node, &xprt->channels);
-		spin_unlock_irqrestore(&xprt->xprt_ctx_lock_lhb1, flags);
+		raw_spin_unlock_irqrestore(&xprt->xprt_ctx_lock_lhb1, flags);
 	}
 
 	mutex_lock(&xprt->xprt_dbgfs_lock_lhb4);
@@ -5232,7 +5232,7 @@ void glink_core_rx_cmd_tx_done(struct glink_transport_if *if_ptr,
 		return;
 	}
 
-	spin_lock_irqsave(&ctx->tx_lists_lock_lhc3, flags);
+	raw_spin_lock_irqsave(&ctx->tx_lists_lock_lhc3, flags);
 	tx_pkt = ch_get_tx_pending_remote_done(ctx, riid);
 	if (IS_ERR_OR_NULL(tx_pkt)) {
 		/*
@@ -5245,7 +5245,7 @@ void glink_core_rx_cmd_tx_done(struct glink_transport_if *if_ptr,
 		GLINK_ERR_CH(ctx, "%s: R[%u]: No matching tx\n",
 				__func__,
 				(unsigned)riid);
-		spin_unlock_irqrestore(&ctx->tx_lists_lock_lhc3, flags);
+		raw_spin_unlock_irqrestore(&ctx->tx_lists_lock_lhc3, flags);
 		rwref_put(&ctx->ch_state_lhb2);
 		return;
 	}
@@ -5256,7 +5256,7 @@ void glink_core_rx_cmd_tx_done(struct glink_transport_if *if_ptr,
 	intent_size = tx_pkt->intent_size;
 	cookie = tx_pkt->cookie;
 	ch_remove_tx_pending_remote_done(ctx, tx_pkt);
-	spin_unlock_irqrestore(&ctx->tx_lists_lock_lhc3, flags);
+	raw_spin_unlock_irqrestore(&ctx->tx_lists_lock_lhc3, flags);
 
 	if (reuse)
 		ch_push_remote_rx_intent(ctx, intent_size, riid, cookie);
@@ -5281,9 +5281,9 @@ static void xprt_schedule_tx(struct glink_core_xprt_ctx *xprt_ptr,
 		return;
 	}
 
-	spin_lock_irqsave(&xprt_ptr->tx_ready_lock_lhb3, flags);
+	raw_spin_lock_irqsave(&xprt_ptr->tx_ready_lock_lhb3, flags);
 	if (unlikely(!ch_is_fully_opened(ch_ptr))) {
-		spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3, flags);
+		raw_spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3, flags);
 		GLINK_ERR_CH(ch_ptr, "%s: Channel closed before tx\n",
 			     __func__);
 		kfree(tx_info);
@@ -5293,15 +5293,15 @@ static void xprt_schedule_tx(struct glink_core_xprt_ctx *xprt_ptr,
 		list_add_tail(&ch_ptr->tx_ready_list_node,
 			&xprt_ptr->prio_bin[ch_ptr->curr_priority].tx_ready);
 
-	spin_lock(&ch_ptr->tx_lists_lock_lhc3);
+	raw_spin_lock(&ch_ptr->tx_lists_lock_lhc3);
 	list_add_tail(&tx_info->list_node, &ch_ptr->tx_active);
 	glink_qos_do_ch_tx(ch_ptr);
 	if (unlikely(tx_info->tracer_pkt))
 		tracer_pkt_log_event((void *)(tx_info->data),
 				     GLINK_QUEUE_TO_SCHEDULER);
 
-	spin_unlock(&ch_ptr->tx_lists_lock_lhc3);
-	spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3, flags);
+	raw_spin_unlock(&ch_ptr->tx_lists_lock_lhc3);
+	raw_spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3, flags);
 	queue_kthread_work(&xprt_ptr->tx_wq, &xprt_ptr->tx_kwork);
 }
 
@@ -5318,7 +5318,7 @@ static int xprt_single_threaded_tx(struct glink_core_xprt_ctx *xprt_ptr,
 	int ret;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ch_ptr->tx_pending_rmt_done_lock_lhc4, flags);
+	raw_spin_lock_irqsave(&ch_ptr->tx_pending_rmt_done_lock_lhc4, flags);
 	do {
 		ret = xprt_ptr->ops->tx(ch_ptr->transport_ptr->ops,
 					ch_ptr->lcid, tx_info);
@@ -5332,7 +5332,7 @@ static int xprt_single_threaded_tx(struct glink_core_xprt_ctx *xprt_ptr,
 			      &ch_ptr->tx_pending_remote_done);
 		ret = 0;
 	}
-	spin_unlock_irqrestore(&ch_ptr->tx_pending_rmt_done_lock_lhc4, flags);
+	raw_spin_unlock_irqrestore(&ch_ptr->tx_pending_rmt_done_lock_lhc4, flags);
 	return ret;
 }
 
@@ -5402,19 +5402,19 @@ static int glink_scheduler_tx(struct channel_ctx *ctx,
 	uint32_t num_pkts = 0;
 	int ret = 0;
 
-	spin_lock_irqsave(&ctx->tx_lists_lock_lhc3, flags);
+	raw_spin_lock_irqsave(&ctx->tx_lists_lock_lhc3, flags);
 	while (txd_len < xprt_ctx->mtu &&
 		!list_empty(&ctx->tx_active)) {
 		tx_info = list_first_entry(&ctx->tx_active,
 				struct glink_core_tx_pkt, list_node);
 		rwref_get(&tx_info->pkt_ref);
 
-		spin_lock(&ctx->tx_pending_rmt_done_lock_lhc4);
+		raw_spin_lock(&ctx->tx_pending_rmt_done_lock_lhc4);
 		if (list_empty(&tx_info->list_done))
 			list_add(&tx_info->list_done,
 				 &ctx->tx_pending_remote_done);
-		spin_unlock(&ctx->tx_pending_rmt_done_lock_lhc4);
-		spin_unlock_irqrestore(&ctx->tx_lists_lock_lhc3, flags);
+		raw_spin_unlock(&ctx->tx_pending_rmt_done_lock_lhc4);
+		raw_spin_unlock_irqrestore(&ctx->tx_lists_lock_lhc3, flags);
 
 		if (unlikely(tx_info->tracer_pkt)) {
 			tracer_pkt_log_event((void *)(tx_info->data),
@@ -5430,7 +5430,7 @@ static int glink_scheduler_tx(struct channel_ctx *ctx,
 			ret = xprt_ctx->ops->tx(xprt_ctx->ops,
 						ctx->lcid, tx_info);
 		}
-		spin_lock_irqsave(&ctx->tx_lists_lock_lhc3, flags);
+		raw_spin_lock_irqsave(&ctx->tx_lists_lock_lhc3, flags);
 		if (!list_empty(&ctx->tx_active)) {
 			/*
 			 * Verify if same tx_info still exist in tx_active
@@ -5493,7 +5493,7 @@ static int glink_scheduler_tx(struct channel_ctx *ctx,
 		else
 			ctx->token_count--;
 	}
-	spin_unlock_irqrestore(&ctx->tx_lists_lock_lhc3, flags);
+	raw_spin_unlock_irqrestore(&ctx->tx_lists_lock_lhc3, flags);
 
 	return ret;
 }
@@ -5517,10 +5517,10 @@ static void tx_func(struct kthread_work *work)
 
 	while (1) {
 		prio = xprt_ptr->num_priority - 1;
-		spin_lock_irqsave(&xprt_ptr->tx_ready_lock_lhb3, flags);
+		raw_spin_lock_irqsave(&xprt_ptr->tx_ready_lock_lhb3, flags);
 		while (list_empty(&xprt_ptr->prio_bin[prio].tx_ready)) {
 			if (prio == 0) {
-				spin_unlock_irqrestore(
+				raw_spin_unlock_irqrestore(
 					&xprt_ptr->tx_ready_lock_lhb3, flags);
 				return;
 			}
@@ -5530,7 +5530,7 @@ static void tx_func(struct kthread_work *work)
 		ch_ptr = list_first_entry(&xprt_ptr->prio_bin[prio].tx_ready,
 				struct channel_ctx, tx_ready_list_node);
 		rwref_get(&ch_ptr->ch_state_lhb2);
-		spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3, flags);
+		raw_spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3, flags);
 
 		if (tx_ready_head == NULL || tx_ready_head_prio < prio) {
 			tx_ready_head = ch_ptr;
@@ -5573,16 +5573,16 @@ static void tx_func(struct kthread_work *work)
 			 * but didn't return an error. Move to the next channel
 			 * and continue.
 			 */
-			spin_lock_irqsave(&xprt_ptr->tx_ready_lock_lhb3, flags);
+			raw_spin_lock_irqsave(&xprt_ptr->tx_ready_lock_lhb3, flags);
 			list_rotate_left(&xprt_ptr->prio_bin[prio].tx_ready);
-			spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3,
+			raw_spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3,
 						flags);
 			rwref_put(&ch_ptr->ch_state_lhb2);
 			continue;
 		}
 
-		spin_lock_irqsave(&xprt_ptr->tx_ready_lock_lhb3, flags);
-		spin_lock(&ch_ptr->tx_lists_lock_lhc3);
+		raw_spin_lock_irqsave(&xprt_ptr->tx_ready_lock_lhb3, flags);
+		raw_spin_lock(&ch_ptr->tx_lists_lock_lhc3);
 
 		glink_scheduler_eval_prio(ch_ptr, xprt_ptr);
 		if (list_empty(&ch_ptr->tx_active)) {
@@ -5590,8 +5590,8 @@ static void tx_func(struct kthread_work *work)
 			glink_qos_done_ch_tx(ch_ptr);
 		}
 
-		spin_unlock(&ch_ptr->tx_lists_lock_lhc3);
-		spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3, flags);
+		raw_spin_unlock(&ch_ptr->tx_lists_lock_lhc3);
+		raw_spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3, flags);
 
 		tx_ready_head = NULL;
 		transmitted_successfully = true;
@@ -5655,7 +5655,7 @@ static void glink_pm_qos_cancel_worker(struct work_struct *work)
 	xprt_ptr = container_of(to_delayed_work(work),
 			struct glink_core_xprt_ctx, pm_qos_work);
 
-	spin_lock_irqsave(&xprt_ptr->tx_ready_lock_lhb3, flags);
+	raw_spin_lock_irqsave(&xprt_ptr->tx_ready_lock_lhb3, flags);
 	if (!xprt_ptr->tx_path_activity) {
 		/* no more tx activity */
 		GLINK_PERF("%s: qos off\n", __func__);
@@ -5664,7 +5664,7 @@ static void glink_pm_qos_cancel_worker(struct work_struct *work)
 		xprt_ptr->qos_req_active = false;
 	}
 	xprt_ptr->tx_path_activity = false;
-	spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3, flags);
+	raw_spin_unlock_irqrestore(&xprt_ptr->tx_ready_lock_lhb3, flags);
 }
 
 /**
@@ -5866,7 +5866,7 @@ void glink_ch_ctx_iterator_init(struct ch_ctx_iterator *ch_iter,
 	if (ch_iter == NULL || xprt == NULL)
 		return;
 
-	spin_lock_irqsave(&xprt->xprt_ctx_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&xprt->xprt_ctx_lock_lhb1, flags);
 	ch_iter->ch_list = &(xprt->channels);
 	ch_iter->i_curr = list_entry(&(xprt->channels),
 				struct channel_ctx, port_list_node);
@@ -5884,7 +5884,7 @@ void glink_ch_ctx_iterator_end(struct ch_ctx_iterator *ch_iter,
 	if (ch_iter == NULL || xprt == NULL)
 		return;
 
-	spin_unlock_irqrestore(&xprt->xprt_ctx_lock_lhb1,
+	raw_spin_unlock_irqrestore(&xprt->xprt_ctx_lock_lhb1,
 			ch_iter->ch_list_flags);
 	ch_iter->ch_list = NULL;
 	ch_iter->i_curr = NULL;
diff --git a/kernel/msm-3.18/drivers/soc/qcom/glink_bgcom_xprt.c b/kernel/msm-3.18/drivers/soc/qcom/glink_bgcom_xprt.c
index c38c3de5f..f0e7e40ad 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/glink_bgcom_xprt.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/glink_bgcom_xprt.c
@@ -167,7 +167,7 @@ struct edge_info {
 	uint32_t bgcom_status;
 	struct work_struct wakeup_work;
 	uint32_t activity_flag;
-	spinlock_t activity_lock;
+	raw_spinlock_t activity_lock;
 	struct bgcom_open_config_type bgcom_config;
 	void *bgcom_handle;
 };
@@ -1356,9 +1356,9 @@ static int power_vote(struct glink_transport_if *if_ptr, uint32_t state)
 	struct edge_info *einfo;
 
 	einfo = container_of(if_ptr, struct edge_info, xprt_if);
-	spin_lock_irqsave(&einfo->activity_lock, flags);
+	raw_spin_lock_irqsave(&einfo->activity_lock, flags);
 	einfo->activity_flag |= ACTIVE_TX;
-	spin_unlock_irqrestore(&einfo->activity_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->activity_lock, flags);
 	return 0;
 }
 
@@ -1374,9 +1374,9 @@ static int power_unvote(struct glink_transport_if *if_ptr)
 	struct edge_info *einfo;
 
 	einfo = container_of(if_ptr, struct edge_info, xprt_if);
-	spin_lock_irqsave(&einfo->activity_lock, flags);
+	raw_spin_lock_irqsave(&einfo->activity_lock, flags);
 	einfo->activity_flag &= ~ACTIVE_TX;
-	spin_unlock_irqrestore(&einfo->activity_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->activity_lock, flags);
 	return 0;
 }
 
@@ -1597,12 +1597,12 @@ static int glink_bgcom_probe(struct platform_device *pdev)
 	init_srcu_struct(&einfo->use_ref);
 	mutex_init(&einfo->write_lock);
 	init_waitqueue_head(&einfo->tx_blocked_queue);
-	spin_lock_init(&einfo->activity_lock);
+	raw_spin_lock_init(&einfo->activity_lock);
 	mutex_init(&einfo->tx_avail_lock);
 
-	spin_lock_irqsave(&edge_infos_lock, flags);
+	raw_spin_lock_irqsave(&edge_infos_lock, flags);
 	list_add_tail(&einfo->list, &edge_infos);
-	spin_unlock_irqrestore(&edge_infos_lock, flags);
+	raw_spin_unlock_irqrestore(&edge_infos_lock, flags);
 
 	einfo->task = kthread_run(kthread_worker_fn, &einfo->kworker,
 				  "bgcom_%s", subsys_name);
@@ -1650,9 +1650,9 @@ reg_xprt_fail:
 	kthread_stop(einfo->task);
 	einfo->task = NULL;
 kthread_fail:
-	spin_lock_irqsave(&edge_infos_lock, flags);
+	raw_spin_lock_irqsave(&edge_infos_lock, flags);
 	list_del(&einfo->list);
-	spin_unlock_irqrestore(&edge_infos_lock, flags);
+	raw_spin_unlock_irqrestore(&edge_infos_lock, flags);
 missing_key:
 	kfree(einfo);
 edge_info_alloc_fail:
@@ -1670,9 +1670,9 @@ static int glink_bgcom_remove(struct platform_device *pdev)
 	flush_kthread_worker(&einfo->kworker);
 	kthread_stop(einfo->task);
 	einfo->task = NULL;
-	spin_lock_irqsave(&edge_infos_lock, flags);
+	raw_spin_lock_irqsave(&edge_infos_lock, flags);
 	list_del(&einfo->list);
-	spin_unlock_irqrestore(&edge_infos_lock, flags);
+	raw_spin_unlock_irqrestore(&edge_infos_lock, flags);
 	return 0;
 }
 
@@ -1693,9 +1693,9 @@ static int glink_bgcom_suspend(struct platform_device *pdev,
 	if (strcmp(einfo->xprt_cfg.edge, "bgcom"))
 		return 0;
 
-	spin_lock_irqsave(&einfo->activity_lock, flags);
+	raw_spin_lock_irqsave(&einfo->activity_lock, flags);
 	suspend = !(einfo->activity_flag);
-	spin_unlock_irqrestore(&einfo->activity_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->activity_lock, flags);
 	if (suspend)
 		rc = bgcom_suspend(&einfo->bgcom_handle);
 	if (rc < 0)
diff --git a/kernel/msm-3.18/drivers/soc/qcom/glink_debugfs.c b/kernel/msm-3.18/drivers/soc/qcom/glink_debugfs.c
index 8e65e4ac9..29317477e 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/glink_debugfs.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/glink_debugfs.c
@@ -62,7 +62,7 @@ struct glink_dbgfs_dent {
 	char self_name[GLINK_DBGFS_NAME_SIZE];
 	struct dentry *parent;
 	struct dentry *self;
-	spinlock_t file_list_lock_lhb0;
+	raw_spinlock_t file_list_lock_lhb0;
 	struct list_head file_list;
 };
 
@@ -259,7 +259,7 @@ static void glink_dfs_update_ch_intent(struct seq_file *s)
 					"INT_SIZE");
 		seq_puts(s,
 		"---------------------------------------------------------------\n");
-		spin_lock_irqsave(ch_intent_info.li_lst_lock, flags);
+		raw_spin_lock_irqsave(ch_intent_info.li_lst_lock, flags);
 		list_for_each_entry_safe(intent, intent_temp,
 				ch_intent_info.li_avail_list, list) {
 			count++;
@@ -272,16 +272,16 @@ static void glink_dfs_update_ch_intent(struct seq_file *s)
 			count++;
 			write_ch_intent(s, intent, "LOCAL_USED_LIST", count);
 		}
-		spin_unlock_irqrestore(ch_intent_info.li_lst_lock, flags);
+		raw_spin_unlock_irqrestore(ch_intent_info.li_lst_lock, flags);
 
 		count = 0;
-		spin_lock_irqsave(ch_intent_info.ri_lst_lock, flags);
+		raw_spin_lock_irqsave(ch_intent_info.ri_lst_lock, flags);
 		list_for_each_entry_safe(intent, intent_temp,
 				ch_intent_info.ri_list, list) {
 			count++;
 			write_ch_intent(s, intent, "REMOTE_LIST", count);
 		}
-		spin_unlock_irqrestore(ch_intent_info.ri_lst_lock,
+		raw_spin_unlock_irqrestore(ch_intent_info.ri_lst_lock,
 					flags);
 		seq_puts(s,
 		"---------------------------------------------------------------\n");
@@ -555,7 +555,7 @@ void glink_dfs_update_list(struct dentry *curr_dent, struct dentry *parent,
 				GFP_KERNEL);
 		if (dbgfs_dent_s != NULL) {
 			INIT_LIST_HEAD(&dbgfs_dent_s->file_list);
-			spin_lock_init(&dbgfs_dent_s->file_list_lock_lhb0);
+			raw_spin_lock_init(&dbgfs_dent_s->file_list_lock_lhb0);
 			dbgfs_dent_s->parent = parent;
 			dbgfs_dent_s->self = curr_dent;
 			strlcpy(dbgfs_dent_s->self_name,
@@ -589,7 +589,7 @@ void glink_remove_dfs_entry(struct glink_dbgfs_dent *entry)
 	if (entry == NULL)
 		return;
 	if (!list_empty(&entry->file_list)) {
-		spin_lock_irqsave(&entry->file_list_lock_lhb0, flags);
+		raw_spin_lock_irqsave(&entry->file_list_lock_lhb0, flags);
 		list_for_each_entry_safe(fentry, fentry_temp,
 				&entry->file_list, flist) {
 			if (fentry->b_priv_free_req)
@@ -598,7 +598,7 @@ void glink_remove_dfs_entry(struct glink_dbgfs_dent *entry)
 			kfree(fentry);
 			fentry = NULL;
 		}
-		spin_unlock_irqrestore(&entry->file_list_lock_lhb0, flags);
+		raw_spin_unlock_irqrestore(&entry->file_list_lock_lhb0, flags);
 	}
 	list_del(&entry->list_node);
 	kfree(entry);
@@ -696,11 +696,11 @@ struct dentry *glink_debugfs_create(const char *name,
 		} else {
 			file_data = glink_dfs_create_file(name, parent, show,
 							dbgfs_data, b_free_req);
-			spin_lock_irqsave(&entry->file_list_lock_lhb0, flags);
+			raw_spin_lock_irqsave(&entry->file_list_lock_lhb0, flags);
 			if (file_data != NULL)
 				list_add_tail(&file_data->flist,
 						&entry->file_list);
-			spin_unlock_irqrestore(&entry->file_list_lock_lhb0,
+			raw_spin_unlock_irqrestore(&entry->file_list_lock_lhb0,
 						flags);
 		}
 	} else {
diff --git a/kernel/msm-3.18/drivers/soc/qcom/glink_private.h b/kernel/msm-3.18/drivers/soc/qcom/glink_private.h
index b8aef7959..1a41910a5 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/glink_private.h
+++ b/kernel/msm-3.18/drivers/soc/qcom/glink_private.h
@@ -87,10 +87,10 @@ struct ch_ctx_iterator {
 };
 
 struct glink_ch_intent_info {
-	spinlock_t *li_lst_lock;
+	raw_spinlock_t *li_lst_lock;
 	struct list_head *li_avail_list;
 	struct list_head *li_used_list;
-	spinlock_t *ri_lst_lock;
+	raw_spinlock_t *ri_lst_lock;
 	struct list_head *ri_list;
 };
 
@@ -733,7 +733,7 @@ struct subsys_info {
 	struct list_head notify_list;
 	int notify_list_len;
 	bool link_up;
-	spinlock_t link_up_lock;
+	raw_spinlock_t link_up_lock;
 };
 
 /**
@@ -889,7 +889,7 @@ struct rwref_lock {
 	struct kref kref;
 	unsigned read_count;
 	unsigned write_count;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	wait_queue_head_t count_zero;
 
 	void (*release)(struct rwref_lock *);
@@ -923,7 +923,7 @@ static inline void rwref_lock_init(struct rwref_lock *lock_ptr,
 	kref_init(&lock_ptr->kref);
 	lock_ptr->read_count = 0;
 	lock_ptr->write_count = 0;
-	spin_lock_init(&lock_ptr->lock);
+	raw_spin_lock_init(&lock_ptr->lock);
 	init_waitqueue_head(&lock_ptr->count_zero);
 	lock_ptr->release = release;
 }
@@ -968,13 +968,13 @@ static inline void rwref_read_get_atomic(struct rwref_lock *lock_ptr,
 
 	kref_get(&lock_ptr->kref);
 	while (1) {
-		spin_lock_irqsave(&lock_ptr->lock, flags);
+		raw_spin_lock_irqsave(&lock_ptr->lock, flags);
 		if (lock_ptr->write_count == 0) {
 			lock_ptr->read_count++;
-			spin_unlock_irqrestore(&lock_ptr->lock, flags);
+			raw_spin_unlock_irqrestore(&lock_ptr->lock, flags);
 			break;
 		}
-		spin_unlock_irqrestore(&lock_ptr->lock, flags);
+		raw_spin_unlock_irqrestore(&lock_ptr->lock, flags);
 		if (!is_atomic) {
 			wait_event(lock_ptr->count_zero,
 					lock_ptr->write_count == 0);
@@ -1005,11 +1005,11 @@ static inline void rwref_read_put(struct rwref_lock *lock_ptr)
 
 	BUG_ON(lock_ptr == NULL);
 
-	spin_lock_irqsave(&lock_ptr->lock, flags);
+	raw_spin_lock_irqsave(&lock_ptr->lock, flags);
 	BUG_ON(lock_ptr->read_count == 0);
 	if (--lock_ptr->read_count == 0)
 		wake_up(&lock_ptr->count_zero);
-	spin_unlock_irqrestore(&lock_ptr->lock, flags);
+	raw_spin_unlock_irqrestore(&lock_ptr->lock, flags);
 	kref_put(&lock_ptr->kref, rwref_lock_release);
 }
 
@@ -1029,13 +1029,13 @@ static inline void rwref_write_get_atomic(struct rwref_lock *lock_ptr,
 
 	kref_get(&lock_ptr->kref);
 	while (1) {
-		spin_lock_irqsave(&lock_ptr->lock, flags);
+		raw_spin_lock_irqsave(&lock_ptr->lock, flags);
 		if (lock_ptr->read_count == 0 && lock_ptr->write_count == 0) {
 			lock_ptr->write_count++;
-			spin_unlock_irqrestore(&lock_ptr->lock, flags);
+			raw_spin_unlock_irqrestore(&lock_ptr->lock, flags);
 			break;
 		}
-		spin_unlock_irqrestore(&lock_ptr->lock, flags);
+		raw_spin_unlock_irqrestore(&lock_ptr->lock, flags);
 		if (!is_atomic) {
 			wait_event(lock_ptr->count_zero,
 					(lock_ptr->read_count == 0 &&
@@ -1067,11 +1067,11 @@ static inline void rwref_write_put(struct rwref_lock *lock_ptr)
 
 	BUG_ON(lock_ptr == NULL);
 
-	spin_lock_irqsave(&lock_ptr->lock, flags);
+	raw_spin_lock_irqsave(&lock_ptr->lock, flags);
 	BUG_ON(lock_ptr->write_count != 1);
 	if (--lock_ptr->write_count == 0)
 		wake_up(&lock_ptr->count_zero);
-	spin_unlock_irqrestore(&lock_ptr->lock, flags);
+	raw_spin_unlock_irqrestore(&lock_ptr->lock, flags);
 	kref_put(&lock_ptr->kref, rwref_lock_release);
 }
 
diff --git a/kernel/msm-3.18/drivers/soc/qcom/glink_smd_xprt.c b/kernel/msm-3.18/drivers/soc/qcom/glink_smd_xprt.c
index 6abe94306..1202012c7 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/glink_smd_xprt.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/glink_smd_xprt.c
@@ -95,7 +95,7 @@ struct edge_info {
 	struct glink_core_transport_cfg xprt_cfg;
 	uint32_t smd_edge;
 	struct list_head channels;
-	spinlock_t channels_lock;
+	raw_spinlock_t channels_lock;
 	bool intentless;
 	bool irq_disabled;
 	struct srcu_struct ssr_sync;
@@ -149,7 +149,7 @@ struct channel {
 	smd_channel_t *smd_ch;
 	struct list_head intents;
 	struct list_head used_intents;
-	spinlock_t intents_lock;
+	raw_spinlock_t intents_lock;
 	uint32_t next_intent_id;
 	struct workqueue_struct *wq;
 	struct tasklet_struct data_tasklet;
@@ -159,7 +159,7 @@ struct channel {
 	bool local_legacy;
 	bool remote_legacy;
 	size_t intent_req_size;
-	spinlock_t rx_data_lock;
+	raw_spinlock_t rx_data_lock;
 	bool streaming_ch;
 	bool tx_resume_needed;
 	bool is_tasklet_enabled;
@@ -309,14 +309,14 @@ static void process_ctl_event(struct work_struct *work)
 			SMDXPRT_INFO(einfo, "%s RX OPEN '%s'\n",
 					__func__, name);
 
-			spin_lock_irqsave(&einfo->channels_lock, flags);
+			raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 			list_for_each_entry(ch, &einfo->channels, node) {
 				if (!strcmp(name, ch->name)) {
 					found = true;
 					break;
 				}
 			}
-			spin_unlock_irqrestore(&einfo->channels_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 			if (!found) {
 				ch = kzalloc(sizeof(*ch), GFP_KERNEL);
@@ -333,8 +333,8 @@ static void process_ctl_event(struct work_struct *work)
 				init_completion(&ch->open_notifier);
 				INIT_LIST_HEAD(&ch->intents);
 				INIT_LIST_HEAD(&ch->used_intents);
-				spin_lock_init(&ch->intents_lock);
-				spin_lock_init(&ch->rx_data_lock);
+				raw_spin_lock_init(&ch->intents_lock);
+				raw_spin_lock_init(&ch->rx_data_lock);
 				mutex_lock(&ch->ch_tasklet_lock);
 				tasklet_init(&ch->data_tasklet,
 				process_data_event, (unsigned long)ch);
@@ -358,7 +358,7 @@ static void process_ctl_event(struct work_struct *work)
 				 * necessary
 				 */
 				temp_ch = ch;
-				spin_lock_irqsave(&einfo->channels_lock, flags);
+				raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 				list_for_each_entry(ch, &einfo->channels, node)
 					if (!strcmp(name, ch->name)) {
 						found = true;
@@ -369,10 +369,10 @@ static void process_ctl_event(struct work_struct *work)
 					ch = temp_ch;
 					list_add_tail(&ch->node,
 							&einfo->channels);
-					spin_unlock_irqrestore(
+					raw_spin_unlock_irqrestore(
 						&einfo->channels_lock, flags);
 				} else {
-					spin_unlock_irqrestore(
+					raw_spin_unlock_irqrestore(
 						&einfo->channels_lock, flags);
 					tasklet_kill(&temp_ch->data_tasklet);
 					destroy_workqueue(temp_ch->wq);
@@ -411,13 +411,13 @@ static void process_ctl_event(struct work_struct *work)
 				"%s RX OPEN ACK lcid %u; xprt_req %u\n",
 				__func__, cmd.id, cmd.priority);
 
-			spin_lock_irqsave(&einfo->channels_lock, flags);
+			raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 			list_for_each_entry(ch, &einfo->channels, node)
 				if (cmd.id == ch->lcid) {
 					found = true;
 					break;
 				}
-			spin_unlock_irqrestore(&einfo->channels_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 			if (!found) {
 				SMDXPRT_ERR(einfo, "%s No channel match %u\n",
 						__func__, cmd.id);
@@ -435,13 +435,13 @@ static void process_ctl_event(struct work_struct *work)
 		} else if (cmd.cmd == CMD_CLOSE) {
 			SMDXPRT_INFO(einfo, "%s RX REMOTE CLOSE rcid %u\n",
 					__func__, cmd.id);
-			spin_lock_irqsave(&einfo->channels_lock, flags);
+			raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 			list_for_each_entry(ch, &einfo->channels, node)
 				if (cmd.id == ch->rcid) {
 					found = true;
 					break;
 				}
-			spin_unlock_irqrestore(&einfo->channels_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 			if (!found)
 				SMDXPRT_ERR(einfo, "%s no matching rcid %u\n",
@@ -473,14 +473,14 @@ static void process_ctl_event(struct work_struct *work)
 			SMDXPRT_INFO(einfo, "%s RX CLOSE ACK lcid %u\n",
 					__func__, cmd.id);
 
-			spin_lock_irqsave(&einfo->channels_lock, flags);
+			raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 			list_for_each_entry(ch, &einfo->channels, node) {
 				if (cmd.id == ch->lcid) {
 					found = true;
 					break;
 				}
 			}
-			spin_unlock_irqrestore(&einfo->channels_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 			if (!found) {
 				SMDXPRT_ERR(einfo, "%s LCID not found %u\n",
 						__func__, cmd.id);
@@ -765,7 +765,7 @@ static void process_data_event(unsigned long param)
 		einfo->xprt_if.glink_core_if_ptr->tx_resume(&einfo->xprt_if);
 	}
 
-	spin_lock_irqsave(&ch->rx_data_lock, rx_data_flags);
+	raw_spin_lock_irqsave(&ch->rx_data_lock, rx_data_flags);
 	while (!ch->is_closing && smd_read_avail(ch->smd_ch)) {
 		if (!ch->streaming_ch)
 			pkt_remaining = smd_cur_packet_size(ch->smd_ch);
@@ -775,7 +775,7 @@ static void process_data_event(unsigned long param)
 				__func__, pkt_remaining, ch->name, ch->lcid,
 				ch->rcid);
 		if (!ch->cur_intent && !einfo->intentless) {
-			spin_lock_irqsave(&ch->intents_lock, intents_flags);
+			raw_spin_lock_irqsave(&ch->intents_lock, intents_flags);
 			ch->intent_req = true;
 			ch->intent_req_size = pkt_remaining;
 			list_for_each_entry(i, &ch->intents, node) {
@@ -786,10 +786,10 @@ static void process_data_event(unsigned long param)
 					break;
 				}
 			}
-			spin_unlock_irqrestore(&ch->intents_lock,
+			raw_spin_unlock_irqrestore(&ch->intents_lock,
 								intents_flags);
 			if (!ch->cur_intent) {
-				spin_unlock_irqrestore(&ch->rx_data_lock,
+				raw_spin_unlock_irqrestore(&ch->rx_data_lock,
 								rx_data_flags);
 				SMDXPRT_DBG(einfo,
 					"%s Reqesting intent '%s' %u:%u\n",
@@ -824,13 +824,13 @@ static void process_data_event(unsigned long param)
 		}
 		smd_read(ch->smd_ch, intent->data + intent->write_offset,
 								read_avail);
-		spin_unlock_irqrestore(&ch->rx_data_lock, rx_data_flags);
+		raw_spin_unlock_irqrestore(&ch->rx_data_lock, rx_data_flags);
 		intent->write_offset += read_avail;
 		intent->pkt_size += read_avail;
 		if (read_avail == pkt_remaining && !einfo->intentless) {
-			spin_lock_irqsave(&ch->intents_lock, intents_flags);
+			raw_spin_lock_irqsave(&ch->intents_lock, intents_flags);
 			list_add_tail(&ch->cur_intent->node, &ch->used_intents);
-			spin_unlock_irqrestore(&ch->intents_lock,
+			raw_spin_unlock_irqrestore(&ch->intents_lock,
 								intents_flags);
 			ch->cur_intent = NULL;
 		}
@@ -839,9 +839,9 @@ static void process_data_event(unsigned long param)
 						ch->rcid,
 						intent,
 						read_avail == pkt_remaining);
-		spin_lock_irqsave(&ch->rx_data_lock, rx_data_flags);
+		raw_spin_lock_irqsave(&ch->rx_data_lock, rx_data_flags);
 	}
-	spin_unlock_irqrestore(&ch->rx_data_lock, rx_data_flags);
+	raw_spin_unlock_irqrestore(&ch->rx_data_lock, rx_data_flags);
 }
 
 /**
@@ -955,7 +955,7 @@ static void smd_data_ch_close(struct channel *ch)
 	mutex_unlock(&ch->ch_probe_lock);
 
 
-	spin_lock_irqsave(&ch->intents_lock, flags);
+	raw_spin_lock_irqsave(&ch->intents_lock, flags);
 	while (!list_empty(&ch->intents)) {
 		intent = list_first_entry(&ch->intents, struct
 				intent_info, node);
@@ -968,7 +968,7 @@ static void smd_data_ch_close(struct channel *ch)
 		list_del(&intent->node);
 		kfree(intent);
 	}
-	spin_unlock_irqrestore(&ch->intents_lock, flags);
+	raw_spin_unlock_irqrestore(&ch->intents_lock, flags);
 	ch->is_closing = false;
 }
 
@@ -1012,14 +1012,14 @@ static int channel_probe(struct platform_device *pdev)
 	einfo = &edge_infos[i];
 
 	found = false;
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node) {
 		if (!strcmp(pdev->name, ch->name)) {
 			found = true;
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 	if (!found)
 		return -EPROBE_DEFER;
@@ -1219,14 +1219,14 @@ static int tx_cmd_ch_open(struct glink_transport_if *if_ptr, uint32_t lcid,
 		return -EFAULT;
 	}
 
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node) {
 		if (!strcmp(name, ch->name)) {
 			found = true;
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 	if (!found) {
 		ch = kzalloc(sizeof(*ch), GFP_KERNEL);
@@ -1244,8 +1244,8 @@ static int tx_cmd_ch_open(struct glink_transport_if *if_ptr, uint32_t lcid,
 		init_completion(&ch->open_notifier);
 		INIT_LIST_HEAD(&ch->intents);
 		INIT_LIST_HEAD(&ch->used_intents);
-		spin_lock_init(&ch->intents_lock);
-		spin_lock_init(&ch->rx_data_lock);
+		raw_spin_lock_init(&ch->intents_lock);
+		raw_spin_lock_init(&ch->rx_data_lock);
 		mutex_lock(&ch->ch_tasklet_lock);
 		tasklet_init(&ch->data_tasklet, process_data_event,
 				(unsigned long)ch);
@@ -1268,7 +1268,7 @@ static int tx_cmd_ch_open(struct glink_transport_if *if_ptr, uint32_t lcid,
 		 * and recheck is necessary
 		 */
 		temp_ch = ch;
-		spin_lock_irqsave(&einfo->channels_lock, flags);
+		raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 		list_for_each_entry(ch, &einfo->channels, node)
 			if (!strcmp(name, ch->name)) {
 				found = true;
@@ -1278,9 +1278,9 @@ static int tx_cmd_ch_open(struct glink_transport_if *if_ptr, uint32_t lcid,
 		if (!found) {
 			ch = temp_ch;
 			list_add_tail(&ch->node, &einfo->channels);
-			spin_unlock_irqrestore(&einfo->channels_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 		} else {
-			spin_unlock_irqrestore(&einfo->channels_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 			tasklet_kill(&temp_ch->data_tasklet);
 			destroy_workqueue(temp_ch->wq);
 			kfree(temp_ch);
@@ -1357,13 +1357,13 @@ static int tx_cmd_ch_close(struct glink_transport_if *if_ptr, uint32_t lcid)
 		return -EFAULT;
 	}
 
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node)
 		if (lcid == ch->lcid) {
 			found = true;
 			break;
 		}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 	if (!found) {
 		SMDXPRT_ERR(einfo, "%s LCID not found %u\n",
@@ -1415,13 +1415,13 @@ static void tx_cmd_ch_remote_open_ack(struct glink_transport_if *if_ptr,
 	if (!einfo->smd_ctl_ch_open)
 		return;
 
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node)
 		if (ch->rcid == rcid) {
 			found = true;
 			break;
 		}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 	if (!found) {
 		SMDXPRT_ERR(einfo, "%s No matching SMD channel for rcid %u\n",
@@ -1472,13 +1472,13 @@ static void tx_cmd_ch_remote_close_ack(struct glink_transport_if *if_ptr,
 
 	einfo = container_of(if_ptr, struct edge_info, xprt_if);
 
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node)
 		if (rcid == ch->rcid) {
 			found = true;
 			break;
 		}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 	if (!found) {
 		SMDXPRT_ERR(einfo,
@@ -1523,9 +1523,9 @@ static int ssr(struct glink_transport_if *if_ptr)
 
 	einfo->smd_ctl_ch_open = false;
 
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node) {
-		spin_unlock_irqrestore(&einfo->channels_lock, flags);
+		raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 		ch->is_closing = true;
 		mutex_lock(&ch->ch_tasklet_lock);
 		if (ch->is_tasklet_enabled) {
@@ -1546,7 +1546,7 @@ static int ssr(struct glink_transport_if *if_ptr)
 		ch->rcid = 0;
 		ch->tx_resume_needed = false;
 
-		spin_lock_irqsave(&ch->intents_lock, flags);
+		raw_spin_lock_irqsave(&ch->intents_lock, flags);
 		while (!list_empty(&ch->intents)) {
 			intent = list_first_entry(&ch->intents,
 							struct intent_info,
@@ -1563,11 +1563,11 @@ static int ssr(struct glink_transport_if *if_ptr)
 		}
 		kfree(ch->cur_intent);
 		ch->cur_intent = NULL;
-		spin_unlock_irqrestore(&ch->intents_lock, flags);
+		raw_spin_unlock_irqrestore(&ch->intents_lock, flags);
 		ch->is_closing = false;
-		spin_lock_irqsave(&einfo->channels_lock, flags);
+		raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 	einfo->xprt_if.glink_core_if_ptr->link_down(&einfo->xprt_if);
 	schedule_delayed_work(&einfo->ssr_work, 5 * HZ);
@@ -1669,12 +1669,12 @@ static int tx_cmd_local_rx_intent(struct glink_transport_if *if_ptr,
 		return -EFAULT;
 	}
 
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node) {
 		if (lcid == ch->lcid)
 			break;
 	}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 	intent = kmalloc(sizeof(*intent), GFP_KERNEL);
 	if (!intent) {
@@ -1685,10 +1685,10 @@ static int tx_cmd_local_rx_intent(struct glink_transport_if *if_ptr,
 
 	intent->liid = liid;
 	intent->size = size;
-	spin_lock_irqsave(&ch->intents_lock, flags);
+	raw_spin_lock_irqsave(&ch->intents_lock, flags);
 	list_add_tail(&intent->node, &ch->intents);
 	check_and_resume_rx(ch, size);
-	spin_unlock_irqrestore(&ch->intents_lock, flags);
+	raw_spin_unlock_irqrestore(&ch->intents_lock, flags);
 
 	srcu_read_unlock(&einfo->ssr_sync, rcu_id);
 	return 0;
@@ -1710,13 +1710,13 @@ static void tx_cmd_local_rx_done(struct glink_transport_if *if_ptr,
 	unsigned long flags;
 
 	einfo = container_of(if_ptr, struct edge_info, xprt_if);
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node) {
 		if (lcid == ch->lcid)
 			break;
 	}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
-	spin_lock_irqsave(&ch->intents_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&ch->intents_lock, flags);
 	list_for_each_entry(i, &ch->used_intents, node) {
 		if (i->liid == liid) {
 			list_del(&i->node);
@@ -1729,7 +1729,7 @@ static void tx_cmd_local_rx_done(struct glink_transport_if *if_ptr,
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&ch->intents_lock, flags);
+	raw_spin_unlock_irqrestore(&ch->intents_lock, flags);
 }
 
 /**
@@ -1760,12 +1760,12 @@ static int tx(struct glink_transport_if *if_ptr, uint32_t lcid,
 		return -EFAULT;
 	}
 
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node) {
 		if (lcid == ch->lcid)
 			break;
 	}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 	data_start = get_tx_vaddr(pctx, pctx->size - pctx->size_remaining,
 				  &tx_size);
@@ -1863,12 +1863,12 @@ static int tx_cmd_rx_intent_req(struct glink_transport_if *if_ptr,
 		srcu_read_unlock(&einfo->ssr_sync, rcu_id);
 		return -EFAULT;
 	}
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node) {
 		if (lcid == ch->lcid)
 			break;
 	}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 	einfo->xprt_if.glink_core_if_ptr->rx_cmd_rx_intent_req_ack(
 								&einfo->xprt_if,
 								ch->rcid,
@@ -1918,12 +1918,12 @@ static int tx_cmd_set_sigs(struct glink_transport_if *if_ptr, uint32_t lcid,
 	unsigned long flags;
 
 	einfo = container_of(if_ptr, struct edge_info, xprt_if);
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node) {
 		if (lcid == ch->lcid)
 			break;
 	}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 
 	if (sigs & SMD_DTR_SIG)
 		set |= TIOCM_DTR;
@@ -1964,12 +1964,12 @@ static int poll(struct glink_transport_if *if_ptr, uint32_t lcid)
 	unsigned long flags;
 
 	einfo = container_of(if_ptr, struct edge_info, xprt_if);
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node) {
 		if (lcid == ch->lcid)
 			break;
 	}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 	rc = smd_is_pkt_avail(ch->smd_ch);
 	if (rc == 1)
 		process_data_event((unsigned long)ch);
@@ -1994,12 +1994,12 @@ static int mask_rx_irq(struct glink_transport_if *if_ptr, uint32_t lcid,
 	unsigned long flags;
 
 	einfo = container_of(if_ptr, struct edge_info, xprt_if);
-	spin_lock_irqsave(&einfo->channels_lock, flags);
+	raw_spin_lock_irqsave(&einfo->channels_lock, flags);
 	list_for_each_entry(ch, &einfo->channels, node) {
 		if (lcid == ch->lcid)
 			break;
 	}
-	spin_unlock_irqrestore(&einfo->channels_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->channels_lock, flags);
 	ret = smd_mask_receive_interrupt(ch->smd_ch, mask, pstruct);
 
 	if (ret == 0)
@@ -2082,7 +2082,7 @@ static int __init glink_smd_xprt_init(void)
 		init_xprt_cfg(einfo);
 		init_xprt_if(einfo);
 		INIT_LIST_HEAD(&einfo->channels);
-		spin_lock_init(&einfo->channels_lock);
+		raw_spin_lock_init(&einfo->channels_lock);
 		init_srcu_struct(&einfo->ssr_sync);
 		mutex_init(&einfo->smd_lock);
 		mutex_init(&einfo->in_ssr_lock);
diff --git a/kernel/msm-3.18/drivers/soc/qcom/glink_smem_native_xprt.c b/kernel/msm-3.18/drivers/soc/qcom/glink_smem_native_xprt.c
index 8a62e4ea6..7cbb9fd29 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/glink_smem_native_xprt.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/glink_smem_native_xprt.c
@@ -206,7 +206,7 @@ struct edge_info {
 	uint32_t rx_fifo_size;
 	void * (*read_from_fifo)(void *dest, const void *src, size_t num_bytes);
 	void * (*write_to_fifo)(void *dest, const void *src, size_t num_bytes);
-	spinlock_t write_lock;
+	raw_spinlock_t write_lock;
 	wait_queue_head_t tx_blocked_queue;
 	bool tx_resume_needed;
 	bool tx_blocked_signal_sent;
@@ -216,7 +216,7 @@ struct edge_info {
 	struct tasklet_struct tasklet;
 	struct srcu_struct use_ref;
 	bool in_ssr;
-	spinlock_t rx_lock;
+	raw_spinlock_t rx_lock;
 	struct list_head deferred_cmds;
 	uint32_t num_pw_states;
 	unsigned long *ramp_time_us;
@@ -624,23 +624,23 @@ static int fifo_tx(struct edge_info *einfo, const void *data, int len)
 
 	DEFINE_WAIT(wait);
 
-	spin_lock_irqsave(&einfo->write_lock, flags);
+	raw_spin_lock_irqsave(&einfo->write_lock, flags);
 	while (fifo_write_avail(einfo) < len) {
 		send_tx_blocked_signal(einfo);
-		spin_unlock_irqrestore(&einfo->write_lock, flags);
+		raw_spin_unlock_irqrestore(&einfo->write_lock, flags);
 		prepare_to_wait(&einfo->tx_blocked_queue, &wait,
 							TASK_UNINTERRUPTIBLE);
 		if (fifo_write_avail(einfo) < len && !einfo->in_ssr)
 			schedule();
 		finish_wait(&einfo->tx_blocked_queue, &wait);
-		spin_lock_irqsave(&einfo->write_lock, flags);
+		raw_spin_lock_irqsave(&einfo->write_lock, flags);
 		if (einfo->in_ssr) {
-			spin_unlock_irqrestore(&einfo->write_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->write_lock, flags);
 			return -EFAULT;
 		}
 	}
 	ret = fifo_write(einfo, data, len);
-	spin_unlock_irqrestore(&einfo->write_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->write_lock, flags);
 
 	return ret;
 }
@@ -855,12 +855,12 @@ static void __rx_worker(struct edge_info *einfo, bool atomic_ctx)
 			einfo->xprt_if.glink_core_if_ptr->tx_resume(
 							&einfo->xprt_if);
 		}
-		spin_lock_irqsave(&einfo->write_lock, flags);
+		raw_spin_lock_irqsave(&einfo->write_lock, flags);
 		if (waitqueue_active(&einfo->tx_blocked_queue)) {
 			einfo->tx_blocked_signal_sent = false;
 			trigger_wakeup = true;
 		}
-		spin_unlock_irqrestore(&einfo->write_lock, flags);
+		raw_spin_unlock_irqrestore(&einfo->write_lock, flags);
 		if (trigger_wakeup)
 			wake_up_all(&einfo->tx_blocked_queue);
 	}
@@ -877,7 +877,7 @@ static void __rx_worker(struct edge_info *einfo, bool atomic_ctx)
 	 * all the channel open commands are processed by the core, thus
 	 * eliminating a race.
 	 */
-	spin_lock_irqsave(&einfo->rx_lock, flags);
+	raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 	while (fifo_read_avail(einfo) ||
 			(!atomic_ctx && !list_empty(&einfo->deferred_cmds))) {
 		if (einfo->in_ssr)
@@ -903,24 +903,24 @@ static void __rx_worker(struct edge_info *einfo, bool atomic_ctx)
 				queue_cmd(einfo, &cmd, NULL);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			einfo->xprt_if.glink_core_if_ptr->rx_cmd_version(
 								&einfo->xprt_if,
 								cmd.param1,
 								cmd.param2);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case VERSION_ACK_CMD:
 			if (atomic_ctx) {
 				queue_cmd(einfo, &cmd, NULL);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			einfo->xprt_if.glink_core_if_ptr->rx_cmd_version_ack(
 								&einfo->xprt_if,
 								cmd.param1,
 								cmd.param2);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case OPEN_CMD:
 			rcid = cmd.param1;
@@ -948,38 +948,38 @@ static void __rx_worker(struct edge_info *einfo, bool atomic_ctx)
 				break;
 			}
 
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			einfo->xprt_if.glink_core_if_ptr->rx_cmd_ch_remote_open(
 								&einfo->xprt_if,
 								rcid,
 								name,
 								SMEM_XPRT_ID);
 			kfree(name);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case CLOSE_CMD:
 			if (atomic_ctx) {
 				queue_cmd(einfo, &cmd, NULL);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			einfo->xprt_if.glink_core_if_ptr->
 							rx_cmd_ch_remote_close(
 								&einfo->xprt_if,
 								cmd.param1);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case OPEN_ACK_CMD:
 			if (atomic_ctx) {
 				queue_cmd(einfo, &cmd, NULL);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			einfo->xprt_if.glink_core_if_ptr->rx_cmd_ch_open_ack(
 								&einfo->xprt_if,
 								cmd.param1,
 								SMEM_XPRT_ID);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case RX_INTENT_CMD:
 			/*
@@ -1015,14 +1015,14 @@ static void __rx_worker(struct edge_info *einfo, bool atomic_ctx)
 						kfree(cmd_data);
 					break;
 				}
-				spin_unlock_irqrestore(&einfo->rx_lock, flags);
+				raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 				einfo->xprt_if.glink_core_if_ptr->
 						rx_cmd_remote_rx_intent_put(
 								&einfo->xprt_if,
 								cmd.param1,
 								intent.id,
 								intent.size);
-				spin_lock_irqsave(&einfo->rx_lock, flags);
+				raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 				break;
 			}
 
@@ -1046,7 +1046,7 @@ static void __rx_worker(struct edge_info *einfo, bool atomic_ctx)
 					kfree(intents);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			for (i = 0; i < cmd.param2; ++i) {
 				einfo->xprt_if.glink_core_if_ptr->
 					rx_cmd_remote_rx_intent_put(
@@ -1056,40 +1056,40 @@ static void __rx_worker(struct edge_info *einfo, bool atomic_ctx)
 							intents[i].size);
 			}
 			kfree(intents);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case RX_DONE_CMD:
 			if (atomic_ctx) {
 				queue_cmd(einfo, &cmd, NULL);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			einfo->xprt_if.glink_core_if_ptr->rx_cmd_tx_done(
 								&einfo->xprt_if,
 								cmd.param1,
 								cmd.param2,
 								false);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case RX_INTENT_REQ_CMD:
 			if (atomic_ctx) {
 				queue_cmd(einfo, &cmd, NULL);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			einfo->xprt_if.glink_core_if_ptr->
 						rx_cmd_remote_rx_intent_req(
 								&einfo->xprt_if,
 								cmd.param1,
 								cmd.param2);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case RX_INTENT_REQ_ACK_CMD:
 			if (atomic_ctx) {
 				queue_cmd(einfo, &cmd, NULL);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			granted = false;
 			if (cmd.param2 == 1)
 				granted = true;
@@ -1098,7 +1098,7 @@ static void __rx_worker(struct edge_info *einfo, bool atomic_ctx)
 								&einfo->xprt_if,
 								cmd.param1,
 								granted);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case TX_DATA_CMD:
 		case TX_DATA_CONT_CMD:
@@ -1111,11 +1111,11 @@ static void __rx_worker(struct edge_info *einfo, bool atomic_ctx)
 				queue_cmd(einfo, &cmd, NULL);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			einfo->xprt_if.glink_core_if_ptr->rx_cmd_ch_close_ack(
 								&einfo->xprt_if,
 								cmd.param1);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case READ_NOTIF_CMD:
 			send_irq(einfo);
@@ -1125,32 +1125,32 @@ static void __rx_worker(struct edge_info *einfo, bool atomic_ctx)
 				queue_cmd(einfo, &cmd, NULL);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			einfo->xprt_if.glink_core_if_ptr->rx_cmd_remote_sigs(
 								&einfo->xprt_if,
 								cmd.param1,
 								cmd.param2);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		case RX_DONE_W_REUSE_CMD:
 			if (atomic_ctx) {
 				queue_cmd(einfo, &cmd, NULL);
 				break;
 			}
-			spin_unlock_irqrestore(&einfo->rx_lock, flags);
+			raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 			einfo->xprt_if.glink_core_if_ptr->rx_cmd_tx_done(
 								&einfo->xprt_if,
 								cmd.param1,
 								cmd.param2,
 								true);
-			spin_lock_irqsave(&einfo->rx_lock, flags);
+			raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 			break;
 		default:
 			pr_err("Unrecognized command: %d\n", cmd.id);
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&einfo->rx_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 	srcu_read_unlock(&einfo->use_ref, rcu_id);
 }
 
@@ -1936,12 +1936,12 @@ static int tx_data(struct glink_transport_if *if_ptr, uint16_t cmd_id,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&einfo->write_lock, flags);
+	raw_spin_lock_irqsave(&einfo->write_lock, flags);
 	size = fifo_write_avail(einfo);
 
 	/* Intentless clients expect a complete commit or instant failure */
 	if (einfo->intentless && size < sizeof(cmd) + pctx->size) {
-		spin_unlock_irqrestore(&einfo->write_lock, flags);
+		raw_spin_unlock_irqrestore(&einfo->write_lock, flags);
 		srcu_read_unlock(&einfo->use_ref, rcu_id);
 		return -ENOSPC;
 	}
@@ -1949,7 +1949,7 @@ static int tx_data(struct glink_transport_if *if_ptr, uint16_t cmd_id,
 	/* Need enough space to write the command and some data */
 	if (size <= sizeof(cmd)) {
 		einfo->tx_resume_needed = true;
-		spin_unlock_irqrestore(&einfo->write_lock, flags);
+		raw_spin_unlock_irqrestore(&einfo->write_lock, flags);
 		srcu_read_unlock(&einfo->use_ref, rcu_id);
 		return -EAGAIN;
 	}
@@ -1969,15 +1969,15 @@ static int tx_data(struct glink_transport_if *if_ptr, uint16_t cmd_id,
 	GLINK_DBG("%s %s: lcid[%u] riid[%u] cmd[%d], size[%d], size_left[%d]\n",
 		"<SMEM>", __func__, cmd.lcid, cmd.riid, cmd.id, cmd.size,
 		cmd.size_left);
-	spin_unlock_irqrestore(&einfo->write_lock, flags);
+	raw_spin_unlock_irqrestore(&einfo->write_lock, flags);
 
 	/* Fake tx_done for intentless since its not supported over the wire */
 	if (einfo->intentless) {
-		spin_lock_irqsave(&einfo->rx_lock, flags);
+		raw_spin_lock_irqsave(&einfo->rx_lock, flags);
 		cmd.id = RX_DONE_CMD;
 		cmd.lcid = pctx->rcid;
 		queue_cmd(einfo, &cmd, NULL);
-		spin_unlock_irqrestore(&einfo->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&einfo->rx_lock, flags);
 	}
 
 	srcu_read_unlock(&einfo->use_ref, rcu_id);
@@ -2262,7 +2262,7 @@ static int glink_smem_native_probe(struct platform_device *pdev)
 
 	init_xprt_cfg(einfo, subsys_name);
 	init_xprt_if(einfo);
-	spin_lock_init(&einfo->write_lock);
+	raw_spin_lock_init(&einfo->write_lock);
 	init_waitqueue_head(&einfo->tx_blocked_queue);
 	init_kthread_work(&einfo->kwork, rx_worker);
 	init_kthread_worker(&einfo->kworker);
@@ -2270,7 +2270,7 @@ static int glink_smem_native_probe(struct platform_device *pdev)
 	einfo->read_from_fifo = read_from_fifo;
 	einfo->write_to_fifo = write_to_fifo;
 	init_srcu_struct(&einfo->use_ref);
-	spin_lock_init(&einfo->rx_lock);
+	raw_spin_lock_init(&einfo->rx_lock);
 	INIT_LIST_HEAD(&einfo->deferred_cmds);
 
 	mutex_lock(&probe_lock);
@@ -2449,7 +2449,7 @@ static int glink_rpm_native_probe(struct platform_device *pdev)
 
 	init_xprt_cfg(einfo, subsys_name);
 	init_xprt_if(einfo);
-	spin_lock_init(&einfo->write_lock);
+	raw_spin_lock_init(&einfo->write_lock);
 	init_waitqueue_head(&einfo->tx_blocked_queue);
 	init_kthread_work(&einfo->kwork, rx_worker);
 	init_kthread_worker(&einfo->kworker);
@@ -2458,7 +2458,7 @@ static int glink_rpm_native_probe(struct platform_device *pdev)
 	einfo->read_from_fifo = memcpy32_fromio;
 	einfo->write_to_fifo = memcpy32_toio;
 	init_srcu_struct(&einfo->use_ref);
-	spin_lock_init(&einfo->rx_lock);
+	raw_spin_lock_init(&einfo->rx_lock);
 	INIT_LIST_HEAD(&einfo->deferred_cmds);
 
 	mutex_lock(&probe_lock);
@@ -2740,7 +2740,7 @@ static int glink_mailbox_probe(struct platform_device *pdev)
 	init_xprt_cfg(einfo, subsys_name);
 	einfo->xprt_cfg.name = "mailbox";
 	init_xprt_if(einfo);
-	spin_lock_init(&einfo->write_lock);
+	raw_spin_lock_init(&einfo->write_lock);
 	init_waitqueue_head(&einfo->tx_blocked_queue);
 	init_kthread_work(&einfo->kwork, rx_worker);
 	init_kthread_worker(&einfo->kworker);
@@ -2748,7 +2748,7 @@ static int glink_mailbox_probe(struct platform_device *pdev)
 	einfo->read_from_fifo = read_from_fifo;
 	einfo->write_to_fifo = write_to_fifo;
 	init_srcu_struct(&einfo->use_ref);
-	spin_lock_init(&einfo->rx_lock);
+	raw_spin_lock_init(&einfo->rx_lock);
 	INIT_LIST_HEAD(&einfo->deferred_cmds);
 
 	mutex_lock(&probe_lock);
diff --git a/kernel/msm-3.18/drivers/soc/qcom/glink_ssr.c b/kernel/msm-3.18/drivers/soc/qcom/glink_ssr.c
index 4d94e6446..869204fb6 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/glink_ssr.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/glink_ssr.c
@@ -136,10 +136,10 @@ static void link_state_cb_worker(struct work_struct *work)
 			ch_open_work->transport);
 
 	if (ss_info && ch_open_work->link_state == GLINK_LINK_STATE_UP) {
-		spin_lock_irqsave(&ss_info->link_up_lock, flags);
+		raw_spin_lock_irqsave(&ss_info->link_up_lock, flags);
 		if (!ss_info->link_up) {
 			ss_info->link_up = true;
-			spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+			raw_spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
 			if (!configure_and_open_channel(ss_info)) {
 				glink_unregister_link_state_cb(
 						ss_info->link_state_handle);
@@ -148,12 +148,12 @@ static void link_state_cb_worker(struct work_struct *work)
 			kfree(ch_open_work);
 			return;
 		}
-		spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+		raw_spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
 	} else {
 		if (ss_info) {
-			spin_lock_irqsave(&ss_info->link_up_lock, flags);
+			raw_spin_lock_irqsave(&ss_info->link_up_lock, flags);
 			ss_info->link_up = false;
-			spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+			raw_spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
 			ss_info->handle = NULL;
 		} else {
 			GLINK_SSR_ERR("<SSR> %s: ss_info is NULL\n", __func__);
@@ -322,9 +322,9 @@ void close_ch_worker(struct work_struct *work)
 	ss_info = get_info_for_edge(close_work->edge);
 	BUG_ON(!ss_info);
 
-	spin_lock_irqsave(&ss_info->link_up_lock, flags);
+	raw_spin_lock_irqsave(&ss_info->link_up_lock, flags);
 	ss_info->link_up = false;
-	spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+	raw_spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
 
 	BUG_ON(ss_info->link_state_handle != NULL);
 	link_state_handle = glink_register_link_state_cb(ss_info->link_info,
@@ -508,7 +508,7 @@ int notify_for_subsystem(struct subsys_info *ss_info)
 		handle = ss_info_channel->handle;
 		ss_leaf_entry->cb_data = ss_info_channel->cb_data;
 
-		spin_lock_irqsave(&ss_info->link_up_lock, flags);
+		raw_spin_lock_irqsave(&ss_info->link_up_lock, flags);
 		if (IS_ERR_OR_NULL(ss_info_channel->handle) ||
 				!ss_info_channel->cb_data ||
 				!ss_info_channel->link_up ||
@@ -523,11 +523,11 @@ int notify_for_subsystem(struct subsys_info *ss_info)
 				ss_info_channel->handle, "link_up",
 				ss_info_channel->link_up);
 
-			spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+			raw_spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
 			atomic_dec(&responses_remaining);
 			continue;
 		}
-		spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
+		raw_spin_unlock_irqrestore(&ss_info->link_up_lock, flags);
 
 		do_cleanup_data = kmalloc(sizeof(struct do_cleanup_msg),
 				GFP_KERNEL);
@@ -874,7 +874,7 @@ static int glink_ssr_probe(struct platform_device *pdev)
 	ss_info->handle = NULL;
 	ss_info->link_state_handle = NULL;
 	ss_info->cb_data = NULL;
-	spin_lock_init(&ss_info->link_up_lock);
+	raw_spin_lock_init(&ss_info->link_up_lock);
 
 	nb = kmalloc(sizeof(struct restart_notifier_block), GFP_KERNEL);
 	if (!nb) {
diff --git a/kernel/msm-3.18/drivers/soc/qcom/icnss.c b/kernel/msm-3.18/drivers/soc/qcom/icnss.c
index caa432812..8872c7c8b 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/icnss.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/icnss.c
@@ -94,7 +94,7 @@ static struct {
 	void __iomem *mem_base_va;
 	struct qmi_handle *wlfw_clnt;
 	struct list_head qmi_event_list;
-	spinlock_t qmi_event_lock;
+	raw_spinlock_t qmi_event_lock;
 	struct work_struct qmi_event_work;
 	struct work_struct qmi_recv_msg_work;
 	struct workqueue_struct *qmi_event_wq;
@@ -129,9 +129,9 @@ static int icnss_qmi_event_post(enum icnss_qmi_event_type type, void *data)
 
 	event->type = type;
 	event->data = data;
-	spin_lock_irqsave(&penv->qmi_event_lock, flags);
+	raw_spin_lock_irqsave(&penv->qmi_event_lock, flags);
 	list_add_tail(&event->list, &penv->qmi_event_list);
-	spin_unlock_irqrestore(&penv->qmi_event_lock, flags);
+	raw_spin_unlock_irqrestore(&penv->qmi_event_lock, flags);
 
 	queue_work(penv->qmi_event_wq, &penv->qmi_event_work);
 
@@ -869,13 +869,13 @@ static void icnss_qmi_wlfw_event_work(struct work_struct *work)
 	struct icnss_qmi_event *event;
 	unsigned long flags;
 
-	spin_lock_irqsave(&penv->qmi_event_lock, flags);
+	raw_spin_lock_irqsave(&penv->qmi_event_lock, flags);
 
 	while (!list_empty(&penv->qmi_event_list)) {
 		event = list_first_entry(&penv->qmi_event_list,
 					 struct icnss_qmi_event, list);
 		list_del(&event->list);
-		spin_unlock_irqrestore(&penv->qmi_event_lock, flags);
+		raw_spin_unlock_irqrestore(&penv->qmi_event_lock, flags);
 
 		switch (event->type) {
 		case ICNSS_QMI_EVENT_SERVER_ARRIVE:
@@ -893,9 +893,9 @@ static void icnss_qmi_wlfw_event_work(struct work_struct *work)
 			break;
 		}
 		kfree(event);
-		spin_lock_irqsave(&penv->qmi_event_lock, flags);
+		raw_spin_lock_irqsave(&penv->qmi_event_lock, flags);
 	}
-	spin_unlock_irqrestore(&penv->qmi_event_lock, flags);
+	raw_spin_unlock_irqrestore(&penv->qmi_event_lock, flags);
 }
 
 static struct notifier_block wlfw_clnt_nb = {
@@ -1446,7 +1446,7 @@ static int icnss_probe(struct platform_device *pdev)
 		goto err_wlan_mode;
 	}
 
-	spin_lock_init(&penv->qmi_event_lock);
+	raw_spin_lock_init(&penv->qmi_event_lock);
 
 	penv->qmi_event_wq = alloc_workqueue("icnss_qmi_event", 0, 0);
 	if (!penv->qmi_event_wq) {
diff --git a/kernel/msm-3.18/drivers/soc/qcom/ipc_router_mhi_xprt.c b/kernel/msm-3.18/drivers/soc/qcom/ipc_router_mhi_xprt.c
index adf407881..283b4945c 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/ipc_router_mhi_xprt.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/ipc_router_mhi_xprt.c
@@ -123,9 +123,9 @@ struct ipc_router_mhi_xprt {
 	struct completion sft_close_complete;
 	unsigned xprt_version;
 	unsigned xprt_option;
-	spinlock_t tx_addr_map_list_lock;
+	raw_spinlock_t tx_addr_map_list_lock;
 	struct list_head tx_addr_map_list;
-	spinlock_t rx_addr_map_list_lock;
+	raw_spinlock_t rx_addr_map_list_lock;
 	struct list_head rx_addr_map_list;
 };
 
@@ -178,7 +178,7 @@ void ipc_router_mhi_release_pkt(struct kref *ref)
  * Return: The mapped virtual Address if found, NULL otherwise.
  */
 void *ipc_router_mhi_xprt_find_addr_map(struct list_head *addr_map_list,
-				spinlock_t *addr_map_list_lock, void *addr)
+				raw_spinlock_t *addr_map_list_lock, void *addr)
 {
 	struct ipc_router_mhi_addr_map *addr_mapping;
 	struct ipc_router_mhi_addr_map *tmp_addr_mapping;
@@ -187,7 +187,7 @@ void *ipc_router_mhi_xprt_find_addr_map(struct list_head *addr_map_list,
 
 	if (!addr_map_list || !addr_map_list_lock)
 		return NULL;
-	spin_lock_irqsave(addr_map_list_lock, flags);
+	raw_spin_lock_irqsave(addr_map_list_lock, flags);
 	list_for_each_entry_safe(addr_mapping, tmp_addr_mapping,
 				addr_map_list, list_node) {
 		if (addr_mapping->virt_addr == addr) {
@@ -197,11 +197,11 @@ void *ipc_router_mhi_xprt_find_addr_map(struct list_head *addr_map_list,
 				kref_put(&addr_mapping->pkt->ref,
 					ipc_router_mhi_release_pkt);
 			kfree(addr_mapping);
-			spin_unlock_irqrestore(addr_map_list_lock, flags);
+			raw_spin_unlock_irqrestore(addr_map_list_lock, flags);
 			return virt_addr;
 		}
 	}
-	spin_unlock_irqrestore(addr_map_list_lock, flags);
+	raw_spin_unlock_irqrestore(addr_map_list_lock, flags);
 	IPC_RTR_ERR(
 		"%s: Virtual address mapping [%p] not found\n",
 		__func__, (void *)addr);
@@ -218,7 +218,7 @@ void *ipc_router_mhi_xprt_find_addr_map(struct list_head *addr_map_list,
  * Return: 0 on success, standard Linux error code otherwise.
  */
 int ipc_router_mhi_xprt_add_addr_map(struct list_head *addr_map_list,
-				spinlock_t *addr_map_list_lock,
+				raw_spinlock_t *addr_map_list_lock,
 				struct rr_packet *pkt, void *virt_addr)
 {
 	struct ipc_router_mhi_addr_map *addr_mapping;
@@ -231,11 +231,11 @@ int ipc_router_mhi_xprt_add_addr_map(struct list_head *addr_map_list,
 		return -ENOMEM;
 	addr_mapping->virt_addr = virt_addr;
 	addr_mapping->pkt = pkt;
-	spin_lock_irqsave(addr_map_list_lock, flags);
+	raw_spin_lock_irqsave(addr_map_list_lock, flags);
 	if (addr_mapping->pkt)
 		kref_get(&addr_mapping->pkt->ref);
 	list_add_tail(&addr_mapping->list_node, addr_map_list);
-	spin_unlock_irqrestore(addr_map_list_lock, flags);
+	raw_spin_unlock_irqrestore(addr_map_list_lock, flags);
 	return 0;
 }
 
@@ -871,9 +871,9 @@ static int ipc_router_mhi_config_init(
 	mhi_xprtp->ch_hndl.num_trbs = IPC_ROUTER_MHI_XPRT_NUM_TRBS;
 	mhi_xprtp->ch_hndl.mhi_xprtp = mhi_xprtp;
 	INIT_LIST_HEAD(&mhi_xprtp->tx_addr_map_list);
-	spin_lock_init(&mhi_xprtp->tx_addr_map_list_lock);
+	raw_spin_lock_init(&mhi_xprtp->tx_addr_map_list_lock);
 	INIT_LIST_HEAD(&mhi_xprtp->rx_addr_map_list);
-	spin_lock_init(&mhi_xprtp->rx_addr_map_list_lock);
+	raw_spin_lock_init(&mhi_xprtp->rx_addr_map_list_lock);
 
 	rc = ipc_router_mhi_driver_register(mhi_xprtp, dev);
 	return rc;
diff --git a/kernel/msm-3.18/drivers/soc/qcom/ipc_router_smd_xprt.c b/kernel/msm-3.18/drivers/soc/qcom/ipc_router_smd_xprt.c
index a94e81556..08be6c413 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/ipc_router_smd_xprt.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/ipc_router_smd_xprt.c
@@ -82,7 +82,7 @@ struct msm_ipc_router_smd_xprt {
 	struct rr_packet *in_pkt;
 	int is_partial_in_pkt;
 	struct delayed_work read_work;
-	spinlock_t ss_reset_lock;	/*Subsystem reset lock*/
+	raw_spinlock_t ss_reset_lock;	/*Subsystem reset lock*/
 	int ss_reset;
 	void *pil;
 	struct completion sft_close_complete;
@@ -202,15 +202,15 @@ static int msm_ipc_router_smd_remote_write(void *data,
 		return -EINVAL;
 
 	do {
-		spin_lock_irqsave(&smd_xprtp->ss_reset_lock, flags);
+		raw_spin_lock_irqsave(&smd_xprtp->ss_reset_lock, flags);
 		if (smd_xprtp->ss_reset) {
-			spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock,
+			raw_spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock,
 						flags);
 			IPC_RTR_ERR("%s: %s chnl reset\n",
 					__func__, xprt->name);
 			return -ENETRESET;
 		}
-		spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock, flags);
+		raw_spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock, flags);
 		ret = smd_write_start(smd_xprtp->channel, len);
 		if (ret < 0 && num_retries >= 5) {
 			IPC_RTR_ERR("%s: Error %d @smd_write_start for %s\n",
@@ -233,15 +233,15 @@ static int msm_ipc_router_smd_remote_write(void *data,
 				(smd_write_segment_avail(smd_xprtp->channel) ||
 				smd_xprtp->ss_reset));
 			smd_disable_read_intr(smd_xprtp->channel);
-			spin_lock_irqsave(&smd_xprtp->ss_reset_lock, flags);
+			raw_spin_lock_irqsave(&smd_xprtp->ss_reset_lock, flags);
 			if (smd_xprtp->ss_reset) {
-				spin_unlock_irqrestore(
+				raw_spin_unlock_irqrestore(
 					&smd_xprtp->ss_reset_lock, flags);
 				IPC_RTR_ERR("%s: %s chnl reset\n",
 					__func__, xprt->name);
 				return -ENETRESET;
 			}
-			spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock,
+			raw_spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock,
 						flags);
 
 			sz_written = smd_write_segment(smd_xprtp->channel,
@@ -291,9 +291,9 @@ static void smd_xprt_read_data(struct work_struct *work)
 	struct msm_ipc_router_smd_xprt *smd_xprtp =
 		container_of(rwork, struct msm_ipc_router_smd_xprt, read_work);
 
-	spin_lock_irqsave(&smd_xprtp->ss_reset_lock, flags);
+	raw_spin_lock_irqsave(&smd_xprtp->ss_reset_lock, flags);
 	if (smd_xprtp->ss_reset) {
-		spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock, flags);
+		raw_spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock, flags);
 		if (smd_xprtp->in_pkt)
 			release_pkt(smd_xprtp->in_pkt);
 		smd_xprtp->is_partial_in_pkt = 0;
@@ -301,7 +301,7 @@ static void smd_xprt_read_data(struct work_struct *work)
 			__func__, smd_xprtp->xprt.name);
 		return;
 	}
-	spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock, flags);
 
 	D("%s pkt_size: %d, read_avail: %d\n", __func__,
 		smd_cur_packet_size(smd_xprtp->channel),
@@ -379,9 +379,9 @@ static void smd_xprt_open_event(struct work_struct *work)
 			     struct msm_ipc_router_smd_xprt, xprt);
 	unsigned long flags;
 
-	spin_lock_irqsave(&smd_xprtp->ss_reset_lock, flags);
+	raw_spin_lock_irqsave(&smd_xprtp->ss_reset_lock, flags);
 	smd_xprtp->ss_reset = 0;
-	spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock, flags);
 	msm_ipc_router_xprt_notify(xprt_work->xprt,
 				IPC_ROUTER_XPRT_EVENT_OPEN, NULL);
 	D("%s: Notified IPC Router of %s OPEN\n",
@@ -445,9 +445,9 @@ static void msm_ipc_router_smd_remote_notify(void *_dev, unsigned event)
 		break;
 
 	case SMD_EVENT_CLOSE:
-		spin_lock_irqsave(&smd_xprtp->ss_reset_lock, flags);
+		raw_spin_lock_irqsave(&smd_xprtp->ss_reset_lock, flags);
 		smd_xprtp->ss_reset = 1;
-		spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock, flags);
+		raw_spin_unlock_irqrestore(&smd_xprtp->ss_reset_lock, flags);
 		wake_up(&smd_xprtp->write_avail_wait_q);
 		xprt_work = kmalloc(sizeof(struct msm_ipc_router_smd_xprt_work),
 				    GFP_ATOMIC);
@@ -688,7 +688,7 @@ static int msm_ipc_router_smd_config_init(
 	smd_xprtp->in_pkt = NULL;
 	smd_xprtp->is_partial_in_pkt = 0;
 	INIT_DELAYED_WORK(&smd_xprtp->read_work, smd_xprt_read_data);
-	spin_lock_init(&smd_xprtp->ss_reset_lock);
+	raw_spin_lock_init(&smd_xprtp->ss_reset_lock);
 	smd_xprtp->ss_reset = 0;
 
 	msm_ipc_router_smd_driver_register(smd_xprtp);
diff --git a/kernel/msm-3.18/drivers/soc/qcom/jtag-mm.c b/kernel/msm-3.18/drivers/soc/qcom/jtag-mm.c
index 3b947abe9..f667e1c69 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/jtag-mm.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/jtag-mm.c
@@ -209,7 +209,7 @@ struct dbg_ctx {
 	bool			enable;
 	void __iomem		*base;
 	uint32_t		*state;
-	spinlock_t		spinlock;
+	raw_spinlock_t		spinlock;
 	struct mutex		mutex;
 };
 static struct dbg_ctx *dbg[NR_CPUS];
@@ -229,7 +229,7 @@ struct etm_ctx {
 	void __iomem		*base;
 	struct device		*dev;
 	uint32_t		*state;
-	spinlock_t		spinlock;
+	raw_spinlock_t		spinlock;
 	struct mutex		mutex;
 };
 
@@ -688,12 +688,12 @@ static int jtag_mm_etm_callback(struct notifier_block *nfb,
 
 	switch (action & (~CPU_TASKS_FROZEN)) {
 	case CPU_STARTING:
-		spin_lock(&etm[cpu]->spinlock);
+		raw_spin_lock(&etm[cpu]->spinlock);
 		if (!etm[cpu]->init) {
 			etm_init_arch_data(etm[cpu]);
 			etm[cpu]->init = true;
 		}
-		spin_unlock(&etm[cpu]->spinlock);
+		raw_spin_unlock(&etm[cpu]->spinlock);
 		break;
 
 	case CPU_ONLINE:
@@ -752,7 +752,7 @@ static int jtag_mm_etm_probe(struct platform_device *pdev, uint32_t cpu)
 	if (!etmdata->state)
 		return -ENOMEM;
 
-	spin_lock_init(&etmdata->spinlock);
+	raw_spin_lock_init(&etmdata->spinlock);
 	mutex_init(&etmdata->mutex);
 
 	if (cnt == 0)
@@ -812,12 +812,12 @@ static int jtag_mm_dbg_callback(struct notifier_block *nfb,
 
 	switch (action & (~CPU_TASKS_FROZEN)) {
 	case CPU_STARTING:
-		spin_lock(&dbg[cpu]->spinlock);
+		raw_spin_lock(&dbg[cpu]->spinlock);
 		if (!dbg[cpu]->init) {
 			dbg_init_arch_data(dbg[cpu]);
 			dbg[cpu]->init = true;
 		}
-		spin_unlock(&dbg[cpu]->spinlock);
+		raw_spin_unlock(&dbg[cpu]->spinlock);
 		break;
 
 	case CPU_ONLINE:
@@ -876,7 +876,7 @@ static int jtag_mm_dbg_probe(struct platform_device *pdev,
 	if (!dbgdata->state)
 		return -ENOMEM;
 
-	spin_lock_init(&dbgdata->spinlock);
+	raw_spin_lock_init(&dbgdata->spinlock);
 	mutex_init(&dbgdata->mutex);
 
 	if (cnt == 0)
diff --git a/kernel/msm-3.18/drivers/soc/qcom/jtagv8-etm.c b/kernel/msm-3.18/drivers/soc/qcom/jtagv8-etm.c
index f6536d365..134c0a106 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/jtagv8-etm.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/jtagv8-etm.c
@@ -230,7 +230,7 @@ struct etm_ctx {
 	void __iomem		*base;
 	struct device		*dev;
 	uint64_t		*state;
-	spinlock_t		spinlock;
+	raw_spinlock_t		spinlock;
 	struct mutex		mutex;
 };
 
@@ -1511,12 +1511,12 @@ static int jtag_mm_etm_callback(struct notifier_block *nfb,
 
 	switch (action & (~CPU_TASKS_FROZEN)) {
 	case CPU_STARTING:
-		spin_lock(&etm[cpu]->spinlock);
+		raw_spin_lock(&etm[cpu]->spinlock);
 		if (!etm[cpu]->init) {
 			etm_init_arch_data(etm[cpu]);
 			etm[cpu]->init = true;
 		}
-		spin_unlock(&etm[cpu]->spinlock);
+		raw_spin_unlock(&etm[cpu]->spinlock);
 		break;
 
 	case CPU_ONLINE:
@@ -1598,7 +1598,7 @@ static int jtag_mm_etm_probe(struct platform_device *pdev, uint32_t cpu)
 	if (!etmdata->state)
 		return -ENOMEM;
 
-	spin_lock_init(&etmdata->spinlock);
+	raw_spin_lock_init(&etmdata->spinlock);
 	mutex_init(&etmdata->mutex);
 
 	if (cnt++ == 0)
diff --git a/kernel/msm-3.18/drivers/soc/qcom/mpm-of.c b/kernel/msm-3.18/drivers/soc/qcom/mpm-of.c
index 97e03253b..b13366783 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/mpm-of.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/mpm-of.c
@@ -385,10 +385,10 @@ static int __msm_mpm_enable_irq(struct irq_data *d, bool enable)
 	if (!msm_mpm_is_initialized())
 		return -EINVAL;
 
-	spin_lock_irqsave(&msm_mpm_lock, flags);
+	raw_spin_lock_irqsave(&msm_mpm_lock, flags);
 
 	rc = msm_mpm_enable_irq_exclusive(d, enable, false);
-	spin_unlock_irqrestore(&msm_mpm_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_mpm_lock, flags);
 
 	return rc;
 }
@@ -411,9 +411,9 @@ static int msm_mpm_set_irq_wake(struct irq_data *d, unsigned int on)
 	if (!msm_mpm_is_initialized())
 		return -EINVAL;
 
-	spin_lock_irqsave(&msm_mpm_lock, flags);
+	raw_spin_lock_irqsave(&msm_mpm_lock, flags);
 	rc = msm_mpm_enable_irq_exclusive(d, (bool)on, true);
-	spin_unlock_irqrestore(&msm_mpm_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_mpm_lock, flags);
 
 	return rc;
 }
@@ -426,9 +426,9 @@ static int msm_mpm_set_irq_type(struct irq_data *d, unsigned int flow_type)
 	if (!msm_mpm_is_initialized())
 		return -EINVAL;
 
-	spin_lock_irqsave(&msm_mpm_lock, flags);
+	raw_spin_lock_irqsave(&msm_mpm_lock, flags);
 	rc = msm_mpm_set_irq_type_exclusive(d, flow_type);
-	spin_unlock_irqrestore(&msm_mpm_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_mpm_lock, flags);
 
 	return rc;
 }
@@ -448,14 +448,14 @@ int msm_mpm_enable_pin(unsigned int pin, unsigned int enable)
 	if (pin >= MSM_MPM_NR_MPM_IRQS)
 		return -EINVAL;
 
-	spin_lock_irqsave(&msm_mpm_lock, flags);
+	raw_spin_lock_irqsave(&msm_mpm_lock, flags);
 
 	if (enable)
 		msm_mpm_enabled_irq[index] |= mask;
 	else
 		msm_mpm_enabled_irq[index] &= ~mask;
 
-	spin_unlock_irqrestore(&msm_mpm_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_mpm_lock, flags);
 	return 0;
 }
 
@@ -471,14 +471,14 @@ int msm_mpm_set_pin_wake(unsigned int pin, unsigned int on)
 	if (pin >= MSM_MPM_NR_MPM_IRQS)
 		return -EINVAL;
 
-	spin_lock_irqsave(&msm_mpm_lock, flags);
+	raw_spin_lock_irqsave(&msm_mpm_lock, flags);
 
 	if (on)
 		msm_mpm_wake_irq[index] |= mask;
 	else
 		msm_mpm_wake_irq[index] &= ~mask;
 
-	spin_unlock_irqrestore(&msm_mpm_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_mpm_lock, flags);
 	return 0;
 }
 
@@ -494,7 +494,7 @@ int msm_mpm_set_pin_type(unsigned int pin, unsigned int flow_type)
 	if (pin >= MSM_MPM_NR_MPM_IRQS)
 		return -EINVAL;
 
-	spin_lock_irqsave(&msm_mpm_lock, flags);
+	raw_spin_lock_irqsave(&msm_mpm_lock, flags);
 
 	msm_mpm_set_edge_ctl(pin, flow_type);
 
@@ -503,7 +503,7 @@ int msm_mpm_set_pin_type(unsigned int pin, unsigned int flow_type)
 	else
 		msm_mpm_polarity[index] &= ~mask;
 
-	spin_unlock_irqrestore(&msm_mpm_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_mpm_lock, flags);
 	return 0;
 }
 
@@ -650,13 +650,13 @@ void msm_mpm_suspend_prepare(void)
 	bool allow;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_mpm_lock, flags);
+	raw_spin_lock_irqsave(&msm_mpm_lock, flags);
 
 	allow = msm_mpm_irqs_detectable(false) &&
 		msm_mpm_gpio_irqs_detectable(false);
 	msm_mpm_in_suspend = true;
 
-	spin_unlock_irqrestore(&msm_mpm_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_mpm_lock, flags);
 	msm_mpm_sys_low_power_modes(allow);
 }
 EXPORT_SYMBOL(msm_mpm_suspend_prepare);
@@ -666,12 +666,12 @@ void msm_mpm_suspend_wake(void)
 	bool allow;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_mpm_lock, flags);
+	raw_spin_lock_irqsave(&msm_mpm_lock, flags);
 
 	allow = msm_mpm_irqs_detectable(true) &&
 		msm_mpm_gpio_irqs_detectable(true);
 
-	spin_unlock_irqrestore(&msm_mpm_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_mpm_lock, flags);
 	msm_mpm_sys_low_power_modes(allow);
 	msm_mpm_in_suspend = false;
 }
@@ -683,15 +683,15 @@ static void msm_mpm_work_fn(struct work_struct *work)
 	while (1) {
 		bool allow;
 		wait_for_completion(&wake_wq);
-		spin_lock_irqsave(&msm_mpm_lock, flags);
+		raw_spin_lock_irqsave(&msm_mpm_lock, flags);
 		allow = msm_mpm_irqs_detectable(true) &&
 				msm_mpm_gpio_irqs_detectable(true);
 		if (msm_mpm_in_suspend) {
-			spin_unlock_irqrestore(&msm_mpm_lock, flags);
+			raw_spin_unlock_irqrestore(&msm_mpm_lock, flags);
 			continue;
 		}
 
-		spin_unlock_irqrestore(&msm_mpm_lock, flags);
+		raw_spin_unlock_irqrestore(&msm_mpm_lock, flags);
 		msm_mpm_sys_low_power_modes(allow);
 	}
 }
diff --git a/kernel/msm-3.18/drivers/soc/qcom/msm_glink_pkt.c b/kernel/msm-3.18/drivers/soc/qcom/msm_glink_pkt.c
index 9490882b6..db6a18bd6 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/msm_glink_pkt.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/msm_glink_pkt.c
@@ -124,11 +124,11 @@ struct glink_pkt_dev {
 	wait_queue_head_t ch_opened_wait_queue;
 	wait_queue_head_t ch_closed_wait_queue;
 	struct list_head pkt_list;
-	spinlock_t pkt_list_lock;
+	raw_spinlock_t pkt_list_lock;
 
 	struct wakeup_source pa_ws;	/* Packet Arrival Wakeup Source */
 	struct work_struct packet_arrival_work;
-	spinlock_t pa_spinlock;
+	raw_spinlock_t pa_spinlock;
 	int ws_locked;
 	int sigs_updated;
 	int open_time_wait;
@@ -298,7 +298,7 @@ static void packet_arrival_worker(struct work_struct *work)
 	devp = container_of(work, struct glink_pkt_dev,
 				    packet_arrival_work);
 	mutex_lock(&devp->ch_lock);
-	spin_lock_irqsave(&devp->pa_spinlock, flags);
+	raw_spin_lock_irqsave(&devp->pa_spinlock, flags);
 	if (devp->ws_locked) {
 		GLINK_PKT_INFO("%s locking glink_pkt_dev id:%d wakeup source\n",
 			__func__, devp->i);
@@ -308,7 +308,7 @@ static void packet_arrival_worker(struct work_struct *work)
 		 */
 		__pm_wakeup_event(&devp->pa_ws, WAKEUPSOURCE_TIMEOUT);
 	}
-	spin_unlock_irqrestore(&devp->pa_spinlock, flags);
+	raw_spin_unlock_irqrestore(&devp->pa_spinlock, flags);
 	mutex_unlock(&devp->ch_lock);
 }
 
@@ -370,14 +370,14 @@ void glink_pkt_notify_rx(void *handle, const void *priv,
 	pkt->data = ptr;
 	pkt->pkt_priv = pkt_priv;
 	pkt->size = size;
-	spin_lock_irqsave(&devp->pkt_list_lock, flags);
+	raw_spin_lock_irqsave(&devp->pkt_list_lock, flags);
 	list_add_tail(&pkt->list, &devp->pkt_list);
-	spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
+	raw_spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
 
-	spin_lock_irqsave(&devp->pa_spinlock, flags);
+	raw_spin_lock_irqsave(&devp->pa_spinlock, flags);
 	__pm_stay_awake(&devp->pa_ws);
 	devp->ws_locked = 1;
-	spin_unlock_irqrestore(&devp->pa_spinlock, flags);
+	raw_spin_unlock_irqrestore(&devp->pa_spinlock, flags);
 	wake_up(&devp->ch_read_wait_queue);
 	schedule_work(&devp->packet_arrival_work);
 	return;
@@ -457,10 +457,10 @@ bool glink_pkt_rmt_rx_intent_req_cb(void *handle, const void *priv, size_t sz)
 	GLINK_PKT_INFO("%s(): QUEUE RX INTENT to receive size[%zu]\n",
 		   __func__, sz);
 	if (devp->auto_intent_enabled) {
-		spin_lock_irqsave(&devp->pkt_list_lock, flags);
+		raw_spin_lock_irqsave(&devp->pkt_list_lock, flags);
 		list_for_each_entry(pkt, &devp->pkt_list, list)
 			pending_pkt_count++;
-		spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
+		raw_spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
 		if (pending_pkt_count > MAX_PENDING_GLINK_PKT) {
 			GLINK_PKT_ERR("%s failed, max limit reached\n",
 					__func__);
@@ -593,9 +593,9 @@ static bool glink_pkt_read_avail(struct glink_pkt_dev *devp)
 	bool list_is_empty;
 	unsigned long flags;
 
-	spin_lock_irqsave(&devp->pkt_list_lock, flags);
+	raw_spin_lock_irqsave(&devp->pkt_list_lock, flags);
 	list_is_empty = list_empty(&devp->pkt_list);
-	spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
+	raw_spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
 	return !list_is_empty;
 }
 
@@ -676,25 +676,25 @@ ssize_t glink_pkt_read(struct file *file,
 		return ret;
 	}
 
-	spin_lock_irqsave(&devp->pkt_list_lock, flags);
+	raw_spin_lock_irqsave(&devp->pkt_list_lock, flags);
 	pkt = list_first_entry(&devp->pkt_list, struct glink_rx_pkt, list);
 	if (pkt->size > count) {
 		GLINK_PKT_ERR("%s: Small Buff on dev Id:%d-[%zu > %zu]\n",
 				__func__, devp->i, pkt->size, count);
-		spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
+		raw_spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
 		return -ETOOSMALL;
 	}
 	list_del(&pkt->list);
-	spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
+	raw_spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
 
 	ret = copy_to_user(buf, pkt->data, pkt->size);
 	 if (ret) {
 		GLINK_PKT_ERR(
 		"%s copy_to_user failed ret[%d] on dev id:%d size %zu\n",
 		 __func__, ret, devp->i, pkt->size);
-		spin_lock_irqsave(&devp->pkt_list_lock, flags);
+		raw_spin_lock_irqsave(&devp->pkt_list_lock, flags);
 		list_add_tail(&pkt->list, &devp->pkt_list);
-		spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
+		raw_spin_unlock_irqrestore(&devp->pkt_list_lock, flags);
 		return -EFAULT;
 	}
 
@@ -704,7 +704,7 @@ ssize_t glink_pkt_read(struct file *file,
 	kfree(pkt);
 
 	mutex_lock(&devp->ch_lock);
-	spin_lock_irqsave(&devp->pa_spinlock, flags);
+	raw_spin_lock_irqsave(&devp->pa_spinlock, flags);
 	if (devp->poll_mode && !glink_pkt_read_avail(devp)) {
 		__pm_relax(&devp->pa_ws);
 		devp->ws_locked = 0;
@@ -712,7 +712,7 @@ ssize_t glink_pkt_read(struct file *file,
 		GLINK_PKT_INFO("%s unlocked pkt_dev id:%d wakeup_source\n",
 			__func__, devp->i);
 	}
-	spin_unlock_irqrestore(&devp->pa_spinlock, flags);
+	raw_spin_unlock_irqrestore(&devp->pa_spinlock, flags);
 	mutex_unlock(&devp->ch_lock);
 
 	GLINK_PKT_INFO("End %s on glink_pkt_dev id:%d ret[%d]\n",
@@ -1116,12 +1116,12 @@ int glink_pkt_release(struct inode *inode, struct file *file)
 			mutex_lock(&devp->ch_lock);
 		}
 		devp->poll_mode = 0;
-		spin_lock_irqsave(&devp->pa_spinlock, flags);
+		raw_spin_lock_irqsave(&devp->pa_spinlock, flags);
 		if (devp->ws_locked) {
 			__pm_relax(&devp->pa_ws);
 			devp->ws_locked = 0;
 		}
-		spin_unlock_irqrestore(&devp->pa_spinlock, flags);
+		raw_spin_unlock_irqrestore(&devp->pa_spinlock, flags);
 		devp->sigs_updated = false;
 		devp->in_reset = 0;
 	}
@@ -1178,9 +1178,9 @@ static int glink_pkt_init_add_device(struct glink_pkt_dev *devp, int i)
 	init_waitqueue_head(&devp->ch_read_wait_queue);
 	init_waitqueue_head(&devp->ch_opened_wait_queue);
 	init_waitqueue_head(&devp->ch_closed_wait_queue);
-	spin_lock_init(&devp->pa_spinlock);
+	raw_spin_lock_init(&devp->pa_spinlock);
 	INIT_LIST_HEAD(&devp->pkt_list);
-	spin_lock_init(&devp->pkt_list_lock);
+	raw_spin_lock_init(&devp->pkt_list_lock);
 	wakeup_source_init(&devp->pa_ws, devp->dev_name);
 	INIT_WORK(&devp->packet_arrival_work, packet_arrival_worker);
 
diff --git a/kernel/msm-3.18/drivers/soc/qcom/msm_performance.c b/kernel/msm-3.18/drivers/soc/qcom/msm_performance.c
index 71f281071..6407c3bc9 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/msm_performance.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/msm_performance.c
@@ -43,7 +43,7 @@ struct cluster {
 	u64 last_io_check_ts;
 	unsigned int iowait_enter_cycle_cnt;
 	unsigned int iowait_exit_cycle_cnt;
-	spinlock_t iowait_lock;
+	raw_spinlock_t iowait_lock;
 	unsigned int cur_io_busy;
 	bool io_change;
 	/* CPU */
@@ -54,14 +54,14 @@ struct cluster {
 	unsigned int single_exit_cycle_cnt;
 	unsigned int multi_enter_cycle_cnt;
 	unsigned int multi_exit_cycle_cnt;
-	spinlock_t mode_lock;
+	raw_spinlock_t mode_lock;
 	/* Perf Cluster Peak Loads */
 	unsigned int perf_cl_peak;
 	u64 last_perf_cl_check_ts;
 	bool perf_cl_detect_state_change;
 	unsigned int perf_cl_peak_enter_cycle_cnt;
 	unsigned int perf_cl_peak_exit_cycle_cnt;
-	spinlock_t perf_cl_peak_lock;
+	raw_spinlock_t perf_cl_peak_lock;
 	/* Tunables */
 	unsigned int single_enter_load;
 	unsigned int pcpu_multi_enter_load;
@@ -76,7 +76,7 @@ struct cluster {
 	unsigned int perf_cl_peak_enter_cycles;
 	unsigned int perf_cl_peak_exit_cycles;
 	unsigned int current_freq;
-	spinlock_t timer_lock;
+	raw_spinlock_t timer_lock;
 	unsigned int timer_rate;
 	struct timer_list mode_exit_timer;
 	struct timer_list perf_cl_peak_mode_exit_timer;
@@ -132,7 +132,7 @@ struct load_stats {
 static DEFINE_PER_CPU(struct load_stats, cpu_load_stats);
 
 struct events {
-	spinlock_t cpu_hotplug_lock;
+	raw_spinlock_t cpu_hotplug_lock;
 	bool cpu_hotplug;
 	bool init_success;
 };
@@ -1402,36 +1402,36 @@ static int set_workload_detect(const char *buf, const struct kernel_param *kp)
 	if (!(workload_detect & IO_DETECT)) {
 		for (i = 0; i < num_clusters; i++) {
 			i_cl = managed_clusters[i];
-			spin_lock_irqsave(&i_cl->iowait_lock, flags);
+			raw_spin_lock_irqsave(&i_cl->iowait_lock, flags);
 			i_cl->iowait_enter_cycle_cnt = 0;
 			i_cl->iowait_exit_cycle_cnt = 0;
 			i_cl->cur_io_busy = 0;
 			i_cl->io_change = true;
-			spin_unlock_irqrestore(&i_cl->iowait_lock, flags);
+			raw_spin_unlock_irqrestore(&i_cl->iowait_lock, flags);
 		}
 	}
 	if (!(workload_detect & MODE_DETECT)) {
 		for (i = 0; i < num_clusters; i++) {
 			i_cl = managed_clusters[i];
-			spin_lock_irqsave(&i_cl->mode_lock, flags);
+			raw_spin_lock_irqsave(&i_cl->mode_lock, flags);
 			i_cl->single_enter_cycle_cnt = 0;
 			i_cl->single_exit_cycle_cnt = 0;
 			i_cl->multi_enter_cycle_cnt = 0;
 			i_cl->multi_exit_cycle_cnt = 0;
 			i_cl->mode = 0;
 			i_cl->mode_change = true;
-			spin_unlock_irqrestore(&i_cl->mode_lock, flags);
+			raw_spin_unlock_irqrestore(&i_cl->mode_lock, flags);
 		}
 	}
 
 	if (!(workload_detect & PERF_CL_PEAK_DETECT)) {
 		for (i = 0; i < num_clusters; i++) {
 			i_cl = managed_clusters[i];
-			spin_lock_irqsave(&i_cl->perf_cl_peak_lock, flags);
+			raw_spin_lock_irqsave(&i_cl->perf_cl_peak_lock, flags);
 			i_cl->perf_cl_peak_enter_cycle_cnt = 0;
 			i_cl->perf_cl_peak_exit_cycle_cnt = 0;
 			i_cl->perf_cl_peak = 0;
-			spin_unlock_irqrestore(&i_cl->perf_cl_peak_lock, flags);
+			raw_spin_unlock_irqrestore(&i_cl->perf_cl_peak_lock, flags);
 		}
 	}
 
@@ -1591,23 +1591,23 @@ static bool check_notify_status(void)
 
 	for (i = 0; i < num_clusters; i++) {
 		cl = managed_clusters[i];
-		spin_lock_irqsave(&cl->iowait_lock, flags);
+		raw_spin_lock_irqsave(&cl->iowait_lock, flags);
 		if (!any_change)
 			any_change = cl->io_change;
 		cl->io_change = false;
-		spin_unlock_irqrestore(&cl->iowait_lock, flags);
+		raw_spin_unlock_irqrestore(&cl->iowait_lock, flags);
 
-		spin_lock_irqsave(&cl->mode_lock, flags);
+		raw_spin_lock_irqsave(&cl->mode_lock, flags);
 		if (!any_change)
 			any_change = cl->mode_change;
 		cl->mode_change = false;
-		spin_unlock_irqrestore(&cl->mode_lock, flags);
+		raw_spin_unlock_irqrestore(&cl->mode_lock, flags);
 
-		spin_lock_irqsave(&cl->perf_cl_peak_lock, flags);
+		raw_spin_lock_irqsave(&cl->perf_cl_peak_lock, flags);
 		if (!any_change)
 			any_change = cl->perf_cl_detect_state_change;
 		cl->perf_cl_detect_state_change = false;
-		spin_unlock_irqrestore(&cl->perf_cl_peak_lock, flags);
+		raw_spin_unlock_irqrestore(&cl->perf_cl_peak_lock, flags);
 	}
 
 	return any_change;
@@ -1667,9 +1667,9 @@ static void hotplug_notify(int action)
 		return;
 
 	if ((action == CPU_ONLINE) || (action == CPU_DEAD)) {
-		spin_lock_irqsave(&(events_group.cpu_hotplug_lock), flags);
+		raw_spin_lock_irqsave(&(events_group.cpu_hotplug_lock), flags);
 		events_group.cpu_hotplug = true;
-		spin_unlock_irqrestore(&(events_group.cpu_hotplug_lock), flags);
+		raw_spin_unlock_irqrestore(&(events_group.cpu_hotplug_lock), flags);
 		wake_up_process(events_notify_thread);
 	}
 }
@@ -1682,23 +1682,23 @@ static int events_notify_userspace(void *data)
 	while (1) {
 
 		set_current_state(TASK_INTERRUPTIBLE);
-		spin_lock_irqsave(&(events_group.cpu_hotplug_lock), flags);
+		raw_spin_lock_irqsave(&(events_group.cpu_hotplug_lock), flags);
 
 		if (!events_group.cpu_hotplug) {
-			spin_unlock_irqrestore(&(events_group.cpu_hotplug_lock),
+			raw_spin_unlock_irqrestore(&(events_group.cpu_hotplug_lock),
 									flags);
 
 			schedule();
 			if (kthread_should_stop())
 				break;
-			spin_lock_irqsave(&(events_group.cpu_hotplug_lock),
+			raw_spin_lock_irqsave(&(events_group.cpu_hotplug_lock),
 									flags);
 		}
 
 		set_current_state(TASK_RUNNING);
 		notify_change = events_group.cpu_hotplug;
 		events_group.cpu_hotplug = false;
-		spin_unlock_irqrestore(&(events_group.cpu_hotplug_lock), flags);
+		raw_spin_unlock_irqrestore(&(events_group.cpu_hotplug_lock), flags);
 
 		if (notify_change)
 			sysfs_notify(events_kobj, NULL, "cpu_hotplug");
@@ -1715,12 +1715,12 @@ static void check_cluster_iowait(struct cluster *cl, u64 now)
 	unsigned int temp_iobusy;
 	u64 max_iowait = 0;
 
-	spin_lock_irqsave(&cl->iowait_lock, flags);
+	raw_spin_lock_irqsave(&cl->iowait_lock, flags);
 
 	if (((now - cl->last_io_check_ts)
 		< (cl->timer_rate - LAST_IO_CHECK_TOL)) ||
 		!(workload_detect & IO_DETECT)) {
-		spin_unlock_irqrestore(&cl->iowait_lock, flags);
+		raw_spin_unlock_irqrestore(&cl->iowait_lock, flags);
 		return;
 	}
 
@@ -1765,7 +1765,7 @@ static void check_cluster_iowait(struct cluster *cl, u64 now)
 		pr_debug("msm_perf: IO changed to %u\n", cl->cur_io_busy);
 	}
 
-	spin_unlock_irqrestore(&cl->iowait_lock, flags);
+	raw_spin_unlock_irqrestore(&cl->iowait_lock, flags);
 	if (cl->io_change)
 		wake_up_process(notify_thread);
 }
@@ -1774,7 +1774,7 @@ static void disable_timer(struct cluster *cl)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&cl->timer_lock, flags);
+	raw_spin_lock_irqsave(&cl->timer_lock, flags);
 
 	if (del_timer(&cl->mode_exit_timer)) {
 		trace_single_cycle_exit_timer_stop(cpumask_first(cl->cpus),
@@ -1785,14 +1785,14 @@ static void disable_timer(struct cluster *cl)
 			cl->timer_rate, cl->mode);
 	}
 
-	spin_unlock_irqrestore(&cl->timer_lock, flags);
+	raw_spin_unlock_irqrestore(&cl->timer_lock, flags);
 }
 
 static void start_timer(struct cluster *cl)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&cl->timer_lock, flags);
+	raw_spin_lock_irqsave(&cl->timer_lock, flags);
 	if ((cl->mode & SINGLE) && !timer_pending(&cl->mode_exit_timer)) {
 		/*Set timer for the Cluster since there is none pending*/
 		cl->mode_exit_timer.expires = get_jiffies_64() +
@@ -1806,7 +1806,7 @@ static void start_timer(struct cluster *cl)
 			cl->multi_exit_cycles, cl->multi_exit_cycle_cnt,
 			cl->timer_rate, cl->mode);
 	}
-	spin_unlock_irqrestore(&cl->timer_lock, flags);
+	raw_spin_unlock_irqrestore(&cl->timer_lock, flags);
 }
 
 
@@ -1966,7 +1966,7 @@ static void check_perf_cl_peak_load(struct cluster *cl, u64 now)
 	unsigned long flags;
 	bool cpu_of_cluster_zero = true;
 
-	spin_lock_irqsave(&cl->perf_cl_peak_lock, flags);
+	raw_spin_lock_irqsave(&cl->perf_cl_peak_lock, flags);
 
 	cpu_of_cluster_zero = cpumask_first(cl->cpus) ? false:true;
 	/*
@@ -1979,7 +1979,7 @@ static void check_perf_cl_peak_load(struct cluster *cl, u64 now)
 		< (cl->timer_rate - LAST_LD_CHECK_TOL)) ||
 		!(workload_detect & PERF_CL_PEAK_DETECT) ||
 		cpu_of_cluster_zero) {
-		spin_unlock_irqrestore(&cl->perf_cl_peak_lock, flags);
+		raw_spin_unlock_irqrestore(&cl->perf_cl_peak_lock, flags);
 		return;
 	}
 	for_each_cpu(i, cl->cpus) {
@@ -2051,7 +2051,7 @@ static void check_perf_cl_peak_load(struct cluster *cl, u64 now)
 		cl->multi_exit_cycle_cnt, cl->perf_cl_peak_enter_cycle_cnt,
 		cl->perf_cl_peak_exit_cycle_cnt, cl->mode, cpu_cnt);
 
-	spin_unlock_irqrestore(&cl->perf_cl_peak_lock, flags);
+	raw_spin_unlock_irqrestore(&cl->perf_cl_peak_lock, flags);
 
 	if (cl->perf_cl_detect_state_change)
 		wake_up_process(notify_thread);
@@ -2065,12 +2065,12 @@ static void check_cpu_load(struct cluster *cl, u64 now)
 	unsigned int total_load_ceil, total_load_floor;
 	unsigned long flags;
 
-	spin_lock_irqsave(&cl->mode_lock, flags);
+	raw_spin_lock_irqsave(&cl->mode_lock, flags);
 
 	if (((now - cl->last_mode_check_ts)
 		< (cl->timer_rate - LAST_LD_CHECK_TOL)) ||
 		!(workload_detect & MODE_DETECT)) {
-		spin_unlock_irqrestore(&cl->mode_lock, flags);
+		raw_spin_unlock_irqrestore(&cl->mode_lock, flags);
 		return;
 	}
 
@@ -2159,7 +2159,7 @@ static void check_cpu_load(struct cluster *cl, u64 now)
 		cl->multi_exit_cycle_cnt, cl->perf_cl_peak_enter_cycle_cnt,
 		cl->perf_cl_peak_exit_cycle_cnt, cl->mode, cpu_cnt);
 
-	spin_unlock_irqrestore(&cl->mode_lock, flags);
+	raw_spin_unlock_irqrestore(&cl->mode_lock, flags);
 
 	if (cl->mode_change)
 		wake_up_process(notify_thread);
@@ -2252,9 +2252,9 @@ static int perf_cputrans_notify(struct notifier_block *nb, unsigned long val,
 		return NOTIFY_OK;
 
 	if (val == CPUFREQ_POSTCHANGE) {
-		spin_lock_irqsave(&cl->perf_cl_peak_lock, flags);
+		raw_spin_lock_irqsave(&cl->perf_cl_peak_lock, flags);
 		cpu_st->freq = freq->new;
-		spin_unlock_irqrestore(&cl->perf_cl_peak_lock, flags);
+		raw_spin_unlock_irqrestore(&cl->perf_cl_peak_lock, flags);
 	}
 	/*
 	* Avoid deadlock in case governor notifier ran in the context
@@ -2536,7 +2536,7 @@ static void single_mod_exit_timer(unsigned long data)
 	if (i_cl == NULL)
 		return;
 
-	spin_lock_irqsave(&i_cl->mode_lock, flags);
+	raw_spin_lock_irqsave(&i_cl->mode_lock, flags);
 	if (i_cl->mode & SINGLE) {
 		/* Disable SINGLE mode and exit since the timer expired */
 		i_cl->mode = i_cl->mode & ~SINGLE;
@@ -2549,7 +2549,7 @@ static void single_mod_exit_timer(unsigned long data)
 			i_cl->multi_exit_cycles, i_cl->multi_exit_cycle_cnt,
 			i_cl->timer_rate, i_cl->mode);
 	}
-	spin_unlock_irqrestore(&i_cl->mode_lock, flags);
+	raw_spin_unlock_irqrestore(&i_cl->mode_lock, flags);
 	wake_up_process(notify_thread);
 }
 
@@ -2573,14 +2573,14 @@ static void perf_cl_peak_mod_exit_timer(unsigned long data)
 	if (i_cl == NULL)
 		return;
 
-	spin_lock_irqsave(&i_cl->perf_cl_peak_lock, flags);
+	raw_spin_lock_irqsave(&i_cl->perf_cl_peak_lock, flags);
 	if (i_cl->perf_cl_peak & PERF_CL_PEAK) {
 		/* Disable PERF_CL_PEAK mode and exit since the timer expired */
 		i_cl->perf_cl_peak = i_cl->perf_cl_peak & ~PERF_CL_PEAK;
 		i_cl->perf_cl_peak_enter_cycle_cnt = 0;
 		i_cl->perf_cl_peak_exit_cycle_cnt = 0;
 	}
-	spin_unlock_irqrestore(&i_cl->perf_cl_peak_lock, flags);
+	raw_spin_unlock_irqrestore(&i_cl->perf_cl_peak_lock, flags);
 	wake_up_process(notify_thread);
 }
 
@@ -2642,10 +2642,10 @@ static int init_cluster_control(void)
 		thr.perf_cl_trigger_threshold = CLUSTER_1_THRESHOLD_FREQ;
 		thr.pwr_cl_trigger_threshold = CLUSTER_0_THRESHOLD_FREQ;
 		thr.ip_evt_threshold = INPUT_EVENT_CNT_THRESHOLD;
-		spin_lock_init(&(managed_clusters[i]->iowait_lock));
-		spin_lock_init(&(managed_clusters[i]->mode_lock));
-		spin_lock_init(&(managed_clusters[i]->timer_lock));
-		spin_lock_init(&(managed_clusters[i]->perf_cl_peak_lock));
+		raw_spin_lock_init(&(managed_clusters[i]->iowait_lock));
+		raw_spin_lock_init(&(managed_clusters[i]->mode_lock));
+		raw_spin_lock_init(&(managed_clusters[i]->timer_lock));
+		raw_spin_lock_init(&(managed_clusters[i]->perf_cl_peak_lock));
 		init_timer(&managed_clusters[i]->mode_exit_timer);
 		managed_clusters[i]->mode_exit_timer.function =
 			single_mod_exit_timer;
@@ -2725,7 +2725,7 @@ static int init_events_group(void)
 		return ret;
 	}
 
-	spin_lock_init(&(events_group.cpu_hotplug_lock));
+	raw_spin_lock_init(&(events_group.cpu_hotplug_lock));
 	events_notify_thread = kthread_run(events_notify_userspace,
 					NULL, "msm_perf:events_notify");
 	if (IS_ERR(events_notify_thread))
diff --git a/kernel/msm-3.18/drivers/soc/qcom/msm_rq_stats.c b/kernel/msm-3.18/drivers/soc/qcom/msm_rq_stats.c
index 15336d033..9c7f7c9b2 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/msm_rq_stats.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/msm_rq_stats.c
@@ -210,11 +210,11 @@ static ssize_t run_queue_avg_show(struct kobject *kobj,
 	unsigned int val = 0;
 	unsigned long flags = 0;
 
-	spin_lock_irqsave(&rq_lock, flags);
+	raw_spin_lock_irqsave(&rq_lock, flags);
 	/* rq avg currently available only on one core */
 	val = rq_info.rq_avg;
 	rq_info.rq_avg = 0;
-	spin_unlock_irqrestore(&rq_lock, flags);
+	raw_spin_unlock_irqrestore(&rq_lock, flags);
 
 	return snprintf(buf, PAGE_SIZE, "%d.%d\n", val/10, val%10);
 }
@@ -227,10 +227,10 @@ static ssize_t show_run_queue_poll_ms(struct kobject *kobj,
 	int ret = 0;
 	unsigned long flags = 0;
 
-	spin_lock_irqsave(&rq_lock, flags);
+	raw_spin_lock_irqsave(&rq_lock, flags);
 	ret = snprintf(buf, MAX_LONG_SIZE, "%u\n",
 		       jiffies_to_msecs(rq_info.rq_poll_jiffies));
-	spin_unlock_irqrestore(&rq_lock, flags);
+	raw_spin_unlock_irqrestore(&rq_lock, flags);
 
 	return ret;
 }
@@ -245,12 +245,12 @@ static ssize_t store_run_queue_poll_ms(struct kobject *kobj,
 
 	mutex_lock(&lock_poll_ms);
 
-	spin_lock_irqsave(&rq_lock, flags);
+	raw_spin_lock_irqsave(&rq_lock, flags);
 	if (kstrtouint(buf, 0, &val))
 		count = -EINVAL;
 	else
 		rq_info.rq_poll_jiffies = msecs_to_jiffies(val);
-	spin_unlock_irqrestore(&rq_lock, flags);
+	raw_spin_unlock_irqrestore(&rq_lock, flags);
 
 	mutex_unlock(&lock_poll_ms);
 
@@ -352,7 +352,7 @@ static int __init msm_rq_stats_init(void)
 	rq_wq = create_singlethread_workqueue("rq_stats");
 	BUG_ON(!rq_wq);
 	INIT_WORK(&rq_info.def_timer_work, def_work_fn);
-	spin_lock_init(&rq_lock);
+	raw_spin_lock_init(&rq_lock);
 	rq_info.rq_poll_jiffies = DEFAULT_RQ_POLL_JIFFIES;
 	rq_info.def_timer_jiffies = DEFAULT_DEF_TIMER_JIFFIES;
 	rq_info.rq_poll_last_jiffy = 0;
diff --git a/kernel/msm-3.18/drivers/soc/qcom/perf_event_l2.c b/kernel/msm-3.18/drivers/soc/qcom/perf_event_l2.c
index 914fd9275..29d9b0a7d 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/perf_event_l2.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/perf_event_l2.c
@@ -34,7 +34,7 @@ struct hml2_pmu {
 	struct perf_event *events[MAX_L2_CTRS];
 	unsigned long used_mask[BITS_TO_LONGS(MAX_L2_CTRS)];
 	atomic64_t prev_count[MAX_L2_CTRS];
-	spinlock_t pmu_lock;
+	raw_spinlock_t pmu_lock;
 };
 
 /*
@@ -246,7 +246,7 @@ void hml2_pmu__set_evres(struct hml2_pmu *slice,
 		group_val |= L2PMRESRH_EN;
 	}
 
-	spin_lock_irqsave(&slice->pmu_lock, iflags);
+	raw_spin_lock_irqsave(&slice->pmu_lock, iflags);
 
 	resr_val = get_l2_indirect_reg(group_reg);
 	resr_val &= group_mask;
@@ -261,7 +261,7 @@ void hml2_pmu__set_evres(struct hml2_pmu *slice,
 			set_l2_indirect_reg(L2PMRESRH, resr_val);
 		}
 	}
-	spin_unlock_irqrestore(&slice->pmu_lock, iflags);
+	raw_spin_unlock_irqrestore(&slice->pmu_lock, iflags);
 }
 
 static void
@@ -914,7 +914,7 @@ static int l2_cache_pmu_probe(struct platform_device *pdev)
 		}
 
 		slice->cluster = affinity_cpu >> 1;
-		spin_lock_init(&slice->pmu_lock);
+		raw_spin_lock_init(&slice->pmu_lock);
 
 		hml2_pmu__init(slice);
 		list_add(&slice->entry, &l2cache_pmu.pmus);
diff --git a/kernel/msm-3.18/drivers/soc/qcom/qmi_interface.c b/kernel/msm-3.18/drivers/soc/qcom/qmi_interface.c
index d6853e4be..41289555e 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/qmi_interface.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/qmi_interface.c
@@ -469,9 +469,9 @@ static void qmi_notify_event_worker(struct work_struct *work)
 
 	switch (notify_work->event) {
 	case IPC_ROUTER_CTRL_CMD_DATA:
-		spin_lock_irqsave(&handle->notify_lock, flags);
+		raw_spin_lock_irqsave(&handle->notify_lock, flags);
 		handle->notify(handle, QMI_RECV_MSG, handle->notify_priv);
-		spin_unlock_irqrestore(&handle->notify_lock, flags);
+		raw_spin_unlock_irqrestore(&handle->notify_lock, flags);
 		break;
 
 	case IPC_ROUTER_CTRL_CMD_RESUME_TX:
@@ -637,9 +637,9 @@ static int handle_rmv_server(struct qmi_handle *handle,
 		handle->handle_reset = 1;
 		clean_txn_info(handle);
 
-		spin_lock_irqsave(&handle->notify_lock, flags);
+		raw_spin_lock_irqsave(&handle->notify_lock, flags);
 		handle->notify(handle, QMI_SERVER_EXIT, handle->notify_priv);
-		spin_unlock_irqrestore(&handle->notify_lock, flags);
+		raw_spin_unlock_irqrestore(&handle->notify_lock, flags);
 	}
 	return 0;
 }
@@ -665,9 +665,9 @@ static int handle_rmv_client(struct qmi_handle *handle,
 	clnt_addr.addr.port_addr.port_id = ctl_msg->cli.port_id;
 	conn_h = find_svc_clnt_conn(handle, &clnt_addr, sizeof(clnt_addr));
 	if (conn_h) {
-		spin_lock_irqsave(&handle->notify_lock, flags);
+		raw_spin_lock_irqsave(&handle->notify_lock, flags);
 		handle->svc_ops_options->disconnect_cb(handle, conn_h);
-		spin_unlock_irqrestore(&handle->notify_lock, flags);
+		raw_spin_unlock_irqrestore(&handle->notify_lock, flags);
 		rmv_svc_clnt_conn(conn_h);
 	}
 	return 0;
@@ -749,7 +749,7 @@ struct qmi_handle *qmi_handle_create(
 	temp_handle->handle_type = QMI_CLIENT_HANDLE;
 	temp_handle->next_txn_id = 1;
 	mutex_init(&temp_handle->handle_lock);
-	spin_lock_init(&temp_handle->notify_lock);
+	raw_spin_lock_init(&temp_handle->notify_lock);
 	temp_handle->notify = notify;
 	temp_handle->notify_priv = notify_priv;
 	init_waitqueue_head(&temp_handle->reset_waitq);
@@ -1834,10 +1834,10 @@ static int qmi_notify_svc_event_arrive(uint32_t service,
 		 * service during registration.
 		 */
 		svc_event_add_svc_addr(temp, node_id, port_id);
-		spin_lock_irqsave(&temp->nb_lock, flags);
+		raw_spin_lock_irqsave(&temp->nb_lock, flags);
 		raw_notifier_call_chain(&temp->svc_event_rcvr_list,
 				QMI_SERVER_ARRIVE, NULL);
-		spin_unlock_irqrestore(&temp->nb_lock, flags);
+		raw_spin_unlock_irqrestore(&temp->nb_lock, flags);
 	}
 	mutex_unlock(&temp->svc_addr_list_lock);
 
@@ -1880,10 +1880,10 @@ static int qmi_notify_svc_event_exit(uint32_t service,
 			 * Notify only if an already notified service has
 			 * gone down.
 			 */
-			spin_lock_irqsave(&temp->nb_lock, flags);
+			raw_spin_lock_irqsave(&temp->nb_lock, flags);
 			raw_notifier_call_chain(&temp->svc_event_rcvr_list,
 						QMI_SERVER_EXIT, NULL);
-			spin_unlock_irqrestore(&temp->nb_lock, flags);
+			raw_spin_unlock_irqrestore(&temp->nb_lock, flags);
 			list_del(&addr->list_node);
 			kfree(addr);
 		}
@@ -1931,7 +1931,7 @@ static struct svc_event_nb *find_and_add_svc_event_nb(uint32_t service_id,
 		return temp;
 	}
 
-	spin_lock_init(&temp->nb_lock);
+	raw_spin_lock_init(&temp->nb_lock);
 	temp->service_id = service_id;
 	temp->instance_id = instance_id;
 	INIT_LIST_HEAD(&temp->list);
@@ -1972,14 +1972,14 @@ int qmi_svc_event_notifier_register(uint32_t service_id,
 	mutex_unlock(&svc_event_nb_list_lock);
 
 	mutex_lock(&temp->svc_addr_list_lock);
-	spin_lock_irqsave(&temp->nb_lock, flags);
+	raw_spin_lock_irqsave(&temp->nb_lock, flags);
 	ret = raw_notifier_chain_register(&temp->svc_event_rcvr_list, nb);
-	spin_unlock_irqrestore(&temp->nb_lock, flags);
+	raw_spin_unlock_irqrestore(&temp->nb_lock, flags);
 	if (!list_empty(&temp->svc_addr_list)) {
 		/* Notify this client only if Some services already exist. */
-		spin_lock_irqsave(&temp->nb_lock, flags);
+		raw_spin_lock_irqsave(&temp->nb_lock, flags);
 		nb->notifier_call(nb, QMI_SERVER_ARRIVE, NULL);
-		spin_unlock_irqrestore(&temp->nb_lock, flags);
+		raw_spin_unlock_irqrestore(&temp->nb_lock, flags);
 	} else {
 		/*
 		 * Check if we have missed a new server event that happened
@@ -2007,10 +2007,10 @@ int qmi_svc_event_notifier_register(uint32_t service_id,
 						svc_info_arr[i].port_id);
 			kfree(svc_info_arr);
 
-			spin_lock_irqsave(&temp->nb_lock, flags);
+			raw_spin_lock_irqsave(&temp->nb_lock, flags);
 			raw_notifier_call_chain(&temp->svc_event_rcvr_list,
 						QMI_SERVER_ARRIVE, NULL);
-			spin_unlock_irqrestore(&temp->nb_lock, flags);
+			raw_spin_unlock_irqrestore(&temp->nb_lock, flags);
 		}
 	}
 	mutex_unlock(&temp->svc_addr_list_lock);
@@ -2037,9 +2037,9 @@ int qmi_svc_event_notifier_unregister(uint32_t service_id,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&temp->nb_lock, flags);
+	raw_spin_lock_irqsave(&temp->nb_lock, flags);
 	ret = raw_notifier_chain_unregister(&temp->svc_event_rcvr_list, nb);
-	spin_unlock_irqrestore(&temp->nb_lock, flags);
+	raw_spin_unlock_irqrestore(&temp->nb_lock, flags);
 	mutex_unlock(&svc_event_nb_list_lock);
 
 	return ret;
diff --git a/kernel/msm-3.18/drivers/soc/qcom/qmi_interface_priv.h b/kernel/msm-3.18/drivers/soc/qcom/qmi_interface_priv.h
index ef3e69246..2fe6877a5 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/qmi_interface_priv.h
+++ b/kernel/msm-3.18/drivers/soc/qcom/qmi_interface_priv.h
@@ -77,7 +77,7 @@ struct svc_addr {
  *			<service_id:instance_id>.
  */
 struct svc_event_nb {
-	spinlock_t nb_lock;
+	raw_spinlock_t nb_lock;
 	uint32_t service_id;
 	uint32_t instance_id;
 	struct raw_notifier_head svc_event_rcvr_list;
diff --git a/kernel/msm-3.18/drivers/soc/qcom/rpm-smd.c b/kernel/msm-3.18/drivers/soc/qcom/rpm-smd.c
index 40711a3b2..2b2a509a3 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/rpm-smd.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/rpm-smd.c
@@ -60,8 +60,8 @@ struct msm_rpm_driver_data {
 	uint32_t ch_type;
 	smd_channel_t *ch_info;
 	struct work_struct work;
-	spinlock_t smd_lock_write;
-	spinlock_t smd_lock_read;
+	raw_spinlock_t smd_lock_write;
+	raw_spinlock_t smd_lock_read;
 	struct completion smd_open;
 };
 
@@ -422,13 +422,13 @@ int msm_rpm_smd_buffer_request(struct msm_rpm_request *cdata,
 	if (size > MAX_SLEEP_BUFFER)
 		return -ENOMEM;
 
-	spin_lock_irqsave(&slp_buffer_lock, flags);
+	raw_spin_lock_irqsave(&slp_buffer_lock, flags);
 	slp = tr_search(&tr_root, buf);
 
 	if (!slp) {
 		slp = kzalloc(sizeof(struct slp_buf), GFP_ATOMIC);
 		if (!slp) {
-			spin_unlock_irqrestore(&slp_buffer_lock, flags);
+			raw_spin_unlock_irqrestore(&slp_buffer_lock, flags);
 			return -ENOMEM;
 		}
 		slp->buf = PTR_ALIGN(&slp->ubuf[0], sizeof(u32));
@@ -443,7 +443,7 @@ int msm_rpm_smd_buffer_request(struct msm_rpm_request *cdata,
 				cdata->msg_hdr.resource_type,
 				cdata->msg_hdr.resource_id);
 
-	spin_unlock_irqrestore(&slp_buffer_lock, flags);
+	raw_spin_unlock_irqrestore(&slp_buffer_lock, flags);
 
 	return 0;
 }
@@ -826,9 +826,9 @@ bool msm_rpm_waiting_for_ack(void)
 	bool ret;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_rpm_list_lock, flags);
+	raw_spin_lock_irqsave(&msm_rpm_list_lock, flags);
 	ret = list_empty(&msm_rpm_wait_list);
-	spin_unlock_irqrestore(&msm_rpm_list_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_rpm_list_lock, flags);
 
 	return !ret;
 }
@@ -839,7 +839,7 @@ static struct msm_rpm_wait_data *msm_rpm_get_entry_from_msg_id(uint32_t msg_id)
 	struct msm_rpm_wait_data *elem = NULL;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_rpm_list_lock, flags);
+	raw_spin_lock_irqsave(&msm_rpm_list_lock, flags);
 
 	list_for_each(ptr, &msm_rpm_wait_list) {
 		elem = list_entry(ptr, struct msm_rpm_wait_data, list);
@@ -847,7 +847,7 @@ static struct msm_rpm_wait_data *msm_rpm_get_entry_from_msg_id(uint32_t msg_id)
 			break;
 		elem = NULL;
 	}
-	spin_unlock_irqrestore(&msm_rpm_list_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_rpm_list_lock, flags);
 	return elem;
 }
 
@@ -883,12 +883,12 @@ static int msm_rpm_add_wait_list(uint32_t msg_id, bool delete_on_ack)
 	data->msg_id = msg_id;
 	data->errno = INIT_ERROR;
 	data->delete_on_ack = delete_on_ack;
-	spin_lock_irqsave(&msm_rpm_list_lock, flags);
+	raw_spin_lock_irqsave(&msm_rpm_list_lock, flags);
 	if (delete_on_ack)
 		list_add_tail(&data->list, &msm_rpm_wait_list);
 	else
 		list_add(&data->list, &msm_rpm_wait_list);
-	spin_unlock_irqrestore(&msm_rpm_list_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_rpm_list_lock, flags);
 
 	return 0;
 }
@@ -897,9 +897,9 @@ static void msm_rpm_free_list_entry(struct msm_rpm_wait_data *elem)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_rpm_list_lock, flags);
+	raw_spin_lock_irqsave(&msm_rpm_list_lock, flags);
 	list_del(&elem->list);
-	spin_unlock_irqrestore(&msm_rpm_list_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_rpm_list_lock, flags);
 	kfree(elem);
 }
 
@@ -909,7 +909,7 @@ static void msm_rpm_process_ack(uint32_t msg_id, int errno)
 	struct msm_rpm_wait_data *elem = NULL;
 	unsigned long flags;
 
-	spin_lock_irqsave(&msm_rpm_list_lock, flags);
+	raw_spin_lock_irqsave(&msm_rpm_list_lock, flags);
 
 	list_for_each_safe(ptr, next, &msm_rpm_wait_list) {
 		elem = list_entry(ptr, struct msm_rpm_wait_data, list);
@@ -932,7 +932,7 @@ static void msm_rpm_process_ack(uint32_t msg_id, int errno)
 	if (!elem)
 		trace_rpm_smd_ack_recvd(0, msg_id, 0xDEADBEEF);
 
-	spin_unlock_irqrestore(&msm_rpm_list_lock, flags);
+	raw_spin_unlock_irqrestore(&msm_rpm_list_lock, flags);
 }
 
 struct msm_rpm_kvp_packet {
@@ -985,7 +985,7 @@ static void data_fn_tasklet(unsigned long data)
 	int errno;
 	char buf[MAX_ERR_BUFFER_SIZE] = {0};
 
-	spin_lock(&msm_rpm_data.smd_lock_read);
+	raw_spin_lock(&msm_rpm_data.smd_lock_read);
 	while (smd_is_pkt_avail(msm_rpm_data.ch_info)) {
 		if (msm_rpm_read_smd_data(buf))
 			break;
@@ -994,7 +994,7 @@ static void data_fn_tasklet(unsigned long data)
 		trace_rpm_smd_ack_recvd(0, msg_id, errno);
 		msm_rpm_process_ack(msg_id, errno);
 	}
-	spin_unlock(&msm_rpm_data.smd_lock_read);
+	raw_spin_unlock(&msm_rpm_data.smd_lock_read);
 }
 
 static void msm_rpm_log_request(struct msm_rpm_request *cdata)
@@ -1135,17 +1135,17 @@ static int msm_rpm_send_smd_buffer(char *buf, uint32_t size, bool noirq)
 	unsigned long flags;
 	int ret;
 
-	spin_lock_irqsave(&msm_rpm_data.smd_lock_write, flags);
+	raw_spin_lock_irqsave(&msm_rpm_data.smd_lock_write, flags);
 	ret = smd_write_avail(msm_rpm_data.ch_info);
 
 	while ((ret = smd_write_avail(msm_rpm_data.ch_info)) < size) {
 		if (ret < 0)
 			break;
 		if (!noirq) {
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 				&msm_rpm_data.smd_lock_write, flags);
 			cpu_relax();
-			spin_lock_irqsave(
+			raw_spin_lock_irqsave(
 				&msm_rpm_data.smd_lock_write, flags);
 		} else
 			udelay(5);
@@ -1153,13 +1153,13 @@ static int msm_rpm_send_smd_buffer(char *buf, uint32_t size, bool noirq)
 
 	if (ret < 0) {
 		pr_err("SMD not initialized\n");
-		spin_unlock_irqrestore(
+		raw_spin_unlock_irqrestore(
 			&msm_rpm_data.smd_lock_write, flags);
 		return ret;
 	}
 
 	ret = smd_write(msm_rpm_data.ch_info, buf, size);
-	spin_unlock_irqrestore(&msm_rpm_data.smd_lock_write, flags);
+	raw_spin_unlock_irqrestore(&msm_rpm_data.smd_lock_write, flags);
 	return ret;
 }
 
@@ -1169,16 +1169,16 @@ static int msm_rpm_glink_send_buffer(char *buf, uint32_t size, bool noirq)
 	unsigned long flags;
 	int timeout = 5;
 
-	spin_lock_irqsave(&msm_rpm_data.smd_lock_write, flags);
+	raw_spin_lock_irqsave(&msm_rpm_data.smd_lock_write, flags);
 	do {
 		ret = glink_tx(glink_data->glink_handle, buf, buf,
 					size, GLINK_TX_SINGLE_THREADED);
 		if (ret == -EBUSY || ret == -ENOSPC) {
 			if (!noirq) {
-				spin_unlock_irqrestore(
+				raw_spin_unlock_irqrestore(
 					&msm_rpm_data.smd_lock_write, flags);
 				cpu_relax();
-				spin_lock_irqsave(
+				raw_spin_lock_irqsave(
 					&msm_rpm_data.smd_lock_write, flags);
 			} else {
 				udelay(100);
@@ -1188,7 +1188,7 @@ static int msm_rpm_glink_send_buffer(char *buf, uint32_t size, bool noirq)
 			ret = 0;
 		}
 	} while (ret && timeout);
-	spin_unlock_irqrestore(&msm_rpm_data.smd_lock_write, flags);
+	raw_spin_unlock_irqrestore(&msm_rpm_data.smd_lock_write, flags);
 
 	if (!timeout)
 		return 0;
@@ -1426,7 +1426,7 @@ int msm_rpm_wait_for_ack_noirq(uint32_t msg_id)
 	if (standalone)
 		return 0;
 
-	spin_lock_irqsave(&msm_rpm_data.smd_lock_read, flags);
+	raw_spin_lock_irqsave(&msm_rpm_data.smd_lock_read, flags);
 
 	elem = msm_rpm_get_entry_from_msg_id(msg_id);
 
@@ -1451,7 +1451,7 @@ int msm_rpm_wait_for_ack_noirq(uint32_t msg_id)
 
 	msm_rpm_free_list_entry(elem);
 wait_ack_cleanup:
-	spin_unlock_irqrestore(&msm_rpm_data.smd_lock_read, flags);
+	raw_spin_unlock_irqrestore(&msm_rpm_data.smd_lock_read, flags);
 
 	if (!glink_enabled)
 		if (smd_is_pkt_avail(msm_rpm_data.ch_info))
@@ -1618,7 +1618,7 @@ static void msm_rpm_trans_notify_rx(void *handle, const void *priv,
 
 	BUG_ON(size > MAX_ERR_BUFFER_SIZE);
 
-	spin_lock_irqsave(&rx_notify_lock, flags);
+	raw_spin_lock_irqsave(&rx_notify_lock, flags);
 	memcpy(buf, ptr, size);
 	msg_id = msm_rpm_get_msg_id_from_ack(buf);
 	errno = msm_rpm_get_error_from_ack(buf);
@@ -1631,13 +1631,13 @@ static void msm_rpm_trans_notify_rx(void *handle, const void *priv,
 	 * run into NULL pointer deferrence issue.
 	 */
 	if (!elem) {
-		spin_unlock_irqrestore(&rx_notify_lock, flags);
+		raw_spin_unlock_irqrestore(&rx_notify_lock, flags);
 		glink_rx_done(handle, ptr, 0);
 		return;
 	}
 
 	msm_rpm_process_ack(msg_id, errno);
-	spin_unlock_irqrestore(&rx_notify_lock, flags);
+	raw_spin_unlock_irqrestore(&rx_notify_lock, flags);
 
 	glink_rx_done(handle, ptr, 0);
 }
@@ -1794,8 +1794,8 @@ static int msm_rpm_glink_link_setup(struct glink_apps_rpm_data *glink_data,
 		return ret;
 	}
 
-	spin_lock_init(&msm_rpm_data.smd_lock_read);
-	spin_lock_init(&msm_rpm_data.smd_lock_write);
+	raw_spin_lock_init(&msm_rpm_data.smd_lock_read);
+	raw_spin_lock_init(&msm_rpm_data.smd_lock_write);
 
 	return ret;
 }
@@ -1889,8 +1889,8 @@ static int msm_rpm_dev_probe(struct platform_device *pdev)
 		goto fail;
 	}
 
-	spin_lock_init(&msm_rpm_data.smd_lock_write);
-	spin_lock_init(&msm_rpm_data.smd_lock_read);
+	raw_spin_lock_init(&msm_rpm_data.smd_lock_write);
+	raw_spin_lock_init(&msm_rpm_data.smd_lock_read);
 	tasklet_init(&data_tasklet, data_fn_tasklet, 0);
 
 	wait_for_completion(&msm_rpm_data.smd_open);
diff --git a/kernel/msm-3.18/drivers/soc/qcom/smd.c b/kernel/msm-3.18/drivers/soc/qcom/smd.c
index d4efd42c8..79df626a6 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smd.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/smd.c
@@ -527,7 +527,7 @@ static struct notifier_block smsm_pm_nb = {
  * list or fiddles with channel state
  */
 static DEFINE_SPINLOCK(smd_lock);
-DEFINE_SPINLOCK(smem_lock);
+DEFINE_RAW_SPINLOCK(smem_lock);
 
 /* the mutex is used during open() and close()
  * operations to avoid races while creating or
@@ -993,14 +993,14 @@ void smd_channel_reset(uint32_t restart_pid)
 
 	/* change all remote states to CLOSING */
 	mutex_lock(&smd_probe_lock);
-	spin_lock_irqsave(&smd_lock, flags);
+	raw_spin_lock_irqsave(&smd_lock, flags);
 	smd_channel_reset_state(shared_pri, PRI_ALLOC_TBL, SMD_SS_CLOSING,
 				restart_pid, pri_size / sizeof(*shared_pri));
 	if (shared_sec)
 		smd_channel_reset_state(shared_sec, SEC_ALLOC_TBL,
 						SMD_SS_CLOSING, restart_pid,
 						sec_size / sizeof(*shared_sec));
-	spin_unlock_irqrestore(&smd_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_lock, flags);
 	mutex_unlock(&smd_probe_lock);
 
 	mb();
@@ -1008,14 +1008,14 @@ void smd_channel_reset(uint32_t restart_pid)
 
 	/* change all remote states to CLOSED */
 	mutex_lock(&smd_probe_lock);
-	spin_lock_irqsave(&smd_lock, flags);
+	raw_spin_lock_irqsave(&smd_lock, flags);
 	smd_channel_reset_state(shared_pri, PRI_ALLOC_TBL, SMD_SS_CLOSED,
 				restart_pid, pri_size / sizeof(*shared_pri));
 	if (shared_sec)
 		smd_channel_reset_state(shared_sec, SEC_ALLOC_TBL,
 						SMD_SS_CLOSED, restart_pid,
 						sec_size / sizeof(*shared_sec));
-	spin_unlock_irqrestore(&smd_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_lock, flags);
 	mutex_unlock(&smd_probe_lock);
 
 	mb();
@@ -1339,7 +1339,7 @@ static void handle_smd_irq_closing_list(void)
 	struct smd_channel *index;
 	unsigned tmp;
 
-	spin_lock_irqsave(&smd_lock, flags);
+	raw_spin_lock_irqsave(&smd_lock, flags);
 	list_for_each_entry_safe(ch, index, &smd_ch_closing_list, ch_list) {
 		if (ch->half_ch->get_fSTATE(ch->recv))
 			ch->half_ch->set_fSTATE(ch->recv, 0);
@@ -1347,7 +1347,7 @@ static void handle_smd_irq_closing_list(void)
 		if (tmp != ch->last_state)
 			smd_state_change(ch, ch->last_state, tmp);
 	}
-	spin_unlock_irqrestore(&smd_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_lock, flags);
 }
 
 static void handle_smd_irq(struct remote_proc_info *r_info,
@@ -1362,7 +1362,7 @@ static void handle_smd_irq(struct remote_proc_info *r_info,
 
 	list = &r_info->ch_list;
 
-	spin_lock_irqsave(&smd_lock, flags);
+	raw_spin_lock_irqsave(&smd_lock, flags);
 	list_for_each_entry(ch, list, ch_list) {
 		state_change = 0;
 		ch_flags = 0;
@@ -1409,7 +1409,7 @@ static void handle_smd_irq(struct remote_proc_info *r_info,
 			ch->notify(ch->priv, SMD_EVENT_STATUS);
 		}
 	}
-	spin_unlock_irqrestore(&smd_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_lock, flags);
 	do_smd_probe(r_info->remote_pid);
 }
 
@@ -1622,10 +1622,10 @@ static int smd_packet_read(smd_channel_t *ch, void *data, int len)
 		if (!read_intr_blocked(ch))
 			ch->notify_other_cpu(ch);
 
-	spin_lock_irqsave(&smd_lock, flags);
+	raw_spin_lock_irqsave(&smd_lock, flags);
 	ch->current_packet -= r;
 	update_packet_state(ch);
-	spin_unlock_irqrestore(&smd_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_lock, flags);
 
 	return r;
 }
@@ -1836,14 +1836,14 @@ static void finalize_channel_close_fn(struct work_struct *work)
 	struct smd_channel *index;
 
 	mutex_lock(&smd_creation_mutex);
-	spin_lock_irqsave(&smd_lock, flags);
+	raw_spin_lock_irqsave(&smd_lock, flags);
 	list_for_each_entry_safe(ch, index,  &smd_ch_to_close_list, ch_list) {
 		list_del(&ch->ch_list);
 		list_add(&ch->ch_list, &smd_ch_closed_list);
 		ch->notify(ch->priv, SMD_EVENT_REOPEN_READY);
 		ch->notify = do_nothing_notify;
 	}
-	spin_unlock_irqrestore(&smd_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_lock, flags);
 	mutex_unlock(&smd_creation_mutex);
 }
 
@@ -1886,14 +1886,14 @@ int smd_named_open_on_edge(const char *name, uint32_t edge,
 
 	ch = smd_get_channel(name, edge);
 	if (!ch) {
-		spin_lock_irqsave(&smd_lock, flags);
+		raw_spin_lock_irqsave(&smd_lock, flags);
 		/* check opened list for port */
 		list_for_each_entry(ch,
 			&remote_info[edge_to_pids[edge].remote_pid].ch_list,
 			ch_list) {
 			if (!strcmp(name, ch->name)) {
 				/* channel is already open */
-				spin_unlock_irqrestore(&smd_lock, flags);
+				raw_spin_unlock_irqrestore(&smd_lock, flags);
 				SMD_DBG("smd_open: channel '%s' already open\n",
 					ch->name);
 				return -EBUSY;
@@ -1905,7 +1905,7 @@ int smd_named_open_on_edge(const char *name, uint32_t edge,
 			if (!strncmp(name, ch->name, 20) &&
 				(edge == ch->type)) {
 				/* channel exists, but is being closed */
-				spin_unlock_irqrestore(&smd_lock, flags);
+				raw_spin_unlock_irqrestore(&smd_lock, flags);
 				return -EAGAIN;
 			}
 		}
@@ -1915,11 +1915,11 @@ int smd_named_open_on_edge(const char *name, uint32_t edge,
 			if (!strncmp(name, ch->name, 20) &&
 				(edge == ch->type)) {
 				/* channel exists, but is being closed */
-				spin_unlock_irqrestore(&smd_lock, flags);
+				raw_spin_unlock_irqrestore(&smd_lock, flags);
 				return -EAGAIN;
 			}
 		}
-		spin_unlock_irqrestore(&smd_lock, flags);
+		raw_spin_unlock_irqrestore(&smd_lock, flags);
 
 		/* one final check to handle closing->closed race condition */
 		ch = smd_get_channel(name, edge);
@@ -1949,7 +1949,7 @@ int smd_named_open_on_edge(const char *name, uint32_t edge,
 
 	SMD_DBG("smd_open: opening '%s'\n", ch->name);
 
-	spin_lock_irqsave(&smd_lock, flags);
+	raw_spin_lock_irqsave(&smd_lock, flags);
 	list_add(&ch->ch_list,
 		       &remote_info[edge_to_pids[ch->type].remote_pid].ch_list);
 
@@ -1957,7 +1957,7 @@ int smd_named_open_on_edge(const char *name, uint32_t edge,
 
 	smd_state_change(ch, ch->last_state, SMD_SS_OPENING);
 
-	spin_unlock_irqrestore(&smd_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_lock, flags);
 
 	return 0;
 }
@@ -1973,7 +1973,7 @@ int smd_close(smd_channel_t *ch)
 
 	SMD_INFO("smd_close(%s)\n", ch->name);
 
-	spin_lock_irqsave(&smd_lock, flags);
+	raw_spin_lock_irqsave(&smd_lock, flags);
 	list_del(&ch->ch_list);
 
 	was_opened = ch->half_ch->get_state(ch->recv) == SMD_SS_OPENED;
@@ -1981,9 +1981,9 @@ int smd_close(smd_channel_t *ch)
 
 	if (was_opened) {
 		list_add(&ch->ch_list, &smd_ch_closing_list);
-		spin_unlock_irqrestore(&smd_lock, flags);
+		raw_spin_unlock_irqrestore(&smd_lock, flags);
 	} else {
-		spin_unlock_irqrestore(&smd_lock, flags);
+		raw_spin_unlock_irqrestore(&smd_lock, flags);
 		ch->notify = do_nothing_notify;
 		mutex_lock(&smd_creation_mutex);
 		list_add(&ch->ch_list, &smd_ch_closed_list);
@@ -2307,9 +2307,9 @@ int smd_tiocmset(smd_channel_t *ch, unsigned int set, unsigned int clear)
 		return -ENODEV;
 	}
 
-	spin_lock_irqsave(&smd_lock, flags);
+	raw_spin_lock_irqsave(&smd_lock, flags);
 	smd_tiocmset_from_cb(ch, set, clear);
-	spin_unlock_irqrestore(&smd_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_lock, flags);
 
 	return 0;
 }
@@ -2325,9 +2325,9 @@ int smd_is_pkt_avail(smd_channel_t *ch)
 	if (ch->current_packet)
 		return 1;
 
-	spin_lock_irqsave(&smd_lock, flags);
+	raw_spin_lock_irqsave(&smd_lock, flags);
 	update_packet_state(ch);
-	spin_unlock_irqrestore(&smd_lock, flags);
+	raw_spin_unlock_irqrestore(&smd_lock, flags);
 
 	return ch->current_packet ? 1 : 0;
 }
@@ -2480,13 +2480,13 @@ static void smsm_cb_snapshot(uint32_t use_wakeup_source)
 	 *   This order ensures that 1 will always occur before abc.
 	 */
 	if (use_wakeup_source) {
-		spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
+		raw_spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
 		if (smsm_snapshot_count == 0) {
 			SMSM_POWER_INFO("SMSM snapshot wake lock\n");
 			__pm_stay_awake(&smsm_snapshot_ws);
 		}
 		++smsm_snapshot_count;
-		spin_unlock_irqrestore(&smsm_snapshot_count_lock, flags);
+		raw_spin_unlock_irqrestore(&smsm_snapshot_count_lock, flags);
 	}
 
 	/* queue state entries */
@@ -2520,7 +2520,7 @@ static void smsm_cb_snapshot(uint32_t use_wakeup_source)
 
 restore_snapshot_count:
 	if (use_wakeup_source) {
-		spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
+		raw_spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
 		if (smsm_snapshot_count) {
 			--smsm_snapshot_count;
 			if (smsm_snapshot_count == 0) {
@@ -2530,7 +2530,7 @@ restore_snapshot_count:
 		} else {
 			pr_err("%s: invalid snapshot count\n", __func__);
 		}
-		spin_unlock_irqrestore(&smsm_snapshot_count_lock, flags);
+		raw_spin_unlock_irqrestore(&smsm_snapshot_count_lock, flags);
 	}
 }
 
@@ -2538,7 +2538,7 @@ static irqreturn_t smsm_irq_handler(int irq, void *data)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&smem_lock, flags);
+	raw_spin_lock_irqsave(&smem_lock, flags);
 	if (!smsm_info.state) {
 		SMSM_INFO("<SM NO STATE>\n");
 	} else {
@@ -2565,7 +2565,7 @@ static irqreturn_t smsm_irq_handler(int irq, void *data)
 
 		smsm_cb_snapshot(1);
 	}
-	spin_unlock_irqrestore(&smem_lock, flags);
+	raw_spin_unlock_irqrestore(&smem_lock, flags);
 	return IRQ_HANDLED;
 }
 
@@ -2628,7 +2628,7 @@ int smsm_change_intr_mask(uint32_t smsm_entry,
 		return -EIO;
 	}
 
-	spin_lock_irqsave(&smem_lock, flags);
+	raw_spin_lock_irqsave(&smem_lock, flags);
 	smsm_states[smsm_entry].intr_mask_clear = clear_mask;
 	smsm_states[smsm_entry].intr_mask_set = set_mask;
 
@@ -2637,7 +2637,7 @@ int smsm_change_intr_mask(uint32_t smsm_entry,
 	__raw_writel(new_mask, SMSM_INTR_MASK_ADDR(smsm_entry, SMSM_APPS));
 
 	wmb();
-	spin_unlock_irqrestore(&smem_lock, flags);
+	raw_spin_unlock_irqrestore(&smem_lock, flags);
 
 	return 0;
 }
@@ -2677,7 +2677,7 @@ int smsm_change_state(uint32_t smsm_entry,
 		pr_err("smsm_change_state <SM NO STATE>\n");
 		return -EIO;
 	}
-	spin_lock_irqsave(&smem_lock, flags);
+	raw_spin_lock_irqsave(&smem_lock, flags);
 
 	old_state = __raw_readl(SMSM_STATE_ADDR(smsm_entry));
 	new_state = (old_state & ~clear_mask) | set_mask;
@@ -2686,7 +2686,7 @@ int smsm_change_state(uint32_t smsm_entry,
 			old_state, new_state);
 	notify_other_smsm(SMSM_APPS_STATE, (old_state ^ new_state));
 
-	spin_unlock_irqrestore(&smem_lock, flags);
+	raw_spin_unlock_irqrestore(&smem_lock, flags);
 
 	return 0;
 }
@@ -2782,7 +2782,7 @@ void notify_smsm_cb_clients_worker(struct work_struct *work)
 		mutex_unlock(&smsm_lock);
 
 		if (use_wakeup_source) {
-			spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
+			raw_spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
 			if (smsm_snapshot_count) {
 				--smsm_snapshot_count;
 				if (smsm_snapshot_count == 0) {
@@ -2794,7 +2794,7 @@ void notify_smsm_cb_clients_worker(struct work_struct *work)
 				pr_err("%s: invalid snapshot count\n",
 						__func__);
 			}
-			spin_unlock_irqrestore(&smsm_snapshot_count_lock,
+			raw_spin_unlock_irqrestore(&smsm_snapshot_count_lock,
 					flags);
 		}
 
@@ -2877,13 +2877,13 @@ int smsm_state_cb_register(uint32_t smsm_entry, uint32_t mask,
 	if (smsm_info.intr_mask) {
 		unsigned long flags;
 
-		spin_lock_irqsave(&smem_lock, flags);
+		raw_spin_lock_irqsave(&smem_lock, flags);
 		new_mask = (new_mask & ~state->intr_mask_clear)
 				| state->intr_mask_set;
 		__raw_writel(new_mask,
 				SMSM_INTR_MASK_ADDR(smsm_entry, SMSM_APPS));
 		wmb();
-		spin_unlock_irqrestore(&smem_lock, flags);
+		raw_spin_unlock_irqrestore(&smem_lock, flags);
 	}
 
 cleanup:
@@ -2952,13 +2952,13 @@ int smsm_state_cb_deregister(uint32_t smsm_entry, uint32_t mask,
 	if (smsm_info.intr_mask) {
 		unsigned long flags;
 
-		spin_lock_irqsave(&smem_lock, flags);
+		raw_spin_lock_irqsave(&smem_lock, flags);
 		new_mask = (new_mask & ~state->intr_mask_clear)
 				| state->intr_mask_set;
 		__raw_writel(new_mask,
 				SMSM_INTR_MASK_ADDR(smsm_entry, SMSM_APPS));
 		wmb();
-		spin_unlock_irqrestore(&smem_lock, flags);
+		raw_spin_unlock_irqrestore(&smem_lock, flags);
 	}
 
 	mutex_unlock(&smsm_lock);
diff --git a/kernel/msm-3.18/drivers/soc/qcom/smd_private.h b/kernel/msm-3.18/drivers/soc/qcom/smd_private.h
index 4beb6b8c1..decaf606c 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smd_private.h
+++ b/kernel/msm-3.18/drivers/soc/qcom/smd_private.h
@@ -169,7 +169,7 @@ struct smd_channel {
 	struct smd_half_channel_access *half_ch;
 };
 
-extern spinlock_t smem_lock;
+extern raw_spinlock_t smem_lock;
 
 struct interrupt_stat {
 	uint32_t smd_in_count;
diff --git a/kernel/msm-3.18/drivers/soc/qcom/smem.c b/kernel/msm-3.18/drivers/soc/qcom/smem.c
index ee0bade50..fa44c9aaf 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smem.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/smem.c
@@ -954,9 +954,9 @@ bool smem_initialized_check(void)
 		return is_inited;
 	}
 
-	spin_lock_irqsave(&smem_init_check_lock, flags);
+	raw_spin_lock_irqsave(&smem_init_check_lock, flags);
 	if (checked) {
-		spin_unlock_irqrestore(&smem_init_check_lock, flags);
+		raw_spin_unlock_irqrestore(&smem_init_check_lock, flags);
 		if (unlikely(!is_inited))
 			LOG_ERR("%s: smem not initialized\n", __func__);
 		return is_inited;
@@ -982,13 +982,13 @@ bool smem_initialized_check(void)
 
 	is_inited = 1;
 	checked = 1;
-	spin_unlock_irqrestore(&smem_init_check_lock, flags);
+	raw_spin_unlock_irqrestore(&smem_init_check_lock, flags);
 	return is_inited;
 
 failed:
 	is_inited = 0;
 	checked = 1;
-	spin_unlock_irqrestore(&smem_init_check_lock, flags);
+	raw_spin_unlock_irqrestore(&smem_init_check_lock, flags);
 	LOG_ERR(
 		"%s: shared memory needs to be initialized by SBL before booting\n",
 								__func__);
diff --git a/kernel/msm-3.18/drivers/soc/qcom/smp2p.c b/kernel/msm-3.18/drivers/soc/qcom/smp2p.c
index 79b8ffbc5..9ab8f6c29 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smp2p.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/smp2p.c
@@ -59,7 +59,7 @@ struct msm_smp2p_out {
  * @restart_ack: Current cached state of the local ack bit
  */
 struct smp2p_out_list_item {
-	spinlock_t out_item_lock_lha1;
+	raw_spinlock_t out_item_lock_lha1;
 
 	struct list_head list;
 	struct smp2p_smem __iomem *smem_edge_out;
@@ -105,7 +105,7 @@ struct smp2p_in {
  * @smem_edge_in: Pointer to the remote smem item.
  */
 struct smp2p_in_list_item {
-	spinlock_t in_item_lock_lhb1;
+	raw_spinlock_t in_item_lock_lhb1;
 	struct list_head list;
 	struct smp2p_smem __iomem *smem_edge_in;
 	uint32_t item_size;
@@ -274,10 +274,10 @@ struct smp2p_smem __iomem *smp2p_get_in_item(int remote_pid)
 	void *ret = NULL;
 	unsigned long flags;
 
-	spin_lock_irqsave(&in_list[remote_pid].in_item_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&in_list[remote_pid].in_item_lock_lhb1, flags);
 	if (remote_pid < SMP2P_NUM_PROCS)
 		ret = in_list[remote_pid].smem_edge_in;
-	spin_unlock_irqrestore(&in_list[remote_pid].in_item_lock_lhb1,
+	raw_spin_unlock_irqrestore(&in_list[remote_pid].in_item_lock_lhb1,
 								flags);
 
 	return ret;
@@ -295,13 +295,13 @@ struct smp2p_smem __iomem *smp2p_get_out_item(int remote_pid, int *state)
 	void *ret = NULL;
 	unsigned long flags;
 
-	spin_lock_irqsave(&out_list[remote_pid].out_item_lock_lha1, flags);
+	raw_spin_lock_irqsave(&out_list[remote_pid].out_item_lock_lha1, flags);
 	if (remote_pid < SMP2P_NUM_PROCS) {
 		ret = out_list[remote_pid].smem_edge_out;
 		if (state)
 			*state = out_list[remote_pid].smem_edge_state;
 	}
-	spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1, flags);
 
 	return ret;
 }
@@ -1039,9 +1039,9 @@ static int smp2p_do_negotiation(int remote_pid,
 	l_smem_ptr = out_item->smem_edge_out;
 
 	/* retrieve remote side and version */
-	spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
+	raw_spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
 	r_smem_ptr = smp2p_get_remote_smem_item(remote_pid, out_item);
-	spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
+	raw_spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
 
 	r_version = 0;
 	if (r_smem_ptr) {
@@ -1112,11 +1112,11 @@ static int smp2p_do_negotiation(int remote_pid,
 		}
 
 		/* update inbound edge */
-		spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
+		raw_spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
 		(void)out_item->ops_ptr->validate_size(remote_pid, r_smem_ptr,
 				in_list[remote_pid].item_size);
 		in_list[remote_pid].smem_edge_in = r_smem_ptr;
-		spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
+		raw_spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
 	} else {
 		SMP2P_INFO("%s: negotiation pid %d: State %d->%d F0x%08x\n",
 			__func__, remote_pid, prev_state,
@@ -1170,11 +1170,11 @@ int msm_smp2p_out_open(int remote_pid, const char *name,
 		return -ENOMEM;
 
 	/* Handle duplicate registration */
-	spin_lock_irqsave(&out_list[remote_pid].out_item_lock_lha1, flags);
+	raw_spin_lock_irqsave(&out_list[remote_pid].out_item_lock_lha1, flags);
 	list_for_each_entry(pos, &out_list[remote_pid].list,
 			out_edge_list) {
 		if (!strcmp(pos->name, name)) {
-			spin_unlock_irqrestore(
+			raw_spin_unlock_irqrestore(
 				&out_list[remote_pid].out_item_lock_lha1,
 				flags);
 			kfree(out_entry);
@@ -1198,14 +1198,14 @@ int msm_smp2p_out_open(int remote_pid, const char *name,
 		raw_notifier_chain_unregister(
 			&out_entry->msm_smp2p_notifier_list,
 			out_entry->open_nb);
-		spin_unlock_irqrestore(
+		raw_spin_unlock_irqrestore(
 			&out_list[remote_pid].out_item_lock_lha1, flags);
 		kfree(out_entry);
 		SMP2P_ERR("%s: unable to open '%s':%d error %d\n",
 				__func__, name, remote_pid, ret);
 		return ret;
 	}
-	spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1,
+	raw_spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1,
 			flags);
 	*handle = out_entry;
 
@@ -1242,11 +1242,11 @@ int msm_smp2p_out_close(struct msm_smp2p_out **handle)
 	}
 
 	out_item = &out_list[out_entry->remote_pid];
-	spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
+	raw_spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
 	list_del(&out_entry->out_edge_list);
 	raw_notifier_chain_unregister(&out_entry->msm_smp2p_notifier_list,
 		out_entry->open_nb);
-	spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
 
 	kfree(out_entry);
 
@@ -1281,9 +1281,9 @@ int msm_smp2p_out_read(struct msm_smp2p_out *handle, uint32_t *data)
 	}
 
 	out_item = &out_list[handle->remote_pid];
-	spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
+	raw_spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
 	ret = out_item->ops_ptr->read_entry(handle, data);
-	spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
 
 	return ret;
 }
@@ -1319,9 +1319,9 @@ int msm_smp2p_out_write(struct msm_smp2p_out *handle, uint32_t data)
 	}
 
 	out_item = &out_list[handle->remote_pid];
-	spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
+	raw_spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
 	ret = out_item->ops_ptr->write_entry(handle, data);
-	spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
 
 	return ret;
 
@@ -1364,10 +1364,10 @@ int msm_smp2p_out_modify(struct msm_smp2p_out *handle, uint32_t set_mask,
 	}
 
 	out_item = &out_list[handle->remote_pid];
-	spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
+	raw_spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
 	ret = out_item->ops_ptr->modify_entry(handle, set_mask,
 						clear_mask, send_irq);
-	spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
 
 	return ret;
 }
@@ -1398,8 +1398,8 @@ int msm_smp2p_in_read(int remote_pid, const char *name, uint32_t *data)
 	}
 
 	out_item = &out_list[remote_pid];
-	spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
-	spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
+	raw_spin_lock_irqsave(&out_item->out_item_lock_lha1, flags);
+	raw_spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
 
 	if (in_list[remote_pid].smem_edge_in)
 		out_item->ops_ptr->find_entry(
@@ -1407,8 +1407,8 @@ int msm_smp2p_in_read(int remote_pid, const char *name, uint32_t *data)
 			in_list[remote_pid].safe_total_entries,
 			(char *)name, &entry_ptr, NULL);
 
-	spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
-	spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
+	raw_spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
+	raw_spin_unlock_irqrestore(&out_item->out_item_lock_lha1, flags);
 
 	if (!entry_ptr)
 		return -ENODEV;
@@ -1459,8 +1459,8 @@ int msm_smp2p_in_register(int pid, const char *name,
 		return -ENOMEM;
 
 	/* Search for existing entry */
-	spin_lock_irqsave(&out_list[pid].out_item_lock_lha1, flags);
-	spin_lock(&in_list[pid].in_item_lock_lhb1);
+	raw_spin_lock_irqsave(&out_list[pid].out_item_lock_lha1, flags);
+	raw_spin_lock(&in_list[pid].in_item_lock_lhb1);
 
 	list_for_each_entry(pos, &in_list[pid].list, in_edge_list) {
 		if (!strncmp(pos->name, name,
@@ -1509,8 +1509,8 @@ int msm_smp2p_in_register(int pid, const char *name,
 	SMP2P_DBG("%s: '%s':%d registered\n", __func__, name, pid);
 
 bail:
-	spin_unlock(&in_list[pid].in_item_lock_lhb1);
-	spin_unlock_irqrestore(&out_list[pid].out_item_lock_lha1, flags);
+	raw_spin_unlock(&in_list[pid].in_item_lock_lhb1);
+	raw_spin_unlock_irqrestore(&out_list[pid].out_item_lock_lha1, flags);
 	return ret;
 
 }
@@ -1542,7 +1542,7 @@ int msm_smp2p_in_unregister(int remote_pid, const char *name,
 		return -EPROBE_DEFER;
 	}
 
-	spin_lock_irqsave(&in_list[remote_pid].in_item_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&in_list[remote_pid].in_item_lock_lhb1, flags);
 	list_for_each_entry(pos, &in_list[remote_pid].list,
 			in_edge_list) {
 		if (!strncmp(pos->name, name, SMP2P_MAX_ENTRY_NAME)) {
@@ -1569,7 +1569,7 @@ int msm_smp2p_in_unregister(int remote_pid, const char *name,
 	}
 
 fail:
-	spin_unlock_irqrestore(&in_list[remote_pid].in_item_lock_lhb1, flags);
+	raw_spin_unlock_irqrestore(&in_list[remote_pid].in_item_lock_lhb1, flags);
 
 	return ret;
 }
@@ -1621,12 +1621,12 @@ static void smp2p_in_edge_notify(int pid)
 	uint32_t curr_data;
 	struct  msm_smp2p_update_notif data;
 
-	spin_lock_irqsave(&in_list[pid].in_item_lock_lhb1, flags);
+	raw_spin_lock_irqsave(&in_list[pid].in_item_lock_lhb1, flags);
 	smem_h_ptr = in_list[pid].smem_edge_in;
 	if (!smem_h_ptr) {
 		SMP2P_DBG("%s: No remote SMEM item for pid %d\n",
 			__func__, pid);
-		spin_unlock_irqrestore(&in_list[pid].in_item_lock_lhb1, flags);
+		raw_spin_unlock_irqrestore(&in_list[pid].in_item_lock_lhb1, flags);
 		return;
 	}
 
@@ -1661,7 +1661,7 @@ static void smp2p_in_edge_notify(int pid)
 			}
 		}
 	}
-	spin_unlock_irqrestore(&in_list[pid].in_item_lock_lhb1, flags);
+	raw_spin_unlock_irqrestore(&in_list[pid].in_item_lock_lhb1, flags);
 }
 
 /**
@@ -1686,7 +1686,7 @@ static irqreturn_t smp2p_interrupt_handler(int irq, void *data)
 		SMP2P_DBG("SMP2P Int %s(%d)->Apps\n",
 			smp2p_int_cfgs[remote_pid].name, remote_pid);
 
-	spin_lock_irqsave(&out_list[remote_pid].out_item_lock_lha1, flags);
+	raw_spin_lock_irqsave(&out_list[remote_pid].out_item_lock_lha1, flags);
 	++smp2p_int_cfgs[remote_pid].in_interrupt_count;
 
 	if (out_list[remote_pid].smem_edge_state != SMP2P_EDGE_STATE_OPENED)
@@ -1707,29 +1707,29 @@ static irqreturn_t smp2p_interrupt_handler(int irq, void *data)
 		 * necessary to handle the race condition exposed by
 		 * unlocking the spinlocks.
 		 */
-		spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
+		raw_spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
 		do_restart_ack = smp2p_ssr_ack_needed(remote_pid);
-		spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
-		spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1,
+		raw_spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
+		raw_spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1,
 			flags);
 
 		smp2p_in_edge_notify(remote_pid);
 
 		if (do_restart_ack) {
-			spin_lock_irqsave(
+			raw_spin_lock_irqsave(
 				&out_list[remote_pid].out_item_lock_lha1,
 				flags);
-			spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
+			raw_spin_lock(&in_list[remote_pid].in_item_lock_lhb1);
 
 			smp2p_do_ssr_ack(remote_pid);
 
-			spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
-			spin_unlock_irqrestore(
+			raw_spin_unlock(&in_list[remote_pid].in_item_lock_lhb1);
+			raw_spin_unlock_irqrestore(
 				&out_list[remote_pid].out_item_lock_lha1,
 				flags);
 		}
 	} else {
-		spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1,
+		raw_spin_unlock_irqrestore(&out_list[remote_pid].out_item_lock_lha1,
 			flags);
 	}
 
@@ -1749,8 +1749,8 @@ int smp2p_reset_mock_edge(void)
 	unsigned long flags;
 	int ret = 0;
 
-	spin_lock_irqsave(&out_list[rpid].out_item_lock_lha1, flags);
-	spin_lock(&in_list[rpid].in_item_lock_lhb1);
+	raw_spin_lock_irqsave(&out_list[rpid].out_item_lock_lha1, flags);
+	raw_spin_lock(&in_list[rpid].in_item_lock_lhb1);
 
 	if (!list_empty(&out_list[rpid].list) ||
 			!list_empty(&in_list[rpid].list)) {
@@ -1770,8 +1770,8 @@ int smp2p_reset_mock_edge(void)
 	in_list[rpid].safe_total_entries = 0;
 
 fail:
-	spin_unlock(&in_list[rpid].in_item_lock_lhb1);
-	spin_unlock_irqrestore(&out_list[rpid].out_item_lock_lha1, flags);
+	raw_spin_unlock(&in_list[rpid].in_item_lock_lhb1);
+	raw_spin_unlock_irqrestore(&out_list[rpid].out_item_lock_lha1, flags);
 
 	return ret;
 }
@@ -1919,7 +1919,7 @@ static int __init msm_smp2p_init(void)
 	int rc;
 
 	for (i = 0; i < SMP2P_NUM_PROCS; i++) {
-		spin_lock_init(&out_list[i].out_item_lock_lha1);
+		raw_spin_lock_init(&out_list[i].out_item_lock_lha1);
 		INIT_LIST_HEAD(&out_list[i].list);
 		out_list[i].smem_edge_out = NULL;
 		out_list[i].smem_edge_state = SMP2P_EDGE_STATE_CLOSED;
@@ -1927,7 +1927,7 @@ static int __init msm_smp2p_init(void)
 		out_list[i].feature_ssr_ack_enabled = false;
 		out_list[i].restart_ack = false;
 
-		spin_lock_init(&in_list[i].in_item_lock_lhb1);
+		raw_spin_lock_init(&in_list[i].in_item_lock_lhb1);
 		INIT_LIST_HEAD(&in_list[i].list);
 		in_list[i].smem_edge_in = NULL;
 	}
diff --git a/kernel/msm-3.18/drivers/soc/qcom/smp2p_spinlock_test.c b/kernel/msm-3.18/drivers/soc/qcom/smp2p_spinlock_test.c
index 27ad4f4c5..418a3da72 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smp2p_spinlock_test.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/smp2p_spinlock_test.c
@@ -532,7 +532,7 @@ static void smp2p_ut_remote_spinlock_ssr(struct seq_file *s)
 
 		remote_spin_lock_irqsave(smem_spinlock, flags);
 		/* Unlock local spin lock and hold HW spinlock */
-		spin_unlock_irqrestore(&((smem_spinlock)->local), flags);
+		raw_spin_unlock_irqrestore(&((smem_spinlock)->local), flags);
 
 		queue_work(ws, &work_item.work);
 		UT_ASSERT_INT(
diff --git a/kernel/msm-3.18/drivers/soc/qcom/smp2p_test_common.h b/kernel/msm-3.18/drivers/soc/qcom/smp2p_test_common.h
index 3be519bc0..fbfa553bf 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smp2p_test_common.h
+++ b/kernel/msm-3.18/drivers/soc/qcom/smp2p_test_common.h
@@ -118,7 +118,7 @@
 /* Structure to track state changes for the notifier callback. */
 struct mock_cb_data {
 	bool initialized;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct notifier_block nb;
 
 	/* events */
@@ -159,7 +159,7 @@ static inline void mock_cb_data_init(struct mock_cb_data *cb)
 {
 	if (!cb->initialized) {
 		init_completion(&cb->cb_completion);
-		spin_lock_init(&cb->lock);
+		raw_spin_lock_init(&cb->lock);
 		cb->initialized = true;
 		cb->nb.notifier_call = smp2p_test_notify;
 		memset(&cb->entry_data, 0,
@@ -184,7 +184,7 @@ static inline int smp2p_test_notify(struct notifier_block *self,
 
 	cb_data_ptr = container_of(self, struct mock_cb_data, nb);
 
-	spin_lock_irqsave(&cb_data_ptr->lock, flags);
+	raw_spin_lock_irqsave(&cb_data_ptr->lock, flags);
 
 	switch (event) {
 	case SMP2P_OPEN:
@@ -208,7 +208,7 @@ static inline int smp2p_test_notify(struct notifier_block *self,
 
 	++cb_data_ptr->cb_count;
 	complete(&cb_data_ptr->cb_completion);
-	spin_unlock_irqrestore(&cb_data_ptr->lock, flags);
+	raw_spin_unlock_irqrestore(&cb_data_ptr->lock, flags);
 	return 0;
 }
 #endif /* _SMP2P_TEST_COMMON_H_ */
diff --git a/kernel/msm-3.18/drivers/soc/qcom/subsystem_restart.c b/kernel/msm-3.18/drivers/soc/qcom/subsystem_restart.c
index fbd8d0a19..7c9f48b43 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/subsystem_restart.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/subsystem_restart.c
@@ -106,7 +106,7 @@ static const char * const restart_levels[] = {
  */
 struct subsys_tracking {
 	enum p_subsys_state p_state;
-	spinlock_t s_lock;
+	raw_spinlock_t s_lock;
 	enum subsys_state state;
 	struct mutex lock;
 };
@@ -342,14 +342,14 @@ static void subsys_set_state(struct subsys_device *subsys,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&subsys->track.s_lock, flags);
+	raw_spin_lock_irqsave(&subsys->track.s_lock, flags);
 	if (subsys->track.state != state) {
 		subsys->track.state = state;
-		spin_unlock_irqrestore(&subsys->track.s_lock, flags);
+		raw_spin_unlock_irqrestore(&subsys->track.s_lock, flags);
 		sysfs_notify(&subsys->dev.kobj, NULL, "state");
 		return;
 	}
-	spin_unlock_irqrestore(&subsys->track.s_lock, flags);
+	raw_spin_unlock_irqrestore(&subsys->track.s_lock, flags);
 }
 
 static ssize_t error_show(struct device *dev,
@@ -1005,9 +1005,9 @@ static void subsystem_restart_wq_func(struct work_struct *work)
 	notify_each_subsys_device(list, count, SUBSYS_RAMDUMP_NOTIFICATION,
 									NULL);
 
-	spin_lock_irqsave(&track->s_lock, flags);
+	raw_spin_lock_irqsave(&track->s_lock, flags);
 	track->p_state = SUBSYS_RESTARTING;
-	spin_unlock_irqrestore(&track->s_lock, flags);
+	raw_spin_unlock_irqrestore(&track->s_lock, flags);
 
 	/* Collect ram dumps for all subsystems in order here */
 	for_each_subsys_device(list, count, NULL, subsystem_ramdump);
@@ -1031,10 +1031,10 @@ err:
 	mutex_unlock(&soc_order_reg_lock);
 	mutex_unlock(&track->lock);
 
-	spin_lock_irqsave(&track->s_lock, flags);
+	raw_spin_lock_irqsave(&track->s_lock, flags);
 	track->p_state = SUBSYS_NORMAL;
 	__pm_relax(&dev->ssr_wlock);
-	spin_unlock_irqrestore(&track->s_lock, flags);
+	raw_spin_unlock_irqrestore(&track->s_lock, flags);
 }
 
 static void __subsystem_restart_dev(struct subsys_device *dev)
@@ -1052,7 +1052,7 @@ static void __subsystem_restart_dev(struct subsys_device *dev)
 	 * Allow drivers to call subsystem_restart{_dev}() as many times as
 	 * they want up until the point where the subsystem is shutdown.
 	 */
-	spin_lock_irqsave(&track->s_lock, flags);
+	raw_spin_lock_irqsave(&track->s_lock, flags);
 	if (track->p_state != SUBSYS_CRASHED &&
 					dev->track.state == SUBSYS_ONLINE) {
 		if (track->p_state != SUBSYS_RESTARTING) {
@@ -1065,7 +1065,7 @@ static void __subsystem_restart_dev(struct subsys_device *dev)
 	} else
 		WARN(dev->track.state == SUBSYS_OFFLINE,
 			"SSR aborted: %s subsystem not online\n", name);
-	spin_unlock_irqrestore(&track->s_lock, flags);
+	raw_spin_unlock_irqrestore(&track->s_lock, flags);
 }
 
 static void device_restart_work_hdlr(struct work_struct *work)
@@ -1426,7 +1426,7 @@ static struct subsys_soc_restart_order *ssr_parse_restart_orders(struct
 
 	order->count = count;
 	mutex_init(&order->track.lock);
-	spin_lock_init(&order->track.s_lock);
+	raw_spin_lock_init(&order->track.s_lock);
 
 	INIT_LIST_HEAD(&order->list);
 	list_add_tail(&order->list, &ssr_order_list);
@@ -1650,7 +1650,7 @@ struct subsys_device *subsys_register(struct subsys_desc *desc)
 	wakeup_source_init(&subsys->ssr_wlock, subsys->wlname);
 	INIT_WORK(&subsys->work, subsystem_restart_wq_func);
 	INIT_WORK(&subsys->device_restart_work, device_restart_work_hdlr);
-	spin_lock_init(&subsys->track.s_lock);
+	raw_spin_lock_init(&subsys->track.s_lock);
 
 	subsys->id = ida_simple_get(&subsys_ida, 0, 0, GFP_KERNEL);
 	if (subsys->id < 0) {
diff --git a/kernel/msm-3.18/drivers/spi/spi_qsd.c b/kernel/msm-3.18/drivers/spi/spi_qsd.c
index 949146b43..56ce716d4 100644
--- a/kernel/msm-3.18/drivers/spi/spi_qsd.c
+++ b/kernel/msm-3.18/drivers/spi/spi_qsd.c
@@ -1658,9 +1658,9 @@ static int msm_spi_transfer_one(struct spi_master *master,
 
 	mutex_lock(&dd->core_lock);
 
-	spin_lock_irqsave(&dd->queue_lock, flags);
+	raw_spin_lock_irqsave(&dd->queue_lock, flags);
 	dd->transfer_pending = 1;
-	spin_unlock_irqrestore(&dd->queue_lock, flags);
+	raw_spin_unlock_irqrestore(&dd->queue_lock, flags);
 	/*
 	 * get local resources for each transfer to ensure we're in a good
 	 * state and not interfering with other EE's using this device
@@ -1692,9 +1692,9 @@ static int msm_spi_transfer_one(struct spi_master *master,
 		status_error =
 			msm_spi_process_transfer(dd);
 
-	spin_lock_irqsave(&dd->queue_lock, flags);
+	raw_spin_lock_irqsave(&dd->queue_lock, flags);
 	dd->transfer_pending = 0;
-	spin_unlock_irqrestore(&dd->queue_lock, flags);
+	raw_spin_unlock_irqrestore(&dd->queue_lock, flags);
 
 	/*
 	 * Put local resources prior to calling finalize to ensure the hw
@@ -2554,7 +2554,7 @@ static int msm_spi_probe(struct platform_device *pdev)
 
 skip_dma_resources:
 
-	spin_lock_init(&dd->queue_lock);
+	raw_spin_lock_init(&dd->queue_lock);
 	mutex_init(&dd->core_lock);
 	init_waitqueue_head(&dd->continue_suspend);
 
@@ -2620,9 +2620,9 @@ static int msm_spi_pm_suspend_runtime(struct device *device)
 	 * Make sure nothing is added to the queue while we're
 	 * suspending
 	 */
-	spin_lock_irqsave(&dd->queue_lock, flags);
+	raw_spin_lock_irqsave(&dd->queue_lock, flags);
 	dd->suspended = 1;
-	spin_unlock_irqrestore(&dd->queue_lock, flags);
+	raw_spin_unlock_irqrestore(&dd->queue_lock, flags);
 
 	/* Wait for transactions to end, or time out */
 	wait_event_interruptible(dd->continue_suspend,
diff --git a/kernel/msm-3.18/drivers/spi/spi_qsd.h b/kernel/msm-3.18/drivers/spi/spi_qsd.h
index d8d13755e..c172f5c26 100644
--- a/kernel/msm-3.18/drivers/spi/spi_qsd.h
+++ b/kernel/msm-3.18/drivers/spi/spi_qsd.h
@@ -286,7 +286,7 @@ struct msm_spi {
 	const u8                *write_buf;
 	void __iomem            *base;
 	struct device           *dev;
-	spinlock_t               queue_lock;
+	raw_spinlock_t               queue_lock;
 	struct mutex             core_lock;
 	struct spi_device       *spi;
 	struct spi_transfer     *cur_transfer;
diff --git a/kernel/msm-3.18/drivers/staging/android/oneshot_sync.c b/kernel/msm-3.18/drivers/staging/android/oneshot_sync.c
index d8c8db61d..2c5f5924f 100644
--- a/kernel/msm-3.18/drivers/staging/android/oneshot_sync.c
+++ b/kernel/msm-3.18/drivers/staging/android/oneshot_sync.c
@@ -34,7 +34,7 @@
  */
 struct oneshot_sync_timeline {
 	struct sync_timeline obj;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct list_head state_list;
 	unsigned int id;
 };
@@ -76,9 +76,9 @@ static void oneshot_state_destroy(struct kref *ref)
 	struct oneshot_sync_state *state =
 		container_of(ref, struct oneshot_sync_state, refcount);
 
-	spin_lock(&state->timeline->lock);
+	raw_spin_lock(&state->timeline->lock);
 	list_del(&state->node);
-	spin_unlock(&state->timeline->lock);
+	raw_spin_unlock(&state->timeline->lock);
 
 	kfree(state);
 }
@@ -106,12 +106,12 @@ oneshot_pt_create(struct oneshot_sync_timeline *timeline)
 	pt->state->signaled = false;
 	pt->state->timeline = timeline;
 
-	spin_lock(&timeline->lock);
+	raw_spin_lock(&timeline->lock);
 	/* assign an id to the state, which could be shared by several pts. */
 	pt->state->id = ++(timeline->id);
 	/* add this pt to the list of pts that can be signaled by userspace */
 	list_add_tail(&pt->state->node, &timeline->state_list);
-	spin_unlock(&timeline->lock);
+	raw_spin_unlock(&timeline->lock);
 
 	return pt;
 error:
@@ -168,7 +168,7 @@ static void oneshot_pt_free(struct sync_pt *sync_pt)
 		to_oneshot_timeline(sync_pt->parent) : NULL;
 
 	if (timeline != NULL) {
-		spin_lock(&timeline->lock);
+		raw_spin_lock(&timeline->lock);
 		/*
 		 * If this is the original pt (and fence), signal to avoid
 		 * deadlock. Unfornately, we can't signal the timeline here
@@ -187,7 +187,7 @@ static void oneshot_pt_free(struct sync_pt *sync_pt)
 				pt->state->signaled = true;
 			}
 		}
-		spin_unlock(&timeline->lock);
+		raw_spin_unlock(&timeline->lock);
 	}
 	oneshot_state_put(pt->state);
 }
@@ -225,7 +225,7 @@ struct oneshot_sync_timeline *oneshot_timeline_create(const char *name)
 		return NULL;
 
 	INIT_LIST_HEAD(&timeline->state_list);
-	spin_lock_init(&timeline->lock);
+	raw_spin_lock_init(&timeline->lock);
 
 	return timeline;
 }
@@ -270,7 +270,7 @@ int oneshot_fence_signal(struct oneshot_sync_timeline *timeline,
 	if (timeline == NULL || fence == NULL)
 		return -EINVAL;
 
-	spin_lock(&timeline->lock);
+	raw_spin_lock(&timeline->lock);
 	list_for_each_entry(state, &timeline->state_list, node) {
 		/*
 		 * If we have the point from this fence on our list,
@@ -286,7 +286,7 @@ int oneshot_fence_signal(struct oneshot_sync_timeline *timeline,
 			break;
 		}
 	}
-	spin_unlock(&timeline->lock);
+	raw_spin_unlock(&timeline->lock);
 	if (ret == -EINVAL)
 		pr_debug("fence: %pK not from this timeline\n", fence);
 
diff --git a/kernel/msm-3.18/drivers/staging/android/sync.c b/kernel/msm-3.18/drivers/staging/android/sync.c
index 2df8a1337..3517c16fe 100644
--- a/kernel/msm-3.18/drivers/staging/android/sync.c
+++ b/kernel/msm-3.18/drivers/staging/android/sync.c
@@ -59,14 +59,14 @@ struct sync_timeline *sync_timeline_create(const struct sync_timeline_ops *ops,
 	strlcpy(obj->name, name, sizeof(obj->name));
 
 	INIT_LIST_HEAD(&obj->child_list_head);
-	spin_lock_init(&obj->child_list_lock);
+	raw_spin_lock_init(&obj->child_list_lock);
 
 	INIT_LIST_HEAD(&obj->active_list_head);
-	spin_lock_init(&obj->active_list_lock);
+	raw_spin_lock_init(&obj->active_list_lock);
 
-	spin_lock_irqsave(&sync_timeline_list_lock, flags);
+	raw_spin_lock_irqsave(&sync_timeline_list_lock, flags);
 	list_add_tail(&obj->sync_timeline_list, &sync_timeline_list_head);
-	spin_unlock_irqrestore(&sync_timeline_list_lock, flags);
+	raw_spin_unlock_irqrestore(&sync_timeline_list_lock, flags);
 
 	return obj;
 }
@@ -78,9 +78,9 @@ static void sync_timeline_free(struct kref *kref)
 		container_of(kref, struct sync_timeline, kref);
 	unsigned long flags;
 
-	spin_lock_irqsave(&sync_timeline_list_lock, flags);
+	raw_spin_lock_irqsave(&sync_timeline_list_lock, flags);
 	list_del(&obj->sync_timeline_list);
-	spin_unlock_irqrestore(&sync_timeline_list_lock, flags);
+	raw_spin_unlock_irqrestore(&sync_timeline_list_lock, flags);
 
 	if (obj->ops->release_obj)
 		obj->ops->release_obj(obj);
@@ -112,9 +112,9 @@ static void sync_timeline_add_pt(struct sync_timeline *obj, struct sync_pt *pt)
 
 	pt->parent = obj;
 
-	spin_lock_irqsave(&obj->child_list_lock, flags);
+	raw_spin_lock_irqsave(&obj->child_list_lock, flags);
 	list_add_tail(&pt->child_list, &obj->child_list_head);
-	spin_unlock_irqrestore(&obj->child_list_lock, flags);
+	raw_spin_unlock_irqrestore(&obj->child_list_lock, flags);
 }
 
 static void sync_timeline_remove_pt(struct sync_pt *pt)
@@ -122,16 +122,16 @@ static void sync_timeline_remove_pt(struct sync_pt *pt)
 	struct sync_timeline *obj = pt->parent;
 	unsigned long flags;
 
-	spin_lock_irqsave(&obj->active_list_lock, flags);
+	raw_spin_lock_irqsave(&obj->active_list_lock, flags);
 	if (!list_empty(&pt->active_list))
 		list_del_init(&pt->active_list);
-	spin_unlock_irqrestore(&obj->active_list_lock, flags);
+	raw_spin_unlock_irqrestore(&obj->active_list_lock, flags);
 
-	spin_lock_irqsave(&obj->child_list_lock, flags);
+	raw_spin_lock_irqsave(&obj->child_list_lock, flags);
 	if (!list_empty(&pt->child_list))
 		list_del_init(&pt->child_list);
 
-	spin_unlock_irqrestore(&obj->child_list_lock, flags);
+	raw_spin_unlock_irqrestore(&obj->child_list_lock, flags);
 }
 
 void sync_timeline_signal(struct sync_timeline *obj)
@@ -142,7 +142,7 @@ void sync_timeline_signal(struct sync_timeline *obj)
 
 	trace_sync_timeline(obj);
 
-	spin_lock_irqsave(&obj->active_list_lock, flags);
+	raw_spin_lock_irqsave(&obj->active_list_lock, flags);
 
 	list_for_each_safe(pos, n, &obj->active_list_head) {
 		struct sync_pt *pt =
@@ -155,7 +155,7 @@ void sync_timeline_signal(struct sync_timeline *obj)
 		}
 	}
 
-	spin_unlock_irqrestore(&obj->active_list_lock, flags);
+	raw_spin_unlock_irqrestore(&obj->active_list_lock, flags);
 
 	list_for_each_safe(pos, n, &signaled_pts) {
 		struct sync_pt *pt =
@@ -229,7 +229,7 @@ static void sync_pt_activate(struct sync_pt *pt)
 	unsigned long flags;
 	int err;
 
-	spin_lock_irqsave(&obj->active_list_lock, flags);
+	raw_spin_lock_irqsave(&obj->active_list_lock, flags);
 
 	err = _sync_pt_has_signaled(pt);
 	if (err != 0)
@@ -238,7 +238,7 @@ static void sync_pt_activate(struct sync_pt *pt)
 	list_add_tail(&pt->active_list, &obj->active_list_head);
 
 out:
-	spin_unlock_irqrestore(&obj->active_list_lock, flags);
+	raw_spin_unlock_irqrestore(&obj->active_list_lock, flags);
 }
 
 static int sync_fence_release(struct inode *inode, struct file *file);
@@ -273,13 +273,13 @@ static struct sync_fence *sync_fence_alloc(const char *name)
 
 	INIT_LIST_HEAD(&fence->pt_list_head);
 	INIT_LIST_HEAD(&fence->waiter_list_head);
-	spin_lock_init(&fence->waiter_list_lock);
+	raw_spin_lock_init(&fence->waiter_list_lock);
 
 	init_waitqueue_head(&fence->wq);
 
-	spin_lock_irqsave(&sync_fence_list_lock, flags);
+	raw_spin_lock_irqsave(&sync_fence_list_lock, flags);
 	list_add_tail(&fence->sync_fence_list, &sync_fence_list_head);
-	spin_unlock_irqrestore(&sync_fence_list_lock, flags);
+	raw_spin_unlock_irqrestore(&sync_fence_list_lock, flags);
 
 	return fence;
 
@@ -505,7 +505,7 @@ static void sync_fence_signal_pt(struct sync_pt *pt)
 
 	status = sync_fence_get_status(fence);
 
-	spin_lock_irqsave(&fence->waiter_list_lock, flags);
+	raw_spin_lock_irqsave(&fence->waiter_list_lock, flags);
 	/*
 	 * this should protect against two threads racing on the signaled
 	 * false -> true transition
@@ -518,7 +518,7 @@ static void sync_fence_signal_pt(struct sync_pt *pt)
 	} else {
 		status = 0;
 	}
-	spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
+	raw_spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
 
 	if (status) {
 		list_for_each_safe(pos, n, &signaled_waiters) {
@@ -539,7 +539,7 @@ int sync_fence_wait_async(struct sync_fence *fence,
 	unsigned long flags;
 	int err = 0;
 
-	spin_lock_irqsave(&fence->waiter_list_lock, flags);
+	raw_spin_lock_irqsave(&fence->waiter_list_lock, flags);
 
 	if (fence->status) {
 		err = fence->status;
@@ -548,7 +548,7 @@ int sync_fence_wait_async(struct sync_fence *fence,
 
 	list_add_tail(&waiter->waiter_list, &fence->waiter_list_head);
 out:
-	spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
+	raw_spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
 
 	return err;
 }
@@ -562,7 +562,7 @@ int sync_fence_cancel_async(struct sync_fence *fence,
 	unsigned long flags;
 	int ret = -ENOENT;
 
-	spin_lock_irqsave(&fence->waiter_list_lock, flags);
+	raw_spin_lock_irqsave(&fence->waiter_list_lock, flags);
 	/*
 	 * Make sure waiter is still in waiter_list because it is possible for
 	 * the waiter to be removed from the list while the callback is still
@@ -578,7 +578,7 @@ int sync_fence_cancel_async(struct sync_fence *fence,
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
+	raw_spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL(sync_fence_cancel_async);
@@ -655,9 +655,9 @@ static int sync_fence_release(struct inode *inode, struct file *file)
 	 *
 	 * start with its membership in the global fence list
 	 */
-	spin_lock_irqsave(&sync_fence_list_lock, flags);
+	raw_spin_lock_irqsave(&sync_fence_list_lock, flags);
 	list_del(&fence->sync_fence_list);
-	spin_unlock_irqrestore(&sync_fence_list_lock, flags);
+	raw_spin_unlock_irqrestore(&sync_fence_list_lock, flags);
 
 	/*
 	 * remove its pts from their parents so that sync_timeline_signal()
@@ -911,13 +911,13 @@ static void sync_print_obj(struct seq_file *s, struct sync_timeline *obj)
 
 	seq_puts(s, "\n");
 
-	spin_lock_irqsave(&obj->child_list_lock, flags);
+	raw_spin_lock_irqsave(&obj->child_list_lock, flags);
 	list_for_each(pos, &obj->child_list_head) {
 		struct sync_pt *pt =
 			container_of(pos, struct sync_pt, child_list);
 		sync_print_pt(s, pt, false);
 	}
-	spin_unlock_irqrestore(&obj->child_list_lock, flags);
+	raw_spin_unlock_irqrestore(&obj->child_list_lock, flags);
 }
 
 static void sync_print_fence(struct seq_file *s, struct sync_fence *fence)
@@ -934,7 +934,7 @@ static void sync_print_fence(struct seq_file *s, struct sync_fence *fence)
 		sync_print_pt(s, pt, true);
 	}
 
-	spin_lock_irqsave(&fence->waiter_list_lock, flags);
+	raw_spin_lock_irqsave(&fence->waiter_list_lock, flags);
 	list_for_each(pos, &fence->waiter_list_head) {
 		struct sync_fence_waiter *waiter =
 			container_of(pos, struct sync_fence_waiter,
@@ -942,7 +942,7 @@ static void sync_print_fence(struct seq_file *s, struct sync_fence *fence)
 
 		seq_printf(s, "waiter %pF\n", waiter->callback);
 	}
-	spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
+	raw_spin_unlock_irqrestore(&fence->waiter_list_lock, flags);
 }
 
 static int sync_debugfs_show(struct seq_file *s, void *unused)
@@ -952,7 +952,7 @@ static int sync_debugfs_show(struct seq_file *s, void *unused)
 
 	seq_puts(s, "objs:\n--------------\n");
 
-	spin_lock_irqsave(&sync_timeline_list_lock, flags);
+	raw_spin_lock_irqsave(&sync_timeline_list_lock, flags);
 	list_for_each(pos, &sync_timeline_list_head) {
 		struct sync_timeline *obj =
 			container_of(pos, struct sync_timeline,
@@ -961,11 +961,11 @@ static int sync_debugfs_show(struct seq_file *s, void *unused)
 		sync_print_obj(s, obj);
 		seq_puts(s, "\n");
 	}
-	spin_unlock_irqrestore(&sync_timeline_list_lock, flags);
+	raw_spin_unlock_irqrestore(&sync_timeline_list_lock, flags);
 
 	seq_puts(s, "fences:\n--------------\n");
 
-	spin_lock_irqsave(&sync_fence_list_lock, flags);
+	raw_spin_lock_irqsave(&sync_fence_list_lock, flags);
 	list_for_each(pos, &sync_fence_list_head) {
 		struct sync_fence *fence =
 			container_of(pos, struct sync_fence, sync_fence_list);
@@ -973,7 +973,7 @@ static int sync_debugfs_show(struct seq_file *s, void *unused)
 		sync_print_fence(s, fence);
 		seq_puts(s, "\n");
 	}
-	spin_unlock_irqrestore(&sync_fence_list_lock, flags);
+	raw_spin_unlock_irqrestore(&sync_fence_list_lock, flags);
 	return 0;
 }
 
diff --git a/kernel/msm-3.18/drivers/thermal/msm-tsens.c b/kernel/msm-3.18/drivers/thermal/msm-tsens.c
index 2a9d6f621..87cc68cd1 100644
--- a/kernel/msm-3.18/drivers/thermal/msm-tsens.c
+++ b/kernel/msm-3.18/drivers/thermal/msm-tsens.c
@@ -858,9 +858,9 @@ struct tsens_tm_device {
 	struct delayed_work		tsens_critical_poll_test;
 	struct completion		tsens_rslt_completion;
 	struct tsens_mtc_sysfs		mtcsys;
-	spinlock_t			tsens_crit_lock;
-	spinlock_t			tsens_upp_low_lock;
-	spinlock_t			tsens_debug_lock;
+	raw_spinlock_t			tsens_crit_lock;
+	raw_spinlock_t			tsens_upp_low_lock;
+	raw_spinlock_t			tsens_debug_lock;
 	bool				crit_set;
 	struct tsens_dbg_counter	crit_timestamp_last_run;
 	struct tsens_dbg_counter	crit_timestamp_last_interrupt_handled;
@@ -1496,7 +1496,7 @@ static int msm_tsens_get_temp(int sensor_client_id, unsigned long *temp)
 		*temp = last_temp;
 	}
 
-	spin_lock_irqsave(&tmdev->tsens_debug_lock, flags);
+	raw_spin_lock_irqsave(&tmdev->tsens_debug_lock, flags);
 	idx = tmdev->sensor_dbg_info[sensor_hw_num].idx;
 	tmdev->sensor_dbg_info[sensor_hw_num].temp[idx%10] = *temp;
 	tmdev->sensor_dbg_info[sensor_hw_num].time_stmp[idx%10] =
@@ -1507,7 +1507,7 @@ static int msm_tsens_get_temp(int sensor_client_id, unsigned long *temp)
 			code;
 	idx++;
 	tmdev->sensor_dbg_info[sensor_hw_num].idx = idx;
-	spin_unlock_irqrestore(&tmdev->tsens_debug_lock, flags);
+	raw_spin_unlock_irqrestore(&tmdev->tsens_debug_lock, flags);
 
 	trace_tsens_read(*temp, sensor_client_id);
 
@@ -1647,7 +1647,7 @@ static int tsens_tm_activate_trip_type(struct thermal_zone_device *thermal,
 	if (!tmdev)
 		return -EINVAL;
 
-	spin_lock_irqsave(&tmdev->tsens_upp_low_lock, flags);
+	raw_spin_lock_irqsave(&tmdev->tsens_upp_low_lock, flags);
 	mask = (tm_sensor->sensor_hw_num);
 	switch (trip) {
 	case TSENS_TM_TRIP_CRITICAL:
@@ -1696,7 +1696,7 @@ static int tsens_tm_activate_trip_type(struct thermal_zone_device *thermal,
 		rc = -EINVAL;
 	}
 
-	spin_unlock_irqrestore(&tmdev->tsens_upp_low_lock, flags);
+	raw_spin_unlock_irqrestore(&tmdev->tsens_upp_low_lock, flags);
 	/* Activate and enable the respective trip threshold setting */
 	mb();
 
@@ -1891,7 +1891,7 @@ static int tsens_tm_set_trip_temp(struct thermal_zone_device *thermal,
 	if (!tmdev)
 		return -EINVAL;
 
-	spin_lock_irqsave(&tmdev->tsens_upp_low_lock, flags);
+	raw_spin_lock_irqsave(&tmdev->tsens_upp_low_lock, flags);
 	switch (trip) {
 	case TSENS_TM_TRIP_CRITICAL:
 		tmdev->sensor[tm_sensor->sensor_hw_num].
@@ -1932,7 +1932,7 @@ static int tsens_tm_set_trip_temp(struct thermal_zone_device *thermal,
 		rc = -EINVAL;
 	}
 
-	spin_unlock_irqrestore(&tmdev->tsens_upp_low_lock, flags);
+	raw_spin_unlock_irqrestore(&tmdev->tsens_upp_low_lock, flags);
 	/* Set trip temperature thresholds */
 	mb();
 	return rc;
@@ -2049,7 +2049,7 @@ static void tsens_poll(struct work_struct *work)
 	tmdev->crit_timestamp_last_run.idx++;
 	tmdev->qtimer_val_detection_start = arch_counter_get_cntpct();
 
-	spin_lock_irqsave(&tmdev->tsens_crit_lock, flags);
+	raw_spin_lock_irqsave(&tmdev->tsens_crit_lock, flags);
 	/* Clear the sensor0 critical status */
 	int_mask_val = 1;
 	writel_relaxed(int_mask_val,
@@ -2069,16 +2069,16 @@ static void tsens_poll(struct work_struct *work)
 		/* Enable the critical int mask */
 		mb();
 	}
-	spin_unlock_irqrestore(&tmdev->tsens_crit_lock, flags);
+	raw_spin_unlock_irqrestore(&tmdev->tsens_crit_lock, flags);
 
 	if (tmdev->tsens_critical_poll) {
 		usleep_range(TSENS_DEBUG_POLL_MIN,
 				TSENS_DEBUG_POLL_MAX);
 		sensor_status_addr = TSENS_TM_SN_STATUS(tmdev->tsens_addr);
 
-		spin_lock_irqsave(&tmdev->tsens_crit_lock, flags);
+		raw_spin_lock_irqsave(&tmdev->tsens_crit_lock, flags);
 		status = readl_relaxed(sensor_status_addr);
-		spin_unlock_irqrestore(&tmdev->tsens_crit_lock, flags);
+		raw_spin_unlock_irqrestore(&tmdev->tsens_crit_lock, flags);
 
 		if (status & TSENS_TM_SN_STATUS_CRITICAL_STATUS)
 			goto re_schedule;
@@ -2099,16 +2099,16 @@ static void tsens_poll(struct work_struct *work)
 		sensor_critical_addr =
 			TSENS_TM_SN_CRITICAL_THRESHOLD(tmdev->tsens_addr);
 
-		spin_lock_irqsave(&tmdev->tsens_crit_lock, flags);
+		raw_spin_lock_irqsave(&tmdev->tsens_crit_lock, flags);
 		if (!tmdev->crit_set) {
 			pr_debug("Ignore this check cycle\n");
-			spin_unlock_irqrestore(&tmdev->tsens_crit_lock, flags);
+			raw_spin_unlock_irqrestore(&tmdev->tsens_crit_lock, flags);
 			goto re_schedule;
 		}
 		status = readl_relaxed(sensor_status_addr);
 		int_mask = readl_relaxed(sensor_int_mask_addr);
 		tmdev->crit_set = false;
-		spin_unlock_irqrestore(&tmdev->tsens_crit_lock, flags);
+		raw_spin_unlock_irqrestore(&tmdev->tsens_crit_lock, flags);
 
 		idx = tmdev->crit_timestamp_last_poll_request.idx;
 		tmdev->crit_timestamp_last_poll_request.time_stmp[idx%10] =
@@ -2118,7 +2118,7 @@ static void tsens_poll(struct work_struct *work)
 						arch_counter_get_cntpct();
 		if (status & TSENS_TM_SN_STATUS_CRITICAL_STATUS) {
 
-			spin_lock_irqsave(&tmdev->tsens_crit_lock, flags);
+			raw_spin_lock_irqsave(&tmdev->tsens_crit_lock, flags);
 			int_mask = readl_relaxed(sensor_int_mask_addr);
 			int_mask_val = 1;
 			/* Mask the corresponding interrupt for the sensors */
@@ -2131,7 +2131,7 @@ static void tsens_poll(struct work_struct *work)
 			writel_relaxed(0,
 				TSENS_TM_CRITICAL_INT_CLEAR(
 					tmdev->tsens_addr));
-			spin_unlock_irqrestore(&tmdev->tsens_crit_lock, flags);
+			raw_spin_unlock_irqrestore(&tmdev->tsens_crit_lock, flags);
 
 			/* Clear critical status */
 			mb();
@@ -2452,7 +2452,7 @@ static irqreturn_t tsens_tm_critical_irq_thread(int irq, void *data)
 		int int_mask, int_mask_val;
 		uint32_t addr_offset;
 
-		spin_lock_irqsave(&tm->tsens_crit_lock, flags);
+		raw_spin_lock_irqsave(&tm->tsens_crit_lock, flags);
 		addr_offset = tm->sensor[i].sensor_hw_num *
 						TSENS_SN_ADDR_OFFSET;
 		status = readl_relaxed(sensor_status_addr + addr_offset);
@@ -2476,7 +2476,7 @@ static irqreturn_t tsens_tm_critical_irq_thread(int irq, void *data)
 			tm->sensor[i].debug_thr_state_copy.
 					crit_th_state = THERMAL_DEVICE_DISABLED;
 		}
-		spin_unlock_irqrestore(&tm->tsens_crit_lock, flags);
+		raw_spin_unlock_irqrestore(&tm->tsens_crit_lock, flags);
 
 		if (critical_thr) {
 			unsigned long temp;
@@ -2529,7 +2529,7 @@ static irqreturn_t tsens_tm_irq_thread(int irq, void *data)
 		bool upper_thr = false, lower_thr = false;
 		int int_mask, int_mask_val = 0;
 
-		spin_lock_irqsave(&tm->tsens_upp_low_lock, flags);
+		raw_spin_lock_irqsave(&tm->tsens_upp_low_lock, flags);
 		addr_offset = tm->sensor[i].sensor_hw_num *
 						TSENS_SN_ADDR_OFFSET;
 		status = readl_relaxed(sensor_status_addr + addr_offset);
@@ -2579,7 +2579,7 @@ static irqreturn_t tsens_tm_irq_thread(int irq, void *data)
 			tm->sensor[i].debug_thr_state_copy.
 					low_th_state = THERMAL_DEVICE_DISABLED;
 		}
-		spin_unlock_irqrestore(&tm->tsens_upp_low_lock, flags);
+		raw_spin_unlock_irqrestore(&tm->tsens_upp_low_lock, flags);
 
 		if (upper_thr || lower_thr) {
 			unsigned long temp;
@@ -5770,9 +5770,9 @@ static int tsens_tm_probe(struct platform_device *pdev)
 	for (i = 0; i < 16; i++)
 		tmdev->sensor_dbg_info[i].idx = 0;
 
-	spin_lock_init(&tmdev->tsens_crit_lock);
-	spin_lock_init(&tmdev->tsens_upp_low_lock);
-	spin_lock_init(&tmdev->tsens_debug_lock);
+	raw_spin_lock_init(&tmdev->tsens_crit_lock);
+	raw_spin_lock_init(&tmdev->tsens_upp_low_lock);
+	raw_spin_lock_init(&tmdev->tsens_debug_lock);
 
 	tmdev->is_ready = true;
 
diff --git a/kernel/msm-3.18/drivers/thermal/qpnp-adc-tm.c b/kernel/msm-3.18/drivers/thermal/qpnp-adc-tm.c
index 943cf214a..b488f2eda 100644
--- a/kernel/msm-3.18/drivers/thermal/qpnp-adc-tm.c
+++ b/kernel/msm-3.18/drivers/thermal/qpnp-adc-tm.c
@@ -217,8 +217,8 @@ struct qpnp_adc_thr_info {
 	u8		adc_tm_high_enable;
 	u8		adc_tm_low_thr_set;
 	u8		adc_tm_high_thr_set;
-	spinlock_t			adc_tm_low_lock;
-	spinlock_t			adc_tm_high_lock;
+	raw_spinlock_t			adc_tm_low_lock;
+	raw_spinlock_t			adc_tm_high_lock;
 };
 
 struct qpnp_adc_thr_client_info {
@@ -2311,10 +2311,10 @@ static int qpnp_adc_tm_read_status(struct qpnp_adc_tm_chip *chip)
 	}
 
 	if (chip->th_info.adc_tm_high_enable) {
-		spin_lock_irqsave(&chip->th_info.adc_tm_high_lock, flags);
+		raw_spin_lock_irqsave(&chip->th_info.adc_tm_high_lock, flags);
 		sensor_notify_num = chip->th_info.adc_tm_high_enable;
 		chip->th_info.adc_tm_high_enable = 0;
-		spin_unlock_irqrestore(&chip->th_info.adc_tm_high_lock, flags);
+		raw_spin_unlock_irqrestore(&chip->th_info.adc_tm_high_lock, flags);
 		while (i < chip->max_channels_available) {
 			if ((sensor_notify_num & 0x1) == 1) {
 				sensor_num = i;
@@ -2331,10 +2331,10 @@ static int qpnp_adc_tm_read_status(struct qpnp_adc_tm_chip *chip)
 	}
 
 	if (chip->th_info.adc_tm_low_enable) {
-		spin_lock_irqsave(&chip->th_info.adc_tm_low_lock, flags);
+		raw_spin_lock_irqsave(&chip->th_info.adc_tm_low_lock, flags);
 		sensor_notify_num = chip->th_info.adc_tm_low_enable;
 		chip->th_info.adc_tm_low_enable = 0;
-		spin_unlock_irqrestore(&chip->th_info.adc_tm_low_lock, flags);
+		raw_spin_unlock_irqrestore(&chip->th_info.adc_tm_low_lock, flags);
 		i = 0;
 		while (i < chip->max_channels_available) {
 			if ((sensor_notify_num & 0x1) == 1) {
@@ -3314,8 +3314,8 @@ static int qpnp_adc_tm_probe(struct spmi_device *spmi)
 	chip->adc_vote_enable = false;
 	dev_set_drvdata(&spmi->dev, chip);
 	list_add(&chip->list, &qpnp_adc_tm_device_list);
-	spin_lock_init(&chip->th_info.adc_tm_low_lock);
-	spin_lock_init(&chip->th_info.adc_tm_high_lock);
+	raw_spin_lock_init(&chip->th_info.adc_tm_low_lock);
+	raw_spin_lock_init(&chip->th_info.adc_tm_high_lock);
 
 	pr_debug("OK\n");
 	return 0;
diff --git a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
index f4e87c7cd..f5ed8b142 100644
--- a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
+++ b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
@@ -444,17 +444,17 @@ static int msm_hsl_loopback_enable_set(void *data, u64 val)
 
 	vid = msm_hsl_port->ver_id;
 	if (val) {
-		spin_lock_irqsave(&port->lock, flags);
+		raw_spin_lock_irqsave(&port->lock, flags);
 		ret = msm_hsl_read(port, regmap[vid][UARTDM_MR2]);
 		ret |= UARTDM_MR2_LOOP_MODE_BMSK;
 		msm_hsl_write(port, ret, regmap[vid][UARTDM_MR2]);
-		spin_unlock_irqrestore(&port->lock, flags);
+		raw_spin_unlock_irqrestore(&port->lock, flags);
 	} else {
-		spin_lock_irqsave(&port->lock, flags);
+		raw_spin_lock_irqsave(&port->lock, flags);
 		ret = msm_hsl_read(port, regmap[vid][UARTDM_MR2]);
 		ret &= ~UARTDM_MR2_LOOP_MODE_BMSK;
 		msm_hsl_write(port, ret, regmap[vid][UARTDM_MR2]);
-		spin_unlock_irqrestore(&port->lock, flags);
+		raw_spin_unlock_irqrestore(&port->lock, flags);
 	}
 
 	clk_en(port, 0);
@@ -476,9 +476,9 @@ static int msm_hsl_loopback_enable_get(void *data, u64 *val)
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 	ret = msm_hsl_read(port, regmap[msm_hsl_port->ver_id][UARTDM_MR2]);
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 	clk_en(port, 0);
 
 	*val = (ret & UARTDM_MR2_LOOP_MODE_BMSK) ? 1 : 0;
@@ -715,7 +715,7 @@ static irqreturn_t msm_hsl_irq(int irq, void *dev_id)
 	unsigned int misr;
 	unsigned long flags;
 
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 	vid = msm_hsl_port->ver_id;
 	misr = msm_hsl_read(port, regmap[vid][UARTDM_MISR]);
 	/* disable interrupt */
@@ -737,7 +737,7 @@ static irqreturn_t msm_hsl_irq(int irq, void *dev_id)
 
 	/* restore interrupt */
 	msm_hsl_write(port, msm_hsl_port->imr, regmap[vid][UARTDM_IMR]);
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 
 	return IRQ_HANDLED;
 }
@@ -1015,7 +1015,7 @@ static int msm_hsl_startup(struct uart_port *port)
 	else
 		rfr_level = port->fifosize;
 
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 
 	vid = msm_hsl_port->ver_id;
 	/* set automatic RFR level */
@@ -1025,7 +1025,7 @@ static int msm_hsl_startup(struct uart_port *port)
 	data |= UARTDM_MR1_AUTO_RFR_LEVEL1_BMSK & (rfr_level << 2);
 	data |= UARTDM_MR1_AUTO_RFR_LEVEL0_BMSK & rfr_level;
 	msm_hsl_write(port, data, regmap[vid][UARTDM_MR1]);
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 
 	ret = request_irq(port->irq, msm_hsl_irq, IRQF_TRIGGER_HIGH,
 			  msm_hsl_port->name, port);
@@ -1458,13 +1458,13 @@ static void msm_hsl_console_write(struct console *co, const char *s,
 		locked = spin_trylock(&port->lock);
 	else {
 		locked = 1;
-		spin_lock(&port->lock);
+		raw_spin_lock(&port->lock);
 	}
 	msm_hsl_write(port, 0, regmap[vid][UARTDM_IMR]);
 	uart_console_write(port, s, count, msm_hsl_console_putchar);
 	msm_hsl_write(port, msm_hsl_port->imr, regmap[vid][UARTDM_IMR]);
 	if (locked == 1)
-		spin_unlock(&port->lock);
+		raw_spin_unlock(&port->lock);
 }
 
 static int msm_hsl_console_setup(struct console *co, char *options)
diff --git a/kernel/msm-3.18/drivers/usb/dwc3/dwc3-msm.c b/kernel/msm-3.18/drivers/usb/dwc3/dwc3-msm.c
index 08b19abb4..954d12ab2 100644
--- a/kernel/msm-3.18/drivers/usb/dwc3/dwc3-msm.c
+++ b/kernel/msm-3.18/drivers/usb/dwc3/dwc3-msm.c
@@ -698,12 +698,12 @@ static int dwc3_msm_ep_queue(struct usb_ep *ep,
 	 * core and ensure that we queuing the request will finish
 	 * as soon as possible so we will release back the lock.
 	 */
-	spin_lock_irqsave(&dwc->lock, flags);
+	raw_spin_lock_irqsave(&dwc->lock, flags);
 	if (!dep->endpoint.desc) {
 		dev_err(mdwc->dev,
 			"%s: trying to queue request %pK to disabled ep %s\n",
 			__func__, request, ep->name);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -EPERM;
 	}
 
@@ -711,13 +711,13 @@ static int dwc3_msm_ep_queue(struct usb_ep *ep,
 		dev_err(mdwc->dev,
 			"ep [%s,%d] was unconfigured as msm endpoint\n",
 			ep->name, dep->number);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -EINVAL;
 	}
 
 	if (!request) {
 		dev_err(mdwc->dev, "%s: request is NULL\n", __func__);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -EINVAL;
 	}
 
@@ -725,14 +725,14 @@ static int dwc3_msm_ep_queue(struct usb_ep *ep,
 		dev_err(mdwc->dev, "%s: sps mode is not set\n",
 					__func__);
 
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -EINVAL;
 	}
 
 	/* HW restriction regarding TRB size (8KB) */
 	if (req->request.length < 0x2000) {
 		dev_err(mdwc->dev, "%s: Min TRB size is 8KB\n", __func__);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -EINVAL;
 	}
 
@@ -740,7 +740,7 @@ static int dwc3_msm_ep_queue(struct usb_ep *ep,
 		dev_err(mdwc->dev,
 			"%s: trying to queue dbm request %pK to control ep %s\n",
 			__func__, request, ep->name);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -EPERM;
 	}
 
@@ -749,7 +749,7 @@ static int dwc3_msm_ep_queue(struct usb_ep *ep,
 		dev_err(mdwc->dev,
 			"%s: trying to queue dbm request %pK tp ep %s\n",
 			__func__, request, ep->name);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -EPERM;
 	}
 	dep->busy_slot = 0;
@@ -762,7 +762,7 @@ static int dwc3_msm_ep_queue(struct usb_ep *ep,
 	req_complete = kzalloc(sizeof(*req_complete), gfp_flags);
 	if (!req_complete) {
 		dev_err(mdwc->dev, "%s: not enough memory\n", __func__);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -ENOMEM;
 	}
 	req_complete->req = request;
@@ -785,7 +785,7 @@ static int dwc3_msm_ep_queue(struct usb_ep *ep,
 		goto err;
 	}
 
-	spin_unlock_irqrestore(&dwc->lock, flags);
+	raw_spin_unlock_irqrestore(&dwc->lock, flags);
 	superspeed = dwc3_msm_is_dev_superspeed(mdwc);
 	dbm_set_speed(mdwc->dbm, (u8)superspeed);
 
@@ -793,7 +793,7 @@ static int dwc3_msm_ep_queue(struct usb_ep *ep,
 
 err:
 	list_del(&req_complete->list_item);
-	spin_unlock_irqrestore(&dwc->lock, flags);
+	raw_spin_unlock_irqrestore(&dwc->lock, flags);
 	kfree(req_complete);
 	return ret;
 }
@@ -1346,15 +1346,15 @@ static int dwc3_msm_gsi_ep_op(struct usb_ep *ep,
 	case GSI_EP_OP_CONFIG:
 		request = (struct usb_gsi_request *)op_data;
 		dbg_print(0xFF, "EP_CONFIG", 0, ep->name);
-		spin_lock_irqsave(&dwc->lock, flags);
+		raw_spin_lock_irqsave(&dwc->lock, flags);
 		gsi_configure_ep(ep, request);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		break;
 	case GSI_EP_OP_STARTXFER:
 		dbg_print(0xFF, "EP_STARTXFER", 0, ep->name);
-		spin_lock_irqsave(&dwc->lock, flags);
+		raw_spin_lock_irqsave(&dwc->lock, flags);
 		ret = gsi_startxfer_for_ep(ep);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		break;
 	case GSI_EP_OP_GET_XFER_IDX:
 		dbg_print(0xFF, "EP_GETXFERID", 0, ep->name);
@@ -1381,16 +1381,16 @@ static int dwc3_msm_gsi_ep_op(struct usb_ep *ep,
 	case GSI_EP_OP_UPDATEXFER:
 		request = (struct usb_gsi_request *)op_data;
 		dbg_print(0xFF, "EP_UPDATEXFER", 0, ep->name);
-		spin_lock_irqsave(&dwc->lock, flags);
+		raw_spin_lock_irqsave(&dwc->lock, flags);
 		ret = gsi_updatexfer_for_ep(ep, request);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		break;
 	case GSI_EP_OP_ENDXFER:
 		request = (struct usb_gsi_request *)op_data;
 		dbg_print(0xFF, "EP_ENDXFER", 0, ep->name);
-		spin_lock_irqsave(&dwc->lock, flags);
+		raw_spin_lock_irqsave(&dwc->lock, flags);
 		gsi_endxfer_for_ep(ep);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		break;
 	case GSI_EP_OP_SET_CLR_BLOCK_DBL:
 		block_db = *((bool *)op_data);
@@ -1438,13 +1438,13 @@ int msm_ep_config(struct usb_ep *ep, struct usb_request *request)
 	unsigned long flags;
 
 
-	spin_lock_irqsave(&dwc->lock, flags);
+	raw_spin_lock_irqsave(&dwc->lock, flags);
 	/* Save original ep ops for future restore*/
 	if (mdwc->original_ep_ops[dep->number]) {
 		dev_err(mdwc->dev,
 			"ep [%s,%d] already configured as msm endpoint\n",
 			ep->name, dep->number);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -EPERM;
 	}
 	mdwc->original_ep_ops[dep->number] = ep->ops;
@@ -1455,7 +1455,7 @@ int msm_ep_config(struct usb_ep *ep, struct usb_request *request)
 		dev_err(mdwc->dev,
 			"%s: unable to allocate mem for new usb ep ops\n",
 			__func__);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -ENOMEM;
 	}
 	(*new_ep_ops) = (*ep->ops);
@@ -1464,7 +1464,7 @@ int msm_ep_config(struct usb_ep *ep, struct usb_request *request)
 	ep->ops = new_ep_ops;
 
 	if (!mdwc->dbm || !request || (dep->endpoint.ep_type == EP_TYPE_GSI)) {
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return 0;
 	}
 
@@ -1482,11 +1482,11 @@ int msm_ep_config(struct usb_ep *ep, struct usb_request *request)
 	if (ret < 0) {
 		dev_err(mdwc->dev,
 			"error %d after calling dbm_ep_config\n", ret);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return ret;
 	}
 
-	spin_unlock_irqrestore(&dwc->lock, flags);
+	raw_spin_unlock_irqrestore(&dwc->lock, flags);
 
 	return 0;
 }
@@ -1509,13 +1509,13 @@ int msm_ep_unconfig(struct usb_ep *ep)
 	struct usb_ep_ops *old_ep_ops;
 	unsigned long flags;
 
-	spin_lock_irqsave(&dwc->lock, flags);
+	raw_spin_lock_irqsave(&dwc->lock, flags);
 	/* Restore original ep ops */
 	if (!mdwc->original_ep_ops[dep->number]) {
 		dev_dbg(mdwc->dev,
 			"ep [%s,%d] was not configured as msm endpoint\n",
 			ep->name, dep->number);
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return -EINVAL;
 	}
 	old_ep_ops = (struct usb_ep_ops	*)ep->ops;
@@ -1528,7 +1528,7 @@ int msm_ep_unconfig(struct usb_ep *ep)
 	 * which are specific to MSM.
 	 */
 	if (!mdwc->dbm || (dep->endpoint.ep_type == EP_TYPE_GSI)) {
-		spin_unlock_irqrestore(&dwc->lock, flags);
+		raw_spin_unlock_irqrestore(&dwc->lock, flags);
 		return 0;
 	}
 
@@ -1551,7 +1551,7 @@ int msm_ep_unconfig(struct usb_ep *ep)
 			dbm_event_buffer_config(mdwc->dbm, 0, 0, 0);
 	}
 
-	spin_unlock_irqrestore(&dwc->lock, flags);
+	raw_spin_unlock_irqrestore(&dwc->lock, flags);
 
 	return 0;
 }
diff --git a/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_msm.c b/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_msm.c
index 6ba3c16ca..86b62f5a5 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_msm.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_msm.c
@@ -495,14 +495,14 @@ void msm_usb_irq_disable(bool disable)
 	struct ci13xxx *udc = _udc;
 	unsigned long flags;
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 
 	if (_udc_ctxt.irq_disabled == disable) {
 		pr_debug("Interrupt state already disable = %d\n", disable);
 		if (disable)
 			mod_timer(&_udc_ctxt.irq_enable_timer,
 					IRQ_ENABLE_DELAY);
-		spin_unlock_irqrestore(udc->lock, flags);
+		raw_spin_unlock_irqrestore(udc->lock, flags);
 		return;
 	}
 
@@ -520,7 +520,7 @@ void msm_usb_irq_disable(bool disable)
 		_udc_ctxt.irq_disabled = false;
 	}
 
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 }
 
 static void enable_usb_irq_timer_func(unsigned long data)
diff --git a/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_udc.c b/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_udc.c
index 02d5cc92c..d97358e1e 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_udc.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_udc.c
@@ -1212,7 +1212,7 @@ static ssize_t show_inters(struct device *dev, struct device_attribute *attr,
 		return 0;
 	}
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 
 	n += scnprintf(buf + n, PAGE_SIZE - n,
 		       "status = %08x\n", hw_read_intr_status());
@@ -1261,7 +1261,7 @@ static ssize_t show_inters(struct device *dev, struct device_attribute *attr,
 			n += scnprintf(buf + n, PAGE_SIZE - n, "\n");
 	}
 
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	return n;
 }
@@ -1290,7 +1290,7 @@ static ssize_t store_inters(struct device *dev, struct device_attribute *attr,
 		goto done;
 	}
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	if (en) {
 		if (hw_intr_force(bit))
 			dev_err(dev, "invalid bit number\n");
@@ -1300,7 +1300,7 @@ static ssize_t store_inters(struct device *dev, struct device_attribute *attr,
 		if (hw_intr_clear(bit))
 			dev_err(dev, "invalid bit number\n");
 	}
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
  done:
 	return count;
@@ -1325,9 +1325,9 @@ static ssize_t show_port_test(struct device *dev,
 		return 0;
 	}
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	mode = hw_port_test_get();
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	return scnprintf(buf, PAGE_SIZE, "mode = %u\n", mode);
 }
@@ -1356,10 +1356,10 @@ static ssize_t store_port_test(struct device *dev,
 		goto done;
 	}
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	if (hw_port_test_set(mode))
 		dev_err(dev, "invalid mode\n");
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
  done:
 	return count;
@@ -1385,7 +1385,7 @@ static ssize_t show_qheads(struct device *dev, struct device_attribute *attr,
 		return 0;
 	}
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	for (i = 0; i < hw_ep_max/2; i++) {
 		struct ci13xxx_ep *mEpRx = &udc->ci13xxx_ep[i];
 		struct ci13xxx_ep *mEpTx = &udc->ci13xxx_ep[i + hw_ep_max/2];
@@ -1399,7 +1399,7 @@ static ssize_t show_qheads(struct device *dev, struct device_attribute *attr,
 				       *((u32 *)mEpTx->qh.ptr + j));
 		}
 	}
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	return n;
 }
@@ -1431,9 +1431,9 @@ static ssize_t show_registers(struct device *dev,
 		return 0;
 	}
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	k = hw_register_read(dump, DUMP_ENTRIES);
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	for (i = 0; i < k; i++) {
 		n += scnprintf(buf + n, PAGE_SIZE - n,
@@ -1468,10 +1468,10 @@ static ssize_t store_registers(struct device *dev,
 		goto done;
 	}
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	if (hw_register_write(addr, data))
 		dev_err(dev, "invalid address range\n");
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
  done:
 	return count;
@@ -1499,7 +1499,7 @@ static ssize_t show_requests(struct device *dev, struct device_attribute *attr,
 		return 0;
 	}
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	for (i = 0; i < hw_ep_max; i++)
 		list_for_each(ptr, &udc->ci13xxx_ep[i].qh.queue)
 		{
@@ -1515,7 +1515,7 @@ static ssize_t show_requests(struct device *dev, struct device_attribute *attr,
 						" %04X:    %08X\n", j,
 						*((u32 *)req->ptr + j));
 		}
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	return n;
 }
@@ -1621,13 +1621,13 @@ static int ci13xxx_wakeup(struct usb_gadget *_gadget)
 
 	trace();
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	if (!udc->gadget.remote_wakeup) {
 		ret = -EOPNOTSUPP;
 		dbg_trace("remote wakeup feature is not enabled\n");
 		goto out;
 	}
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	pm_runtime_get_sync(&_gadget->dev);
 
@@ -1637,7 +1637,7 @@ static int ci13xxx_wakeup(struct usb_gadget *_gadget)
 	if (udc->transceiver)
 		usb_phy_set_suspend(udc->transceiver, 0);
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	if (!hw_cread(CAP_PORTSC, PORTSC_SUSP)) {
 		ret = -EINVAL;
 		dbg_trace("port is not suspended\n");
@@ -1649,7 +1649,7 @@ static int ci13xxx_wakeup(struct usb_gadget *_gadget)
 	pm_runtime_mark_last_busy(&_gadget->dev);
 	pm_runtime_put_autosuspend(&_gadget->dev);
 out:
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 	return ret;
 }
 
@@ -1663,9 +1663,9 @@ static void usb_do_remote_wakeup(struct work_struct *w)
 	 * This work can not be canceled from interrupt handler. Check
 	 * if wakeup conditions are still met.
 	 */
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	do_wake = udc->suspended && udc->gadget.remote_wakeup;
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	if (do_wake)
 		ci13xxx_wakeup(&udc->gadget);
@@ -1797,7 +1797,7 @@ static void ep_prime_timer_func(unsigned long data)
 	unsigned long flags;
 
 
-	spin_lock_irqsave(mep->lock, flags);
+	raw_spin_lock_irqsave(mep->lock, flags);
 
 	if (_udc && (!_udc->vbus_active || _udc->suspended)) {
 		pr_debug("ep%d%s prime timer when vbus_active=%d,suspend=%d\n",
@@ -1838,12 +1838,12 @@ static void ep_prime_timer_func(unsigned long data)
 		mod_timer(&mep->prime_timer, EP_PRIME_CHECK_DELAY);
 	}
 
-	spin_unlock_irqrestore(mep->lock, flags);
+	raw_spin_unlock_irqrestore(mep->lock, flags);
 	return;
 
 out:
 	mep->prime_timer_count = 0;
-	spin_unlock_irqrestore(mep->lock, flags);
+	raw_spin_unlock_irqrestore(mep->lock, flags);
 
 }
 
@@ -2259,14 +2259,14 @@ static void release_ep_request(struct ci13xxx_ep  *mEp,
 	}
 
 	if (mReq->req.complete != NULL) {
-		spin_unlock(mEp->lock);
+		raw_spin_unlock(mEp->lock);
 		if ((mEp->type == USB_ENDPOINT_XFER_CONTROL) &&
 			mReq->req.length)
 			mEpTemp = &_udc->ep0in;
 		mReq->req.complete(&mEpTemp->ep, &mReq->req);
 		if (mEp->type == USB_ENDPOINT_XFER_CONTROL)
 			mReq->req.complete = NULL;
-		spin_lock(mEp->lock);
+		raw_spin_lock(mEp->lock);
 	}
 }
 
@@ -2333,20 +2333,20 @@ static int _gadget_stop_activity(struct usb_gadget *gadget)
 	if (gadget == NULL)
 		return -EINVAL;
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	udc->gadget.speed = USB_SPEED_UNKNOWN;
 	udc->gadget.remote_wakeup = 0;
 	udc->suspended = 0;
 	udc->configured = 0;
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	gadget->xfer_isr_count = 0;
 	udc->driver->disconnect(gadget);
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	_ep_nuke(&udc->ep0out);
 	_ep_nuke(&udc->ep0in);
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	if (udc->ep0in.last_zptr) {
 		dma_pool_free(udc->ep0in.td_pool, udc->ep0in.last_zptr,
@@ -2381,7 +2381,7 @@ __acquires(udc->lock)
 
 	dbg_event(0xFF, "BUS RST", 0);
 
-	spin_unlock(udc->lock);
+	raw_spin_unlock(udc->lock);
 
 	if (udc->suspended) {
 		if (udc->udc_driver->notify_event)
@@ -2409,7 +2409,7 @@ __acquires(udc->lock)
 	if (retval)
 		goto done;
 
-	spin_lock(udc->lock);
+	raw_spin_lock(udc->lock);
 
  done:
 	if (retval)
@@ -2426,7 +2426,7 @@ static void isr_resume_handler(struct ci13xxx *udc)
 	udc->gadget.speed = hw_port_is_high_speed() ?
 		USB_SPEED_HIGH : USB_SPEED_FULL;
 	if (udc->suspended) {
-		spin_unlock(udc->lock);
+		raw_spin_unlock(udc->lock);
 		if (udc->udc_driver->notify_event)
 			udc->udc_driver->notify_event(udc,
 			  CI13XXX_CONTROLLER_RESUME_EVENT);
@@ -2434,7 +2434,7 @@ static void isr_resume_handler(struct ci13xxx *udc)
 			usb_phy_set_suspend(udc->transceiver, 0);
 		udc->suspended = 0;
 		udc->driver->resume(&udc->gadget);
-		spin_lock(udc->lock);
+		raw_spin_lock(udc->lock);
 
 		if (udc->rw_pending)
 			purge_rw_queue(udc);
@@ -2452,14 +2452,14 @@ static void isr_suspend_handler(struct ci13xxx *udc)
 	if (udc->gadget.speed != USB_SPEED_UNKNOWN &&
 		udc->vbus_active) {
 		if (udc->suspended == 0) {
-			spin_unlock(udc->lock);
+			raw_spin_unlock(udc->lock);
 			udc->driver->suspend(&udc->gadget);
 			if (udc->udc_driver->notify_event)
 				udc->udc_driver->notify_event(udc,
 				CI13XXX_CONTROLLER_SUSPEND_EVENT);
 			if (udc->transceiver)
 				usb_phy_set_suspend(udc->transceiver, 1);
-			spin_lock(udc->lock);
+			raw_spin_lock(udc->lock);
 			udc->suspended = 1;
 		}
 	}
@@ -2523,9 +2523,9 @@ __acquires(mEp->lock)
 	}
 	/* else do nothing; reserved for future use */
 
-	spin_unlock(mEp->lock);
+	raw_spin_unlock(mEp->lock);
 	retval = usb_ep_queue(&mEp->ep, req, GFP_ATOMIC);
-	spin_lock(mEp->lock);
+	raw_spin_lock(mEp->lock);
 	return retval;
 }
 
@@ -2545,10 +2545,10 @@ isr_setup_status_complete(struct usb_ep *ep, struct usb_request *req)
 
 	trace("%pK, %pK", ep, req);
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	if (udc->test_mode)
 		hw_port_test_set(udc->test_mode);
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 }
 
 /**
@@ -2571,9 +2571,9 @@ __acquires(mEp->lock)
 	udc->status->complete = isr_setup_status_complete;
 	udc->status->length = 0;
 
-	spin_unlock(mEp->lock);
+	raw_spin_unlock(mEp->lock);
 	retval = usb_ep_queue(&mEp->ep, udc->status, GFP_ATOMIC);
-	spin_lock(mEp->lock);
+	raw_spin_lock(mEp->lock);
 
 	return retval;
 }
@@ -2663,12 +2663,12 @@ done:
 		dbg_done(_usb_addr(mEp), mReq->ptr->token, retval);
 
 		if (mReq->req.complete != NULL) {
-			spin_unlock(mEp->lock);
+			raw_spin_unlock(mEp->lock);
 			if ((mEp->type == USB_ENDPOINT_XFER_CONTROL) &&
 					mReq->req.length)
 				mEpTemp = &_udc->ep0in;
 			mReq->req.complete(&mEpTemp->ep, &mReq->req);
-			spin_lock(mEp->lock);
+			raw_spin_lock(mEp->lock);
 		}
 	}
 
@@ -2716,10 +2716,10 @@ __acquires(udc->lock)
 				if (err < 0) {
 					dbg_event(_usb_addr(mEp),
 						  "ERROR", err);
-					spin_unlock(udc->lock);
+					raw_spin_unlock(udc->lock);
 					if (usb_ep_set_halt(&mEp->ep))
 						err("error: ep_set_halt");
-					spin_lock(udc->lock);
+					raw_spin_lock(udc->lock);
 				}
 			}
 		}
@@ -2767,10 +2767,10 @@ __acquires(udc->lock)
 				if (dir) /* TX */
 					num += hw_ep_max/2;
 				if (!udc->ci13xxx_ep[num].wedge) {
-					spin_unlock(udc->lock);
+					raw_spin_unlock(udc->lock);
 					err = usb_ep_clear_halt(
 						&udc->ci13xxx_ep[num].ep);
-					spin_lock(udc->lock);
+					raw_spin_lock(udc->lock);
 					if (err)
 						break;
 				}
@@ -2823,9 +2823,9 @@ __acquires(udc->lock)
 				if (dir) /* TX */
 					num += hw_ep_max/2;
 
-				spin_unlock(udc->lock);
+				raw_spin_unlock(udc->lock);
 				err = usb_ep_set_halt(&udc->ci13xxx_ep[num].ep);
-				spin_lock(udc->lock);
+				raw_spin_lock(udc->lock);
 				if (!err)
 					isr_setup_status_phase(udc);
 			} else if (type == (USB_DIR_OUT|USB_RECIP_DEVICE)) {
@@ -2863,19 +2863,19 @@ delegate:
 			if (req.wLength == 0)   /* no data phase */
 				udc->ep0_dir = TX;
 
-			spin_unlock(udc->lock);
+			raw_spin_unlock(udc->lock);
 			err = udc->driver->setup(&udc->gadget, &req);
-			spin_lock(udc->lock);
+			raw_spin_lock(udc->lock);
 			break;
 		}
 
 		if (err < 0) {
 			dbg_event(_usb_addr(mEp), "ERROR", err);
 
-			spin_unlock(udc->lock);
+			raw_spin_unlock(udc->lock);
 			if (usb_ep_set_halt(&mEp->ep))
 				err("error: ep_set_halt");
-			spin_lock(udc->lock);
+			raw_spin_lock(udc->lock);
 		}
 	}
 }
@@ -2901,7 +2901,7 @@ static int ep_enable(struct usb_ep *ep,
 	if (ep == NULL || desc == NULL)
 		return -EINVAL;
 
-	spin_lock_irqsave(mEp->lock, flags);
+	raw_spin_lock_irqsave(mEp->lock, flags);
 
 	/* only internal SW should enable ctrl endpts */
 
@@ -2944,7 +2944,7 @@ static int ep_enable(struct usb_ep *ep,
 	if (mEp->num)
 		retval |= hw_ep_enable(mEp->num, mEp->dir, mEp->type);
 
-	spin_unlock_irqrestore(mEp->lock, flags);
+	raw_spin_unlock_irqrestore(mEp->lock, flags);
 	return retval;
 }
 
@@ -2966,7 +2966,7 @@ static int ep_disable(struct usb_ep *ep)
 	else if (mEp->desc == NULL)
 		return -EBUSY;
 
-	spin_lock_irqsave(mEp->lock, flags);
+	raw_spin_lock_irqsave(mEp->lock, flags);
 
 	/* only internal SW should disable ctrl endpts */
 
@@ -2992,7 +2992,7 @@ static int ep_disable(struct usb_ep *ep)
 	mEp->ep.desc = NULL;
 	mEp->ep.maxpacket = USHRT_MAX;
 
-	spin_unlock_irqrestore(mEp->lock, flags);
+	raw_spin_unlock_irqrestore(mEp->lock, flags);
 	return retval;
 }
 
@@ -3052,7 +3052,7 @@ static void ep_free_request(struct usb_ep *ep, struct usb_request *req)
 		return;
 	}
 
-	spin_lock_irqsave(mEp->lock, flags);
+	raw_spin_lock_irqsave(mEp->lock, flags);
 
 	if (mReq->ptr)
 		dma_pool_free(mEp->td_pool, mReq->ptr, mReq->dma);
@@ -3060,7 +3060,7 @@ static void ep_free_request(struct usb_ep *ep, struct usb_request *req)
 
 	dbg_event(_usb_addr(mEp), "FREE", 0);
 
-	spin_unlock_irqrestore(mEp->lock, flags);
+	raw_spin_unlock_irqrestore(mEp->lock, flags);
 }
 
 /**
@@ -3082,7 +3082,7 @@ static int ep_queue(struct usb_ep *ep, struct usb_request *req,
 	if (ep == NULL)
 		return -EINVAL;
 
-	spin_lock_irqsave(mEp->lock, flags);
+	raw_spin_lock_irqsave(mEp->lock, flags);
 	if (req == NULL || mEp->desc == NULL) {
 		retval = -EINVAL;
 		goto done;
@@ -3197,7 +3197,7 @@ static int ep_queue(struct usb_ep *ep, struct usb_request *req,
 		mEp->multi_req = false;
 
  done:
-	spin_unlock_irqrestore(mEp->lock, flags);
+	raw_spin_unlock_irqrestore(mEp->lock, flags);
 	return retval;
 }
 
@@ -3226,7 +3226,7 @@ static int ep_dequeue(struct usb_ep *ep, struct usb_request *req)
 	if (ep == NULL)
 		return -EINVAL;
 
-	spin_lock_irqsave(mEp->lock, flags);
+	raw_spin_lock_irqsave(mEp->lock, flags);
 	/*
 	 * Only ep0 IN is exposed to composite.  When a req is dequeued
 	 * on ep0, check both ep0 IN and ep0 OUT queues.
@@ -3236,7 +3236,7 @@ static int ep_dequeue(struct usb_ep *ep, struct usb_request *req)
 		(list_empty(&mEp->qh.queue) && ((mEp->type !=
 			USB_ENDPOINT_XFER_CONTROL) ||
 			list_empty(&_udc->ep0out.qh.queue)))) {
-		spin_unlock_irqrestore(mEp->lock, flags);
+		raw_spin_unlock_irqrestore(mEp->lock, flags);
 		return -EINVAL;
 	}
 
@@ -3277,17 +3277,17 @@ static int ep_dequeue(struct usb_ep *ep, struct usb_request *req)
 	}
 
 	if (mReq->req.complete != NULL) {
-		spin_unlock(mEp->lock);
+		raw_spin_unlock(mEp->lock);
 		if ((mEp->type == USB_ENDPOINT_XFER_CONTROL) &&
 				mReq->req.length)
 			mEpTemp = &_udc->ep0in;
 		mReq->req.complete(&mEpTemp->ep, &mReq->req);
 		if (mEp->type == USB_ENDPOINT_XFER_CONTROL)
 			mReq->req.complete = NULL;
-		spin_lock(mEp->lock);
+		raw_spin_lock(mEp->lock);
 	}
 
-	spin_unlock_irqrestore(mEp->lock, flags);
+	raw_spin_unlock_irqrestore(mEp->lock, flags);
 	return 0;
 }
 
@@ -3320,7 +3320,7 @@ static int ep_set_halt(struct usb_ep *ep, int value)
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(mEp->lock, flags);
+	raw_spin_lock_irqsave(mEp->lock, flags);
 
 #ifndef STALL_IN
 	/* g_file_storage MS compliant but g_zero fails chapter 9 compliance */
@@ -3328,7 +3328,7 @@ static int ep_set_halt(struct usb_ep *ep, int value)
 		!list_empty(&mEp->qh.queue) &&
 		!is_sps_req(list_entry(mEp->qh.queue.next, struct ci13xxx_req,
 							   queue))){
-		spin_unlock_irqrestore(mEp->lock, flags);
+		raw_spin_unlock_irqrestore(mEp->lock, flags);
 		return -EAGAIN;
 	}
 #endif
@@ -3346,7 +3346,7 @@ static int ep_set_halt(struct usb_ep *ep, int value)
 
 	} while (mEp->dir != direction);
 
-	spin_unlock_irqrestore(mEp->lock, flags);
+	raw_spin_unlock_irqrestore(mEp->lock, flags);
 	return retval;
 }
 
@@ -3365,12 +3365,12 @@ static int ep_set_wedge(struct usb_ep *ep)
 	if (ep == NULL || mEp->desc == NULL)
 		return -EINVAL;
 
-	spin_lock_irqsave(mEp->lock, flags);
+	raw_spin_lock_irqsave(mEp->lock, flags);
 
 	dbg_event(_usb_addr(mEp), "WEDGE", 0);
 	mEp->wedge = 1;
 
-	spin_unlock_irqrestore(mEp->lock, flags);
+	raw_spin_unlock_irqrestore(mEp->lock, flags);
 
 	return usb_ep_set_halt(ep);
 }
@@ -3400,7 +3400,7 @@ static void ep_fifo_flush(struct usb_ep *ep)
 		return;
 	}
 
-	spin_lock_irqsave(mEp->lock, flags);
+	raw_spin_lock_irqsave(mEp->lock, flags);
 
 	dbg_event(_usb_addr(mEp), "FFLUSH", 0);
 	/*
@@ -3411,7 +3411,7 @@ static void ep_fifo_flush(struct usb_ep *ep)
 	 */
 	_ep_nuke(mEp);
 
-	spin_unlock_irqrestore(mEp->lock, flags);
+	raw_spin_unlock_irqrestore(mEp->lock, flags);
 }
 
 /**
@@ -3442,11 +3442,11 @@ static int ci13xxx_vbus_session(struct usb_gadget *_gadget, int is_active)
 	if (!(udc->udc_driver->flags & CI13XXX_PULLUP_ON_VBUS))
 		return -EOPNOTSUPP;
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	udc->vbus_active = is_active;
 	if (udc->driver)
 		gadget_ready = 1;
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	if (!gadget_ready)
 		return 0;
@@ -3505,14 +3505,14 @@ static int ci13xxx_pullup(struct usb_gadget *_gadget, int is_active)
 	struct ci13xxx *udc = container_of(_gadget, struct ci13xxx, gadget);
 	unsigned long flags;
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	udc->softconnect = is_active;
 	if (((udc->udc_driver->flags & CI13XXX_PULLUP_ON_VBUS) &&
 			!udc->vbus_active) || !udc->driver) {
-		spin_unlock_irqrestore(udc->lock, flags);
+		raw_spin_unlock_irqrestore(udc->lock, flags);
 		return 0;
 	}
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	pm_runtime_get_sync(&_gadget->dev);
 
@@ -3522,23 +3522,23 @@ static int ci13xxx_pullup(struct usb_gadget *_gadget, int is_active)
 		msm_usb_bam_enable(CI_CTRL, _gadget->bam2bam_func_enabled);
 	}
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 	if (!udc->vbus_active) {
-		spin_unlock_irqrestore(udc->lock, flags);
+		raw_spin_unlock_irqrestore(udc->lock, flags);
 		pm_runtime_put_sync(&_gadget->dev);
 		return 0;
 	}
 	if (is_active) {
-		spin_unlock(udc->lock);
+		raw_spin_unlock(udc->lock);
 		if (udc->udc_driver->notify_event)
 			udc->udc_driver->notify_event(udc,
 				CI13XXX_CONTROLLER_CONNECT_EVENT);
-		spin_lock(udc->lock);
+		raw_spin_lock(udc->lock);
 		hw_device_state(udc->ep0out.qh.dma);
 	} else {
 		hw_device_state(0);
 	}
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	pm_runtime_mark_last_busy(&_gadget->dev);
 	pm_runtime_put_autosuspend(&_gadget->dev);
@@ -3590,13 +3590,13 @@ static int ci13xxx_start(struct usb_gadget *gadget,
 	else if (udc->driver != NULL)
 		return -EBUSY;
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 
 	info("hw_ep_max = %d", hw_ep_max);
 
 	udc->gadget.dev.driver = NULL;
 
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	pm_runtime_get_sync(&udc->gadget.dev);
 
@@ -3622,7 +3622,7 @@ static int ci13xxx_start(struct usb_gadget *gadget,
 		retval = -ENOMEM;
 		goto pm_put;
 	}
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 
 	udc->gadget.ep0 = &udc->ep0in.ep;
 	/* bind gadget */
@@ -3645,7 +3645,7 @@ static int ci13xxx_start(struct usb_gadget *gadget,
 	retval = hw_device_state(udc->ep0out.qh.dma);
 
 done:
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	if (udc->udc_driver->notify_event)
 			udc->udc_driver->notify_event(udc,
@@ -3676,17 +3676,17 @@ static int ci13xxx_stop(struct usb_gadget *gadget,
 	    driver             != udc->driver)
 		return -EINVAL;
 
-	spin_lock_irqsave(udc->lock, flags);
+	raw_spin_lock_irqsave(udc->lock, flags);
 
 	if (!(udc->udc_driver->flags & CI13XXX_PULLUP_ON_VBUS) ||
 			udc->vbus_active) {
 		hw_device_state(0);
-		spin_unlock_irqrestore(udc->lock, flags);
+		raw_spin_unlock_irqrestore(udc->lock, flags);
 		_gadget_stop_activity(&udc->gadget);
-		spin_lock_irqsave(udc->lock, flags);
+		raw_spin_lock_irqsave(udc->lock, flags);
 	}
 
-	spin_unlock_irqrestore(udc->lock, flags);
+	raw_spin_unlock_irqrestore(udc->lock, flags);
 
 	usb_ep_free_request(&udc->ep0in.ep, udc->status);
 	kfree(udc->status_buf);
@@ -3716,17 +3716,17 @@ static irqreturn_t udc_irq(void)
 		return IRQ_HANDLED;
 	}
 
-	spin_lock(udc->lock);
+	raw_spin_lock(udc->lock);
 
 	if (udc->udc_driver->in_lpm && udc->udc_driver->in_lpm(udc)) {
-		spin_unlock(udc->lock);
+		raw_spin_unlock(udc->lock);
 		return IRQ_NONE;
 	}
 
 	if (udc->udc_driver->flags & CI13XXX_REGS_SHARED) {
 		if (hw_cread(CAP_USBMODE, USBMODE_CM) !=
 				USBMODE_CM_DEVICE) {
-			spin_unlock(udc->lock);
+			raw_spin_unlock(udc->lock);
 			return IRQ_NONE;
 		}
 	}
@@ -3764,7 +3764,7 @@ static irqreturn_t udc_irq(void)
 		isr_statistics.none++;
 		retval = IRQ_NONE;
 	}
-	spin_unlock(udc->lock);
+	raw_spin_unlock(udc->lock);
 
 	return retval;
 }
diff --git a/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_udc.h b/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_udc.h
index 7983bfdee..9deb4093d 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_udc.h
+++ b/kernel/msm-3.18/drivers/usb/gadget/ci13xxx_udc.h
@@ -113,7 +113,7 @@ struct ci13xxx_ep {
 	int                                    wedge;
 
 	/* global resources */
-	spinlock_t                            *lock;
+	raw_spinlock_t                            *lock;
 	struct device                         *device;
 	struct dma_pool                       *td_pool;
 	struct ci13xxx_td                     *last_zptr;
@@ -154,7 +154,7 @@ struct ci13xxx_udc_driver {
 
 /* CI13XXX UDC descriptor & global resources */
 struct ci13xxx {
-	spinlock_t		  *lock;      /* ctrl register bank access */
+	raw_spinlock_t		  *lock;      /* ctrl register bank access */
 	void __iomem              *regs;      /* registers address space */
 
 	struct dma_pool           *qh_pool;   /* DMA pool for queue heads */
diff --git a/kernel/msm-3.18/drivers/usb/gadget/f_ccid.c b/kernel/msm-3.18/drivers/usb/gadget/f_ccid.c
index d99f8318e..b22999422 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/f_ccid.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/f_ccid.c
@@ -55,7 +55,7 @@ struct f_ccid {
 	struct usb_function function;
 	struct usb_composite_dev *cdev;
 	int ifc_id;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	atomic_t online;
 	/* usb eps*/
 	struct usb_ep *notify;
@@ -198,9 +198,9 @@ static void ccid_req_put(struct f_ccid *ccid_dev, struct list_head *head,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&ccid_dev->lock, flags);
+	raw_spin_lock_irqsave(&ccid_dev->lock, flags);
 	list_add_tail(&req->list, head);
-	spin_unlock_irqrestore(&ccid_dev->lock, flags);
+	raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 }
 
 static struct usb_request *ccid_req_get(struct f_ccid *ccid_dev,
@@ -209,12 +209,12 @@ static struct usb_request *ccid_req_get(struct f_ccid *ccid_dev,
 	unsigned long flags;
 	struct usb_request *req = NULL;
 
-	spin_lock_irqsave(&ccid_dev->lock, flags);
+	raw_spin_lock_irqsave(&ccid_dev->lock, flags);
 	if (!list_empty(head)) {
 		req = list_first_entry(head, struct usb_request, list);
 		list_del(&req->list);
 	}
-	spin_unlock_irqrestore(&ccid_dev->lock, flags);
+	raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 	return req;
 }
 
@@ -586,9 +586,9 @@ static int ccid_bulk_open(struct inode *ip, struct file *fp)
 	atomic_set(&bulk_dev->opened, 1);
 	/* clear the error latch */
 	atomic_set(&bulk_dev->error, 0);
-	spin_lock_irqsave(&ccid_dev->lock, flags);
+	raw_spin_lock_irqsave(&ccid_dev->lock, flags);
 	fp->private_data = ccid_dev;
-	spin_unlock_irqrestore(&ccid_dev->lock, flags);
+	raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 
 	return 0;
 }
@@ -628,7 +628,7 @@ static ssize_t ccid_bulk_read(struct file *fp, char __user *buf,
 	}
 
 requeue_req:
-	spin_lock_irqsave(&ccid_dev->lock, flags);
+	raw_spin_lock_irqsave(&ccid_dev->lock, flags);
 	if (!atomic_read(&ccid_dev->online)) {
 		pr_debug("%s: USB cable not connected\n", __func__);
 		return -ENODEV;
@@ -637,7 +637,7 @@ requeue_req:
 	req = bulk_dev->rx_req;
 	req->length = count;
 	bulk_dev->rx_done = 0;
-	spin_unlock_irqrestore(&ccid_dev->lock, flags);
+	raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 	ret = usb_ep_queue(ccid_dev->out, req, GFP_KERNEL);
 	if (ret < 0) {
 		r = -EIO;
@@ -656,37 +656,37 @@ requeue_req:
 		goto done;
 	}
 	if (!atomic_read(&bulk_dev->error)) {
-		spin_lock_irqsave(&ccid_dev->lock, flags);
+		raw_spin_lock_irqsave(&ccid_dev->lock, flags);
 		if (!atomic_read(&ccid_dev->online)) {
-			spin_unlock_irqrestore(&ccid_dev->lock, flags);
+			raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 			pr_debug("%s: USB cable not connected\n", __func__);
 			r = -ENODEV;
 			goto done;
 		}
 		/* If we got a 0-len packet, throw it back and try again. */
 		if (req->actual == 0) {
-			spin_unlock_irqrestore(&ccid_dev->lock, flags);
+			raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 			goto requeue_req;
 		}
 		xfer = (req->actual < count) ? req->actual : count;
 		atomic_set(&bulk_dev->rx_req_busy, 1);
-		spin_unlock_irqrestore(&ccid_dev->lock, flags);
+		raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 
 		if (copy_to_user(buf, req->buf, xfer))
 			r = -EFAULT;
 
-		spin_lock_irqsave(&ccid_dev->lock, flags);
+		raw_spin_lock_irqsave(&ccid_dev->lock, flags);
 		atomic_set(&bulk_dev->rx_req_busy, 0);
 		if (!atomic_read(&ccid_dev->online)) {
 			ccid_request_free(bulk_dev->rx_req, ccid_dev->out);
-			spin_unlock_irqrestore(&ccid_dev->lock, flags);
+			raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 			pr_debug("%s: USB cable not connected\n", __func__);
 			r = -ENODEV;
 			goto done;
 		} else {
 			r = xfer;
 		}
-		spin_unlock_irqrestore(&ccid_dev->lock, flags);
+		raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 	} else {
 		r = -EIO;
 	}
@@ -757,9 +757,9 @@ static ssize_t ccid_bulk_write(struct file *fp, const char __user *buf,
 		atomic_set(&bulk_dev->error, 1);
 		ccid_req_put(ccid_dev, &bulk_dev->tx_idle, req);
 		r = -EIO;
-		spin_lock_irqsave(&ccid_dev->lock, flags);
+		raw_spin_lock_irqsave(&ccid_dev->lock, flags);
 		if (!atomic_read(&ccid_dev->online)) {
-			spin_unlock_irqrestore(&ccid_dev->lock, flags);
+			raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 			pr_debug("%s: USB cable not connected\n",
 							__func__);
 			while ((req = ccid_req_get(ccid_dev,
@@ -767,7 +767,7 @@ static ssize_t ccid_bulk_write(struct file *fp, const char __user *buf,
 				ccid_request_free(req, ccid_dev->in);
 			r = -ENODEV;
 		}
-		spin_unlock_irqrestore(&ccid_dev->lock, flags);
+		raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 		goto done;
 	}
 done:
@@ -822,9 +822,9 @@ static int ccid_ctrl_open(struct inode *inode, struct file *fp)
 		return -EBUSY;
 	}
 	atomic_set(&ctrl_dev->opened, 1);
-	spin_lock_irqsave(&ccid_dev->lock, flags);
+	raw_spin_lock_irqsave(&ccid_dev->lock, flags);
 	fp->private_data = ccid_dev;
-	spin_unlock_irqrestore(&ccid_dev->lock, flags);
+	raw_spin_unlock_irqrestore(&ccid_dev->lock, flags);
 
 	return 0;
 }
@@ -966,7 +966,7 @@ static int ccid_setup(void)
 		return -ENOMEM;
 
 	_ccid_dev = ccid_dev;
-	spin_lock_init(&ccid_dev->lock);
+	raw_spin_lock_init(&ccid_dev->lock);
 
 	ret = ccid_ctrl_device_init(ccid_dev);
 	if (ret) {
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_accessory.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_accessory.c
index 27b2926f1..bf27c7ccf 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_accessory.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_accessory.c
@@ -72,7 +72,7 @@ struct acc_hid_dev {
 struct acc_dev {
 	struct usb_function function;
 	struct usb_composite_dev *cdev;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	struct usb_ep *ep_in;
 	struct usb_ep *ep_out;
@@ -282,9 +282,9 @@ static void req_put(struct acc_dev *dev, struct list_head *head,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	list_add_tail(&req->list, head);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 }
 
 /* remove a request from the head of a list */
@@ -293,14 +293,14 @@ static struct usb_request *req_get(struct acc_dev *dev, struct list_head *head)
 	unsigned long flags;
 	struct usb_request *req;
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	if (list_empty(head)) {
 		req = 0;
 	} else {
 		req = list_first_entry(head, struct usb_request, list);
 		list_del(&req->list);
 	}
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 	return req;
 }
 
@@ -374,11 +374,11 @@ static void acc_complete_set_string(struct usb_ep *ep, struct usb_request *req)
 		if (length >= ACC_STRING_SIZE)
 			length = ACC_STRING_SIZE - 1;
 
-		spin_lock_irqsave(&dev->lock, flags);
+		raw_spin_lock_irqsave(&dev->lock, flags);
 		memcpy(string_dest, req->buf, length);
 		/* ensure zero termination */
 		string_dest[length] = 0;
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 	} else {
 		pr_err("unknown accessory string index %d\n",
 			dev->string_index);
@@ -503,7 +503,7 @@ static int acc_register_hid(struct acc_dev *dev, int id, int desc_length)
 	if (desc_length <= 0)
 		return -EINVAL;
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	/* replace HID if one already exists with this ID */
 	hid = acc_hid_get(&dev->hid_list, id);
 	if (!hid)
@@ -513,12 +513,12 @@ static int acc_register_hid(struct acc_dev *dev, int id, int desc_length)
 
 	hid = acc_hid_new(dev, id, desc_length);
 	if (!hid) {
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		return -ENOMEM;
 	}
 
 	list_add(&hid->list, &dev->new_hid_list);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	/* schedule work to register the HID device */
 	schedule_work(&dev->hid_work);
@@ -530,17 +530,17 @@ static int acc_unregister_hid(struct acc_dev *dev, int id)
 	struct acc_hid_dev *hid;
 	unsigned long flags;
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	hid = acc_hid_get(&dev->hid_list, id);
 	if (!hid)
 		hid = acc_hid_get(&dev->new_hid_list, id);
 	if (!hid) {
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		return -EINVAL;
 	}
 
 	list_move(&hid->list, &dev->dead_hid_list);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	schedule_work(&dev->hid_work);
 	return 0;
@@ -905,9 +905,9 @@ int acc_ctrlrequest(struct usb_composite_dev *cdev,
 		} else if (b_request == ACCESSORY_UNREGISTER_HID) {
 			value = acc_unregister_hid(dev, w_value);
 		} else if (b_request == ACCESSORY_SET_HID_REPORT_DESC) {
-			spin_lock_irqsave(&dev->lock, flags);
+			raw_spin_lock_irqsave(&dev->lock, flags);
 			hid = acc_hid_get(&dev->new_hid_list, w_value);
-			spin_unlock_irqrestore(&dev->lock, flags);
+			raw_spin_unlock_irqrestore(&dev->lock, flags);
 			if (!hid) {
 				value = -EINVAL;
 				goto err;
@@ -922,9 +922,9 @@ int acc_ctrlrequest(struct usb_composite_dev *cdev,
 			cdev->req->complete = acc_complete_set_hid_report_desc;
 			value = w_length;
 		} else if (b_request == ACCESSORY_SEND_HID_EVENT) {
-			spin_lock_irqsave(&dev->lock, flags);
+			raw_spin_lock_irqsave(&dev->lock, flags);
 			hid = acc_hid_get(&dev->hid_list, w_value);
-			spin_unlock_irqrestore(&dev->lock, flags);
+			raw_spin_unlock_irqrestore(&dev->lock, flags);
 			if (!hid) {
 				value = -EINVAL;
 				goto err;
@@ -1055,7 +1055,7 @@ kill_all_hid_devices(struct acc_dev *dev)
 	if (!dev)
 		return;
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	list_for_each_safe(entry, temp, &dev->hid_list) {
 		hid = list_entry(entry, struct acc_hid_dev, list);
 		list_del(&hid->list);
@@ -1066,7 +1066,7 @@ kill_all_hid_devices(struct acc_dev *dev)
 		list_del(&hid->list);
 		list_add(&hid->list, &dev->dead_hid_list);
 	}
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	schedule_work(&dev->hid_work);
 }
@@ -1142,7 +1142,7 @@ static void acc_hid_work(struct work_struct *data)
 
 	INIT_LIST_HEAD(&new_list);
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 
 	/* copy hids that are ready for initialization to new_list */
 	list_for_each_safe(entry, temp, &dev->new_hid_list) {
@@ -1162,7 +1162,7 @@ static void acc_hid_work(struct work_struct *data)
 		INIT_LIST_HEAD(&dev->dead_hid_list);
 	}
 
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	/* register new HID devices */
 	list_for_each_safe(entry, temp, &new_list) {
@@ -1171,9 +1171,9 @@ static void acc_hid_work(struct work_struct *data)
 			pr_err("can't add HID device %pK\n", hid);
 			acc_hid_delete(hid);
 		} else {
-			spin_lock_irqsave(&dev->lock, flags);
+			raw_spin_lock_irqsave(&dev->lock, flags);
 			list_move(&hid->list, &dev->hid_list);
-			spin_unlock_irqrestore(&dev->lock, flags);
+			raw_spin_unlock_irqrestore(&dev->lock, flags);
 		}
 	}
 
@@ -1289,7 +1289,7 @@ static int acc_setup(void)
 	if (!dev)
 		return -ENOMEM;
 
-	spin_lock_init(&dev->lock);
+	raw_spin_lock_init(&dev->lock);
 	init_waitqueue_head(&dev->read_wq);
 	init_waitqueue_head(&dev->write_wq);
 	atomic_set(&dev->open_excl, 0);
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_audio_source.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_audio_source.c
index 1295efbad..7d8431ece 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_audio_source.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_audio_source.c
@@ -254,7 +254,7 @@ struct audio_dev {
 	struct list_head		idle_reqs;
 	struct usb_ep			*in_ep;
 
-	spinlock_t			lock;
+	raw_spinlock_t			lock;
 
 	/* beginning, end and current position in our buffer */
 	void				*buffer_start;
@@ -337,9 +337,9 @@ static void audio_req_put(struct audio_dev *audio, struct usb_request *req)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&audio->lock, flags);
+	raw_spin_lock_irqsave(&audio->lock, flags);
 	list_add_tail(&req->list, &audio->idle_reqs);
-	spin_unlock_irqrestore(&audio->lock, flags);
+	raw_spin_unlock_irqrestore(&audio->lock, flags);
 }
 
 static struct usb_request *audio_req_get(struct audio_dev *audio)
@@ -347,7 +347,7 @@ static struct usb_request *audio_req_get(struct audio_dev *audio)
 	unsigned long flags;
 	struct usb_request *req;
 
-	spin_lock_irqsave(&audio->lock, flags);
+	raw_spin_lock_irqsave(&audio->lock, flags);
 	if (list_empty(&audio->idle_reqs)) {
 		req = 0;
 	} else {
@@ -355,7 +355,7 @@ static struct usb_request *audio_req_get(struct audio_dev *audio)
 				list);
 		list_del(&req->list);
 	}
-	spin_unlock_irqrestore(&audio->lock, flags);
+	raw_spin_unlock_irqrestore(&audio->lock, flags);
 	return req;
 }
 
@@ -370,20 +370,20 @@ static void audio_send(struct audio_dev *audio)
 	ktime_t now;
 	unsigned long flags;
 
-	spin_lock_irqsave(&audio->lock, flags);
+	raw_spin_lock_irqsave(&audio->lock, flags);
 	/* audio->substream will be null if we have been closed */
 	if (!audio->substream) {
-		spin_unlock_irqrestore(&audio->lock, flags);
+		raw_spin_unlock_irqrestore(&audio->lock, flags);
 		return;
 	}
 	/* audio->buffer_pos will be null if we have been stopped */
 	if (!audio->buffer_pos) {
-		spin_unlock_irqrestore(&audio->lock, flags);
+		raw_spin_unlock_irqrestore(&audio->lock, flags);
 		return;
 	}
 
 	runtime = audio->substream->runtime;
-	spin_unlock_irqrestore(&audio->lock, flags);
+	raw_spin_unlock_irqrestore(&audio->lock, flags);
 
 	/* compute number of frames to send */
 	now = ktime_get();
@@ -407,19 +407,19 @@ static void audio_send(struct audio_dev *audio)
 
 	while (frames > 0) {
 		req = audio_req_get(audio);
-		spin_lock_irqsave(&audio->lock, flags);
+		raw_spin_lock_irqsave(&audio->lock, flags);
 		/* audio->substream will be null if we have been closed */
 		if (!audio->substream) {
-			spin_unlock_irqrestore(&audio->lock, flags);
+			raw_spin_unlock_irqrestore(&audio->lock, flags);
 			return;
 		}
 		/* audio->buffer_pos will be null if we have been stopped */
 		if (!audio->buffer_pos) {
-			spin_unlock_irqrestore(&audio->lock, flags);
+			raw_spin_unlock_irqrestore(&audio->lock, flags);
 			return;
 		}
 		if (!req) {
-			spin_unlock_irqrestore(&audio->lock, flags);
+			raw_spin_unlock_irqrestore(&audio->lock, flags);
 			break;
 		}
 
@@ -447,7 +447,7 @@ static void audio_send(struct audio_dev *audio)
 		}
 
 		req->length = length;
-		spin_unlock_irqrestore(&audio->lock, flags);
+		raw_spin_unlock_irqrestore(&audio->lock, flags);
 		ret = usb_ep_queue(audio->in_ep, req, GFP_ATOMIC);
 		if (ret < 0) {
 			pr_err("usb_ep_queue failed ret: %d\n", ret);
@@ -751,11 +751,11 @@ static void audio_pcm_playback_stop(struct audio_dev *audio)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&audio->lock, flags);
+	raw_spin_lock_irqsave(&audio->lock, flags);
 	audio->buffer_start = 0;
 	audio->buffer_end = 0;
 	audio->buffer_pos = 0;
-	spin_unlock_irqrestore(&audio->lock, flags);
+	raw_spin_unlock_irqrestore(&audio->lock, flags);
 }
 
 static int audio_pcm_open(struct snd_pcm_substream *substream)
@@ -777,9 +777,9 @@ static int audio_pcm_close(struct snd_pcm_substream *substream)
 	struct audio_dev *audio = substream->private_data;
 	unsigned long flags;
 
-	spin_lock_irqsave(&audio->lock, flags);
+	raw_spin_lock_irqsave(&audio->lock, flags);
 	audio->substream = NULL;
-	spin_unlock_irqrestore(&audio->lock, flags);
+	raw_spin_unlock_irqrestore(&audio->lock, flags);
 
 	return 0;
 }
@@ -897,7 +897,7 @@ static struct audio_dev _audio_dev = {
 		.setup = audio_setup,
 		.disable = audio_disable,
 	},
-	.lock = __SPIN_LOCK_UNLOCKED(_audio_dev.lock),
+	.lock = __RAW_SPIN_LOCK_UNLOCKED(_audio_dev.lock),
 	.idle_reqs = LIST_HEAD_INIT(_audio_dev.idle_reqs),
 };
 
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_diag.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_diag.c
index 7594fb143..0df91d22a 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_diag.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_diag.c
@@ -151,7 +151,7 @@ struct diag_context {
 	struct usb_ep *in;
 	struct list_head read_pool;
 	struct list_head write_pool;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	unsigned configured;
 	struct usb_composite_dev *cdev;
 	int (*update_pid_and_serial_num)(uint32_t, const char *);
@@ -180,7 +180,7 @@ static void diag_context_release(struct kref *kref)
 	struct diag_context *ctxt =
 		container_of(kref, struct diag_context, kref);
 
-	spin_unlock(&ctxt->lock);
+	raw_spin_unlock(&ctxt->lock);
 	kfree(ctxt);
 }
 
@@ -246,13 +246,13 @@ static void diag_write_complete(struct usb_ep *ep,
 		}
 	}
 
-	spin_lock_irqsave(&ctxt->lock, flags);
+	raw_spin_lock_irqsave(&ctxt->lock, flags);
 	list_add_tail(&req->list, &ctxt->write_pool);
 	if (req->length != 0) {
 		d_req->actual = req->actual;
 		d_req->status = req->status;
 	}
-	spin_unlock_irqrestore(&ctxt->lock, flags);
+	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 
 	if (ctxt->ch && ctxt->ch->notify)
 		ctxt->ch->notify(ctxt->ch->priv, USB_DIAG_WRITE_DONE, d_req);
@@ -271,9 +271,9 @@ static void diag_read_complete(struct usb_ep *ep,
 	d_req->actual = req->actual;
 	d_req->status = req->status;
 
-	spin_lock_irqsave(&ctxt->lock, flags);
+	raw_spin_lock_irqsave(&ctxt->lock, flags);
 	list_add_tail(&req->list, &ctxt->read_pool);
-	spin_unlock_irqrestore(&ctxt->lock, flags);
+	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 
 	ctxt->dpkts_tomodem++;
 
@@ -302,7 +302,7 @@ struct usb_diag_ch *usb_diag_open(const char *name, void *priv,
 	unsigned long flags;
 	int found = 0;
 
-	spin_lock_irqsave(&ch_lock, flags);
+	raw_spin_lock_irqsave(&ch_lock, flags);
 	/* Check if we already have a channel with this name */
 	list_for_each_entry(ch, &usb_diag_ch_list, list) {
 		if (!strcmp(name, ch->name)) {
@@ -310,7 +310,7 @@ struct usb_diag_ch *usb_diag_open(const char *name, void *priv,
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&ch_lock, flags);
+	raw_spin_unlock_irqrestore(&ch_lock, flags);
 
 	if (!found) {
 		ch = kzalloc(sizeof(*ch), GFP_KERNEL);
@@ -323,9 +323,9 @@ struct usb_diag_ch *usb_diag_open(const char *name, void *priv,
 	ch->notify = notify;
 
 	if (!found) {
-		spin_lock_irqsave(&ch_lock, flags);
+		raw_spin_lock_irqsave(&ch_lock, flags);
 		list_add_tail(&ch->list, &usb_diag_ch_list);
-		spin_unlock_irqrestore(&ch_lock, flags);
+		raw_spin_unlock_irqrestore(&ch_lock, flags);
 	}
 
 	return ch;
@@ -344,7 +344,7 @@ void usb_diag_close(struct usb_diag_ch *ch)
 	struct diag_context *dev = NULL;
 	unsigned long flags;
 
-	spin_lock_irqsave(&ch_lock, flags);
+	raw_spin_lock_irqsave(&ch_lock, flags);
 	ch->priv = NULL;
 	ch->notify = NULL;
 	/* Free-up the resources if channel is no more active */
@@ -354,7 +354,7 @@ void usb_diag_close(struct usb_diag_ch *ch)
 			dev->ch = NULL;
 	kfree(ch);
 
-	spin_unlock_irqrestore(&ch_lock, flags);
+	raw_spin_unlock_irqrestore(&ch_lock, flags);
 }
 EXPORT_SYMBOL(usb_diag_close);
 
@@ -397,7 +397,7 @@ int usb_diag_alloc_req(struct usb_diag_ch *ch, int n_write, int n_read)
 	if (!ctxt)
 		return -ENODEV;
 
-	spin_lock_irqsave(&ctxt->lock, flags);
+	raw_spin_lock_irqsave(&ctxt->lock, flags);
 	/* Free previous session's stale requests */
 	free_reqs(ctxt);
 	for (i = 0; i < n_write; i++) {
@@ -417,11 +417,11 @@ int usb_diag_alloc_req(struct usb_diag_ch *ch, int n_write, int n_read)
 		req->complete = diag_read_complete;
 		list_add_tail(&req->list, &ctxt->read_pool);
 	}
-	spin_unlock_irqrestore(&ctxt->lock, flags);
+	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 	return 0;
 fail:
 	free_reqs(ctxt);
-	spin_unlock_irqrestore(&ctxt->lock, flags);
+	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 	return -ENOMEM;
 
 }
@@ -472,17 +472,17 @@ int usb_diag_read(struct usb_diag_ch *ch, struct diag_request *d_req)
 	if (!ctxt)
 		return -ENODEV;
 
-	spin_lock_irqsave(&ctxt->lock, flags);
+	raw_spin_lock_irqsave(&ctxt->lock, flags);
 
 	if (!ctxt->configured || !ctxt->out) {
-		spin_unlock_irqrestore(&ctxt->lock, flags);
+		raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 		return -EIO;
 	}
 
 	out = ctxt->out;
 
 	if (list_empty(&ctxt->read_pool)) {
-		spin_unlock_irqrestore(&ctxt->lock, flags);
+		raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 		ERROR(ctxt->cdev, "%s: no requests available\n", __func__);
 		return -EAGAIN;
 	}
@@ -490,7 +490,7 @@ int usb_diag_read(struct usb_diag_ch *ch, struct diag_request *d_req)
 	req = list_first_entry(&ctxt->read_pool, struct usb_request, list);
 	list_del(&req->list);
 	kref_get(&ctxt->kref); /* put called in complete callback */
-	spin_unlock_irqrestore(&ctxt->lock, flags);
+	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 
 	req->buf = d_req->buf;
 	req->length = d_req->length;
@@ -506,7 +506,7 @@ int usb_diag_read(struct usb_diag_ch *ch, struct diag_request *d_req)
 
 	if (usb_ep_queue(out, req, GFP_ATOMIC)) {
 		/* If error add the link to linked list again*/
-		spin_lock_irqsave(&ctxt->lock, flags);
+		raw_spin_lock_irqsave(&ctxt->lock, flags);
 		list_add_tail(&req->list, &ctxt->read_pool);
 		/* 1 error message for every 10 sec */
 		if (__ratelimit(&rl))
@@ -514,10 +514,10 @@ int usb_diag_read(struct usb_diag_ch *ch, struct diag_request *d_req)
 				" read request\n", __func__);
 
 		if (kref_put(&ctxt->kref, diag_context_release))
-			/* diag_context_release called spin_unlock already */
+			/* diag_context_release called raw_spin_unlock already */
 			local_irq_restore(flags);
 		else
-			spin_unlock_irqrestore(&ctxt->lock, flags);
+			raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 		return -EIO;
 	}
 
@@ -549,17 +549,17 @@ int usb_diag_write(struct usb_diag_ch *ch, struct diag_request *d_req)
 	if (!ctxt)
 		return -ENODEV;
 
-	spin_lock_irqsave(&ctxt->lock, flags);
+	raw_spin_lock_irqsave(&ctxt->lock, flags);
 
 	if (!ctxt->configured || !ctxt->in) {
-		spin_unlock_irqrestore(&ctxt->lock, flags);
+		raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 		return -EIO;
 	}
 
 	in = ctxt->in;
 
 	if (list_empty(&ctxt->write_pool)) {
-		spin_unlock_irqrestore(&ctxt->lock, flags);
+		raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 		ERROR(ctxt->cdev, "%s: no requests available\n", __func__);
 		return -EAGAIN;
 	}
@@ -567,7 +567,7 @@ int usb_diag_write(struct usb_diag_ch *ch, struct diag_request *d_req)
 	req = list_first_entry(&ctxt->write_pool, struct usb_request, list);
 	list_del(&req->list);
 	kref_get(&ctxt->kref); /* put called in complete callback */
-	spin_unlock_irqrestore(&ctxt->lock, flags);
+	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 
 	req->buf = d_req->buf;
 	req->length = d_req->length;
@@ -584,7 +584,7 @@ int usb_diag_write(struct usb_diag_ch *ch, struct diag_request *d_req)
 	ctxt->dpkts_tolaptop_pending++;
 	if (usb_ep_queue(in, req, GFP_ATOMIC)) {
 		/* If error add the link to linked list again*/
-		spin_lock_irqsave(&ctxt->lock, flags);
+		raw_spin_lock_irqsave(&ctxt->lock, flags);
 		list_add_tail(&req->list, &ctxt->write_pool);
 		ctxt->dpkts_tolaptop_pending--;
 		/* 1 error message for every 10 sec */
@@ -593,10 +593,10 @@ int usb_diag_write(struct usb_diag_ch *ch, struct diag_request *d_req)
 				" read request\n", __func__);
 
 		if (kref_put(&ctxt->kref, diag_context_release))
-			/* diag_context_release called spin_unlock already */
+			/* diag_context_release called raw_spin_unlock already */
 			local_irq_restore(flags);
 		else
-			spin_unlock_irqrestore(&ctxt->lock, flags);
+			raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 		return -EIO;
 	}
 
@@ -617,9 +617,9 @@ static void diag_function_disable(struct usb_function *f)
 
 	DBG(dev->cdev, "diag_function_disable\n");
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	dev->configured = 0;
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	if (dev->ch && dev->ch->notify)
 		dev->ch->notify(dev->ch->priv, USB_DIAG_DISCONNECT, NULL);
@@ -677,9 +677,9 @@ static int diag_function_set_alt(struct usb_function *f,
 	dev->dpkts_tomodem = 0;
 	dev->dpkts_tolaptop_pending = 0;
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	dev->configured = 1;
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	if (dev->ch->notify)
 		dev->ch->notify(dev->ch->priv, USB_DIAG_CONNECT, NULL);
@@ -709,14 +709,14 @@ static void diag_function_unbind(struct usb_configuration *c,
 		ctxt->ch->priv_usb = NULL;
 	list_del(&ctxt->list_item);
 	/* Free any pending USB requests from last session */
-	spin_lock_irqsave(&ctxt->lock, flags);
+	raw_spin_lock_irqsave(&ctxt->lock, flags);
 	free_reqs(ctxt);
 
 	if (kref_put(&ctxt->kref, diag_context_release))
-		/* diag_context_release called spin_unlock already */
+		/* diag_context_release called raw_spin_unlock already */
 		local_irq_restore(flags);
 	else
-		spin_unlock_irqrestore(&ctxt->lock, flags);
+		raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 }
 
 static int diag_function_bind(struct usb_configuration *c,
@@ -813,9 +813,9 @@ int diag_function_add(struct usb_configuration *c, const char *name,
 
 		_ch->name = name;
 
-		spin_lock_irqsave(&ch_lock, flags);
+		raw_spin_lock_irqsave(&ch_lock, flags);
 		list_add_tail(&_ch->list, &usb_diag_ch_list);
-		spin_unlock_irqrestore(&ch_lock, flags);
+		raw_spin_unlock_irqrestore(&ch_lock, flags);
 	}
 
 	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
@@ -842,7 +842,7 @@ int diag_function_add(struct usb_configuration *c, const char *name,
 	dev->function.set_alt = diag_function_set_alt;
 	dev->function.disable = diag_function_disable;
 	kref_init(&dev->kref);
-	spin_lock_init(&dev->lock);
+	raw_spin_lock_init(&dev->lock);
 	INIT_LIST_HEAD(&dev->read_pool);
 	INIT_LIST_HEAD(&dev->write_pool);
 
@@ -871,7 +871,7 @@ static ssize_t debug_read_stats(struct file *file, char __user *ubuf,
 		unsigned long flags;
 
 		if (ctxt) {
-			spin_lock_irqsave(&ctxt->lock, flags);
+			raw_spin_lock_irqsave(&ctxt->lock, flags);
 			temp += scnprintf(buf + temp, PAGE_SIZE - temp,
 					"---Name: %s---\n"
 					"endpoints: %s, %s\n"
@@ -883,7 +883,7 @@ static ssize_t debug_read_stats(struct file *file, char __user *ubuf,
 					ctxt->dpkts_tolaptop,
 					ctxt->dpkts_tomodem,
 					ctxt->dpkts_tolaptop_pending);
-			spin_unlock_irqrestore(&ctxt->lock, flags);
+			raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 		}
 	}
 
@@ -900,11 +900,11 @@ static ssize_t debug_reset_stats(struct file *file, const char __user *buf,
 		unsigned long flags;
 
 		if (ctxt) {
-			spin_lock_irqsave(&ctxt->lock, flags);
+			raw_spin_lock_irqsave(&ctxt->lock, flags);
 			ctxt->dpkts_tolaptop = 0;
 			ctxt->dpkts_tomodem = 0;
 			ctxt->dpkts_tolaptop_pending = 0;
-			spin_unlock_irqrestore(&ctxt->lock, flags);
+			raw_spin_unlock_irqrestore(&ctxt->lock, flags);
 		}
 	}
 
@@ -960,13 +960,13 @@ static void diag_cleanup(void)
 	list_for_each_safe(act, tmp, &usb_diag_ch_list) {
 		_ch = list_entry(act, struct usb_diag_ch, list);
 
-		spin_lock_irqsave(&ch_lock, flags);
+		raw_spin_lock_irqsave(&ch_lock, flags);
 		/* Free if diagchar is not using the channel anymore */
 		if (!_ch->priv) {
 			list_del(&_ch->list);
 			kfree(_ch);
 		}
-		spin_unlock_irqrestore(&ch_lock, flags);
+		raw_spin_unlock_irqrestore(&ch_lock, flags);
 	}
 }
 
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_gps.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_gps.c
index f25c79a68..c562edb5d 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_gps.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_gps.c
@@ -37,7 +37,7 @@ struct f_gps {
 	atomic_t			ctrl_online;
 	struct usb_composite_dev	*cdev;
 
-	spinlock_t			lock;
+	raw_spinlock_t			lock;
 
 	/* usb eps */
 	struct usb_ep			*notify;
@@ -256,7 +256,7 @@ static void gps_purge_responses(struct f_gps *dev)
 	pr_debug("%s: port#%d\n", __func__, dev->port_num);
 
 	usb_ep_dequeue(dev->notify, dev->notify_req);
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	while (!list_empty(&dev->cpkt_resp_q)) {
 		cpkt = list_first_entry(&dev->cpkt_resp_q,
 				struct rmnet_ctrl_pkt, list);
@@ -265,7 +265,7 @@ static void gps_purge_responses(struct f_gps *dev)
 		rmnet_free_ctrl_pkt(cpkt);
 	}
 	atomic_set(&dev->notify_count, 0);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 }
 
 static void gps_suspend(struct usb_function *f)
@@ -298,12 +298,12 @@ static void gps_resume(struct usb_function *f)
 		return;
 
 	dev->is_suspended = false;
-	spin_lock(&dev->lock);
+	raw_spin_lock(&dev->lock);
 	if (list_empty(&dev->cpkt_resp_q)) {
-		spin_unlock(&dev->lock);
+		raw_spin_unlock(&dev->lock);
 		return;
 	}
-	spin_unlock(&dev->lock);
+	raw_spin_unlock(&dev->lock);
 	gps_ctrl_response_available(dev);
 }
 
@@ -437,14 +437,14 @@ static void gps_ctrl_response_available(struct f_gps *dev)
 
 	pr_debug("%s:dev:%pK\n", __func__, dev);
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	if (!atomic_read(&dev->online) || !req || !req->buf) {
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		return;
 	}
 
 	if (atomic_inc_return(&dev->notify_count) != 1) {
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		return;
 	}
 
@@ -455,7 +455,7 @@ static void gps_ctrl_response_available(struct f_gps *dev)
 	event->wValue = cpu_to_le16(0);
 	event->wIndex = cpu_to_le16(dev->ifc_id);
 	event->wLength = cpu_to_le16(0);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	ret = usb_ep_queue(dev->notify, dev->notify_req, GFP_ATOMIC);
 	if (ret) {
@@ -464,7 +464,7 @@ static void gps_ctrl_response_available(struct f_gps *dev)
 				__func__, atomic_read(&dev->notify_count));
 			WARN_ON(1);
 		}
-		spin_lock_irqsave(&dev->lock, flags);
+		raw_spin_lock_irqsave(&dev->lock, flags);
 		if (!list_empty(&dev->cpkt_resp_q)) {
 			atomic_dec(&dev->notify_count);
 			cpkt = list_first_entry(&dev->cpkt_resp_q,
@@ -472,7 +472,7 @@ static void gps_ctrl_response_available(struct f_gps *dev)
 			list_del(&cpkt->list);
 			gps_free_ctrl_pkt(cpkt);
 		}
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		pr_debug("ep enqueue error %d\n", ret);
 	}
 }
@@ -543,9 +543,9 @@ gps_send_cpkt_response(void *gr, void *buf, size_t len)
 		return 0;
 	}
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	list_add_tail(&cpkt->list, &dev->cpkt_resp_q);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	if (dev->is_suspended && dev->is_rw_allowed) {
 		pr_debug("%s: calling gps_wakeup_host\n", __func__);
@@ -614,7 +614,7 @@ static void gps_notify_complete(struct usb_ep *ep, struct usb_request *req)
 				WARN_ON(1);
 			}
 
-			spin_lock_irqsave(&dev->lock, flags);
+			raw_spin_lock_irqsave(&dev->lock, flags);
 			if (!list_empty(&dev->cpkt_resp_q)) {
 				atomic_dec(&dev->notify_count);
 				cpkt = list_first_entry(&dev->cpkt_resp_q,
@@ -622,7 +622,7 @@ static void gps_notify_complete(struct usb_ep *ep, struct usb_request *req)
 				list_del(&cpkt->list);
 				gps_free_ctrl_pkt(cpkt);
 			}
-			spin_unlock_irqrestore(&dev->lock, flags);
+			raw_spin_unlock_irqrestore(&dev->lock, flags);
 			pr_debug("ep enqueue error %d\n", status);
 		}
 		break;
@@ -663,9 +663,9 @@ gps_setup(struct usb_function *f, const struct usb_ctrlrequest *ctrl)
 			unsigned len;
 			struct rmnet_ctrl_pkt *cpkt;
 
-			spin_lock(&dev->lock);
+			raw_spin_lock(&dev->lock);
 			if (list_empty(&dev->cpkt_resp_q)) {
-				spin_unlock(&dev->lock);
+				raw_spin_unlock(&dev->lock);
 				pr_debug("%s: ctrl resp queue empty", __func__);
 				ret = 0;
 				goto invalid;
@@ -674,7 +674,7 @@ gps_setup(struct usb_function *f, const struct usb_ctrlrequest *ctrl)
 			cpkt = list_first_entry(&dev->cpkt_resp_q,
 					struct rmnet_ctrl_pkt, list);
 			list_del(&cpkt->list);
-			spin_unlock(&dev->lock);
+			raw_spin_unlock(&dev->lock);
 
 			len = min_t(unsigned, w_length, cpkt->len);
 			memcpy(req->buf, cpkt->buf, len);
@@ -825,11 +825,11 @@ static int gps_bind_config(struct usb_configuration *c)
 
 	dev = gps_port.port;
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	dev->cdev = c->cdev;
 	f = &dev->port.func;
 	f->name = kasprintf(GFP_ATOMIC, "gps");
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 	if (!f->name) {
 		pr_err("%s: cannot allocate memory for name\n", __func__);
 		return -ENOMEM;
@@ -877,7 +877,7 @@ static int gps_init_port(void)
 		return -ENOMEM;
 	}
 
-	spin_lock_init(&dev->lock);
+	raw_spin_lock_init(&dev->lock);
 	INIT_LIST_HEAD(&dev->cpkt_resp_q);
 	dev->port_num = 0;
 
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_gsi.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_gsi.c
index 64f010a7e..421c905f3 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_gsi.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_gsi.c
@@ -84,7 +84,7 @@ void post_event(struct gsi_data_port *port, u8 event)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&port->evt_q.q_lock, flags);
+	raw_spin_lock_irqsave(&port->evt_q.q_lock, flags);
 
 	port->evt_q.tail++;
 	/* Check for wraparound and make room */
@@ -93,12 +93,12 @@ void post_event(struct gsi_data_port *port, u8 event)
 	/* Check for overflow */
 	if (port->evt_q.tail == port->evt_q.head) {
 		log_event_err("%s: event queue overflow error", __func__);
-		spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
+		raw_spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
 		return;
 	}
 	/* Add event to queue */
 	port->evt_q.event[port->evt_q.tail] = event;
-	spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
+	raw_spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
 }
 
 void post_event_to_evt_queue(struct gsi_data_port *port, u8 event)
@@ -112,10 +112,10 @@ u8 read_event(struct gsi_data_port *port)
 	u8 event;
 	unsigned long flags;
 
-	spin_lock_irqsave(&port->evt_q.q_lock, flags);
+	raw_spin_lock_irqsave(&port->evt_q.q_lock, flags);
 	if (port->evt_q.head == port->evt_q.tail) {
 		log_event_dbg("%s: event queue empty", __func__);
-		spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
+		raw_spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
 		return EVT_NONE;
 	}
 
@@ -124,7 +124,7 @@ u8 read_event(struct gsi_data_port *port)
 	port->evt_q.head = port->evt_q.head % MAXQUEUELEN;
 
 	 event = port->evt_q.event[port->evt_q.head];
-	 spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
+	 raw_spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
 
 	 return event;
 }
@@ -135,16 +135,16 @@ u8 peek_event(struct gsi_data_port *port)
 	unsigned long flags;
 	u8 peek_index = 0;
 
-	spin_lock_irqsave(&port->evt_q.q_lock, flags);
+	raw_spin_lock_irqsave(&port->evt_q.q_lock, flags);
 	if (port->evt_q.head == port->evt_q.tail) {
 		log_event_dbg("%s: event queue empty", __func__);
-		spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
+		raw_spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
 		return EVT_NONE;
 	}
 
 	peek_index = (port->evt_q.head + 1) % MAXQUEUELEN;
 	event = port->evt_q.event[peek_index];
-	spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
+	raw_spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
 
 	return event;
 }
@@ -153,10 +153,10 @@ void reset_event_queue(struct gsi_data_port *port)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&port->evt_q.q_lock, flags);
+	raw_spin_lock_irqsave(&port->evt_q.q_lock, flags);
 	port->evt_q.head = port->evt_q.tail = MAXQUEUELEN - 1;
 	memset(&port->evt_q.event[0], EVT_NONE, MAXQUEUELEN);
-	spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
+	raw_spin_unlock_irqrestore(&port->evt_q.q_lock, flags);
 }
 
 int gsi_wakeup_host(struct f_gsi *gsi)
@@ -597,13 +597,13 @@ int ipa_usb_notify_cb(enum ipa_usb_notify_event event,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&gsi->d_port.lock, flags);
+	raw_spin_lock_irqsave(&gsi->d_port.lock, flags);
 
 	switch (event) {
 	case IPA_USB_DEVICE_READY:
 
 		if (gsi->d_port.net_ready_trigger) {
-			spin_unlock_irqrestore(&gsi->d_port.lock, flags);
+			raw_spin_unlock_irqrestore(&gsi->d_port.lock, flags);
 			log_event_dbg("%s: Already triggered", __func__);
 			return 1;
 		}
@@ -614,7 +614,7 @@ int ipa_usb_notify_cb(enum ipa_usb_notify_event event,
 		if (gsi->prot_id == IPA_USB_ECM) {
 			cpkt_notify_connect = gsi_ctrl_pkt_alloc(0, GFP_ATOMIC);
 			if (IS_ERR(cpkt_notify_connect)) {
-				spin_unlock_irqrestore(&gsi->d_port.lock,
+				raw_spin_unlock_irqrestore(&gsi->d_port.lock,
 								flags);
 				log_event_dbg("%s: err cpkt_notify_connect\n",
 								__func__);
@@ -624,7 +624,7 @@ int ipa_usb_notify_cb(enum ipa_usb_notify_event event,
 
 			cpkt_notify_speed = gsi_ctrl_pkt_alloc(0, GFP_ATOMIC);
 			if (IS_ERR(cpkt_notify_speed)) {
-				spin_unlock_irqrestore(&gsi->d_port.lock,
+				raw_spin_unlock_irqrestore(&gsi->d_port.lock,
 								flags);
 				gsi_ctrl_pkt_free(cpkt_notify_connect);
 				log_event_dbg("%s: err cpkt_notify_speed\n",
@@ -632,12 +632,12 @@ int ipa_usb_notify_cb(enum ipa_usb_notify_event event,
 				return -ENOMEM;
 			}
 			cpkt_notify_speed->type = GSI_CTRL_NOTIFY_SPEED;
-			spin_lock_irqsave(&gsi->c_port.lock, flags);
+			raw_spin_lock_irqsave(&gsi->c_port.lock, flags);
 			list_add_tail(&cpkt_notify_connect->list,
 					&gsi->c_port.cpkt_resp_q);
 			list_add_tail(&cpkt_notify_speed->list,
 					&gsi->c_port.cpkt_resp_q);
-			spin_unlock_irqrestore(&gsi->c_port.lock, flags);
+			raw_spin_unlock_irqrestore(&gsi->c_port.lock, flags);
 			gsi_ctrl_send_notification(gsi);
 		}
 
@@ -661,7 +661,7 @@ int ipa_usb_notify_cb(enum ipa_usb_notify_event event,
 		break;
 	}
 
-	spin_unlock_irqrestore(&gsi->d_port.lock, flags);
+	raw_spin_unlock_irqrestore(&gsi->d_port.lock, flags);
 	return 1;
 }
 
@@ -1283,7 +1283,7 @@ static void gsi_ctrl_clear_cpkt_queues(struct f_gsi *gsi, bool skip_req_q)
 	struct gsi_ctrl_pkt *cpkt = NULL;
 	struct list_head *act, *tmp;
 
-	spin_lock(&gsi->c_port.lock);
+	raw_spin_lock(&gsi->c_port.lock);
 	if (skip_req_q)
 		goto clean_resp_q;
 
@@ -1298,7 +1298,7 @@ clean_resp_q:
 		list_del(&cpkt->list);
 		gsi_ctrl_pkt_free(cpkt);
 	}
-	spin_unlock(&gsi->c_port.lock);
+	raw_spin_unlock(&gsi->c_port.lock);
 }
 
 static int gsi_ctrl_send_cpkt_tomodem(struct f_gsi *gsi, void *buf, size_t len)
@@ -1307,20 +1307,20 @@ static int gsi_ctrl_send_cpkt_tomodem(struct f_gsi *gsi, void *buf, size_t len)
 	struct gsi_ctrl_port *c_port = &gsi->c_port;
 	struct gsi_ctrl_pkt *cpkt;
 
-	spin_lock_irqsave(&c_port->lock, flags);
+	raw_spin_lock_irqsave(&c_port->lock, flags);
 	/* drop cpkt if port is not open */
 	if (!gsi->c_port.is_open) {
 		log_event_dbg("%s: ctrl device %s is not open",
 			   __func__, gsi->c_port.name);
 		c_port->cpkt_drop_cnt++;
-		spin_unlock_irqrestore(&c_port->lock, flags);
+		raw_spin_unlock_irqrestore(&c_port->lock, flags);
 		return -ENODEV;
 	}
 
 	cpkt = gsi_ctrl_pkt_alloc(len, GFP_ATOMIC);
 	if (IS_ERR(cpkt)) {
 		log_event_err("%s: Reset func pkt allocation failed", __func__);
-		spin_unlock_irqrestore(&c_port->lock, flags);
+		raw_spin_unlock_irqrestore(&c_port->lock, flags);
 		return -ENOMEM;
 	}
 
@@ -1329,7 +1329,7 @@ static int gsi_ctrl_send_cpkt_tomodem(struct f_gsi *gsi, void *buf, size_t len)
 
 	list_add_tail(&cpkt->list, &c_port->cpkt_req_q);
 	c_port->host_to_modem++;
-	spin_unlock_irqrestore(&c_port->lock, flags);
+	raw_spin_unlock_irqrestore(&c_port->lock, flags);
 
 	log_event_dbg("%s: Wake up read queue", __func__);
 	wake_up(&c_port->read_wq);
@@ -1403,10 +1403,10 @@ gsi_ctrl_dev_read(struct file *fp, char __user *buf, size_t count, loff_t *pos)
 	}
 
 	/* block until a new packet is available */
-	spin_lock_irqsave(&c_port->lock, flags);
+	raw_spin_lock_irqsave(&c_port->lock, flags);
 	while (list_empty(&c_port->cpkt_req_q)) {
 		log_event_dbg("Requests list is empty. Wait.");
-		spin_unlock_irqrestore(&c_port->lock, flags);
+		raw_spin_unlock_irqrestore(&c_port->lock, flags);
 		ret = wait_event_interruptible(c_port->read_wq,
 			!list_empty(&c_port->cpkt_req_q));
 		if (ret < 0) {
@@ -1414,13 +1414,13 @@ gsi_ctrl_dev_read(struct file *fp, char __user *buf, size_t count, loff_t *pos)
 			return -ERESTARTSYS;
 		}
 		log_event_dbg("Received request packet");
-		spin_lock_irqsave(&c_port->lock, flags);
+		raw_spin_lock_irqsave(&c_port->lock, flags);
 	}
 
 	cpkt = list_first_entry(&c_port->cpkt_req_q, struct gsi_ctrl_pkt,
 							list);
 	list_del(&cpkt->list);
-	spin_unlock_irqrestore(&c_port->lock, flags);
+	raw_spin_unlock_irqrestore(&c_port->lock, flags);
 
 	if (cpkt->len > count) {
 		log_event_err("cpkt size large:%d > buf size:%zu",
@@ -1507,9 +1507,9 @@ static ssize_t gsi_ctrl_dev_write(struct file *fp, const char __user *buf,
 		print_hex_dump(KERN_DEBUG, "WRITE:", DUMP_PREFIX_OFFSET, 16, 1,
 			cpkt->buf, min_t(int, 30, count), false);
 
-	spin_lock_irqsave(&c_port->lock, flags);
+	raw_spin_lock_irqsave(&c_port->lock, flags);
 	list_add_tail(&cpkt->list, &c_port->cpkt_resp_q);
-	spin_unlock_irqrestore(&c_port->lock, flags);
+	raw_spin_unlock_irqrestore(&c_port->lock, flags);
 
 	if (!gsi_ctrl_send_notification(gsi))
 		c_port->modem_to_host++;
@@ -1550,9 +1550,9 @@ static long gsi_ctrl_dev_ioctl(struct file *fp, unsigned cmd,
 			return -ENOMEM;
 		}
 		cpkt->type = GSI_CTRL_NOTIFY_OFFLINE;
-		spin_lock_irqsave(&c_port->lock, flags);
+		raw_spin_lock_irqsave(&c_port->lock, flags);
 		list_add_tail(&cpkt->list, &c_port->cpkt_resp_q);
-		spin_unlock_irqrestore(&c_port->lock, flags);
+		raw_spin_unlock_irqrestore(&c_port->lock, flags);
 		gsi_ctrl_send_notification(gsi);
 		break;
 	case QTI_CTRL_MODEM_ONLINE:
@@ -1664,12 +1664,12 @@ static unsigned int gsi_ctrl_dev_poll(struct file *fp, poll_table *wait)
 
 	poll_wait(fp, &c_port->read_wq, wait);
 
-	spin_lock_irqsave(&c_port->lock, flags);
+	raw_spin_lock_irqsave(&c_port->lock, flags);
 	if (!list_empty(&c_port->cpkt_req_q)) {
 		mask |= POLLIN | POLLRDNORM;
 		log_event_dbg("%s sets POLLIN for %s", __func__, c_port->name);
 	}
-	spin_unlock_irqrestore(&c_port->lock, flags);
+	raw_spin_unlock_irqrestore(&c_port->lock, flags);
 
 	return mask;
 }
@@ -1714,7 +1714,7 @@ int gsi_function_ctrl_port_init(enum ipa_usb_teth_prot prot_id)
 	INIT_LIST_HEAD(&gsi->c_port.cpkt_req_q);
 	INIT_LIST_HEAD(&gsi->c_port.cpkt_resp_q);
 
-	spin_lock_init(&gsi->c_port.lock);
+	raw_spin_lock_init(&gsi->c_port.lock);
 
 	init_waitqueue_head(&gsi->c_port.read_wq);
 
@@ -1781,15 +1781,15 @@ static void gsi_rndis_ipa_reset_trigger(void)
 		return;
 	}
 
-	spin_lock_irqsave(&rndis->d_port.lock, flags);
+	raw_spin_lock_irqsave(&rndis->d_port.lock, flags);
 	if (!rndis) {
 		log_event_err("%s: No RNDIS instance", __func__);
-		spin_unlock_irqrestore(&rndis->d_port.lock, flags);
+		raw_spin_unlock_irqrestore(&rndis->d_port.lock, flags);
 		return;
 	}
 
 	rndis->d_port.net_ready_trigger = false;
-	spin_unlock_irqrestore(&rndis->d_port.lock, flags);
+	raw_spin_unlock_irqrestore(&rndis->d_port.lock, flags);
 }
 
 void gsi_rndis_flow_ctrl_enable(bool enable)
@@ -1882,9 +1882,9 @@ static int queue_notification_request(struct f_gsi *gsi)
 	ret = usb_func_ep_queue(&gsi->function, gsi->c_port.notify,
 			   gsi->c_port.notify_req, GFP_ATOMIC);
 	if (ret < 0) {
-		spin_lock_irqsave(&gsi->c_port.lock, flags);
+		raw_spin_lock_irqsave(&gsi->c_port.lock, flags);
 		gsi->c_port.notify_req_queued = false;
-		spin_unlock_irqrestore(&gsi->c_port.lock, flags);
+		raw_spin_unlock_irqrestore(&gsi->c_port.lock, flags);
 	}
 
 	log_event_dbg("%s: ret:%d req_queued:%d",
@@ -1908,9 +1908,9 @@ static int gsi_ctrl_send_notification(struct f_gsi *gsi)
 		return -ENODEV;
 	}
 
-	spin_lock_irqsave(&gsi->c_port.lock, flags);
+	raw_spin_lock_irqsave(&gsi->c_port.lock, flags);
 	if (list_empty(&gsi->c_port.cpkt_resp_q)) {
-		spin_unlock_irqrestore(&gsi->c_port.lock, flags);
+		raw_spin_unlock_irqrestore(&gsi->c_port.lock, flags);
 		log_event_dbg("%s: cpkt_resp_q is empty\n", __func__);
 		return 0;
 	}
@@ -1919,7 +1919,7 @@ static int gsi_ctrl_send_notification(struct f_gsi *gsi)
 		__func__, gsi->c_port.notify_req_queued);
 
 	if (gsi->c_port.notify_req_queued) {
-		spin_unlock_irqrestore(&gsi->c_port.lock, flags);
+		raw_spin_unlock_irqrestore(&gsi->c_port.lock, flags);
 		log_event_dbg("%s: notify_req is already queued.\n", __func__);
 		return 0;
 	}
@@ -1981,7 +1981,7 @@ static int gsi_ctrl_send_notification(struct f_gsi *gsi)
 		}
 		break;
 	default:
-		spin_unlock_irqrestore(&gsi->c_port.lock, flags);
+		raw_spin_unlock_irqrestore(&gsi->c_port.lock, flags);
 		log_event_err("%s:unknown notify state", __func__);
 		WARN_ON(1);
 		return -EINVAL;
@@ -1998,7 +1998,7 @@ static int gsi_ctrl_send_notification(struct f_gsi *gsi)
 	}
 
 	gsi->c_port.notify_req_queued = true;
-	spin_unlock_irqrestore(&gsi->c_port.lock, flags);
+	raw_spin_unlock_irqrestore(&gsi->c_port.lock, flags);
 	log_event_dbg("send Notify type %02x", event->bNotificationType);
 
 	return queue_notification_request(gsi);
@@ -2012,9 +2012,9 @@ static void gsi_ctrl_notify_resp_complete(struct usb_ep *ep,
 	int status = req->status;
 	unsigned long flags;
 
-	spin_lock_irqsave(&gsi->c_port.lock, flags);
+	raw_spin_lock_irqsave(&gsi->c_port.lock, flags);
 	gsi->c_port.notify_req_queued = false;
-	spin_unlock_irqrestore(&gsi->c_port.lock, flags);
+	raw_spin_unlock_irqrestore(&gsi->c_port.lock, flags);
 
 	switch (status) {
 	case -ECONNRESET:
@@ -2046,9 +2046,9 @@ static void gsi_rndis_response_available(void *_rndis)
 	}
 
 	cpkt->type = GSI_CTRL_NOTIFY_RESPONSE_AVAILABLE;
-	spin_lock_irqsave(&gsi->c_port.lock, flags);
+	raw_spin_lock_irqsave(&gsi->c_port.lock, flags);
 	list_add_tail(&cpkt->list, &gsi->c_port.cpkt_resp_q);
-	spin_unlock_irqrestore(&gsi->c_port.lock, flags);
+	raw_spin_unlock_irqrestore(&gsi->c_port.lock, flags);
 	gsi_ctrl_send_notification(gsi);
 }
 
@@ -2205,10 +2205,10 @@ gsi_setup(struct usb_function *f, const struct usb_ctrlrequest *ctrl)
 			break;
 		}
 
-		spin_lock(&gsi->c_port.lock);
+		raw_spin_lock(&gsi->c_port.lock);
 		if (list_empty(&gsi->c_port.cpkt_resp_q)) {
 			log_event_dbg("ctrl resp queue empty");
-			spin_unlock(&gsi->c_port.lock);
+			raw_spin_unlock(&gsi->c_port.lock);
 			break;
 		}
 
@@ -2216,7 +2216,7 @@ gsi_setup(struct usb_function *f, const struct usb_ctrlrequest *ctrl)
 					struct gsi_ctrl_pkt, list);
 		list_del(&cpkt->list);
 		gsi->c_port.get_encap_cnt++;
-		spin_unlock(&gsi->c_port.lock);
+		raw_spin_unlock(&gsi->c_port.lock);
 
 		value = min_t(unsigned, w_length, cpkt->len);
 		memcpy(req->buf, cpkt->buf, value);
@@ -2844,7 +2844,7 @@ skip_string_id_alloc:
 	}
 
 	/* Initialize event queue */
-	spin_lock_init(&gsi->d_port.evt_q.q_lock);
+	raw_spin_lock_init(&gsi->d_port.evt_q.q_lock);
 	gsi->d_port.evt_q.head = gsi->d_port.evt_q.tail = MAXQUEUELEN - 1;
 
 	/* copy descriptors, and track endpoint copies */
@@ -3315,7 +3315,7 @@ static int gsi_function_init(enum ipa_usb_teth_prot prot_id)
 		goto error;
 	}
 
-	spin_lock_init(&gsi->d_port.lock);
+	raw_spin_lock_init(&gsi->d_port.lock);
 
 	init_waitqueue_head(&gsi->d_port.wait_for_ipa_ready);
 
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_gsi.h b/kernel/msm-3.18/drivers/usb/gadget/function/f_gsi.h
index 2a4944cb1..471d7f033 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_gsi.h
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_gsi.h
@@ -89,7 +89,7 @@ enum gsi_ctrl_notify_state {
 struct event_queue {
 	u8 event[MAXQUEUELEN];
 	u8 head, tail;
-	spinlock_t q_lock;
+	raw_spinlock_t q_lock;
 };
 
 struct gsi_ntb_info {
@@ -158,7 +158,7 @@ struct gsi_ctrl_port {
 	struct list_head cpkt_resp_q;
 	unsigned long cpkts_len;
 
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	int ipa_cons_clnt_hdl;
 	int ipa_prod_clnt_hdl;
@@ -196,7 +196,7 @@ struct gsi_data_port {
 	bool net_ready_trigger;
 	struct gsi_ntb_info ntb_info;
 
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	struct work_struct usb_ipa_w;
 	struct workqueue_struct *ipa_usb_wq;
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_mbim.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_mbim.c
index f2f95a315..e15e68bef 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_mbim.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_mbim.c
@@ -122,7 +122,7 @@ struct f_mbim {
 	u8				ctrl_id, data_id;
 	bool				data_interface_up;
 
-	spinlock_t			lock;
+	raw_spinlock_t			lock;
 
 	struct list_head	cpkt_req_q;
 	struct list_head	cpkt_resp_q;
@@ -625,17 +625,17 @@ static void mbim_reset_function_queue(struct f_mbim *dev)
 
 	pr_debug("Queue empty packet for QBI\n");
 
-	spin_lock(&dev->lock);
+	raw_spin_lock(&dev->lock);
 
 	cpkt = mbim_alloc_ctrl_pkt(0, GFP_ATOMIC);
 	if (!cpkt) {
 		pr_err("%s: Unable to allocate reset function pkt\n", __func__);
-		spin_unlock(&dev->lock);
+		raw_spin_unlock(&dev->lock);
 		return;
 	}
 
 	list_add_tail(&cpkt->list, &dev->cpkt_req_q);
-	spin_unlock(&dev->lock);
+	raw_spin_unlock(&dev->lock);
 
 	pr_debug("%s: Wake up read queue\n", __func__);
 	wake_up(&dev->read_wq);
@@ -653,7 +653,7 @@ static void mbim_clear_queues(struct f_mbim *mbim)
 	struct ctrl_pkt	*cpkt = NULL;
 	struct list_head *act, *tmp;
 
-	spin_lock(&mbim->lock);
+	raw_spin_lock(&mbim->lock);
 	list_for_each_safe(act, tmp, &mbim->cpkt_req_q) {
 		cpkt = list_entry(act, struct ctrl_pkt, list);
 		list_del(&cpkt->list);
@@ -664,7 +664,7 @@ static void mbim_clear_queues(struct f_mbim *mbim)
 		list_del(&cpkt->list);
 		mbim_free_ctrl_pkt(cpkt);
 	}
-	spin_unlock(&mbim->lock);
+	raw_spin_unlock(&mbim->lock);
 }
 
 /*
@@ -701,11 +701,11 @@ static void mbim_do_notify(struct f_mbim *mbim)
 			return;
 		}
 
-		spin_unlock(&mbim->lock);
+		raw_spin_unlock(&mbim->lock);
 		status = usb_func_ep_queue(&mbim->function,
 				mbim->not_port.notify,
 				req, GFP_ATOMIC);
-		spin_lock(&mbim->lock);
+		raw_spin_lock(&mbim->lock);
 		if (status) {
 			atomic_dec(&mbim->not_port.notify_count);
 			pr_err("Queue notify request failed, err: %d\n",
@@ -726,10 +726,10 @@ static void mbim_do_notify(struct f_mbim *mbim)
 	atomic_inc(&mbim->not_port.notify_count);
 	pr_debug("queue request: notify_count = %d\n",
 		atomic_read(&mbim->not_port.notify_count));
-	spin_unlock(&mbim->lock);
+	raw_spin_unlock(&mbim->lock);
 	status = usb_func_ep_queue(&mbim->function, mbim->not_port.notify, req,
 			GFP_ATOMIC);
-	spin_lock(&mbim->lock);
+	raw_spin_lock(&mbim->lock);
 	if (status) {
 		atomic_dec(&mbim->not_port.notify_count);
 		pr_err("usb_func_ep_queue failed, err: %d\n", status);
@@ -743,7 +743,7 @@ static void mbim_notify_complete(struct usb_ep *ep, struct usb_request *req)
 
 	pr_debug("dev:%pK\n", mbim);
 
-	spin_lock(&mbim->lock);
+	raw_spin_lock(&mbim->lock);
 	switch (req->status) {
 	case 0:
 		atomic_dec(&mbim->not_port.notify_count);
@@ -757,10 +757,10 @@ static void mbim_notify_complete(struct usb_ep *ep, struct usb_request *req)
 		mbim->not_port.notify_state = MBIM_NOTIFY_NONE;
 		atomic_set(&mbim->not_port.notify_count, 0);
 		pr_info("ESHUTDOWN/ECONNRESET, connection gone\n");
-		spin_unlock(&mbim->lock);
+		raw_spin_unlock(&mbim->lock);
 		mbim_clear_queues(mbim);
 		mbim_reset_function_queue(mbim);
-		spin_lock(&mbim->lock);
+		raw_spin_lock(&mbim->lock);
 		break;
 	default:
 		pr_err("Unknown event %02x --> %d\n",
@@ -768,7 +768,7 @@ static void mbim_notify_complete(struct usb_ep *ep, struct usb_request *req)
 		break;
 	}
 
-	spin_unlock(&mbim->lock);
+	raw_spin_unlock(&mbim->lock);
 
 	pr_debug("dev:%pK Exit\n", mbim);
 }
@@ -865,10 +865,10 @@ fmbim_cmd_complete(struct usb_ep *ep, struct usb_request *req)
 	pr_debug("Add to cpkt_req_q packet with len = %d\n", len);
 	memcpy(cpkt->buf, req->buf, len);
 
-	spin_lock(&dev->lock);
+	raw_spin_lock(&dev->lock);
 
 	list_add_tail(&cpkt->list, &dev->cpkt_req_q);
-	spin_unlock(&dev->lock);
+	raw_spin_unlock(&dev->lock);
 
 	/* wakeup read thread */
 	pr_debug("Wake up read queue\n");
@@ -883,9 +883,9 @@ static void mbim_response_complete(struct usb_ep *ep, struct usb_request *req)
 
 	pr_debug("%s: queue notify request if any new response available\n"
 			, __func__);
-	spin_lock(&mbim->lock);
+	raw_spin_lock(&mbim->lock);
 	mbim_do_notify(mbim);
-	spin_unlock(&mbim->lock);
+	raw_spin_unlock(&mbim->lock);
 }
 
 static int
@@ -948,17 +948,17 @@ mbim_setup(struct usb_function *f, const struct usb_ctrlrequest *ctrl)
 			ctrl->bRequestType, ctrl->bRequest,
 			w_value, w_index, w_length);
 
-		spin_lock(&mbim->lock);
+		raw_spin_lock(&mbim->lock);
 		if (list_empty(&mbim->cpkt_resp_q)) {
 			pr_err("ctrl resp queue empty\n");
-			spin_unlock(&mbim->lock);
+			raw_spin_unlock(&mbim->lock);
 			break;
 		}
 
 		cpkt = list_first_entry(&mbim->cpkt_resp_q,
 					struct ctrl_pkt, list);
 		list_del(&cpkt->list);
-		spin_unlock(&mbim->lock);
+		raw_spin_unlock(&mbim->lock);
 
 		value = min_t(unsigned, w_length, cpkt->len);
 		memcpy(req->buf, cpkt->buf, value);
@@ -1243,9 +1243,9 @@ static int mbim_set_alt(struct usb_function *f, unsigned intf, unsigned alt)
 		}
 notify_ready:
 		mbim->data_interface_up = alt;
-		spin_lock(&mbim->lock);
+		raw_spin_lock(&mbim->lock);
 		mbim->not_port.notify_state = MBIM_NOTIFY_RESPONSE_AVAILABLE;
-		spin_unlock(&mbim->lock);
+		raw_spin_unlock(&mbim->lock);
 	} else {
 		goto fail;
 	}
@@ -1380,9 +1380,9 @@ static void mbim_resume(struct usb_function *f)
 		return;
 
 	/* resume control path by queuing notify req */
-	spin_lock(&mbim->lock);
+	raw_spin_lock(&mbim->lock);
 	mbim_do_notify(mbim);
-	spin_unlock(&mbim->lock);
+	raw_spin_unlock(&mbim->lock);
 
 	/* MBIM data interface is up only when alt setting is set to 1. */
 	if (!mbim->data_interface_up) {
@@ -1787,10 +1787,10 @@ mbim_read(struct file *fp, char __user *buf, size_t count, loff_t *pos)
 		return -EIO;
 	}
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	while (list_empty(&dev->cpkt_req_q)) {
 		pr_debug("Requests list is empty. Wait.\n");
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		ret = wait_event_interruptible(dev->read_wq,
 			!list_empty(&dev->cpkt_req_q));
 		if (ret < 0) {
@@ -1799,13 +1799,13 @@ mbim_read(struct file *fp, char __user *buf, size_t count, loff_t *pos)
 			return -ERESTARTSYS;
 		}
 		pr_debug("Received request packet\n");
-		spin_lock_irqsave(&dev->lock, flags);
+		raw_spin_lock_irqsave(&dev->lock, flags);
 	}
 
 	cpkt = list_first_entry(&dev->cpkt_req_q, struct ctrl_pkt,
 							list);
 	if (cpkt->len > count) {
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		mbim_unlock(&dev->read_excl);
 		pr_err("cpkt size too big:%d > buf size:%zu\n",
 				cpkt->len, count);
@@ -1815,7 +1815,7 @@ mbim_read(struct file *fp, char __user *buf, size_t count, loff_t *pos)
 	pr_debug("cpkt size:%d\n", cpkt->len);
 
 	list_del(&cpkt->list);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 	mbim_unlock(&dev->read_excl);
 
 	ret = copy_to_user(buf, cpkt->buf, cpkt->len);
@@ -1894,22 +1894,22 @@ mbim_write(struct file *fp, const char __user *buf, size_t count, loff_t *pos)
 		return ret;
 	}
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	list_add_tail(&cpkt->list, &dev->cpkt_resp_q);
 
 	if (atomic_inc_return(&dev->not_port.notify_count) != 1) {
 		pr_debug("delay ep_queue: notifications queue is busy[%d]\n",
 			atomic_read(&dev->not_port.notify_count));
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		mbim_unlock(&dev->write_excl);
 		return count;
 	}
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	ret = usb_func_ep_queue(&dev->function, dev->not_port.notify,
 			   req, GFP_ATOMIC);
 	if (ret == -ENOTSUPP || (ret < 0 && ret != -EAGAIN)) {
-		spin_lock_irqsave(&dev->lock, flags);
+		raw_spin_lock_irqsave(&dev->lock, flags);
 		/* check if device disconnected while we dropped lock */
 		if (atomic_read(&dev->online)) {
 			list_del(&cpkt->list);
@@ -1917,7 +1917,7 @@ mbim_write(struct file *fp, const char __user *buf, size_t count, loff_t *pos)
 			mbim_free_ctrl_pkt(cpkt);
 		}
 		dev->cpkt_drop_cnt++;
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		pr_err("drop ctrl pkt of len %d error %d\n", cpkt->len, ret);
 	} else {
 		ret = 0;
@@ -2105,7 +2105,7 @@ static int mbim_init(int instances)
 		dev->bam_port.ipa_consumer_ep = -1;
 		dev->bam_port.ipa_producer_ep = -1;
 
-		spin_lock_init(&dev->lock);
+		raw_spin_lock_init(&dev->lock);
 		INIT_LIST_HEAD(&dev->cpkt_req_q);
 		INIT_LIST_HEAD(&dev->cpkt_resp_q);
 
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_mtp.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_mtp.c
index 0677e493c..dcd863ecf 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_mtp.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_mtp.c
@@ -92,7 +92,7 @@ static const char mtp_shortname[] = DRIVER_NAME "_usb";
 struct mtp_dev {
 	struct usb_function function;
 	struct usb_composite_dev *cdev;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 
 	struct usb_ep *ep_in;
 	struct usb_ep *ep_out;
@@ -450,9 +450,9 @@ static void mtp_req_put(struct mtp_dev *dev, struct list_head *head,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	list_add_tail(&req->list, head);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 }
 
 /* remove a request from the head of a list */
@@ -462,14 +462,14 @@ static struct usb_request
 	unsigned long flags;
 	struct usb_request *req;
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	if (list_empty(head)) {
 		req = 0;
 	} else {
 		req = list_first_entry(head, struct usb_request, list);
 		list_del(&req->list);
 	}
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 	return req;
 }
 
@@ -634,15 +634,15 @@ static ssize_t mtp_read(struct file *fp, char __user *buf,
 	if (len > mtp_rx_req_len)
 		return -EINVAL;
 
-	spin_lock_irq(&dev->lock);
+	raw_spin_lock_irq(&dev->lock);
 	if (dev->state == STATE_CANCELED) {
 		/* report cancelation to userspace */
 		dev->state = STATE_READY;
-		spin_unlock_irq(&dev->lock);
+		raw_spin_unlock_irq(&dev->lock);
 		return -ECANCELED;
 	}
 	dev->state = STATE_BUSY;
-	spin_unlock_irq(&dev->lock);
+	raw_spin_unlock_irq(&dev->lock);
 
 requeue_req:
 	/* queue a request */
@@ -664,9 +664,9 @@ requeue_req:
 		r = -ECANCELED;
 		if (!dev->rx_done)
 			usb_ep_dequeue(dev->ep_out, req);
-		spin_lock_irq(&dev->lock);
+		raw_spin_lock_irq(&dev->lock);
 		dev->state = STATE_CANCELED;
-		spin_unlock_irq(&dev->lock);
+		raw_spin_unlock_irq(&dev->lock);
 		goto done;
 	}
 	if (ret < 0) {
@@ -688,12 +688,12 @@ requeue_req:
 		r = -EIO;
 
 done:
-	spin_lock_irq(&dev->lock);
+	raw_spin_lock_irq(&dev->lock);
 	if (dev->state == STATE_CANCELED)
 		r = -ECANCELED;
 	else if (dev->state != STATE_OFFLINE)
 		dev->state = STATE_READY;
-	spin_unlock_irq(&dev->lock);
+	raw_spin_unlock_irq(&dev->lock);
 
 	DBG(cdev, "mtp_read returning %zd state:%d\n", r, dev->state);
 	return r;
@@ -712,19 +712,19 @@ static ssize_t mtp_write(struct file *fp, const char __user *buf,
 
 	DBG(cdev, "mtp_write(%zu) state:%d\n", count, dev->state);
 
-	spin_lock_irq(&dev->lock);
+	raw_spin_lock_irq(&dev->lock);
 	if (dev->state == STATE_CANCELED) {
 		/* report cancelation to userspace */
 		dev->state = STATE_READY;
-		spin_unlock_irq(&dev->lock);
+		raw_spin_unlock_irq(&dev->lock);
 		return -ECANCELED;
 	}
 	if (dev->state == STATE_OFFLINE) {
-		spin_unlock_irq(&dev->lock);
+		raw_spin_unlock_irq(&dev->lock);
 		return -ENODEV;
 	}
 	dev->state = STATE_BUSY;
-	spin_unlock_irq(&dev->lock);
+	raw_spin_unlock_irq(&dev->lock);
 
 	/* we need to send a zero length packet to signal the end of transfer
 	 * if the transfer size is aligned to a packet boundary.
@@ -782,12 +782,12 @@ static ssize_t mtp_write(struct file *fp, const char __user *buf,
 	if (req)
 		mtp_req_put(dev, &dev->tx_idle, req);
 
-	spin_lock_irq(&dev->lock);
+	raw_spin_lock_irq(&dev->lock);
 	if (dev->state == STATE_CANCELED)
 		r = -ECANCELED;
 	else if (dev->state != STATE_OFFLINE)
 		dev->state = STATE_READY;
-	spin_unlock_irq(&dev->lock);
+	raw_spin_unlock_irq(&dev->lock);
 
 	DBG(cdev, "mtp_write returning %zd state:%d\n", r, dev->state);
 	return r;
@@ -1065,21 +1065,21 @@ static long mtp_send_receive_ioctl(struct file *fp, unsigned code,
 		return -EBUSY;
 	}
 
-	spin_lock_irq(&dev->lock);
+	raw_spin_lock_irq(&dev->lock);
 	if (dev->state == STATE_CANCELED) {
 		/* report cancelation to userspace */
 		dev->state = STATE_READY;
-		spin_unlock_irq(&dev->lock);
+		raw_spin_unlock_irq(&dev->lock);
 		ret = -ECANCELED;
 		goto out;
 	}
 	if (dev->state == STATE_OFFLINE) {
-		spin_unlock_irq(&dev->lock);
+		raw_spin_unlock_irq(&dev->lock);
 		ret = -ENODEV;
 		goto out;
 	}
 	dev->state = STATE_BUSY;
-	spin_unlock_irq(&dev->lock);
+	raw_spin_unlock_irq(&dev->lock);
 
 	/* hold a reference to the file while we are working with it */
 	filp = fget(mfr->fd);
@@ -1120,12 +1120,12 @@ static long mtp_send_receive_ioctl(struct file *fp, unsigned code,
 	ret = dev->xfer_result;
 
 fail:
-	spin_lock_irq(&dev->lock);
+	raw_spin_lock_irq(&dev->lock);
 	if (dev->state == STATE_CANCELED)
 		ret = -ECANCELED;
 	else if (dev->state != STATE_OFFLINE)
 		dev->state = STATE_READY;
-	spin_unlock_irq(&dev->lock);
+	raw_spin_unlock_irq(&dev->lock);
 out:
 	mtp_unlock(&dev->ioctl_excl);
 	DBG(dev->cdev, "ioctl returning %d state:%d\n", ret, dev->state);
@@ -1337,13 +1337,13 @@ static int mtp_ctrlrequest(struct usb_composite_dev *cdev,
 				&& w_value == 0) {
 			DBG(cdev, "MTP_REQ_CANCEL\n");
 
-			spin_lock_irqsave(&dev->lock, flags);
+			raw_spin_lock_irqsave(&dev->lock, flags);
 			if (dev->state == STATE_BUSY) {
 				dev->state = STATE_CANCELED;
 				wake_up(&dev->read_wq);
 				wake_up(&dev->write_wq);
 			}
-			spin_unlock_irqrestore(&dev->lock, flags);
+			raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 			/* We need to queue a request to read the remaining
 			 *  bytes, but we don't actually need to look at
@@ -1357,7 +1357,7 @@ static int mtp_ctrlrequest(struct usb_composite_dev *cdev,
 				__constant_cpu_to_le16(sizeof(*status));
 
 			DBG(cdev, "MTP_REQ_GET_DEVICE_STATUS\n");
-			spin_lock_irqsave(&dev->lock, flags);
+			raw_spin_lock_irqsave(&dev->lock, flags);
 			/* device status is "busy" until we report
 			 * the cancelation to userspace
 			 */
@@ -1367,7 +1367,7 @@ static int mtp_ctrlrequest(struct usb_composite_dev *cdev,
 			else
 				status->wCode =
 					__cpu_to_le16(MTP_RESPONSE_OK);
-			spin_unlock_irqrestore(&dev->lock, flags);
+			raw_spin_unlock_irqrestore(&dev->lock, flags);
 			value = sizeof(*status);
 		}
 	}
@@ -1571,7 +1571,7 @@ static int debug_mtp_read_stats(struct seq_file *s, void *unused)
 	seq_puts(s, "\n=======================\n");
 	seq_puts(s, "MTP Write Stats:\n");
 	seq_puts(s, "\n=======================\n");
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	min = dev->perf[0].vfs_wtime;
 	for (i = 0; i < MAX_ITERATION; i++) {
 		seq_printf(s, "vfs write: bytes:%ld\t\t time:%d\n",
@@ -1611,7 +1611,7 @@ static int debug_mtp_read_stats(struct seq_file *s, void *unused)
 
 	seq_printf(s, "vfs_read(time in usec) min:%d\t max:%d\t avg:%d\n",
 				min, max, (iteration ? (sum / iteration) : 0));
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 	return 0;
 }
 
@@ -1641,11 +1641,11 @@ static ssize_t debug_mtp_reset_stats(struct file *file, const char __user *buf,
 		return ret;
 	}
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	memset(&dev->perf[0], 0, MAX_ITERATION * sizeof(dev->perf[0]));
 	dev->dbg_read_index = 0;
 	dev->dbg_write_index = 0;
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	return count;
 }
@@ -1696,7 +1696,7 @@ static int __mtp_setup(struct mtp_instance *fi_mtp)
 	if (!dev)
 		return -ENOMEM;
 
-	spin_lock_init(&dev->lock);
+	raw_spin_lock_init(&dev->lock);
 	init_waitqueue_head(&dev->read_wq);
 	init_waitqueue_head(&dev->write_wq);
 	init_waitqueue_head(&dev->intr_wq);
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_qc_rndis.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_qc_rndis.c
index b36236e04..64a30e8fa 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_qc_rndis.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_qc_rndis.c
@@ -108,7 +108,7 @@ struct f_rndis_qc {
 };
 
 static struct ipa_usb_init_params rndis_ipa_params;
-static spinlock_t rndis_lock;
+static raw_spinlock_t rndis_lock;
 static bool rndis_ipa_supported;
 static void rndis_qc_open(struct qc_gether *geth);
 
@@ -498,16 +498,16 @@ static void rndis_qc_response_complete(struct usb_ep *ep,
 	struct usb_composite_dev	*cdev;
 	struct usb_ep *notify_ep;
 
-	spin_lock(&rndis_lock);
+	raw_spin_lock(&rndis_lock);
 	rndis = _rndis_qc;
 	if (!rndis || !rndis->notify || !rndis->notify->driver_data) {
-		spin_unlock(&rndis_lock);
+		raw_spin_unlock(&rndis_lock);
 		return;
 	}
 
 	if (!rndis->port.func.config || !rndis->port.func.config->cdev) {
 		pr_err("%s(): cdev or config is NULL.\n", __func__);
-		spin_unlock(&rndis_lock);
+		raw_spin_unlock(&rndis_lock);
 		return;
 	}
 
@@ -538,22 +538,22 @@ static void rndis_qc_response_complete(struct usb_ep *ep,
 		if (atomic_dec_and_test(&rndis->notify_count))
 			goto out;
 		notify_ep = rndis->notify;
-		spin_unlock(&rndis_lock);
+		raw_spin_unlock(&rndis_lock);
 		status = usb_ep_queue(notify_ep, req, GFP_ATOMIC);
 		if (status) {
-			spin_lock(&rndis_lock);
+			raw_spin_lock(&rndis_lock);
 			if (!_rndis_qc)
 				goto out;
 			atomic_dec(&_rndis_qc->notify_count);
 			DBG(cdev, "notify/1 --> %d\n", status);
-			spin_unlock(&rndis_lock);
+			raw_spin_unlock(&rndis_lock);
 		}
 	}
 
 	return;
 
 out:
-	spin_unlock(&rndis_lock);
+	raw_spin_unlock(&rndis_lock);
 }
 
 static void rndis_qc_command_complete(struct usb_ep *ep,
@@ -570,10 +570,10 @@ static void rndis_qc_command_complete(struct usb_ep *ep,
 		return;
 	}
 
-	spin_lock(&rndis_lock);
+	raw_spin_lock(&rndis_lock);
 	rndis = _rndis_qc;
 	if (!rndis || !rndis->notify || !rndis->notify->driver_data) {
-		spin_unlock(&rndis_lock);
+		raw_spin_unlock(&rndis_lock);
 		return;
 	}
 
@@ -605,7 +605,7 @@ static void rndis_qc_command_complete(struct usb_ep *ep,
 				rndis_get_dl_max_xfer_size(rndis->config);
 		u_bam_data_set_dl_max_xfer_size(dl_max_xfer_size);
 	}
-	spin_unlock(&rndis_lock);
+	raw_spin_unlock(&rndis_lock);
 }
 
 static int
@@ -791,9 +791,9 @@ static void rndis_qc_disable(struct usb_function *f)
 
 	pr_info("rndis deactivated\n");
 
-	spin_lock_irqsave(&rndis_lock, flags);
+	raw_spin_lock_irqsave(&rndis_lock, flags);
 	rndis_uninit(rndis->config);
-	spin_unlock_irqrestore(&rndis_lock, flags);
+	raw_spin_unlock_irqrestore(&rndis_lock, flags);
 	bam_data_disconnect(&rndis->bam_port, USB_FUNC_RNDIS, rndis->port_num);
 	if (rndis->xport != USB_GADGET_XPORT_BAM2BAM_IPA)
 		gether_qc_disconnect_name(&rndis->port, "rndis0");
@@ -1107,10 +1107,10 @@ rndis_qc_unbind(struct usb_configuration *c, struct usb_function *f)
 		rndis_ipa_supported = false;
 	}
 
-	spin_lock_irqsave(&rndis_lock, flags);
+	raw_spin_lock_irqsave(&rndis_lock, flags);
 	kfree(rndis);
 	_rndis_qc = NULL;
-	spin_unlock_irqrestore(&rndis_lock, flags);
+	raw_spin_unlock_irqrestore(&rndis_lock, flags);
 }
 
 void rndis_ipa_reset_trigger(void)
@@ -1136,22 +1136,22 @@ void rndis_net_ready_notify(void)
 	unsigned long flags;
 	int port_num;
 
-	spin_lock_irqsave(&rndis_lock, flags);
+	raw_spin_lock_irqsave(&rndis_lock, flags);
 	rndis = _rndis_qc;
 	if (!rndis) {
 		pr_err("%s: No RNDIS instance", __func__);
-		spin_unlock_irqrestore(&rndis_lock, flags);
+		raw_spin_unlock_irqrestore(&rndis_lock, flags);
 		return;
 	}
 	if (rndis->net_ready_trigger) {
 		pr_err("%s: Already triggered", __func__);
-		spin_unlock_irqrestore(&rndis_lock, flags);
+		raw_spin_unlock_irqrestore(&rndis_lock, flags);
 		return;
 	}
 
 	pr_debug("%s: Set net_ready_trigger", __func__);
 	rndis->net_ready_trigger = true;
-	spin_unlock_irqrestore(&rndis_lock, flags);
+	raw_spin_unlock_irqrestore(&rndis_lock, flags);
 	port_num = (u_bam_data_func_to_port(USB_FUNC_RNDIS,
 					    RNDIS_QC_ACTIVE_PORT));
 	if (port_num < 0)
@@ -1328,7 +1328,7 @@ static int rndis_qc_open_dev(struct inode *ip, struct file *fp)
 	unsigned long flags;
 	pr_info("Open rndis QC driver\n");
 
-	spin_lock_irqsave(&rndis_lock, flags);
+	raw_spin_lock_irqsave(&rndis_lock, flags);
 	if (!_rndis_qc) {
 		pr_err("rndis_qc_dev not created yet\n");
 		ret = -ENODEV;
@@ -1343,7 +1343,7 @@ static int rndis_qc_open_dev(struct inode *ip, struct file *fp)
 
 	fp->private_data = _rndis_qc;
 fail:
-	spin_unlock_irqrestore(&rndis_lock, flags);
+	raw_spin_unlock_irqrestore(&rndis_lock, flags);
 
 	if (!ret)
 		pr_info("rndis QC file opened\n");
@@ -1356,15 +1356,15 @@ static int rndis_qc_release_dev(struct inode *ip, struct file *fp)
 	unsigned long flags;
 	pr_info("Close rndis QC file\n");
 
-	spin_lock_irqsave(&rndis_lock, flags);
+	raw_spin_lock_irqsave(&rndis_lock, flags);
 
 	if (!_rndis_qc) {
 		pr_err("rndis_qc_dev not present\n");
-		spin_unlock_irqrestore(&rndis_lock, flags);
+		raw_spin_unlock_irqrestore(&rndis_lock, flags);
 		return -ENODEV;
 	}
 	rndis_qc_unlock(&_rndis_qc->open_excl);
-	spin_unlock_irqrestore(&rndis_lock, flags);
+	raw_spin_unlock_irqrestore(&rndis_lock, flags);
 	return 0;
 }
 
@@ -1375,7 +1375,7 @@ static long rndis_qc_ioctl(struct file *fp, unsigned cmd, unsigned long arg)
 	int ret = 0;
 	unsigned long flags;
 
-	spin_lock_irqsave(&rndis_lock, flags);
+	raw_spin_lock_irqsave(&rndis_lock, flags);
 	if (!_rndis_qc) {
 		pr_err("rndis_qc_dev not present\n");
 		ret = -ENODEV;
@@ -1390,7 +1390,7 @@ static long rndis_qc_ioctl(struct file *fp, unsigned cmd, unsigned long arg)
 		goto fail;
 	}
 
-	spin_unlock_irqrestore(&rndis_lock, flags);
+	raw_spin_unlock_irqrestore(&rndis_lock, flags);
 
 	pr_info("Received command %d\n", cmd);
 
@@ -1422,7 +1422,7 @@ static long rndis_qc_ioctl(struct file *fp, unsigned cmd, unsigned long arg)
 		ret = -EINVAL;
 	}
 
-	spin_lock_irqsave(&rndis_lock, flags);
+	raw_spin_lock_irqsave(&rndis_lock, flags);
 
 	if (!_rndis_qc) {
 		pr_err("rndis_qc_dev not present\n");
@@ -1432,7 +1432,7 @@ static long rndis_qc_ioctl(struct file *fp, unsigned cmd, unsigned long arg)
 	rndis_qc_unlock(&_rndis_qc->ioctl_excl);
 
 fail:
-	spin_unlock_irqrestore(&rndis_lock, flags);
+	raw_spin_unlock_irqrestore(&rndis_lock, flags);
 	return ret;
 }
 
@@ -1455,7 +1455,7 @@ static int rndis_qc_init(void)
 
 	pr_info("initialize rndis QC instance\n");
 
-	spin_lock_init(&rndis_lock);
+	raw_spin_lock_init(&rndis_lock);
 
 	ret = misc_register(&rndis_qc_device);
 	if (ret)
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_qdss.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_qdss.c
index 14c09760a..bca51d551 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_qdss.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_qdss.c
@@ -46,7 +46,7 @@ static struct qdss_ports {
 } qdss_ports[NR_QDSS_PORTS];
 
 
-static DEFINE_SPINLOCK(qdss_lock);
+static DEFINE_RAW_SPINLOCK(qdss_lock);
 static LIST_HEAD(usb_qdss_ch_list);
 
 static struct usb_interface_descriptor qdss_data_intf_desc = {
@@ -236,13 +236,13 @@ static void qdss_write_complete(struct usb_ep *ep,
 		}
 	}
 
-	spin_lock_irqsave(&qdss->lock, flags);
+	raw_spin_lock_irqsave(&qdss->lock, flags);
 	list_add_tail(&req->list, list_pool);
 	if (req->length != 0) {
 		d_req->actual = req->actual;
 		d_req->status = req->status;
 	}
-	spin_unlock_irqrestore(&qdss->lock, flags);
+	raw_spin_unlock_irqrestore(&qdss->lock, flags);
 
 	if (qdss->ch.notify)
 		qdss->ch.notify(qdss->ch.priv, state, d_req,
@@ -261,9 +261,9 @@ static void qdss_ctrl_read_complete(struct usb_ep *ep,
 	d_req->actual = req->actual;
 	d_req->status = req->status;
 
-	spin_lock_irqsave(&qdss->lock, flags);
+	raw_spin_lock_irqsave(&qdss->lock, flags);
 	list_add_tail(&req->list, &qdss->ctrl_read_pool);
-	spin_unlock_irqrestore(&qdss->lock, flags);
+	raw_spin_unlock_irqrestore(&qdss->lock, flags);
 
 	if (qdss->ch.notify)
 		qdss->ch.notify(qdss->ch.priv, USB_QDSS_CTRL_READ_DONE, d_req,
@@ -674,9 +674,9 @@ static void qdss_disable(struct usb_function *f)
 		return;
 	}
 	pr_debug("qdss_disable\n");
-	spin_lock_irqsave(&qdss->lock, flags);
+	raw_spin_lock_irqsave(&qdss->lock, flags);
 	if (!qdss->usb_connected) {
-		spin_unlock_irqrestore(&qdss->lock, flags);
+		raw_spin_unlock_irqrestore(&qdss->lock, flags);
 		return;
 	}
 
@@ -685,7 +685,7 @@ static void qdss_disable(struct usb_function *f)
 	switch (dxport) {
 	case USB_GADGET_XPORT_BAM2BAM_IPA:
 	case USB_GADGET_XPORT_BAM_DMUX:
-		spin_unlock_irqrestore(&qdss->lock, flags);
+		raw_spin_unlock_irqrestore(&qdss->lock, flags);
 		/* Disable usb irq for CI gadget. It will be enabled in
 		 * usb_bam_disconnect_pipe() after disconnecting all pipes
 		 * and USB BAM reset is done.
@@ -699,7 +699,7 @@ static void qdss_disable(struct usb_function *f)
 						xport_to_str(dxport));
 	}
 
-	spin_unlock_irqrestore(&qdss->lock, flags);
+	raw_spin_unlock_irqrestore(&qdss->lock, flags);
 	/*cancell all active xfers*/
 	qdss_eps_disable(f);
 	if (!gadget_is_dwc3(qdss->cdev->gadget))
@@ -721,7 +721,7 @@ static int qdss_dpl_ipa_connect(int port_num)
 	ipa_data_port_select(port_num, USB_GADGET_DPL);
 	qdss = qdss_ports[port_num].port;
 
-	spin_lock_irqsave(&qdss->lock, flags);
+	raw_spin_lock_irqsave(&qdss->lock, flags);
 	g_qdss = &qdss->port;
 	gp = &qdss_ports[port_num].ipa_port;
 	gp->cdev = qdss->cdev;
@@ -731,7 +731,7 @@ static int qdss_dpl_ipa_connect(int port_num)
 	gp->func = &g_qdss->function;
 	gadget = qdss->cdev->gadget;
 
-	spin_unlock_irqrestore(&qdss->lock, flags);
+	raw_spin_unlock_irqrestore(&qdss->lock, flags);
 
 	usb_bam_type = usb_bam_get_bam_type(gadget->name);
 	dst_connection_idx = usb_bam_get_connection_idx(usb_bam_type, IPA_P_BAM,
@@ -1031,7 +1031,7 @@ static int qdss_bind_config(struct usb_configuration *c, unsigned char portno)
 	if (!name)
 		return -ENOMEM;
 
-	spin_lock_irqsave(&qdss_lock, flags);
+	raw_spin_lock_irqsave(&qdss_lock, flags);
 
 	list_for_each_entry(ch, &usb_qdss_ch_list, list) {
 		if (!strcmp(name, ch->name)) {
@@ -1041,17 +1041,17 @@ static int qdss_bind_config(struct usb_configuration *c, unsigned char portno)
 	}
 	if (!found) {
 		if (!qdss) {
-			spin_unlock_irqrestore(&qdss_lock, flags);
+			raw_spin_unlock_irqrestore(&qdss_lock, flags);
 			return -ENOMEM;
 		}
-		spin_unlock_irqrestore(&qdss_lock, flags);
+		raw_spin_unlock_irqrestore(&qdss_lock, flags);
 		qdss->wq = create_singlethread_workqueue(name);
 		if (!qdss->wq) {
 			kfree(name);
 			kfree(qdss);
 			return -ENOMEM;
 		}
-		spin_lock_irqsave(&qdss_lock, flags);
+		raw_spin_lock_irqsave(&qdss_lock, flags);
 		ch = &qdss->ch;
 		ch->name = name;
 		list_add_tail(&ch->list, &usb_qdss_ch_list);
@@ -1065,7 +1065,7 @@ static int qdss_bind_config(struct usb_configuration *c, unsigned char portno)
 			qdss_ports[portno].port = qdss;
 		}
 	}
-	spin_unlock_irqrestore(&qdss_lock, flags);
+	raw_spin_unlock_irqrestore(&qdss_lock, flags);
 	qdss->cdev = c->cdev;
 	qdss->port_num = portno;
 	qdss->port.function.name = name;
@@ -1087,7 +1087,7 @@ static int qdss_bind_config(struct usb_configuration *c, unsigned char portno)
 	qdss->port.function.unbind = qdss_unbind;
 	qdss->port.function.set_alt = qdss_set_alt;
 	qdss->port.function.disable = qdss_disable;
-	spin_lock_init(&qdss->lock);
+	raw_spin_lock_init(&qdss->lock);
 	INIT_LIST_HEAD(&qdss->ctrl_read_pool);
 	INIT_LIST_HEAD(&qdss->ctrl_write_pool);
 	INIT_LIST_HEAD(&qdss->data_write_pool);
@@ -1118,22 +1118,22 @@ int usb_qdss_ctrl_read(struct usb_qdss_ch *ch, struct qdss_request *d_req)
 	if (!qdss)
 		return -ENODEV;
 
-	spin_lock_irqsave(&qdss->lock, flags);
+	raw_spin_lock_irqsave(&qdss->lock, flags);
 
 	if (qdss->usb_connected == 0) {
-		spin_unlock_irqrestore(&qdss->lock, flags);
+		raw_spin_unlock_irqrestore(&qdss->lock, flags);
 		return -EIO;
 	}
 
 	if (list_empty(&qdss->ctrl_read_pool)) {
-		spin_unlock_irqrestore(&qdss->lock, flags);
+		raw_spin_unlock_irqrestore(&qdss->lock, flags);
 		pr_err("error: usb_qdss_ctrl_read list is empty\n");
 		return -EAGAIN;
 	}
 
 	req = list_first_entry(&qdss->ctrl_read_pool, struct usb_request, list);
 	list_del(&req->list);
-	spin_unlock_irqrestore(&qdss->lock, flags);
+	raw_spin_unlock_irqrestore(&qdss->lock, flags);
 
 	req->buf = d_req->buf;
 	req->length = d_req->length;
@@ -1141,9 +1141,9 @@ int usb_qdss_ctrl_read(struct usb_qdss_ch *ch, struct qdss_request *d_req)
 
 	if (usb_ep_queue(qdss->port.ctrl_out, req, GFP_ATOMIC)) {
 		/* If error add the link to linked list again*/
-		spin_lock_irqsave(&qdss->lock, flags);
+		raw_spin_lock_irqsave(&qdss->lock, flags);
 		list_add_tail(&req->list, &qdss->ctrl_read_pool);
-		spin_unlock_irqrestore(&qdss->lock, flags);
+		raw_spin_unlock_irqrestore(&qdss->lock, flags);
 		pr_err("qdss usb_ep_queue failed\n");
 		return -EIO;
 	}
@@ -1163,31 +1163,31 @@ int usb_qdss_ctrl_write(struct usb_qdss_ch *ch, struct qdss_request *d_req)
 	if (!qdss)
 		return -ENODEV;
 
-	spin_lock_irqsave(&qdss->lock, flags);
+	raw_spin_lock_irqsave(&qdss->lock, flags);
 
 	if (qdss->usb_connected == 0) {
-		spin_unlock_irqrestore(&qdss->lock, flags);
+		raw_spin_unlock_irqrestore(&qdss->lock, flags);
 		return -EIO;
 	}
 
 	if (list_empty(&qdss->ctrl_write_pool)) {
 		pr_err("error: usb_qdss_ctrl_write list is empty\n");
-		spin_unlock_irqrestore(&qdss->lock, flags);
+		raw_spin_unlock_irqrestore(&qdss->lock, flags);
 		return -EAGAIN;
 	}
 
 	req = list_first_entry(&qdss->ctrl_write_pool, struct usb_request,
 		list);
 	list_del(&req->list);
-	spin_unlock_irqrestore(&qdss->lock, flags);
+	raw_spin_unlock_irqrestore(&qdss->lock, flags);
 
 	req->buf = d_req->buf;
 	req->length = d_req->length;
 	req->context = d_req;
 	if (usb_ep_queue(qdss->port.ctrl_in, req, GFP_ATOMIC)) {
-		spin_lock_irqsave(&qdss->lock, flags);
+		raw_spin_lock_irqsave(&qdss->lock, flags);
 		list_add_tail(&req->list, &qdss->ctrl_write_pool);
-		spin_unlock_irqrestore(&qdss->lock, flags);
+		raw_spin_unlock_irqrestore(&qdss->lock, flags);
 		pr_err("qdss usb_ep_queue failed\n");
 		return -EIO;
 	}
@@ -1207,31 +1207,31 @@ int usb_qdss_data_write(struct usb_qdss_ch *ch, struct qdss_request *d_req)
 	if (!qdss)
 		return -ENODEV;
 
-	spin_lock_irqsave(&qdss->lock, flags);
+	raw_spin_lock_irqsave(&qdss->lock, flags);
 
 	if (qdss->usb_connected == 0) {
-		spin_unlock_irqrestore(&qdss->lock, flags);
+		raw_spin_unlock_irqrestore(&qdss->lock, flags);
 		return -EIO;
 	}
 
 	if (list_empty(&qdss->data_write_pool)) {
 		pr_err_ratelimited("error: usb_qdss_data_write list is empty\n");
-		spin_unlock_irqrestore(&qdss->lock, flags);
+		raw_spin_unlock_irqrestore(&qdss->lock, flags);
 		return -EAGAIN;
 	}
 
 	req = list_first_entry(&qdss->data_write_pool, struct usb_request,
 			list);
 	list_del(&req->list);
-	spin_unlock_irqrestore(&qdss->lock, flags);
+	raw_spin_unlock_irqrestore(&qdss->lock, flags);
 
 	req->buf = d_req->buf;
 	req->length = d_req->length;
 	req->context = d_req;
 	if (usb_ep_queue(qdss->port.data, req, GFP_ATOMIC)) {
-			spin_lock_irqsave(&qdss->lock, flags);
+			raw_spin_lock_irqsave(&qdss->lock, flags);
 			list_add_tail(&req->list, &qdss->data_write_pool);
-			spin_unlock_irqrestore(&qdss->lock, flags);
+			raw_spin_unlock_irqrestore(&qdss->lock, flags);
 			pr_err("qdss usb_ep_queue failed\n");
 			return -EIO;
 	}
@@ -1256,7 +1256,7 @@ struct usb_qdss_ch *usb_qdss_open(const char *name, void *priv,
 		return NULL;
 	}
 
-	spin_lock_irqsave(&qdss_lock, flags);
+	raw_spin_lock_irqsave(&qdss_lock, flags);
 	/* Check if we already have a channel with this name */
 	list_for_each_entry(ch, &usb_qdss_ch_list, list) {
 		if (!strcmp(name, ch->name)) {
@@ -1269,16 +1269,16 @@ struct usb_qdss_ch *usb_qdss_open(const char *name, void *priv,
 		pr_debug("usb_qdss_open: allocation qdss ctx\n");
 		qdss = kzalloc(sizeof(*qdss), GFP_ATOMIC);
 		if (!qdss) {
-			spin_unlock_irqrestore(&qdss_lock, flags);
+			raw_spin_unlock_irqrestore(&qdss_lock, flags);
 			return ERR_PTR(-ENOMEM);
 		}
-		spin_unlock_irqrestore(&qdss_lock, flags);
+		raw_spin_unlock_irqrestore(&qdss_lock, flags);
 		qdss->wq = create_singlethread_workqueue(name);
 		if (!qdss->wq) {
 			kfree(qdss);
 			return ERR_PTR(-ENOMEM);
 		}
-		spin_lock_irqsave(&qdss_lock, flags);
+		raw_spin_lock_irqsave(&qdss_lock, flags);
 		ch = &qdss->ch;
 		list_add_tail(&ch->list, &usb_qdss_ch_list);
 	} else {
@@ -1291,7 +1291,7 @@ struct usb_qdss_ch *usb_qdss_open(const char *name, void *priv,
 	ch->priv = priv;
 	ch->notify = notify;
 	ch->app_conn = 1;
-	spin_unlock_irqrestore(&qdss_lock, flags);
+	raw_spin_unlock_irqrestore(&qdss_lock, flags);
 
 	/* the case USB cabel was connected befor qdss called  qdss_open*/
 	if (qdss->usb_connected == 1)
@@ -1314,12 +1314,12 @@ void usb_qdss_close(struct usb_qdss_ch *ch)
 	if (!qdss)
 		return;
 
-	spin_lock_irqsave(&qdss_lock, flags);
+	raw_spin_lock_irqsave(&qdss_lock, flags);
 	dxport = qdss_ports[qdss->port_num].data_xport;
 	ch->priv_usb = NULL;
 	if (!qdss->usb_connected || (dxport == USB_GADGET_XPORT_PCIE)) {
 		ch->app_conn = 0;
-		spin_unlock_irqrestore(&qdss_lock, flags);
+		raw_spin_unlock_irqrestore(&qdss_lock, flags);
 		return;
 	}
 
@@ -1328,7 +1328,7 @@ void usb_qdss_close(struct usb_qdss_ch *ch)
 	qdss->endless_req = NULL;
 	gadget = qdss->cdev->gadget;
 	ch->app_conn = 0;
-	spin_unlock_irqrestore(&qdss_lock, flags);
+	raw_spin_unlock_irqrestore(&qdss_lock, flags);
 
 	status = uninit_data(qdss->port.data);
 	if (status)
@@ -1357,13 +1357,13 @@ static void qdss_cleanup(void)
 	list_for_each_safe(act, tmp, &usb_qdss_ch_list) {
 		_ch = list_entry(act, struct usb_qdss_ch, list);
 		qdss = container_of(_ch, struct f_qdss, ch);
-		spin_lock_irqsave(&qdss_lock, flags);
+		raw_spin_lock_irqsave(&qdss_lock, flags);
 		destroy_workqueue(qdss->wq);
 		if (!_ch->priv) {
 			list_del(&_ch->list);
 			kfree(qdss);
 		}
-		spin_unlock_irqrestore(&qdss_lock, flags);
+		raw_spin_unlock_irqrestore(&qdss_lock, flags);
 	}
 }
 
@@ -1399,7 +1399,7 @@ static int qdss_init_port(const char *ctrl_name, const char *data_name,
 	}
 
 	dev->port_num = nr_qdss_ports;
-	spin_lock_init(&dev->lock);
+	raw_spin_lock_init(&dev->lock);
 
 	qdss_port = &qdss_ports[nr_qdss_ports];
 	qdss_port->port = dev;
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_qdss.h b/kernel/msm-3.18/drivers/usb/gadget/function/f_qdss.h
index 62bb3a386..b74eec4f4 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_qdss.h
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_qdss.h
@@ -49,7 +49,7 @@ struct f_qdss {
 
 	struct work_struct connect_w;
 	struct work_struct disconnect_w;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	unsigned int data_enabled:1;
 	unsigned int ctrl_in_enabled:1;
 	unsigned int ctrl_out_enabled:1;
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_rmnet.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_rmnet.c
index e9def8aff..fa8c1239d 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_rmnet.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_rmnet.c
@@ -46,7 +46,7 @@ struct f_rmnet {
 	atomic_t			ctrl_online;
 	struct usb_composite_dev	*cdev;
 
-	spinlock_t			lock;
+	raw_spinlock_t			lock;
 
 	/* usb eps*/
 	struct usb_ep			*notify;
@@ -577,7 +577,7 @@ static void frmnet_purge_responses(struct f_rmnet *dev)
 
 	pr_debug("%s: port#%d\n", __func__, dev->port_num);
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	while (!list_empty(&dev->cpkt_resp_q)) {
 		cpkt = list_first_entry(&dev->cpkt_resp_q,
 				struct rmnet_ctrl_pkt, list);
@@ -586,7 +586,7 @@ static void frmnet_purge_responses(struct f_rmnet *dev)
 		rmnet_free_ctrl_pkt(cpkt);
 	}
 	dev->notify_count = 0;
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 }
 
 static void frmnet_suspend(struct usb_function *f)
@@ -845,14 +845,14 @@ static void frmnet_ctrl_response_available(struct f_rmnet *dev)
 
 	pr_debug("%s:dev:%pK portno#%d\n", __func__, dev, dev->port_num);
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	if (!atomic_read(&dev->online) || !req || !req->buf) {
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		return;
 	}
 
 	if (++dev->notify_count != 1) {
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		return;
 	}
 
@@ -863,18 +863,18 @@ static void frmnet_ctrl_response_available(struct f_rmnet *dev)
 	event->wValue = cpu_to_le16(0);
 	event->wIndex = cpu_to_le16(dev->ifc_id);
 	event->wLength = cpu_to_le16(0);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	ret = usb_ep_queue(dev->notify, dev->notify_req, GFP_ATOMIC);
 	if (ret) {
-		spin_lock_irqsave(&dev->lock, flags);
+		raw_spin_lock_irqsave(&dev->lock, flags);
 		if (!list_empty(&dev->cpkt_resp_q)) {
 			if (dev->notify_count > 0)
 				dev->notify_count--;
 			else {
 				pr_debug("%s: Invalid notify_count=%lu to decrement\n",
 					 __func__, dev->notify_count);
-				spin_unlock_irqrestore(&dev->lock, flags);
+				raw_spin_unlock_irqrestore(&dev->lock, flags);
 				return;
 			}
 			cpkt = list_first_entry(&dev->cpkt_resp_q,
@@ -882,7 +882,7 @@ static void frmnet_ctrl_response_available(struct f_rmnet *dev)
 			list_del(&cpkt->list);
 			rmnet_free_ctrl_pkt(cpkt);
 		}
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		pr_debug("ep enqueue error %d\n", ret);
 	}
 }
@@ -971,9 +971,9 @@ frmnet_send_cpkt_response(void *gr, void *buf, size_t len)
 		return 0;
 	}
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	list_add_tail(&cpkt->list, &dev->cpkt_resp_q);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	frmnet_ctrl_response_available(dev);
 
@@ -1015,9 +1015,9 @@ static void frmnet_notify_complete(struct usb_ep *ep, struct usb_request *req)
 	case -ECONNRESET:
 	case -ESHUTDOWN:
 		/* connection gone */
-		spin_lock_irqsave(&dev->lock, flags);
+		raw_spin_lock_irqsave(&dev->lock, flags);
 		dev->notify_count = 0;
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 		break;
 	default:
 		pr_err("rmnet notify ep error %d\n", status);
@@ -1026,31 +1026,31 @@ static void frmnet_notify_complete(struct usb_ep *ep, struct usb_request *req)
 		if (!atomic_read(&dev->ctrl_online))
 			break;
 
-		spin_lock_irqsave(&dev->lock, flags);
+		raw_spin_lock_irqsave(&dev->lock, flags);
 		if (dev->notify_count > 0) {
 			dev->notify_count--;
 			if (dev->notify_count == 0) {
-				spin_unlock_irqrestore(&dev->lock, flags);
+				raw_spin_unlock_irqrestore(&dev->lock, flags);
 				break;
 			}
 		} else {
 			pr_debug("%s: Invalid notify_count=%lu to decrement\n",
 					__func__, dev->notify_count);
-			spin_unlock_irqrestore(&dev->lock, flags);
+			raw_spin_unlock_irqrestore(&dev->lock, flags);
 			break;
 		}
-		spin_unlock_irqrestore(&dev->lock, flags);
+		raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 		status = usb_ep_queue(dev->notify, req, GFP_ATOMIC);
 		if (status) {
-			spin_lock_irqsave(&dev->lock, flags);
+			raw_spin_lock_irqsave(&dev->lock, flags);
 			if (!list_empty(&dev->cpkt_resp_q)) {
 				if (dev->notify_count > 0)
 					dev->notify_count--;
 				else {
 					pr_err("%s: Invalid notify_count=%lu to decrement\n",
 						__func__, dev->notify_count);
-					spin_unlock_irqrestore(&dev->lock,
+					raw_spin_unlock_irqrestore(&dev->lock,
 								flags);
 					break;
 				}
@@ -1059,7 +1059,7 @@ static void frmnet_notify_complete(struct usb_ep *ep, struct usb_request *req)
 				list_del(&cpkt->list);
 				rmnet_free_ctrl_pkt(cpkt);
 			}
-			spin_unlock_irqrestore(&dev->lock, flags);
+			raw_spin_unlock_irqrestore(&dev->lock, flags);
 			pr_debug("ep enqueue error %d\n", status);
 		}
 		break;
@@ -1108,21 +1108,21 @@ frmnet_setup(struct usb_function *f, const struct usb_ctrlrequest *ctrl)
 			unsigned len;
 			struct rmnet_ctrl_pkt *cpkt;
 
-			spin_lock(&dev->lock);
+			raw_spin_lock(&dev->lock);
 			if (list_empty(&dev->cpkt_resp_q)) {
 				pr_err("ctrl resp queue empty "
 					" req%02x.%02x v%04x i%04x l%d\n",
 					ctrl->bRequestType, ctrl->bRequest,
 					w_value, w_index, w_length);
 				ret = 0;
-				spin_unlock(&dev->lock);
+				raw_spin_unlock(&dev->lock);
 				goto invalid;
 			}
 
 			cpkt = list_first_entry(&dev->cpkt_resp_q,
 					struct rmnet_ctrl_pkt, list);
 			list_del(&cpkt->list);
-			spin_unlock(&dev->lock);
+			raw_spin_unlock(&dev->lock);
 
 			len = min_t(unsigned, w_length, cpkt->len);
 			memcpy(req->buf, cpkt->buf, len);
@@ -1340,12 +1340,12 @@ static int frmnet_bind_config(struct usb_configuration *c, unsigned portno)
 		rmnet_string_defs[0].id = status;
 	}
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	dev->cdev = c->cdev;
 	f = &dev->gether_port.func;
 	dev->port.f = f;
 	f->name = kasprintf(GFP_ATOMIC, "rmnet%d", portno);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 	if (!f->name) {
 		pr_err("%s: cannot allocate memory for name\n", __func__);
 		return -ENOMEM;
@@ -1443,7 +1443,7 @@ static int frmnet_init_port(const char *ctrl_name, const char *data_name,
 	}
 
 	dev->port_num = nr_rmnet_ports;
-	spin_lock_init(&dev->lock);
+	raw_spin_lock_init(&dev->lock);
 	INIT_LIST_HEAD(&dev->cpkt_resp_q);
 
 	rmnet_port = &rmnet_ports[nr_rmnet_ports];
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/u_bam.c b/kernel/msm-3.18/drivers/usb/gadget/function/u_bam.c
index 166804e12..fefd39181 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/u_bam.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/u_bam.c
@@ -180,9 +180,9 @@ struct gbam_port {
 	bool			is_connected;
 	enum u_bam_event_type	last_event;
 	unsigned		port_num;
-	spinlock_t		port_lock_ul;
-	spinlock_t		port_lock_dl;
-	spinlock_t		port_lock;
+	raw_spinlock_t		port_lock_ul;
+	raw_spinlock_t		port_lock_dl;
+	raw_spinlock_t		port_lock;
 
 	struct grmnet		*port_usb;
 	struct usb_gadget	*gadget;
@@ -405,9 +405,9 @@ static void gbam_write_data_tohost(struct gbam_port *port)
 	struct usb_request		*req;
 	struct usb_ep			*ep;
 
-	spin_lock_irqsave(&port->port_lock_dl, flags);
+	raw_spin_lock_irqsave(&port->port_lock_dl, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock_dl, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 		return;
 	}
 
@@ -460,9 +460,9 @@ static void gbam_write_data_tohost(struct gbam_port *port)
 
 		list_del(&req->list);
 
-		spin_unlock(&port->port_lock_dl);
+		raw_spin_unlock(&port->port_lock_dl);
 		ret = usb_ep_queue(ep, req, GFP_ATOMIC);
-		spin_lock(&port->port_lock_dl);
+		raw_spin_lock(&port->port_lock_dl);
 		if (ret) {
 			pr_err_ratelimited("%s: usb epIn failed with %d\n",
 					 __func__, ret);
@@ -472,7 +472,7 @@ static void gbam_write_data_tohost(struct gbam_port *port)
 		}
 		d->to_host++;
 	}
-	spin_unlock_irqrestore(&port->port_lock_dl, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 }
 
 static void gbam_write_data_tohost_w(struct work_struct *w)
@@ -498,9 +498,9 @@ void gbam_data_recv_cb(void *p, struct sk_buff *skb)
 	pr_debug("%s: p:%pK#%d d:%pK skb_len:%d\n", __func__,
 			port, port->port_num, d, skb->len);
 
-	spin_lock_irqsave(&port->port_lock_dl, flags);
+	raw_spin_lock_irqsave(&port->port_lock_dl, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock_dl, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 		dev_kfree_skb_any(skb);
 		return;
 	}
@@ -510,13 +510,13 @@ void gbam_data_recv_cb(void *p, struct sk_buff *skb)
 		if (printk_ratelimit())
 			pr_err("%s: tx pkt dropped: tx_drop_cnt:%u\n",
 					__func__, d->tohost_drp_cnt);
-		spin_unlock_irqrestore(&port->port_lock_dl, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 		dev_kfree_skb_any(skb);
 		return;
 	}
 
 	__skb_queue_tail(&d->tx_skb_q, skb);
-	spin_unlock_irqrestore(&port->port_lock_dl, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 
 	gbam_write_data_tohost(port);
 }
@@ -530,7 +530,7 @@ void gbam_data_write_done(void *p, struct sk_buff *skb)
 	if (!skb)
 		return;
 
-	spin_lock_irqsave(&port->port_lock_ul, flags);
+	raw_spin_lock_irqsave(&port->port_lock_ul, flags);
 
 	d->pending_pkts_with_bam--;
 	d->pending_bytes_with_bam -= skb->len;
@@ -540,7 +540,7 @@ void gbam_data_write_done(void *p, struct sk_buff *skb)
 			port, d, d->to_modem, d->pending_pkts_with_bam,
 			d->pending_bytes_with_bam, port->port_num);
 
-	spin_unlock_irqrestore(&port->port_lock_ul, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 
 	/*
 	 * If BAM doesn't have much pending data then push new data from here:
@@ -587,14 +587,14 @@ static void gbam_data_write_tobam(struct work_struct *w)
 	d = container_of(w, struct bam_ch_info, write_tobam_w);
 	port = d->port;
 
-	spin_lock_irqsave(&port->port_lock_ul, flags);
+	raw_spin_lock_irqsave(&port->port_lock_ul, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock_ul, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 		return;
 	}
 	/* Bail out if already in progress */
 	if (test_bit(BAM_CH_WRITE_INPROGRESS, &d->flags)) {
-		spin_unlock_irqrestore(&port->port_lock_ul, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 		return;
 	}
 
@@ -617,7 +617,7 @@ static void gbam_data_write_tobam(struct work_struct *w)
 				d->to_modem, d->pending_pkts_with_bam,
 				d->pending_bytes_with_bam, port->port_num);
 
-		spin_unlock_irqrestore(&port->port_lock_ul, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 		if (d->src_pipe_type == USB_BAM_PIPE_SYS2BAM) {
 			dma_addr_t         skb_dma_addr;
 			struct ipa_tx_meta ipa_meta = {0x0};
@@ -635,7 +635,7 @@ static void gbam_data_write_tobam(struct work_struct *w)
 			ret = msm_bam_dmux_write(d->id, skb);
 		}
 
-		spin_lock_irqsave(&port->port_lock_ul, flags);
+		raw_spin_lock_irqsave(&port->port_lock_ul, flags);
 		if (ret) {
 			pr_debug("%s: write error:%d\n", __func__, ret);
 			d->pending_pkts_with_bam--;
@@ -656,7 +656,7 @@ static void gbam_data_write_tobam(struct work_struct *w)
 	qlen = d->rx_skb_q.qlen;
 
 	clear_bit(BAM_CH_WRITE_INPROGRESS, &d->flags);
-	spin_unlock_irqrestore(&port->port_lock_ul, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 
 	if (qlen < bam_mux_rx_fctrl_dis_thld) {
 		if (d->rx_flow_control_triggered) {
@@ -696,10 +696,10 @@ static void gbam_epin_complete(struct usb_ep *ep, struct usb_request *req)
 	if (!port)
 		return;
 
-	spin_lock(&port->port_lock_dl);
+	raw_spin_lock(&port->port_lock_dl);
 	d = &port->data_ch;
 	list_add_tail(&req->list, &d->tx_idle);
-	spin_unlock(&port->port_lock_dl);
+	raw_spin_unlock(&port->port_lock_dl);
 
 	queue_work(gbam_wq, &d->write_tohost_w);
 }
@@ -721,9 +721,9 @@ gbam_epout_complete(struct usb_ep *ep, struct usb_request *req)
 	case -ECONNRESET:
 	case -ESHUTDOWN:
 		/* cable disconnection */
-		spin_lock(&port->port_lock_ul);
+		raw_spin_lock(&port->port_lock_ul);
 		gbam_free_skb_to_pool(port, skb);
-		spin_unlock(&port->port_lock_ul);
+		raw_spin_unlock(&port->port_lock_ul);
 		req->buf = 0;
 		usb_ep_free_request(ep, req);
 		return;
@@ -732,13 +732,13 @@ gbam_epout_complete(struct usb_ep *ep, struct usb_request *req)
 			pr_err("%s: %s response error %d, %d/%d\n",
 				__func__, ep->name, status,
 				req->actual, req->length);
-		spin_lock(&port->port_lock_ul);
+		raw_spin_lock(&port->port_lock_ul);
 		gbam_free_skb_to_pool(port, skb);
-		spin_unlock(&port->port_lock_ul);
+		raw_spin_unlock(&port->port_lock_ul);
 		break;
 	}
 
-	spin_lock(&port->port_lock_ul);
+	raw_spin_lock(&port->port_lock_ul);
 
 	if (queue) {
 		__skb_queue_tail(&d->rx_skb_q, skb);
@@ -746,7 +746,7 @@ gbam_epout_complete(struct usb_ep *ep, struct usb_request *req)
 			!usb_bam_get_prod_granted(d->usb_bam_type,
 					d->dst_connection_idx)) {
 			list_add_tail(&req->list, &d->rx_idle);
-			spin_unlock(&port->port_lock_ul);
+			raw_spin_unlock(&port->port_lock_ul);
 			return;
 		} else
 			queue_work(gbam_wq, &d->write_tobam_w);
@@ -762,17 +762,17 @@ gbam_epout_complete(struct usb_ep *ep, struct usb_request *req)
 			d->rx_flow_control_enable++;
 		}
 		list_add_tail(&req->list, &d->rx_idle);
-		spin_unlock(&port->port_lock_ul);
+		raw_spin_unlock(&port->port_lock_ul);
 		return;
 	}
 
 	skb = gbam_alloc_skb_from_pool(port);
 	if (!skb) {
 		list_add_tail(&req->list, &d->rx_idle);
-		spin_unlock(&port->port_lock_ul);
+		raw_spin_unlock(&port->port_lock_ul);
 		return;
 	}
-	spin_unlock(&port->port_lock_ul);
+	raw_spin_unlock(&port->port_lock_ul);
 
 	req->buf = skb->data;
 	req->dma = gbam_get_dma_from_skb(skb);
@@ -787,17 +787,17 @@ gbam_epout_complete(struct usb_ep *ep, struct usb_request *req)
 
 	status = usb_ep_queue(ep, req, GFP_ATOMIC);
 	if (status) {
-		spin_lock(&port->port_lock_ul);
+		raw_spin_lock(&port->port_lock_ul);
 		gbam_free_skb_to_pool(port, skb);
-		spin_unlock(&port->port_lock_ul);
+		raw_spin_unlock(&port->port_lock_ul);
 
 		if (printk_ratelimit())
 			pr_err("%s: data rx enqueue err %d\n",
 					__func__, status);
 
-		spin_lock(&port->port_lock_ul);
+		raw_spin_lock(&port->port_lock_ul);
 		list_add_tail(&req->list, &d->rx_idle);
-		spin_unlock(&port->port_lock_ul);
+		raw_spin_unlock(&port->port_lock_ul);
 	}
 }
 
@@ -824,9 +824,9 @@ static void gbam_start_rx(struct gbam_port *port)
 	int				ret;
 	struct sk_buff			*skb;
 
-	spin_lock_irqsave(&port->port_lock_ul, flags);
+	raw_spin_lock_irqsave(&port->port_lock_ul, flags);
 	if (!port->port_usb || !port->port_usb->out) {
-		spin_unlock_irqrestore(&port->port_lock_ul, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 		return;
 	}
 
@@ -857,9 +857,9 @@ static void gbam_start_rx(struct gbam_port *port)
 
 		req->context = skb;
 
-		spin_unlock_irqrestore(&port->port_lock_ul, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 		ret = usb_ep_queue(ep, req, GFP_ATOMIC);
-		spin_lock_irqsave(&port->port_lock_ul, flags);
+		raw_spin_lock_irqsave(&port->port_lock_ul, flags);
 		if (ret) {
 			gbam_free_skb_to_pool(port, skb);
 
@@ -875,7 +875,7 @@ static void gbam_start_rx(struct gbam_port *port)
 		}
 	}
 
-	spin_unlock_irqrestore(&port->port_lock_ul, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 }
 
 static void gbam_start_endless_rx(struct gbam_port *port)
@@ -885,15 +885,15 @@ static void gbam_start_endless_rx(struct gbam_port *port)
 	struct usb_ep *ep;
 	unsigned long flags;
 
-	spin_lock_irqsave(&port->port_lock_ul, flags);
+	raw_spin_lock_irqsave(&port->port_lock_ul, flags);
 	if (!port->port_usb || !d->rx_req) {
-		spin_unlock_irqrestore(&port->port_lock_ul, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 		pr_err("%s: port->port_usb is NULL", __func__);
 		return;
 	}
 
 	ep = port->port_usb->out;
-	spin_unlock_irqrestore(&port->port_lock_ul, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 	pr_debug("%s: enqueue\n", __func__);
 	status = usb_ep_queue(ep, d->rx_req, GFP_ATOMIC);
 	if (status)
@@ -907,15 +907,15 @@ static void gbam_start_endless_tx(struct gbam_port *port)
 	struct usb_ep *ep;
 	unsigned long flags;
 
-	spin_lock_irqsave(&port->port_lock_dl, flags);
+	raw_spin_lock_irqsave(&port->port_lock_dl, flags);
 	if (!port->port_usb || !d->tx_req) {
-		spin_unlock_irqrestore(&port->port_lock_dl, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 		pr_err("%s: port->port_usb is NULL", __func__);
 		return;
 	}
 
 	ep = port->port_usb->in;
-	spin_unlock_irqrestore(&port->port_lock_dl, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 	pr_debug("%s: enqueue\n", __func__);
 	status = usb_ep_queue(ep, d->tx_req, GFP_ATOMIC);
 	if (status)
@@ -929,16 +929,16 @@ static void gbam_stop_endless_rx(struct gbam_port *port)
 	unsigned long flags;
 	struct usb_ep *ep;
 
-	spin_lock_irqsave(&port->port_lock_ul, flags);
+	raw_spin_lock_irqsave(&port->port_lock_ul, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock_ul, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 		pr_err("%s: port->port_usb is NULL", __func__);
 		return;
 	}
 
 	ep = port->port_usb->out;
 	d->rx_req_dequeued = true;
-	spin_unlock_irqrestore(&port->port_lock_ul, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 	pr_debug("%s: dequeue\n", __func__);
 	status = usb_ep_dequeue(ep, d->rx_req);
 	if (status)
@@ -952,16 +952,16 @@ static void gbam_stop_endless_tx(struct gbam_port *port)
 	unsigned long flags;
 	struct usb_ep *ep;
 
-	spin_lock_irqsave(&port->port_lock_dl, flags);
+	raw_spin_lock_irqsave(&port->port_lock_dl, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock_dl, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 		pr_err("%s: port->port_usb is NULL", __func__);
 		return;
 	}
 
 	ep = port->port_usb->in;
 	d->tx_req_dequeued = true;
-	spin_unlock_irqrestore(&port->port_lock_dl, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 	pr_debug("%s: dequeue\n", __func__);
 	status = usb_ep_dequeue(ep, d->tx_req);
 	if (status)
@@ -1004,16 +1004,16 @@ static void gbam_start(void *param, enum usb_bam_pipe_dir dir)
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (port->port_usb == NULL) {
 		pr_err("%s: port_usb is NULL, disconnected\n", __func__);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
 	gadget = port->port_usb->gadget;
 	d = &port->data_ch;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	if (gadget == NULL) {
 		pr_err("%s: gadget is NULL\n", __func__);
@@ -1061,7 +1061,7 @@ static int _gbam_start_io(struct gbam_port *port, bool in)
 	struct usb_ep		*ep;
 	struct list_head	*idle;
 	unsigned		queue_size;
-	spinlock_t		*spinlock;
+	raw_spinlock_t		*spinlock;
 	void		(*ep_complete)(struct usb_ep *, struct usb_request *);
 
 	if (in)
@@ -1069,9 +1069,9 @@ static int _gbam_start_io(struct gbam_port *port, bool in)
 	else
 		spinlock = &port->port_lock_ul;
 
-	spin_lock_irqsave(spinlock, flags);
+	raw_spin_lock_irqsave(spinlock, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(spinlock, flags);
+		raw_spin_unlock_irqrestore(spinlock, flags);
 		return -EBUSY;
 	}
 
@@ -1092,7 +1092,7 @@ static int _gbam_start_io(struct gbam_port *port, bool in)
 	ret = gbam_alloc_requests(ep, idle, queue_size, ep_complete,
 			GFP_ATOMIC);
 out:
-	spin_unlock_irqrestore(spinlock, flags);
+	raw_spin_unlock_irqrestore(spinlock, flags);
 	if (ret)
 		pr_err("%s: allocation failed\n", __func__);
 
@@ -1109,11 +1109,11 @@ static void gbam_start_io(struct gbam_port *port)
 		return;
 
 	if (_gbam_start_io(port, false)) {
-		spin_lock_irqsave(&port->port_lock_dl, flags);
+		raw_spin_lock_irqsave(&port->port_lock_dl, flags);
 		if (port->port_usb)
 			gbam_free_requests(port->port_usb->in,
 				&port->data_ch.tx_idle);
-		spin_unlock_irqrestore(&port->port_lock_dl, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 		return;
 	}
 
@@ -1161,7 +1161,7 @@ static void gbam_free_rx_buffers(struct gbam_port *port)
 	unsigned long		flags;
 	struct bam_ch_info	*d;
 
-	spin_lock_irqsave(&port->port_lock_ul, flags);
+	raw_spin_lock_irqsave(&port->port_lock_ul, flags);
 
 	if (!port->port_usb || !port->port_usb->out)
 		goto free_rx_buf_out;
@@ -1175,7 +1175,7 @@ static void gbam_free_rx_buffers(struct gbam_port *port)
 	gbam_free_rx_skb_idle_list(port);
 
 free_rx_buf_out:
-	spin_unlock_irqrestore(&port->port_lock_ul, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 }
 
 static void gbam_free_tx_buffers(struct gbam_port *port)
@@ -1184,7 +1184,7 @@ static void gbam_free_tx_buffers(struct gbam_port *port)
 	unsigned long		flags;
 	struct bam_ch_info	*d;
 
-	spin_lock_irqsave(&port->port_lock_dl, flags);
+	raw_spin_lock_irqsave(&port->port_lock_dl, flags);
 
 	if (!port->port_usb)
 		goto free_tx_buf_out;
@@ -1196,7 +1196,7 @@ static void gbam_free_tx_buffers(struct gbam_port *port)
 		dev_kfree_skb_any(skb);
 
 free_tx_buf_out:
-	spin_unlock_irqrestore(&port->port_lock_dl, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock_dl, flags);
 }
 
 static void gbam_free_buffers(struct gbam_port *port)
@@ -1230,12 +1230,12 @@ static void gbam2bam_disconnect_work(struct work_struct *w)
 	int ret;
 	unsigned long flags;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	if (!port->is_connected) {
 		pr_debug("%s: Port already disconnected. Bailing out.\n",
 			__func__);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
@@ -1250,7 +1250,7 @@ static void gbam2bam_disconnect_work(struct work_struct *w)
 	 * and event functions (as bam_data_connect) will not influance
 	 * while lower layers connect pipes, etc.
 	*/
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	if (d->trans == USB_GADGET_XPORT_BAM2BAM_IPA) {
 		ret = usb_bam_disconnect_ipa(d->usb_bam_type, &d->ipa_params);
@@ -1275,15 +1275,15 @@ static void gbam_connect_work(struct work_struct *w)
 	int ret;
 	unsigned long flags;
 
-	spin_lock_irqsave(&port->port_lock_ul, flags);
-	spin_lock(&port->port_lock_dl);
+	raw_spin_lock_irqsave(&port->port_lock_ul, flags);
+	raw_spin_lock(&port->port_lock_dl);
 	if (!port->port_usb) {
-		spin_unlock(&port->port_lock_dl);
-		spin_unlock_irqrestore(&port->port_lock_ul, flags);
+		raw_spin_unlock(&port->port_lock_dl);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 		return;
 	}
-	spin_unlock(&port->port_lock_dl);
-	spin_unlock_irqrestore(&port->port_lock_ul, flags);
+	raw_spin_unlock(&port->port_lock_dl);
+	raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 
 	if (!test_bit(BAM_CH_READY, &d->flags)) {
 		pr_err("%s: Bam channel is not ready\n", __func__);
@@ -1315,38 +1315,38 @@ static void gbam2bam_connect_work(struct work_struct *w)
 	int ret;
 	unsigned long flags, flags_ul;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	if (port->last_event == U_BAM_DISCONNECT_E) {
 		pr_debug("%s: Port is about to disconnected. Bailing out.\n",
 			__func__);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
 	if (port->is_connected) {
 		pr_debug("%s: Port already connected. Bail out.\n",
 			__func__);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 	port->is_connected = true;
 
-	spin_lock_irqsave(&port->port_lock_ul, flags_ul);
-	spin_lock(&port->port_lock_dl);
+	raw_spin_lock_irqsave(&port->port_lock_ul, flags_ul);
+	raw_spin_lock(&port->port_lock_dl);
 	if (!port->port_usb) {
 		pr_debug("%s: usb cable is disconnected, exiting\n", __func__);
-		spin_unlock(&port->port_lock_dl);
-		spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock(&port->port_lock_dl);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
 	gadget = port->port_usb->gadget;
 	if (!gadget) {
-		spin_unlock(&port->port_lock_dl);
-		spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock(&port->port_lock_dl);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s: port_usb.gadget is NULL, exiting\n", __func__);
 		return;
 	}
@@ -1360,8 +1360,8 @@ static void gbam2bam_connect_work(struct work_struct *w)
 	 * and event functions (as bam_data_connect) will not influance
 	 * while lower layers connect pipes, etc.
 	*/
-	spin_unlock(&port->port_lock_dl);
-	spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
+	raw_spin_unlock(&port->port_lock_dl);
+	raw_spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
 
 	d->ipa_params.usb_connection_speed = gadget->speed;
 
@@ -1376,27 +1376,27 @@ static void gbam2bam_connect_work(struct work_struct *w)
 			&d->src_pipe_type) ||
 		usb_bam_get_pipe_type(d->usb_bam_type, d->ipa_params.dst_idx,
 				&d->dst_pipe_type)) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s:usb_bam_get_pipe_type() failed\n", __func__);
 		return;
 	}
 	if (d->dst_pipe_type != USB_BAM_PIPE_BAM2BAM) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s: no software preparation for DL not using bam2bam\n",
 				__func__);
 		return;
 	}
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	usb_bam_alloc_fifos(d->usb_bam_type, d->src_connection_idx);
 	usb_bam_alloc_fifos(d->usb_bam_type, d->dst_connection_idx);
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	/* check if USB cable is disconnected or not */
 	if (!port || !port->port_usb) {
 		pr_debug("%s: cable is disconnected.\n",
 						 __func__);
-		spin_unlock_irqrestore(&port->port_lock,
+		raw_spin_unlock_irqrestore(&port->port_lock,
 							flags);
 		goto free_fifos;
 	}
@@ -1442,7 +1442,7 @@ static void gbam2bam_connect_work(struct work_struct *w)
 	teth_bridge_params.client = d->ipa_params.src_client;
 	ret = teth_bridge_init(&teth_bridge_params);
 	if (ret) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s:teth_bridge_init() failed\n", __func__);
 		goto ep_unconfig;
 	}
@@ -1469,7 +1469,7 @@ static void gbam2bam_connect_work(struct work_struct *w)
 	d->ipa_params.ipa_ep_cfg.mode.mode = IPA_BASIC;
 	d->ipa_params.skip_ep_cfg = teth_bridge_params.skip_ep_cfg;
 	d->ipa_params.dir = USB_TO_PEER_PERIPHERAL;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	ret = usb_bam_connect_ipa(d->usb_bam_type, &d->ipa_params);
 	if (ret) {
 		pr_err("%s: usb_bam_connect_ipa failed: err:%d\n",
@@ -1477,10 +1477,10 @@ static void gbam2bam_connect_work(struct work_struct *w)
 		goto ep_unconfig;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	/* check if USB cable is disconnected or not */
 	if (port->last_event ==  U_BAM_DISCONNECT_E) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_debug("%s:%d: cable is disconnected.\n",
 						 __func__, __LINE__);
 		goto ep_unconfig;
@@ -1498,7 +1498,7 @@ static void gbam2bam_connect_work(struct work_struct *w)
 	else
 		d->ipa_params.reset_pipe_after_lpm = false;
 	d->ipa_params.dir = PEER_PERIPHERAL_TO_USB;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	ret = usb_bam_connect_ipa(d->usb_bam_type, &d->ipa_params);
 	if (ret) {
 		pr_err("%s: usb_bam_connect_ipa failed: err:%d\n",
@@ -1506,16 +1506,16 @@ static void gbam2bam_connect_work(struct work_struct *w)
 		goto ep_unconfig;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	/* check if USB cable is disconnected or not */
 	if (port->last_event ==  U_BAM_DISCONNECT_E) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_debug("%s:%d: cable is disconnected.\n",
 						 __func__, __LINE__);
 		goto ep_unconfig;
 	}
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	gqti_ctrl_update_ipa_pipes(port->port_usb, port->port_num,
 					d->ipa_params.ipa_prod_ep_idx ,
 					d->ipa_params.ipa_cons_ep_idx);
@@ -1552,13 +1552,13 @@ static void gbam2bam_connect_work(struct work_struct *w)
 
 ep_unconfig:
 	if (gadget_is_dwc3(gadget)) {
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		/* check if USB cable is disconnected or not */
 		if (port->port_usb) {
 			msm_ep_unconfig(port->port_usb->in);
 			msm_ep_unconfig(port->port_usb->out);
 		}
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	}
 free_fifos:
 	usb_bam_free_fifos(d->usb_bam_type, d->src_connection_idx);
@@ -1574,17 +1574,17 @@ static int gbam_wake_cb(void *param)
 	struct usb_function *func;
 	int ret;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb) {
 		pr_debug("%s: usb cable is disconnected, exiting\n",
 				__func__);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return -ENODEV;
 	}
 
 	gadget = port->port_usb->gadget;
 	func = port->port_usb->f;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	pr_debug("%s: woken up by peer\n", __func__);
 
@@ -1611,7 +1611,7 @@ static void gbam2bam_suspend_work(struct work_struct *w)
 
 	pr_debug("%s: suspend work started\n", __func__);
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	if ((port->last_event == U_BAM_DISCONNECT_E) ||
 	    (port->last_event == U_BAM_RESUME_E)) {
@@ -1639,9 +1639,9 @@ static void gbam2bam_suspend_work(struct work_struct *w)
 		 * gbam_stop() called from usb_bam_suspend()
 		 * re-acquires port lock.
 		 */
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		usb_bam_suspend(d->usb_bam_type, &d->ipa_params);
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 	}
 
 exit:
@@ -1651,7 +1651,7 @@ exit:
 	 */
 	usb_gadget_autopm_put_async(port->gadget);
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 static void gbam2bam_resume_work(struct work_struct *w)
@@ -1664,7 +1664,7 @@ static void gbam2bam_resume_work(struct work_struct *w)
 
 	pr_debug("%s: resume work started\n", __func__);
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	if (port->last_event == U_BAM_DISCONNECT_E || !port->port_usb) {
 		pr_debug("%s: usb cable is disconnected, exiting\n",
@@ -1699,12 +1699,12 @@ static void gbam2bam_resume_work(struct work_struct *w)
 					port->port_usb->out, d->src_pipe_type);
 			}
 
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			if (d->tx_req_dequeued)
 				msm_dwc3_reset_dbm_ep(port->port_usb->in);
 			if (d->rx_req_dequeued)
 				msm_dwc3_reset_dbm_ep(port->port_usb->out);
-			spin_lock_irqsave(&port->port_lock, flags);
+			raw_spin_lock_irqsave(&port->port_lock, flags);
 			if (port->port_usb) {
 				if (d->tx_req_dequeued)
 					msm_ep_config(port->port_usb->in,
@@ -1720,7 +1720,7 @@ static void gbam2bam_resume_work(struct work_struct *w)
 	}
 
 exit:
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 /* BAM data channel ready, allow attempt to open */
@@ -1743,12 +1743,12 @@ static int gbam_data_ch_probe(struct platform_device *pdev)
 			set_bit(BAM_CH_READY, &d->flags);
 
 			/* if usb is online, try opening bam_ch */
-			spin_lock_irqsave(&port->port_lock_ul, flags);
-			spin_lock(&port->port_lock_dl);
+			raw_spin_lock_irqsave(&port->port_lock_ul, flags);
+			raw_spin_lock(&port->port_lock_dl);
 			if (port->port_usb)
 				do_work = true;
-			spin_unlock(&port->port_lock_dl);
-			spin_unlock_irqrestore(&port->port_lock_ul, flags);
+			raw_spin_unlock(&port->port_lock_dl);
+			raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 
 			if (do_work)
 				queue_work(gbam_wq, &port->connect_w);
@@ -1777,14 +1777,14 @@ static int gbam_data_ch_remove(struct platform_device *pdev)
 			port = bam_ports[i].port;
 			d = &port->data_ch;
 
-			spin_lock_irqsave(&port->port_lock_ul, flags);
-			spin_lock(&port->port_lock_dl);
+			raw_spin_lock_irqsave(&port->port_lock_ul, flags);
+			raw_spin_lock(&port->port_lock_dl);
 			if (port->port_usb) {
 				ep_in = port->port_usb->in;
 				ep_out = port->port_usb->out;
 			}
-			spin_unlock(&port->port_lock_dl);
-			spin_unlock_irqrestore(&port->port_lock_ul, flags);
+			raw_spin_unlock(&port->port_lock_dl);
+			raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 
 			if (ep_in)
 				usb_ep_fifo_flush(ep_in);
@@ -1839,9 +1839,9 @@ static int gbam_port_alloc(int portno)
 
 	/* port initialization */
 	port->is_connected = false;
-	spin_lock_init(&port->port_lock_ul);
-	spin_lock_init(&port->port_lock_dl);
-	spin_lock_init(&port->port_lock);
+	raw_spin_lock_init(&port->port_lock_ul);
+	raw_spin_lock_init(&port->port_lock_dl);
+	raw_spin_lock_init(&port->port_lock);
 	INIT_WORK(&port->connect_w, gbam_connect_work);
 	INIT_WORK(&port->disconnect_w, gbam_disconnect_work);
 
@@ -1886,9 +1886,9 @@ static int gbam2bam_port_alloc(int portno)
 
 	/* port initialization */
 	port->is_connected = false;
-	spin_lock_init(&port->port_lock_ul);
-	spin_lock_init(&port->port_lock_dl);
-	spin_lock_init(&port->port_lock);
+	raw_spin_lock_init(&port->port_lock_ul);
+	raw_spin_lock_init(&port->port_lock_dl);
+	raw_spin_lock_init(&port->port_lock);
 
 	INIT_WORK(&port->connect_w, gbam2bam_connect_work);
 	INIT_WORK(&port->disconnect_w, gbam2bam_disconnect_work);
@@ -1934,8 +1934,8 @@ static ssize_t gbam_read_stats(struct file *file, char __user *ubuf,
 		port = bam_ports[i].port;
 		if (!port)
 			continue;
-		spin_lock_irqsave(&port->port_lock_ul, flags);
-		spin_lock(&port->port_lock_dl);
+		raw_spin_lock_irqsave(&port->port_lock_ul, flags);
+		raw_spin_lock(&port->port_lock_dl);
 
 		d = &port->data_ch;
 
@@ -1974,8 +1974,8 @@ static ssize_t gbam_read_stats(struct file *file, char __user *ubuf,
 				test_bit(BAM_CH_READY, &d->flags),
 				d->skb_expand_cnt);
 
-		spin_unlock(&port->port_lock_dl);
-		spin_unlock_irqrestore(&port->port_lock_ul, flags);
+		raw_spin_unlock(&port->port_lock_dl);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 	}
 
 	ret = simple_read_from_buffer(ubuf, count, ppos, buf, temp);
@@ -1998,8 +1998,8 @@ static ssize_t gbam_reset_stats(struct file *file, const char __user *buf,
 		if (!port)
 			continue;
 
-		spin_lock_irqsave(&port->port_lock_ul, flags);
-		spin_lock(&port->port_lock_dl);
+		raw_spin_lock_irqsave(&port->port_lock_ul, flags);
+		raw_spin_lock(&port->port_lock_dl);
 
 		d = &port->data_ch;
 
@@ -2017,8 +2017,8 @@ static ssize_t gbam_reset_stats(struct file *file, const char __user *buf,
 		d->delayed_bam_mux_write_done = 0;
 		d->skb_expand_cnt = 0;
 
-		spin_unlock(&port->port_lock_dl);
-		spin_unlock_irqrestore(&port->port_lock_ul, flags);
+		raw_spin_unlock(&port->port_lock_dl);
+		raw_spin_unlock_irqrestore(&port->port_lock_ul, flags);
 	}
 	return count;
 }
@@ -2039,17 +2039,17 @@ static ssize_t gbam_rw_write(struct file *file, const char __user *ubuf,
 	if (!port)
 		return -ENODEV;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb) {
 		pr_debug("%s: usb cable is disconnected, exiting\n",
 				__func__);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return -ENODEV;
 	}
 
 	gadget = port->port_usb->gadget;
 	func = port->port_usb->f;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	if ((gadget->speed == USB_SPEED_SUPER) && (func->func_is_suspended)) {
 		pr_debug("%s Initiating usb_func rwakeup\n", __func__);
@@ -2136,12 +2136,12 @@ void gbam_disconnect(struct grmnet *gr, u8 port_num, enum transport_type trans)
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	d = &port->data_ch;
 	/* Already disconnected due to suspend with remote wake disabled */
 	if (port->last_event == U_BAM_DISCONNECT_E) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 	/*
@@ -2161,32 +2161,32 @@ void gbam_disconnect(struct grmnet *gr, u8 port_num, enum transport_type trans)
 	else if (trans == USB_GADGET_XPORT_BAM2BAM_IPA)
 		gbam_free_rx_buffers(port);
 
-	spin_lock_irqsave(&port->port_lock_ul, flags_ul);
-	spin_lock(&port->port_lock_dl);
+	raw_spin_lock_irqsave(&port->port_lock_ul, flags_ul);
+	raw_spin_lock(&port->port_lock_dl);
 	port->port_usb = 0;
 	n_tx_req_queued = 0;
-	spin_unlock(&port->port_lock_dl);
-	spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
+	raw_spin_unlock(&port->port_lock_dl);
+	raw_spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
 
 	usb_ep_disable(gr->in);
 	if (trans == USB_GADGET_XPORT_BAM2BAM_IPA) {
-		spin_lock_irqsave(&port->port_lock_dl, flags_dl);
+		raw_spin_lock_irqsave(&port->port_lock_dl, flags_dl);
 		if (d->tx_req) {
 			usb_ep_free_request(gr->in, d->tx_req);
 			d->tx_req = NULL;
 		}
-		spin_unlock_irqrestore(&port->port_lock_dl, flags_dl);
+		raw_spin_unlock_irqrestore(&port->port_lock_dl, flags_dl);
 	}
 	/* disable endpoints */
 	if (gr->out) {
 		usb_ep_disable(gr->out);
 		if (trans == USB_GADGET_XPORT_BAM2BAM_IPA) {
-			spin_lock_irqsave(&port->port_lock_ul, flags_ul);
+			raw_spin_lock_irqsave(&port->port_lock_ul, flags_ul);
 			if (d->rx_req) {
 				usb_ep_free_request(gr->out, d->rx_req);
 				d->rx_req = NULL;
 			}
-			spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
+			raw_spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
 		}
 	}
 
@@ -2218,7 +2218,7 @@ void gbam_disconnect(struct grmnet *gr, u8 port_num, enum transport_type trans)
 
 	queue_work(gbam_wq, &port->disconnect_w);
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 int gbam_connect(struct grmnet *gr, u8 port_num,
@@ -2268,13 +2268,13 @@ int gbam_connect(struct grmnet *gr, u8 port_num,
 		return -ENODEV;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	d = &port->data_ch;
 	d->trans = trans;
 
-	spin_lock_irqsave(&port->port_lock_ul, flags_ul);
-	spin_lock(&port->port_lock_dl);
+	raw_spin_lock_irqsave(&port->port_lock_ul, flags_ul);
+	raw_spin_lock(&port->port_lock_dl);
 	port->port_usb = gr;
 	port->gadget = port->port_usb->gadget;
 
@@ -2284,9 +2284,9 @@ int gbam_connect(struct grmnet *gr, u8 port_num,
 		if (!d->rx_req) {
 			pr_err("%s: RX request allocation failed\n", __func__);
 			d->rx_req = NULL;
-			spin_unlock(&port->port_lock_dl);
-			spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock(&port->port_lock_dl);
+			raw_spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			return -ENOMEM;
 		}
 
@@ -2302,9 +2302,9 @@ int gbam_connect(struct grmnet *gr, u8 port_num,
 			d->tx_req = NULL;
 			usb_ep_free_request(port->port_usb->out, d->rx_req);
 			d->rx_req = NULL;
-			spin_unlock(&port->port_lock_dl);
-			spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock(&port->port_lock_dl);
+			raw_spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			return -ENOMEM;
 		}
 
@@ -2329,8 +2329,8 @@ int gbam_connect(struct grmnet *gr, u8 port_num,
 		d->delayed_bam_mux_write_done = 0;
 	}
 
-	spin_unlock(&port->port_lock_dl);
-	spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
+	raw_spin_unlock(&port->port_lock_dl);
+	raw_spin_unlock_irqrestore(&port->port_lock_ul, flags_ul);
 
 	if (d->trans == USB_GADGET_XPORT_BAM2BAM_IPA) {
 		d->src_connection_idx = src_connection_idx;
@@ -2428,7 +2428,7 @@ int gbam_connect(struct grmnet *gr, u8 port_num,
 
 	ret = 0;
 exit:
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	return ret;
 }
 
@@ -2555,7 +2555,7 @@ void gbam_suspend(struct grmnet *gr, u8 port_num, enum transport_type trans)
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	d = &port->data_ch;
 
@@ -2564,7 +2564,7 @@ void gbam_suspend(struct grmnet *gr, u8 port_num, enum transport_type trans)
 	port->last_event = U_BAM_SUSPEND_E;
 	queue_work(gbam_wq, &port->suspend_w);
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 void gbam_resume(struct grmnet *gr, u8 port_num, enum transport_type trans)
@@ -2583,7 +2583,7 @@ void gbam_resume(struct grmnet *gr, u8 port_num, enum transport_type trans)
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	d = &port->data_ch;
 
@@ -2599,7 +2599,7 @@ void gbam_resume(struct grmnet *gr, u8 port_num, enum transport_type trans)
 	usb_gadget_autopm_get_noresume(port->gadget);
 	queue_work(gbam_wq, &port->resume_w);
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 int gbam_mbim_connect(struct usb_gadget *g, struct usb_ep *in,
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/u_bam_data.c b/kernel/msm-3.18/drivers/usb/gadget/function/u_bam_data.c
index c9bfb4763..e0c05abe0 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/u_bam_data.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/u_bam_data.c
@@ -130,7 +130,7 @@ struct bam_data_port {
 	bool                            is_ipa_connected;
 	enum u_bam_data_event_type	last_event;
 	unsigned			port_num;
-	spinlock_t			port_lock;
+	raw_spinlock_t			port_lock;
 	unsigned int                    ref_count;
 	struct data_port		*port_usb;
 	struct usb_gadget		*gadget;
@@ -287,7 +287,7 @@ static void bam_data_write_done(void *p, struct sk_buff *skb)
 	if (!skb)
 		return;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	bam_data_free_skb_to_pool(port, skb);
 
 	d->pending_with_bam--;
@@ -295,7 +295,7 @@ static void bam_data_write_done(void *p, struct sk_buff *skb)
 	pr_debug("%s: port:%pK d:%pK pbam:%u, pno:%d\n", __func__,
 			port, d, d->pending_with_bam, port->port_num);
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	queue_work(bam_data_wq, &d->write_tobam_w);
 }
@@ -345,7 +345,7 @@ static void bam_data_start_rx(struct bam_data_port *port)
 	d = &port->data_ch;
 	ep = port->port_usb->out;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	while (port->port_usb && !list_empty(&d->rx_idle)) {
 
 		if (bam_ipa_rx_fctrl_support &&
@@ -367,9 +367,9 @@ static void bam_data_start_rx(struct bam_data_port *port)
 			req->dma_pre_mapped = false;
 
 		req->context = skb;
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		ret = usb_ep_queue(ep, req, GFP_ATOMIC);
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		if (ret) {
 			bam_data_free_skb_to_pool(port, skb);
 
@@ -382,7 +382,7 @@ static void bam_data_start_rx(struct bam_data_port *port)
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 static void bam_data_epout_complete(struct usb_ep *ep, struct usb_request *req)
@@ -402,29 +402,29 @@ static void bam_data_epout_complete(struct usb_ep *ep, struct usb_request *req)
 	case -ECONNRESET:
 	case -ESHUTDOWN:
 		/* cable disconnection */
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		bam_data_free_skb_to_pool(port, skb);
 		d->freed_rx_reqs++;
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		req->buf = 0;
 		usb_ep_free_request(ep, req);
 		return;
 	default:
 		pr_err("%s: %s response error %d, %d/%d\n", __func__,
 			ep->name, status, req->actual, req->length);
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		bam_data_free_skb_to_pool(port, skb);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		break;
 	}
 
-	spin_lock(&port->port_lock);
+	raw_spin_lock(&port->port_lock);
 	if (queue) {
 		__skb_queue_tail(&d->rx_skb_q, skb);
 		if (!usb_bam_get_prod_granted(d->usb_bam_type,
 					d->dst_connection_idx)) {
 			list_add_tail(&req->list, &d->rx_idle);
-			spin_unlock(&port->port_lock);
+			raw_spin_unlock(&port->port_lock);
 			pr_err_ratelimited("usb bam prod is not granted.\n");
 			return;
 		} else
@@ -438,17 +438,17 @@ static void bam_data_epout_complete(struct usb_ep *ep, struct usb_request *req)
 			d->rx_flow_control_enable++;
 		}
 		list_add_tail(&req->list, &d->rx_idle);
-		spin_unlock(&port->port_lock);
+		raw_spin_unlock(&port->port_lock);
 		return;
 	}
 
 	skb = bam_data_alloc_skb_from_pool(port);
 	if (!skb) {
 		list_add_tail(&req->list, &d->rx_idle);
-		spin_unlock(&port->port_lock);
+		raw_spin_unlock(&port->port_lock);
 		return;
 	}
-	spin_unlock(&port->port_lock);
+	raw_spin_unlock(&port->port_lock);
 
 	req->buf = skb->data;
 	req->dma = bam_data_get_dma_from_skb(skb);
@@ -465,10 +465,10 @@ static void bam_data_epout_complete(struct usb_ep *ep, struct usb_request *req)
 	if (status) {
 		pr_err_ratelimited("%s: data rx enqueue err %d\n",
 						__func__, status);
-		spin_lock(&port->port_lock);
+		raw_spin_lock(&port->port_lock);
 		bam_data_free_skb_to_pool(port, skb);
 		list_add_tail(&req->list, &d->rx_idle);
-		spin_unlock(&port->port_lock);
+		raw_spin_unlock(&port->port_lock);
 	}
 }
 /* It should be called with port_lock acquire. */
@@ -513,9 +513,9 @@ static void bam_data_write_toipa(struct work_struct *w)
 	d = container_of(w, struct bam_data_ch_info, write_tobam_w);
 	port = d->port;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
@@ -531,7 +531,7 @@ static void bam_data_write_toipa(struct work_struct *w)
 		pr_debug("%s: port:%pK d:%pK pbam:%u pno:%d\n", __func__,
 				port, d, d->pending_with_bam, port->port_num);
 
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 		skb_dma_addr = bam_data_get_dma_from_skb(skb);
 		if (skb_dma_addr != DMA_ERROR_CODE) {
@@ -541,7 +541,7 @@ static void bam_data_write_toipa(struct work_struct *w)
 
 		ret = ipa_tx_dp(IPA_CLIENT_USB_PROD, skb, &ipa_meta);
 
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		if (ret) {
 			pr_debug_ratelimited("%s: write error:%d\n",
 							__func__, ret);
@@ -552,7 +552,7 @@ static void bam_data_write_toipa(struct work_struct *w)
 	}
 
 	qlen = d->rx_skb_q.qlen;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	if (qlen < bam_ipa_rx_fctrl_dis_thld) {
 		if (d->rx_flow_control_triggered) {
@@ -589,13 +589,13 @@ static void bam_data_start_endless_rx(struct bam_data_port *port)
 	unsigned long flags;
 	int status;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb || !d->rx_req) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 	ep = port->port_usb->out;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	pr_debug("%s: enqueue\n", __func__);
 	status = usb_ep_queue(ep, d->rx_req, GFP_ATOMIC);
@@ -610,13 +610,13 @@ static void bam_data_start_endless_tx(struct bam_data_port *port)
 	unsigned long flags;
 	int status;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb || !d->tx_req) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 	ep = port->port_usb->in;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	pr_debug("%s: enqueue\n", __func__);
 	status = usb_ep_queue(ep, d->tx_req, GFP_ATOMIC);
@@ -630,9 +630,9 @@ static void bam_data_stop_endless_rx(struct bam_data_port *port)
 	unsigned long flags;
 	int status;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
@@ -643,7 +643,7 @@ static void bam_data_stop_endless_rx(struct bam_data_port *port)
 	if (status)
 		pr_err("%s: error dequeuing transfer, %d\n", __func__, status);
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 static void bam_data_stop_endless_tx(struct bam_data_port *port)
@@ -653,14 +653,14 @@ static void bam_data_stop_endless_tx(struct bam_data_port *port)
 	unsigned long flags;
 	int status;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 	ep = port->port_usb->in;
 	d->tx_req_dequeued = true;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	pr_debug("%s: dequeue\n", __func__);
 	status = usb_ep_dequeue(ep, d->tx_req);
@@ -767,11 +767,11 @@ static void bam2bam_data_disconnect_work(struct work_struct *w)
 	unsigned long flags;
 	int ret;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	if (!port->is_ipa_connected) {
 		pr_debug("%s: Already disconnected. Bailing out.\n", __func__);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
@@ -785,7 +785,7 @@ static void bam2bam_data_disconnect_work(struct work_struct *w)
 	 * and event functions (as bam_data_connect) will not influance
 	 * while lower layers connect pipes, etc.
 	*/
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	ret = usb_bam_disconnect_ipa(d->usb_bam_type, &d->ipa_params);
 	if (ret)
@@ -801,7 +801,7 @@ static void bam2bam_data_disconnect_work(struct work_struct *w)
 	 * there is pending data on USB BAM producer pipe.
 	 */
 	bam_data_ipa_disconnect(d);
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->is_ipa_connected = false;
 
 	/*
@@ -809,7 +809,7 @@ static void bam2bam_data_disconnect_work(struct work_struct *w)
 	 * upon cable connect or cable disconnect in suspended state.
 	 */
 	usb_gadget_autopm_put_async(port->gadget);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	pr_debug("Disconnect workqueue done (port %pK)\n", port);
 }
@@ -860,7 +860,7 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 
 	pr_debug("%s: Connect workqueue started", __func__);
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	d = &port->data_ch;
 	d_port = port->port_usb;
@@ -868,7 +868,7 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 	if (port->last_event == U_BAM_DATA_DISCONNECT_E) {
 		pr_debug("%s: Port is about to disconnect. Bail out.\n",
 			__func__);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
@@ -877,19 +877,19 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 
 	if (!gadget) {
 		pr_err("%s: NULL gadget\n", __func__);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
 	if (!port->port_usb) {
 		pr_err("port_usb is NULL");
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
 	if (!port->port_usb->out) {
 		pr_err("port_usb->out (bulk out ep) is NULL");
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
@@ -900,7 +900,7 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 	 */
 	if (port->is_ipa_connected) {
 		pr_debug("IPA connect is already done & Transfers started\n");
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		usb_gadget_autopm_put_async(port->gadget);
 		return;
 	}
@@ -910,21 +910,21 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 	d->ipa_params.prod_clnt_hdl = -1;
 
 	if (d->dst_pipe_type != USB_BAM_PIPE_BAM2BAM) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s: no software preparation for DL not using bam2bam\n",
 				__func__);
 		return;
 	}
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	usb_bam_alloc_fifos(d->usb_bam_type, d->src_connection_idx);
 	usb_bam_alloc_fifos(d->usb_bam_type, d->dst_connection_idx);
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb) {
 		pr_err("Disconnected.port_usb is NULL\n");
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		goto free_fifos;
 	}
 
@@ -973,7 +973,7 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 		teth_bridge_params.client = d->ipa_params.src_client;
 		ret = teth_bridge_init(&teth_bridge_params);
 		if (ret) {
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			pr_err("%s:teth_bridge_init() failed\n",
 			      __func__);
 			goto free_fifos;
@@ -1014,7 +1014,7 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 			msm_dwc3_reset_ep_after_lpm(gadget));
 	}
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	ret = usb_bam_connect_ipa(d->usb_bam_type, &d->ipa_params);
 	if (ret) {
 		pr_err("%s: usb_bam_connect_ipa failed: err:%d\n",
@@ -1022,9 +1022,9 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 		goto free_fifos;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (port->last_event ==  U_BAM_DATA_DISCONNECT_E) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s:%d: Port is being disconnected.\n",
 					__func__, __LINE__);
 		goto disconnect_ipa;
@@ -1058,7 +1058,7 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 	} else {
 		d->ipa_params.reset_pipe_after_lpm = false;
 	}
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	ret = usb_bam_connect_ipa(d->usb_bam_type, &d->ipa_params);
 	if (ret) {
 		pr_err("%s: usb_bam_connect_ipa failed: err:%d\n",
@@ -1070,9 +1070,9 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 	 * Cable might have been disconnected after releasing the
 	 * spinlock and re-enabling IRQs. Hence check again.
 	 */
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (port->last_event ==  U_BAM_DATA_DISCONNECT_E) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s:%d: port is beind disconnected.\n",
 					__func__, __LINE__);
 		goto disconnect_ipa;
@@ -1084,7 +1084,7 @@ static void bam2bam_data_connect_work(struct work_struct *w)
 	pr_debug("%s(): ipa_producer_ep:%d ipa_consumer_ep:%d\n",
 			__func__, d_port->ipa_producer_ep,
 			d_port->ipa_consumer_ep);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	if (d->func_type == USB_FUNC_MBIM) {
 		connect_params.ipa_usb_pipe_hdl =
@@ -1186,7 +1186,7 @@ void bam_data_start_rx_tx(u8 port_num)
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	d = &port->data_ch;
 
 	if (!port->port_usb || !port->port_usb->in->driver_data
@@ -1205,7 +1205,7 @@ void bam_data_start_rx_tx(u8 port_num)
 		goto out;
 	}
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	/* queue in & out requests */
 	pr_debug("%s: Starting rx", __func__);
@@ -1216,7 +1216,7 @@ void bam_data_start_rx_tx(u8 port_num)
 
 	return;
 out:
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 inline int u_bam_data_func_to_port(enum function_type func, u8 func_port)
@@ -1249,7 +1249,7 @@ static int bam2bam_data_port_alloc(int portno)
 	d = &port->data_ch;
 	d->port = port;
 
-	spin_lock_init(&port->port_lock);
+	raw_spin_lock_init(&port->port_lock);
 
 	INIT_WORK(&port->connect_w, bam2bam_data_connect_work);
 	INIT_WORK(&port->disconnect_w, bam2bam_data_disconnect_work);
@@ -1314,13 +1314,13 @@ void u_bam_data_stop_rndis_ipa(void)
 		bam_data_stop_endless_tx(port);
 		bam_data_stop_endless_rx(port);
 		if (gadget_is_dwc3(port->gadget)) {
-			spin_lock_irqsave(&port->port_lock, flags);
+			raw_spin_lock_irqsave(&port->port_lock, flags);
 			/* check if USB cable is disconnected or not */
 			if (port->port_usb) {
 				msm_ep_unconfig(port->port_usb->in);
 				msm_ep_unconfig(port->port_usb->out);
 			}
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		}
 		queue_work(bam_data_wq, &port->disconnect_w);
 	}
@@ -1382,13 +1382,13 @@ void bam_data_disconnect(struct data_port *gr, enum function_type func,
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	d = &port->data_ch;
 
 	/* Already disconnected due to suspend with remote wake disabled */
 	if (port->last_event == U_BAM_DATA_DISCONNECT_E) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
@@ -1421,18 +1421,18 @@ void bam_data_disconnect(struct data_port *gr, enum function_type func,
 			 * complete function will be called, where we try
 			 * to obtain the spinlock as well.
 			 */
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			usb_ep_disable(port->port_usb->in);
-			spin_lock_irqsave(&port->port_lock, flags);
+			raw_spin_lock_irqsave(&port->port_lock, flags);
 			if (d->tx_req) {
 				usb_ep_free_request(port->port_usb->in,
 								d->tx_req);
 				d->tx_req = NULL;
 			}
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 			usb_ep_disable(port->port_usb->out);
-			spin_lock_irqsave(&port->port_lock, flags);
+			raw_spin_lock_irqsave(&port->port_lock, flags);
 			if (d->rx_req) {
 				usb_ep_free_request(port->port_usb->out,
 								d->rx_req);
@@ -1488,7 +1488,7 @@ void bam_data_disconnect(struct data_port *gr, enum function_type func,
 
 	queue_work(bam_data_wq, &port->disconnect_w);
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 int bam_data_connect(struct data_port *gr, enum transport_type trans,
@@ -1534,7 +1534,7 @@ int bam_data_connect(struct data_port *gr, enum transport_type trans,
 
 	port = bam2bam_data_ports[port_num];
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	port->port_usb = gr;
 	port->gadget = gr->cdev->gadget;
@@ -1664,7 +1664,7 @@ int bam_data_connect(struct data_port *gr, enum transport_type trans,
 	usb_gadget_autopm_get_noresume(port->gadget);
 
 	queue_work(bam_data_wq, &port->connect_w);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	return 0;
 
 ep_out_req_free:
@@ -1678,7 +1678,7 @@ disable_in_ep:
 	gr->in->driver_data = 0;
 	usb_ep_disable(gr->in);
 exit:
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	return ret;
 }
 
@@ -1881,10 +1881,10 @@ void bam_data_suspend(struct data_port *port_usb, u8 dev_port_num,
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->last_event = U_BAM_DATA_SUSPEND_E;
 	queue_work(bam_data_wq, &port->suspend_w);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 void bam_data_resume(struct data_port *port_usb, u8 dev_port_num,
@@ -1923,7 +1923,7 @@ void bam_data_resume(struct data_port *port_usb, u8 dev_port_num,
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->last_event = U_BAM_DATA_RESUME_E;
 
 	/*
@@ -1935,7 +1935,7 @@ void bam_data_resume(struct data_port *port_usb, u8 dev_port_num,
 	 */
 	usb_gadget_autopm_get_noresume(port->gadget);
 	queue_work(bam_data_wq, &port->resume_w);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 void bam_data_flush_workqueue(void)
@@ -1954,7 +1954,7 @@ static void bam2bam_data_suspend_work(struct work_struct *w)
 
 	pr_debug("%s: suspend work started\n", __func__);
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	d = &port->data_ch;
 
@@ -1968,7 +1968,7 @@ static void bam2bam_data_suspend_work(struct work_struct *w)
 	 */
 	if (!port->is_ipa_connected) {
 		pr_err("%s: Not yet connected. SUSPEND pending.\n", __func__);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
@@ -1996,9 +1996,9 @@ static void bam2bam_data_suspend_work(struct work_struct *w)
 	 * bam_data_stop() called from usb_bam_suspend()
 	 * re-acquires port lock.
 	 */
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	usb_bam_suspend(d->usb_bam_type, &d->ipa_params);
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 exit:
 	/*
@@ -2008,7 +2008,7 @@ exit:
 	 */
 	usb_gadget_autopm_put_async(port->gadget);
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 static void bam2bam_data_resume_work(struct work_struct *w)
@@ -2021,7 +2021,7 @@ static void bam2bam_data_resume_work(struct work_struct *w)
 	int ret;
 	unsigned long flags;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	if (!port->port_usb) {
 		pr_err("port->port_usb is NULL");
@@ -2078,12 +2078,12 @@ static void bam2bam_data_resume_work(struct work_struct *w)
 				d->src_connection_idx,
 				port->port_usb->out, d->src_pipe_type);
 		}
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		if (d->tx_req_dequeued)
 			msm_dwc3_reset_dbm_ep(port->port_usb->in);
 		if (d->rx_req_dequeued)
 			msm_dwc3_reset_dbm_ep(port->port_usb->out);
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		if (port->port_usb) {
 			if (d->tx_req_dequeued)
 				msm_ep_config(port->port_usb->in, d->tx_req);
@@ -2096,7 +2096,7 @@ static void bam2bam_data_resume_work(struct work_struct *w)
 	d->rx_req_dequeued = false;
 	usb_bam_resume(d->usb_bam_type, &d->ipa_params);
 exit:
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 void u_bam_data_set_dl_max_xfer_size(u32 max_transfer_size)
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/u_ctrl_hsic.c b/kernel/msm-3.18/drivers/usb/gadget/function/u_ctrl_hsic.c
index 6a1242276..0bb461163 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/u_ctrl_hsic.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/u_ctrl_hsic.c
@@ -47,7 +47,7 @@ struct gctrl_port {
 	unsigned		port_num;
 
 	/* gadget */
-	spinlock_t		port_lock;
+	raw_spinlock_t		port_lock;
 	void			*port_usb;
 
 	/* work queue*/
@@ -193,10 +193,10 @@ static void ghsic_ctrl_connect_w(struct work_struct *w)
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb) {
 		ctrl_bridge_close(port->brdg.ch_id);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 	set_bit(CH_OPENED, &port->bridge_sts);
@@ -204,7 +204,7 @@ static void ghsic_ctrl_connect_w(struct work_struct *w)
 	if (port->cbits_tomodem)
 		ctrl_bridge_set_cbits(port->brdg.ch_id, port->cbits_tomodem);
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	cbits = ctrl_bridge_get_cbits_tohost(port->brdg.ch_id);
 
@@ -242,7 +242,7 @@ int ghsic_ctrl_connect(void *gptr, int port_num)
 		return -ENODEV;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (port->gtype == USB_GADGET_SERIAL) {
 		gser = gptr;
 		gser->notify_modem = ghsic_send_cbits_tomodem;
@@ -259,7 +259,7 @@ int ghsic_ctrl_connect(void *gptr, int port_num)
 	port->to_host = 0;
 	port->to_modem = 0;
 	port->drp_cpkt_cnt = 0;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	queue_work(port->wq, &port->connect_w);
 
@@ -305,7 +305,7 @@ void ghsic_ctrl_disconnect(void *gptr, int port_num)
 	 else
 		gr = gptr;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (gr) {
 		gr->send_encap_cmd = 0;
 		gr->notify_modem = 0;
@@ -316,7 +316,7 @@ void ghsic_ctrl_disconnect(void *gptr, int port_num)
 	port->cbits_tomodem = 0;
 	port->port_usb = 0;
 	port->send_cpkt_response = 0;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	queue_work(port->wq, &port->disconnect_w);
 }
@@ -374,10 +374,10 @@ static int ghsic_ctrl_probe(struct platform_device *pdev)
 	set_bit(CH_READY, &port->bridge_sts);
 
 	/* if usb is online, start read */
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (port->port_usb)
 		queue_work(port->wq, &port->connect_w);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	return 0;
 }
@@ -400,9 +400,9 @@ static int ghsic_ctrl_remove(struct platform_device *pdev)
 
 	port = gctrl_ports[id].port;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		goto not_ready;
 	}
 
@@ -412,7 +412,7 @@ static int ghsic_ctrl_remove(struct platform_device *pdev)
 		gr = port->port_usb;
 
 	port->cbits_tohost = 0;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	if (gr && gr->disconnect)
 		gr->disconnect(gr);
@@ -462,7 +462,7 @@ static int gctrl_port_alloc(int portno, enum gadget_type gtype)
 	port->port_num = portno;
 	port->gtype = gtype;
 
-	spin_lock_init(&port->port_lock);
+	raw_spin_lock_init(&port->port_lock);
 
 	INIT_WORK(&port->connect_w, ghsic_ctrl_connect_w);
 	INIT_WORK(&port->disconnect_w, gctrl_disconnect_w);
@@ -570,7 +570,7 @@ static ssize_t gctrl_read_stats(struct file *file, char __user *ubuf,
 		if (!port)
 			continue;
 		pdrv = &gctrl_ports[i].pdrv;
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 
 		temp += scnprintf(buf + temp, DEBUG_BUF_SIZE - temp,
 				"\nName:        %s\n"
@@ -589,7 +589,7 @@ static ssize_t gctrl_read_stats(struct file *file, char __user *ubuf,
 				test_bit(CH_OPENED, &port->bridge_sts),
 				test_bit(CH_READY, &port->bridge_sts));
 
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	}
 
 	ret = simple_read_from_buffer(ubuf, count, ppos, buf, temp);
@@ -611,11 +611,11 @@ static ssize_t gctrl_reset_stats(struct file *file,
 		if (!port)
 			continue;
 
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		port->to_host = 0;
 		port->to_modem = 0;
 		port->drp_cpkt_cnt = 0;
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	}
 	return count;
 }
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/u_ctrl_qti.c b/kernel/msm-3.18/drivers/usb/gadget/function/u_ctrl_qti.c
index eb3e3d237..3a91b2b48 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/u_ctrl_qti.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/u_ctrl_qti.c
@@ -53,7 +53,7 @@ struct qti_ctrl_port {
 
 	struct list_head	cpkt_req_q;
 
-	spinlock_t	lock;
+	raw_spinlock_t	lock;
 	enum gadget_type	gtype;
 	unsigned	host_to_modem;
 	unsigned	copied_to_modem;
@@ -86,23 +86,23 @@ static void qti_ctrl_queue_notify(struct qti_ctrl_port *port)
 	pr_debug("%s: Queue empty packet for QTI for port%d",
 		 __func__, port->index);
 
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 	if (!port->is_open) {
 		pr_err("%s: rmnet ctrl file handler %pK is not open",
 			   __func__, port);
-		spin_unlock_irqrestore(&port->lock, flags);
+		raw_spin_unlock_irqrestore(&port->lock, flags);
 		return;
 	}
 
 	cpkt = alloc_rmnet_ctrl_pkt(0, GFP_ATOMIC);
 	if (IS_ERR(cpkt)) {
 		pr_err("%s: Unable to allocate reset function pkt\n", __func__);
-		spin_unlock_irqrestore(&port->lock, flags);
+		raw_spin_unlock_irqrestore(&port->lock, flags);
 		return;
 	}
 
 	list_add_tail(&cpkt->list, &port->cpkt_req_q);
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 
 	pr_debug("%s: Wake up read queue", __func__);
 	wake_up(&port->read_wq);
@@ -137,21 +137,21 @@ static int gqti_ctrl_send_cpkt_tomodem(u8 portno, void *buf, size_t len)
 
 	pr_debug("%s: gtype:%d: Add to cpkt_req_q packet with len = %zu\n",
 			__func__, port->gtype, len);
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 
 	/* drop cpkt if port is not open */
 	if (!port->is_open) {
 		pr_debug("rmnet file handler %pK(index=%d) is not open",
 		       port, port->index);
 		port->drp_cpkt_cnt++;
-		spin_unlock_irqrestore(&port->lock, flags);
+		raw_spin_unlock_irqrestore(&port->lock, flags);
 		free_rmnet_ctrl_pkt(cpkt);
 		return 0;
 	}
 
 	list_add_tail(&cpkt->list, &port->cpkt_req_q);
 	port->host_to_modem++;
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 
 	/* wakeup read thread */
 	pr_debug("%s: Wake up read queue", __func__);
@@ -197,7 +197,7 @@ int gqti_ctrl_connect(void *gr, u8 port_num, unsigned intf,
 		return -ENODEV;
 	}
 
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 	port->gtype = gtype;
 	if (dxport == USB_GADGET_XPORT_BAM_DMUX) {
 		/*
@@ -232,7 +232,7 @@ int gqti_ctrl_connect(void *gr, u8 port_num, unsigned intf,
 		g_dpl->notify_modem = gqti_ctrl_notify_modem;
 		atomic_set(&port->line_state, 1);
 	} else {
-		spin_unlock_irqrestore(&port->lock, flags);
+		raw_spin_unlock_irqrestore(&port->lock, flags);
 		pr_err("%s(): Port is used without gtype.\n", __func__);
 		return -ENODEV;
 	}
@@ -243,7 +243,7 @@ int gqti_ctrl_connect(void *gr, u8 port_num, unsigned intf,
 	port->modem_to_host = 0;
 	port->drp_cpkt_cnt = 0;
 
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 
 	atomic_set(&port->connected, 1);
 	wake_up(&port->read_wq);
@@ -277,7 +277,7 @@ void gqti_ctrl_disconnect(void *gr, u8 port_num)
 
 	atomic_set(&port->connected, 0);
 	atomic_set(&port->line_state, 0);
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 
 	/* reset ipa eps to -1 */
 	port->ipa_prod_idx = -1;
@@ -305,7 +305,7 @@ void gqti_ctrl_disconnect(void *gr, u8 port_num)
 		free_rmnet_ctrl_pkt(cpkt);
 	}
 
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 
 	/* send 0 len pkt to qti to notify state change */
 	qti_ctrl_queue_notify(port);
@@ -344,9 +344,9 @@ static int qti_ctrl_open(struct inode *ip, struct file *fp)
 		return -EBUSY;
 	}
 
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 	port->is_open = true;
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 
 	return 0;
 }
@@ -360,9 +360,9 @@ static int qti_ctrl_release(struct inode *ip, struct file *fp)
 
 	pr_debug("Close rmnet control file");
 
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 	port->is_open = false;
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 
 	qti_ctrl_unlock(&port->open_excl);
 
@@ -394,10 +394,10 @@ qti_ctrl_read(struct file *fp, char __user *buf, size_t count, loff_t *pos)
 
 	/* block until a new packet is available */
 	do {
-		spin_lock_irqsave(&port->lock, flags);
+		raw_spin_lock_irqsave(&port->lock, flags);
 		if (!list_empty(&port->cpkt_req_q))
 			break;
-		spin_unlock_irqrestore(&port->lock, flags);
+		raw_spin_unlock_irqrestore(&port->lock, flags);
 
 		pr_debug("%s: Requests list is empty. Wait.\n", __func__);
 		ret = wait_event_interruptible(port->read_wq,
@@ -412,7 +412,7 @@ qti_ctrl_read(struct file *fp, char __user *buf, size_t count, loff_t *pos)
 	cpkt = list_first_entry(&port->cpkt_req_q, struct rmnet_ctrl_pkt,
 							list);
 	list_del(&cpkt->list);
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 
 	if (cpkt->len > count) {
 		pr_err("cpkt size too big:%d > buf size:%zu\n",
@@ -493,12 +493,12 @@ qti_ctrl_write(struct file *fp, const char __user *buf, size_t count,
 	}
 	port->copied_from_modem++;
 
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 	if (port && port->port_usb) {
 		if (port->gtype == USB_GADGET_RMNET) {
 			g_rmnet = (struct grmnet *)port->port_usb;
 		} else {
-			spin_unlock_irqrestore(&port->lock, flags);
+			raw_spin_unlock_irqrestore(&port->lock, flags);
 			pr_err("%s(): unrecognized gadget type(%d).\n",
 						__func__, port->gtype);
 			return -EINVAL;
@@ -516,7 +516,7 @@ qti_ctrl_write(struct file *fp, const char __user *buf, size_t count,
 		}
 	}
 
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 	kfree(kbuf);
 	qti_ctrl_unlock(&port->write_excl);
 
@@ -637,12 +637,12 @@ static unsigned int qti_ctrl_poll(struct file *file, poll_table *wait)
 
 	poll_wait(file, &port->read_wq, wait);
 
-	spin_lock_irqsave(&port->lock, flags);
+	raw_spin_lock_irqsave(&port->lock, flags);
 	if (!list_empty(&port->cpkt_req_q)) {
 		mask |= POLLIN | POLLRDNORM;
 		pr_debug("%s sets POLLIN for rmnet_ctrl_qti_port\n", __func__);
 	}
-	spin_unlock_irqrestore(&port->lock, flags);
+	raw_spin_unlock_irqrestore(&port->lock, flags);
 
 	return mask;
 }
@@ -657,7 +657,7 @@ static int qti_ctrl_read_stats(struct seq_file *s, void *unused)
 		port = ctrl_port[i];
 		if (!port)
 			continue;
-		spin_lock_irqsave(&port->lock, flags);
+		raw_spin_lock_irqsave(&port->lock, flags);
 
 		seq_printf(s, "\n#PORT:%d port: %pK\n", i, port);
 		seq_printf(s, "name:			%s\n", port->name);
@@ -671,7 +671,7 @@ static int qti_ctrl_read_stats(struct seq_file *s, void *unused)
 				port->modem_to_host);
 		seq_printf(s, "cpkt_drp_cnt:		%d\n",
 				port->drp_cpkt_cnt);
-		spin_unlock_irqrestore(&port->lock, flags);
+		raw_spin_unlock_irqrestore(&port->lock, flags);
 	}
 
 	return 0;
@@ -695,13 +695,13 @@ static ssize_t qti_ctrl_reset_stats(struct file *file,
 		if (!port)
 			continue;
 
-		spin_lock_irqsave(&port->lock, flags);
+		raw_spin_lock_irqsave(&port->lock, flags);
 		port->host_to_modem = 0;
 		port->copied_to_modem = 0;
 		port->copied_from_modem = 0;
 		port->modem_to_host = 0;
 		port->drp_cpkt_cnt = 0;
-		spin_unlock_irqrestore(&port->lock, flags);
+		raw_spin_unlock_irqrestore(&port->lock, flags);
 	}
 	return count;
 }
@@ -774,7 +774,7 @@ int gqti_ctrl_init(void)
 		}
 
 		INIT_LIST_HEAD(&port->cpkt_req_q);
-		spin_lock_init(&port->lock);
+		raw_spin_lock_init(&port->lock);
 
 		atomic_set(&port->open_excl, 0);
 		atomic_set(&port->read_excl, 0);
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/u_data_bridge.c b/kernel/msm-3.18/drivers/usb/gadget/function/u_data_bridge.c
index f5ecf3e27..206cd3f0a 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/u_data_bridge.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/u_data_bridge.c
@@ -56,7 +56,7 @@ struct gbridge_port {
 	unsigned		port_num;
 	char			name[sizeof(DEVICE_NAME) + 2];
 
-	spinlock_t		port_lock;
+	raw_spinlock_t		port_lock;
 
 	wait_queue_head_t	open_wq;
 	wait_queue_head_t	read_wq;
@@ -167,9 +167,9 @@ static void gbridge_start_rx(struct gbridge_port *port)
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!(port->is_connected && port->port_open)) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_debug("can't start rx.\n");
 		return;
 	}
@@ -184,9 +184,9 @@ static void gbridge_start_rx(struct gbridge_port *port)
 		list_del_init(&req->list);
 		req->length = BRIDGE_RX_BUF_SIZE;
 		req->complete = gbridge_read_complete;
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		ret = usb_ep_queue(ep, req, GFP_KERNEL);
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		if (ret) {
 			pr_err("port(%d):%pK usb ep(%s) queue failed\n",
 					port->port_num, port, ep->name);
@@ -195,7 +195,7 @@ static void gbridge_start_rx(struct gbridge_port *port)
 		}
 	}
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 static void gbridge_read_complete(struct usb_ep *ep, struct usb_request *req)
@@ -210,16 +210,16 @@ static void gbridge_read_complete(struct usb_ep *ep, struct usb_request *req)
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_open || req->status || !req->actual) {
 		list_add_tail(&req->list, &port->read_pool);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
 	port->nbytes_from_host += req->actual;
 	list_add_tail(&req->list, &port->read_queued);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	wake_up(&port->read_wq);
 	return;
@@ -233,9 +233,9 @@ static void gbridge_write_complete(struct usb_ep *ep, struct usb_request *req)
 	pr_debug("ep:(%pK)(%s) port:%pK req_stats:%d\n",
 			ep, ep->name, port, req->status);
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("port is null\n");
 		return;
 	}
@@ -257,7 +257,7 @@ static void gbridge_write_complete(struct usb_ep *ep, struct usb_request *req)
 		break;
 	}
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	return;
 }
 
@@ -268,7 +268,7 @@ static void gbridge_start_io(struct gbridge_port *port)
 
 	pr_debug("port: %pK\n", port);
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb)
 		goto start_io_out;
 
@@ -296,7 +296,7 @@ static void gbridge_start_io(struct gbridge_port *port)
 	}
 
 start_io_out:
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	if (ret)
 		return;
 
@@ -310,14 +310,14 @@ static void gbridge_stop_io(struct gbridge_port *port)
 	unsigned long	flags;
 
 	pr_debug("port:%pK\n", port);
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 	in = port->port_usb->in;
 	out = port->port_usb->out;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	/* disable endpoints, aborting down any active I/O */
 	usb_ep_disable(out);
@@ -325,7 +325,7 @@ static void gbridge_stop_io(struct gbridge_port *port)
 	usb_ep_disable(in);
 	in->driver_data = NULL;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (port->current_rx_req != NULL) {
 		kfree(port->current_rx_req->buf);
 		usb_ep_free_request(out, port->current_rx_req);
@@ -336,7 +336,7 @@ static void gbridge_stop_io(struct gbridge_port *port)
 	gbridge_free_requests(out, &port->read_queued);
 	gbridge_free_requests(out, &port->read_pool);
 	gbridge_free_requests(in, &port->write_pool);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 int gbridge_port_open(struct inode *inode, struct file *file)
@@ -366,9 +366,9 @@ int gbridge_port_open(struct inode *inode, struct file *file)
 		return ret;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->port_open = true;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	gbridge_start_rx(port);
 
 	pr_debug("port(%pK) open is success\n", port);
@@ -388,10 +388,10 @@ int gbridge_port_release(struct inode *inode, struct file *file)
 	}
 
 	pr_debug("closing port(%pK)\n", port);
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->port_open = false;
 	port->cbits_updated = false;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	pr_debug("port(%pK) is closed.\n", port);
 
 	return 0;
@@ -417,7 +417,7 @@ ssize_t gbridge_port_read(struct file *file,
 	}
 
 	pr_debug("read on port(%pK) count:%zu\n", port, count);
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	current_rx_req = port->current_rx_req;
 	pending_rx_bytes = port->pending_rx_bytes;
 	current_rx_buf = port->current_rx_buf;
@@ -427,7 +427,7 @@ ssize_t gbridge_port_read(struct file *file,
 	bytes_copied = 0;
 
 	if (list_empty(&port->read_queued) && !pending_rx_bytes) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_debug("%s(): read_queued list is empty.\n", __func__);
 		goto start_rx;
 	}
@@ -452,7 +452,7 @@ ssize_t gbridge_port_read(struct file *file,
 			current_rx_buf = req->buf;
 		}
 
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		size = count;
 		if (size > pending_rx_bytes)
 			size = pending_rx_bytes;
@@ -465,10 +465,10 @@ ssize_t gbridge_port_read(struct file *file,
 		count -= size;
 		buf += size;
 
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		if (!port->is_connected) {
 			list_add_tail(&current_rx_req->list, &port->read_pool);
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			return -EAGAIN;
 		}
 
@@ -490,7 +490,7 @@ ssize_t gbridge_port_read(struct file *file,
 	port->pending_rx_bytes = pending_rx_bytes;
 	port->current_rx_buf = current_rx_buf;
 	port->current_rx_req = current_rx_req;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 start_rx:
 	gbridge_start_rx(port);
@@ -516,17 +516,17 @@ ssize_t gbridge_port_write(struct file *file,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	pr_debug("write on port(%pK)\n", port);
 
 	if (!port->is_connected || !port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s: cable is disconnected.\n", __func__);
 		return -ENODEV;
 	}
 
 	if (list_empty(&port->write_pool)) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_debug("%s: Request list is empty.\n", __func__);
 		return 0;
 	}
@@ -535,7 +535,7 @@ ssize_t gbridge_port_write(struct file *file,
 	pool = &port->write_pool;
 	req = list_first_entry(pool, struct usb_request, list);
 	list_del_init(&req->list);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	pr_debug("%s: write buf size:%zu\n", __func__, count);
 	if (count > BRIDGE_TX_BUF_SIZE)
@@ -555,20 +555,20 @@ ssize_t gbridge_port_write(struct file *file,
 			ret = -EIO;
 			goto err_exit;
 		}
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		port->nbytes_from_port_bridge += req->length;
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	}
 
 err_exit:
 	if (ret) {
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		/* USB cable is connected, add it back otherwise free request */
 		if (port->is_connected)
 			list_add(&req->list, &port->write_pool);
 		else
 			gbridge_free_req(in, req);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return ret;
 	}
 
@@ -584,7 +584,7 @@ static unsigned int gbridge_port_poll(struct file *file, poll_table *wait)
 	port = file->private_data;
 	if (port && port->is_connected) {
 		poll_wait(file, &port->read_wq, wait);
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		if (!list_empty(&port->read_queued)) {
 			mask |= POLLIN | POLLRDNORM;
 			pr_debug("sets POLLIN for gbridge_port\n");
@@ -594,7 +594,7 @@ static unsigned int gbridge_port_poll(struct file *file, poll_table *wait)
 			mask |= POLLPRI;
 			pr_debug("sets POLLPRI for gbridge_port\n");
 		}
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	} else {
 		pr_err("Failed due to NULL device or disconnected.\n");
 		mask = POLLERR;
@@ -614,7 +614,7 @@ static int gbridge_port_tiocmget(struct gbridge_port *port)
 		return -ENODEV;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	gser = port->port_usb;
 	if (!gser) {
 		pr_err("gser is null.\n");
@@ -634,7 +634,7 @@ static int gbridge_port_tiocmget(struct gbridge_port *port)
 	if (gser->serial_state & TIOCM_RI)
 		result |= TIOCM_RI;
 fail:
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	return result;
 }
 
@@ -650,7 +650,7 @@ static int gbridge_port_tiocmset(struct gbridge_port *port,
 		return -ENODEV;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	gser = port->port_usb;
 	if (!gser) {
 		pr_err("gser is NULL.\n");
@@ -683,7 +683,7 @@ static int gbridge_port_tiocmset(struct gbridge_port *port,
 		}
 	}
 fail:
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	return status;
 }
 
@@ -744,17 +744,17 @@ static void gbridge_notify_modem(void *gptr, u8 portno, int ctrl_bits)
 	}
 
 	port = ports[portno];
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	temp = convert_acm_sigs_to_uart(ctrl_bits);
 
 	if (temp == port->cbits_to_modem) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 
 	port->cbits_to_modem = temp;
 	port->cbits_updated = true;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	wake_up(&port->read_wq);
 }
 
@@ -775,7 +775,7 @@ static ssize_t debug_gbridge_read_stats(struct file *file, char __user *ubuf,
 
 	for (i = 0; i < n_bridge_ports; i++) {
 		port = ports[i];
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		temp += scnprintf(buf + temp, 512 - temp,
 				"###PORT:%d###\n"
 				"nbytes_to_host: %lu\n"
@@ -790,7 +790,7 @@ static ssize_t debug_gbridge_read_stats(struct file *file, char __user *ubuf,
 				port->nbytes_from_port_bridge,
 				port->cbits_to_modem,
 				(port->port_open ? "Opened" : "Closed"));
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	}
 
 	ret = simple_read_from_buffer(ubuf, count, ppos, buf, temp);
@@ -809,10 +809,10 @@ static ssize_t debug_gbridge_reset_stats(struct file *file,
 
 	for (i = 0; i < n_bridge_ports; i++) {
 		port = ports[i];
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		port->nbytes_to_host = port->nbytes_from_host = 0;
 		port->nbytes_to_port_bridge = port->nbytes_from_port_bridge = 0;
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	}
 
 	return count;
@@ -914,10 +914,10 @@ int gbridge_connect(void *gptr, u8 portno)
 	port = ports[portno];
 	gser = gptr;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->port_usb = gser;
 	gser->notify_modem = gbridge_notify_modem;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	ret = usb_ep_enable(gser->in);
 	if (ret) {
@@ -938,9 +938,9 @@ int gbridge_connect(void *gptr, u8 portno)
 	}
 	gser->out->driver_data = port;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->is_connected = true;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	gbridge_start_io(port);
 	wake_up(&port->open_wq);
@@ -972,12 +972,12 @@ void gbridge_disconnect(void *gptr, u8 portno)
 	/* lower DTR to modem */
 	gbridge_notify_modem(gser, portno, 0);
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->is_connected = false;
 	port->port_usb = NULL;
 	port->nbytes_from_host = port->nbytes_to_host = 0;
 	port->nbytes_to_port_bridge = 0;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 static void gbridge_port_free(int portno)
@@ -1003,7 +1003,7 @@ static int gbridge_port_alloc(int portno)
 	ports[portno]->port_num = portno;
 	snprintf(ports[portno]->name, sizeof(ports[portno]->name),
 			"%s%d", DEVICE_NAME, portno);
-	spin_lock_init(&ports[portno]->port_lock);
+	raw_spin_lock_init(&ports[portno]->port_lock);
 
 	init_waitqueue_head(&ports[portno]->open_wq);
 	init_waitqueue_head(&ports[portno]->read_wq);
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/u_data_hsic.c b/kernel/msm-3.18/drivers/usb/gadget/function/u_data_hsic.c
index 403f8ffbf..09cae999f 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/u_data_hsic.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/u_data_hsic.c
@@ -94,12 +94,12 @@ struct gdata_port {
 	unsigned int		tx_q_size;
 	struct list_head	tx_idle;
 	struct sk_buff_head	tx_skb_q;
-	spinlock_t		tx_lock;
+	raw_spinlock_t		tx_lock;
 
 	unsigned int		rx_q_size;
 	struct list_head	rx_idle;
 	struct sk_buff_head	rx_skb_q;
-	spinlock_t		rx_lock;
+	raw_spinlock_t		rx_lock;
 
 	/* work */
 	struct workqueue_struct	*wq;
@@ -150,7 +150,7 @@ static void ghsic_data_free_requests(struct usb_ep *ep, struct list_head *head)
 static int ghsic_data_alloc_requests(struct usb_ep *ep, struct list_head *head,
 		int num,
 		void (*cb)(struct usb_ep *ep, struct usb_request *),
-		spinlock_t *lock)
+		raw_spinlock_t *lock)
 {
 	int			i;
 	struct usb_request	*req;
@@ -166,9 +166,9 @@ static int ghsic_data_alloc_requests(struct usb_ep *ep, struct list_head *head,
 			return list_empty(head) ? -ENOMEM : 0;
 		}
 		req->complete = cb;
-		spin_lock_irqsave(lock, flags);
+		raw_spin_lock_irqsave(lock, flags);
 		list_add(&req->list, head);
-		spin_unlock_irqrestore(lock, flags);
+		raw_spin_unlock_irqrestore(lock, flags);
 	}
 
 	return 0;
@@ -182,9 +182,9 @@ static void ghsic_data_unthrottle_tx(void *ctx)
 	if (!port || !atomic_read(&port->connected))
 		return;
 
-	spin_lock_irqsave(&port->rx_lock, flags);
+	raw_spin_lock_irqsave(&port->rx_lock, flags);
 	port->tx_unthrottled_cnt++;
-	spin_unlock_irqrestore(&port->rx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 
 	queue_work(port->wq, &port->write_tomdm_w);
 	pr_debug("%s: port num =%d unthrottled\n", __func__,
@@ -206,10 +206,10 @@ static void ghsic_data_write_tohost(struct work_struct *w)
 	if (!port)
 		return;
 
-	spin_lock_irqsave(&port->tx_lock, flags);
+	raw_spin_lock_irqsave(&port->tx_lock, flags);
 	ep = port->in;
 	if (!ep) {
-		spin_unlock_irqrestore(&port->tx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 		return;
 	}
 
@@ -237,9 +237,9 @@ static void ghsic_data_write_tohost(struct work_struct *w)
 
 		info = (struct timestamp_info *)skb->cb;
 		info->tx_queued = get_timestamp();
-		spin_unlock_irqrestore(&port->tx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 		ret = usb_ep_queue(ep, req, GFP_KERNEL);
-		spin_lock_irqsave(&port->tx_lock, flags);
+		raw_spin_lock_irqsave(&port->tx_lock, flags);
 		if (ret) {
 			pr_err("%s: usb epIn failed\n", __func__);
 			list_add(&req->list, &port->tx_idle);
@@ -258,7 +258,7 @@ static void ghsic_data_write_tohost(struct work_struct *w)
 			data_bridge_unthrottle_rx(port->brdg.ch_id);
 		}
 	}
-	spin_unlock_irqrestore(&port->tx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 }
 
 static int ghsic_data_receive(void *p, void *data, size_t len)
@@ -275,7 +275,7 @@ static int ghsic_data_receive(void *p, void *data, size_t len)
 	pr_debug("%s: p:%pK#%d skb_len:%d\n", __func__,
 			port, port->port_num, skb->len);
 
-	spin_lock_irqsave(&port->tx_lock, flags);
+	raw_spin_lock_irqsave(&port->tx_lock, flags);
 	__skb_queue_tail(&port->tx_skb_q, skb);
 
 	if (ghsic_data_fctrl_support &&
@@ -284,12 +284,12 @@ static int ghsic_data_receive(void *p, void *data, size_t len)
 		port->rx_throttled_cnt++;
 		pr_debug_ratelimited("%s: flow ctrl enabled: tx skbq len: %u\n",
 					__func__, port->tx_skb_q.qlen);
-		spin_unlock_irqrestore(&port->tx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 		queue_work(port->wq, &port->write_tohost_w);
 		return -EBUSY;
 	}
 
-	spin_unlock_irqrestore(&port->tx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 
 	queue_work(port->wq, &port->write_tohost_w);
 
@@ -309,9 +309,9 @@ static void ghsic_data_write_tomdm(struct work_struct *w)
 	if (!port || !atomic_read(&port->connected))
 		return;
 
-	spin_lock_irqsave(&port->rx_lock, flags);
+	raw_spin_lock_irqsave(&port->rx_lock, flags);
 	if (test_bit(TX_THROTTLED, &port->brdg.flags)) {
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 		goto start_rx;
 	}
 
@@ -321,9 +321,9 @@ static void ghsic_data_write_tomdm(struct work_struct *w)
 
 		info = (struct timestamp_info *)skb->cb;
 		info->rx_done_sent = get_timestamp();
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 		ret = data_bridge_write(port->brdg.ch_id, skb);
-		spin_lock_irqsave(&port->rx_lock, flags);
+		raw_spin_lock_irqsave(&port->rx_lock, flags);
 		if (ret < 0) {
 			if (ret == -EBUSY) {
 				/*flow control*/
@@ -338,7 +338,7 @@ static void ghsic_data_write_tomdm(struct work_struct *w)
 		}
 		port->to_modem++;
 	}
-	spin_unlock_irqrestore(&port->rx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 start_rx:
 	ghsic_data_start_rx(port);
 }
@@ -368,9 +368,9 @@ static void ghsic_data_epin_complete(struct usb_ep *ep, struct usb_request *req)
 
 	dev_kfree_skb_any(skb);
 
-	spin_lock(&port->tx_lock);
+	raw_spin_lock(&port->tx_lock);
 	list_add_tail(&req->list, &port->tx_idle);
-	spin_unlock(&port->tx_lock);
+	raw_spin_unlock(&port->tx_lock);
 
 	queue_work(port->wq, &port->write_tohost_w);
 }
@@ -404,14 +404,14 @@ ghsic_data_epout_complete(struct usb_ep *ep, struct usb_request *req)
 		break;
 	}
 
-	spin_lock(&port->rx_lock);
+	raw_spin_lock(&port->rx_lock);
 	if (queue) {
 		info->rx_done = get_timestamp();
 		__skb_queue_tail(&port->rx_skb_q, skb);
 		list_add_tail(&req->list, &port->rx_idle);
 		queue_work(port->wq, &port->write_tomdm_w);
 	}
-	spin_unlock(&port->rx_lock);
+	raw_spin_unlock(&port->rx_lock);
 }
 
 static void ghsic_data_start_rx(struct gdata_port *port)
@@ -428,10 +428,10 @@ static void ghsic_data_start_rx(struct gdata_port *port)
 	if (!port)
 		return;
 
-	spin_lock_irqsave(&port->rx_lock, flags);
+	raw_spin_lock_irqsave(&port->rx_lock, flags);
 	ep = port->out;
 	if (!ep) {
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 		return;
 	}
 
@@ -442,12 +442,12 @@ static void ghsic_data_start_rx(struct gdata_port *port)
 		req = list_first_entry(&port->rx_idle,
 					struct usb_request, list);
 		list_del(&req->list);
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 
 		created = get_timestamp();
 		skb = alloc_skb(ghsic_data_rx_req_size, GFP_KERNEL);
 		if (!skb) {
-			spin_lock_irqsave(&port->rx_lock, flags);
+			raw_spin_lock_irqsave(&port->rx_lock, flags);
 			list_add(&req->list, &port->rx_idle);
 			break;
 		}
@@ -459,7 +459,7 @@ static void ghsic_data_start_rx(struct gdata_port *port)
 
 		info->rx_queued = get_timestamp();
 		ret = usb_ep_queue(ep, req, GFP_KERNEL);
-		spin_lock_irqsave(&port->rx_lock, flags);
+		raw_spin_lock_irqsave(&port->rx_lock, flags);
 		if (ret) {
 			dev_kfree_skb_any(skb);
 
@@ -472,7 +472,7 @@ static void ghsic_data_start_rx(struct gdata_port *port)
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&port->rx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 }
 
 static void ghsic_data_start_io(struct gdata_port *port)
@@ -486,9 +486,9 @@ static void ghsic_data_start_io(struct gdata_port *port)
 	if (!port)
 		return;
 
-	spin_lock_irqsave(&port->rx_lock, flags);
+	raw_spin_lock_irqsave(&port->rx_lock, flags);
 	ep_out = port->out;
-	spin_unlock_irqrestore(&port->rx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 
 	if (ep_out) {
 		ret = ghsic_data_alloc_requests(ep_out,
@@ -505,16 +505,16 @@ static void ghsic_data_start_io(struct gdata_port *port)
 		}
 	}
 
-	spin_lock_irqsave(&port->tx_lock, flags);
+	raw_spin_lock_irqsave(&port->tx_lock, flags);
 	ep_in = port->in;
-	spin_unlock_irqrestore(&port->tx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 	pr_debug("%s: ep_in:%pK\n", __func__, ep_in);
 
 	if (!ep_in) {
-		spin_lock_irqsave(&port->rx_lock, flags);
+		raw_spin_lock_irqsave(&port->rx_lock, flags);
 		if (ep_out)
 			ghsic_data_free_requests(ep_out, &port->rx_idle);
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 		return;
 	}
 
@@ -522,10 +522,10 @@ static void ghsic_data_start_io(struct gdata_port *port)
 		port->tx_q_size, ghsic_data_epin_complete, &port->tx_lock);
 	if (ret) {
 		pr_err("%s: tx req allocation failed\n", __func__);
-		spin_lock_irqsave(&port->rx_lock, flags);
+		raw_spin_lock_irqsave(&port->rx_lock, flags);
 		if (ep_out)
 			ghsic_data_free_requests(ep_out, &port->rx_idle);
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 		return;
 	}
 
@@ -577,9 +577,9 @@ static void ghsic_data_free_buffers(struct gdata_port *port)
 	if (!port)
 		return;
 
-	spin_lock_irqsave(&port->tx_lock, flags);
+	raw_spin_lock_irqsave(&port->tx_lock, flags);
 	if (!port->in) {
-		spin_unlock_irqrestore(&port->tx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 		return;
 	}
 
@@ -587,11 +587,11 @@ static void ghsic_data_free_buffers(struct gdata_port *port)
 
 	while ((skb = __skb_dequeue(&port->tx_skb_q)))
 		dev_kfree_skb_any(skb);
-	spin_unlock_irqrestore(&port->tx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 
-	spin_lock_irqsave(&port->rx_lock, flags);
+	raw_spin_lock_irqsave(&port->rx_lock, flags);
 	if (!port->out) {
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 		return;
 	}
 
@@ -599,7 +599,7 @@ static void ghsic_data_free_buffers(struct gdata_port *port)
 
 	while ((skb = __skb_dequeue(&port->rx_skb_q)))
 		dev_kfree_skb_any(skb);
-	spin_unlock_irqrestore(&port->rx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 }
 
 static int ghsic_data_get_port_id(const char *pdev_name)
@@ -712,8 +712,8 @@ static int ghsic_data_port_alloc(unsigned port_num, enum gadget_type gtype)
 	port->port_num = port_num;
 
 	/* port initialization */
-	spin_lock_init(&port->rx_lock);
-	spin_lock_init(&port->tx_lock);
+	raw_spin_lock_init(&port->rx_lock);
+	raw_spin_lock_init(&port->tx_lock);
 
 	INIT_WORK(&port->connect_w, ghsic_data_connect_w);
 	INIT_WORK(&port->disconnect_w, ghsic_data_disconnect_w);
@@ -777,16 +777,16 @@ void ghsic_data_disconnect(void *gptr, int port_num)
 
 	atomic_set(&port->connected, 0);
 
-	spin_lock_irqsave(&port->tx_lock, flags);
+	raw_spin_lock_irqsave(&port->tx_lock, flags);
 	port->in = NULL;
 	port->n_tx_req_queued = 0;
 	clear_bit(RX_THROTTLED, &port->brdg.flags);
-	spin_unlock_irqrestore(&port->tx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 
-	spin_lock_irqsave(&port->rx_lock, flags);
+	raw_spin_lock_irqsave(&port->rx_lock, flags);
 	port->out = NULL;
 	clear_bit(TX_THROTTLED, &port->brdg.flags);
-	spin_unlock_irqrestore(&port->rx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 
 	queue_work(port->wq, &port->disconnect_w);
 }
@@ -817,13 +817,13 @@ int ghsic_data_connect(void *gptr, int port_num)
 	if (port->gtype == USB_GADGET_SERIAL) {
 		gser = gptr;
 
-		spin_lock_irqsave(&port->tx_lock, flags);
+		raw_spin_lock_irqsave(&port->tx_lock, flags);
 		port->in = gser->in;
-		spin_unlock_irqrestore(&port->tx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 
-		spin_lock_irqsave(&port->rx_lock, flags);
+		raw_spin_lock_irqsave(&port->rx_lock, flags);
 		port->out = gser->out;
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 
 		port->tx_q_size = ghsic_data_serial_tx_q_size;
 		port->rx_q_size = ghsic_data_serial_rx_q_size;
@@ -831,13 +831,13 @@ int ghsic_data_connect(void *gptr, int port_num)
 		gser->out->driver_data = port;
 	} else if (port->gtype == USB_GADGET_RMNET) {
 		gr = gptr;
-		spin_lock_irqsave(&port->tx_lock, flags);
+		raw_spin_lock_irqsave(&port->tx_lock, flags);
 		port->in = gr->in;
-		spin_unlock_irqrestore(&port->tx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 
-		spin_lock_irqsave(&port->rx_lock, flags);
+		raw_spin_lock_irqsave(&port->rx_lock, flags);
 		port->out = gr->out;
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 
 		port->tx_q_size = ghsic_data_rmnet_tx_q_size;
 		port->rx_q_size = ghsic_data_rmnet_rx_q_size;
@@ -846,9 +846,9 @@ int ghsic_data_connect(void *gptr, int port_num)
 	} else if (port->gtype == USB_GADGET_QDSS) {
 		pr_debug("%s:: port type = USB_GADGET_QDSS\n", __func__);
 		qdss = gptr;
-		spin_lock_irqsave(&port->tx_lock, flags);
+		raw_spin_lock_irqsave(&port->tx_lock, flags);
 		port->in = qdss->data;
-		spin_unlock_irqrestore(&port->tx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 		port->tx_q_size = ghsic_data_qdss_tx_q_size;
 		qdss->data->driver_data = port;
 	}
@@ -871,19 +871,19 @@ int ghsic_data_connect(void *gptr, int port_num)
 
 	atomic_set(&port->connected, 1);
 
-	spin_lock_irqsave(&port->tx_lock, flags);
+	raw_spin_lock_irqsave(&port->tx_lock, flags);
 	port->to_host = 0;
 	port->rx_throttled_cnt = 0;
 	port->rx_unthrottled_cnt = 0;
 	port->unthrottled_pnd_skbs = 0;
-	spin_unlock_irqrestore(&port->tx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 
-	spin_lock_irqsave(&port->rx_lock, flags);
+	raw_spin_lock_irqsave(&port->rx_lock, flags);
 	port->to_modem = 0;
 	port->tomodem_drp_cnt = 0;
 	port->tx_throttled_cnt = 0;
 	port->tx_unthrottled_cnt = 0;
-	spin_unlock_irqrestore(&port->rx_lock, flags);
+	raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 
 	queue_work(port->wq, &port->connect_w);
 fail:
@@ -1010,7 +1010,7 @@ static ssize_t ghsic_data_read_stats(struct file *file,
 			continue;
 		pdrv = &gdata_ports[i].pdrv;
 
-		spin_lock_irqsave(&port->rx_lock, flags);
+		raw_spin_lock_irqsave(&port->rx_lock, flags);
 		temp += scnprintf(buf + temp, DEBUG_DATA_BUF_SIZE - temp,
 				"\nName:           %s\n"
 				"#PORT:%d port#:   %pK\n"
@@ -1033,9 +1033,9 @@ static ssize_t ghsic_data_read_stats(struct file *file,
 				port->tx_throttled_cnt,
 				port->tx_unthrottled_cnt,
 				test_bit(TX_THROTTLED, &port->brdg.flags));
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 
-		spin_lock_irqsave(&port->tx_lock, flags);
+		raw_spin_lock_irqsave(&port->tx_lock, flags);
 		temp += scnprintf(buf + temp, DEBUG_DATA_BUF_SIZE - temp,
 				"\n******DL INFO******\n\n"
 				"dpkts_to_usbhost: %lu\n"
@@ -1050,7 +1050,7 @@ static ssize_t ghsic_data_read_stats(struct file *file,
 				port->rx_unthrottled_cnt,
 				port->unthrottled_pnd_skbs,
 				test_bit(RX_THROTTLED, &port->brdg.flags));
-		spin_unlock_irqrestore(&port->tx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 
 	}
 
@@ -1073,19 +1073,19 @@ static ssize_t ghsic_data_reset_stats(struct file *file,
 		if (!port)
 			continue;
 
-		spin_lock_irqsave(&port->rx_lock, flags);
+		raw_spin_lock_irqsave(&port->rx_lock, flags);
 		port->to_modem = 0;
 		port->tomodem_drp_cnt = 0;
 		port->tx_throttled_cnt = 0;
 		port->tx_unthrottled_cnt = 0;
-		spin_unlock_irqrestore(&port->rx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->rx_lock, flags);
 
-		spin_lock_irqsave(&port->tx_lock, flags);
+		raw_spin_lock_irqsave(&port->tx_lock, flags);
 		port->to_host = 0;
 		port->rx_throttled_cnt = 0;
 		port->rx_unthrottled_cnt = 0;
 		port->unthrottled_pnd_skbs = 0;
-		spin_unlock_irqrestore(&port->tx_lock, flags);
+		raw_spin_unlock_irqrestore(&port->tx_lock, flags);
 	}
 	return count;
 }
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/u_data_ipa.c b/kernel/msm-3.18/drivers/usb/gadget/function/u_data_ipa.c
index 19f1aea68..d62eb2f24 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/u_data_ipa.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/u_data_ipa.c
@@ -34,7 +34,7 @@ struct ipa_data_ch_info {
 	enum gadget_type	gtype;
 	bool			is_connected;
 	unsigned		port_num;
-	spinlock_t		port_lock;
+	raw_spinlock_t		port_lock;
 
 	struct work_struct	connect_w;
 	struct work_struct	disconnect_w;
@@ -150,9 +150,9 @@ static void ipa_data_disconnect_work(struct work_struct *w)
 	unsigned long flags;
 	int ret;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->is_connected) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_debug("Already disconnected.\n");
 		return;
 	}
@@ -161,7 +161,7 @@ static void ipa_data_disconnect_work(struct work_struct *w)
 			port->ipa_params.prod_clnt_hdl,
 			port->ipa_params.cons_clnt_hdl);
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	ret = usb_bam_disconnect_ipa(port->usb_bam_type, &port->ipa_params);
 	if (ret)
 		pr_err("usb_bam_disconnect_ipa failed: err:%d\n", ret);
@@ -209,7 +209,7 @@ void ipa_data_disconnect(struct gadget_ipa_port *gp, u8 port_num)
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (port->port_usb) {
 		gadget = port->port_usb->cdev->gadget;
 		port->port_usb->ipa_consumer_ep = -1;
@@ -225,24 +225,24 @@ void ipa_data_disconnect(struct gadget_ipa_port *gp, u8 port_num)
 			 */
 			if (gadget_is_dwc3(gadget))
 				msm_ep_unconfig(port->port_usb->in);
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			usb_ep_disable(port->port_usb->in);
-			spin_lock_irqsave(&port->port_lock, flags);
+			raw_spin_lock_irqsave(&port->port_lock, flags);
 			port->port_usb->in->endless = false;
 		}
 
 		if (port->port_usb->out) {
 			if (gadget_is_dwc3(gadget))
 				msm_ep_unconfig(port->port_usb->out);
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			usb_ep_disable(port->port_usb->out);
-			spin_lock_irqsave(&port->port_lock, flags);
+			raw_spin_lock_irqsave(&port->port_lock, flags);
 			port->port_usb->out->endless = false;
 		}
 
 		port->port_usb = NULL;
 	}
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	queue_work(ipa_data_wq, &port->disconnect_w);
 }
 
@@ -290,10 +290,10 @@ static void ipa_data_connect_work(struct work_struct *w)
 
 	pr_debug("%s: Connect workqueue started", __func__);
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s(): port_usb is NULL.\n", __func__);
 		return;
 	}
@@ -303,7 +303,7 @@ static void ipa_data_connect_work(struct work_struct *w)
 		gadget = gport->cdev->gadget;
 
 	if (!gadget) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s: gport is NULL.\n", __func__);
 		return;
 	}
@@ -313,7 +313,7 @@ static void ipa_data_connect_work(struct work_struct *w)
 	if (gport->out) {
 		port->rx_req = usb_ep_alloc_request(gport->out, GFP_ATOMIC);
 		if (!port->rx_req) {
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			pr_err("%s: failed to allocate rx_req\n", __func__);
 			return;
 		}
@@ -326,7 +326,7 @@ static void ipa_data_connect_work(struct work_struct *w)
 	if (gport->in) {
 		port->tx_req = usb_ep_alloc_request(gport->in, GFP_ATOMIC);
 		if (!port->tx_req) {
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			pr_err("%s: failed to allocate tx_req\n", __func__);
 			goto free_rx_req;
 		}
@@ -337,7 +337,7 @@ static void ipa_data_connect_work(struct work_struct *w)
 	}
 
 	port->is_connected = true;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	/* update IPA Parameteres here. */
 	port->ipa_params.usb_connection_speed = gadget->speed;
@@ -479,9 +479,9 @@ unconfig_msm_ep_out:
 						port->src_connection_idx);
 	}
 free_rx_tx_req:
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->is_connected = false;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	if (gport->in && port->tx_req)
 		usb_ep_free_request(gport->in, port->tx_req);
 free_rx_req:
@@ -524,7 +524,7 @@ int ipa_data_connect(struct gadget_ipa_port *gp, u8 port_num,
 
 	port = ipa_data_ports[port_num];
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->port_usb = gp;
 	port->src_connection_idx = src_connection_idx;
 	port->dst_connection_idx = dst_connection_idx;
@@ -570,7 +570,7 @@ int ipa_data_connect(struct gadget_ipa_port *gp, u8 port_num,
 	}
 
 	queue_work(ipa_data_wq, &port->connect_w);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	return ret;
 
@@ -578,7 +578,7 @@ err_usb_out:
 	if (port->port_usb->in)
 		port->port_usb->in->endless = false;
 err_usb_in:
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 err:
 	pr_debug("%s(): failed with error:%d\n", __func__, ret);
 	return ret;
@@ -726,10 +726,10 @@ void ipa_data_resume(struct gadget_ipa_port *gp, u8 port_num)
 	}
 
 	pr_debug("%s: resume started\n", __func__);
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	gadget = port->port_usb->cdev->gadget;
 	if (!gadget) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s(): Gadget is NULL.\n", __func__);
 		return;
 	}
@@ -737,7 +737,7 @@ void ipa_data_resume(struct gadget_ipa_port *gp, u8 port_num)
 	ret = usb_bam_register_wake_cb(port->usb_bam_type,
 				port->dst_connection_idx, NULL, NULL);
 	if (ret) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		pr_err("%s(): Failed to register BAM wake callback.\n",
 								__func__);
 		return;
@@ -748,13 +748,13 @@ void ipa_data_resume(struct gadget_ipa_port *gp, u8 port_num)
 				port->port_usb->out);
 		configure_fifo(port->usb_bam_type, port->dst_connection_idx,
 				port->port_usb->in);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		msm_dwc3_reset_dbm_ep(port->port_usb->in);
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		usb_bam_resume(port->usb_bam_type, &port->ipa_params);
 	}
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 /**
@@ -806,7 +806,7 @@ void ipa_data_port_select(int portno, enum gadget_type gtype)
 	port->port_num  = portno;
 	port->is_connected = false;
 
-	spin_lock_init(&port->port_lock);
+	raw_spin_lock_init(&port->port_lock);
 
 	if (!work_pending(&port->connect_w))
 		INIT_WORK(&port->connect_w, ipa_data_connect_work);
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/u_qc_ether.c b/kernel/msm-3.18/drivers/usb/gadget/function/u_qc_ether.c
index 223f33a6c..4a0998440 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/u_qc_ether.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/u_qc_ether.c
@@ -61,7 +61,7 @@ struct eth_qc_dev {
 	/* lock is held while accessing port_usb
 	 * or updating its backlink port_usb->ioport
 	 */
-	spinlock_t		lock;
+	raw_spinlock_t		lock;
 	struct qc_gether	*port_usb;
 
 	struct net_device	*net;
@@ -114,14 +114,14 @@ static int ueth_qc_change_mtu(struct net_device *net, int new_mtu)
 	int		status = 0;
 
 	/* don't change MTU on "live" link (peer won't know) */
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	if (dev->port_usb)
 		status = -EBUSY;
 	else if (new_mtu <= ETH_HLEN || new_mtu > ETH_FRAME_LEN)
 		status = -ERANGE;
 	else
 		net->mtu = new_mtu;
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	return status;
 }
@@ -163,11 +163,11 @@ static int eth_qc_open(struct net_device *net)
 		netif_wake_queue(dev->net);
 	}
 
-	spin_lock_irq(&dev->lock);
+	raw_spin_lock_irq(&dev->lock);
 	link = dev->port_usb;
 	if (link && link->open)
 		link->open(link);
-	spin_unlock_irq(&dev->lock);
+	raw_spin_unlock_irq(&dev->lock);
 
 	return 0;
 }
@@ -181,10 +181,10 @@ static int eth_qc_stop(struct net_device *net)
 	VDBG(dev, "%s\n", __func__);
 	netif_stop_queue(net);
 
-	spin_lock_irqsave(&dev->lock, flags);
+	raw_spin_lock_irqsave(&dev->lock, flags);
 	if (dev->port_usb && link->close)
 			link->close(link);
-	spin_unlock_irqrestore(&dev->lock, flags);
+	raw_spin_unlock_irqrestore(&dev->lock, flags);
 
 	return 0;
 }
@@ -287,7 +287,7 @@ int gether_qc_setup_name(struct usb_gadget *g, u8 ethaddr[ETH_ALEN],
 		return -ENOMEM;
 
 	dev = netdev_priv(net);
-	spin_lock_init(&dev->lock);
+	raw_spin_lock_init(&dev->lock);
 
 	/* network device setup */
 	dev->net = net;
@@ -393,7 +393,7 @@ struct net_device *gether_qc_connect_name(struct qc_gether *link,
 	dev->zlp = link->is_zlp_ok;
 	dev->header_len = link->header_len;
 
-	spin_lock(&dev->lock);
+	raw_spin_lock(&dev->lock);
 	dev->port_usb = link;
 	link->ioport = dev;
 	if (netif_running(dev->net)) {
@@ -403,7 +403,7 @@ struct net_device *gether_qc_connect_name(struct qc_gether *link,
 		if (link->close)
 			link->close(link);
 	}
-	spin_unlock(&dev->lock);
+	raw_spin_unlock(&dev->lock);
 
 	if (netif_enable) {
 		netif_carrier_on(dev->net);
@@ -447,8 +447,8 @@ void gether_qc_disconnect_name(struct qc_gether *link, const char *netname)
 	netif_stop_queue(dev->net);
 	netif_carrier_off(dev->net);
 
-	spin_lock(&dev->lock);
+	raw_spin_lock(&dev->lock);
 	dev->port_usb = NULL;
 	link->ioport = NULL;
-	spin_unlock(&dev->lock);
+	raw_spin_unlock(&dev->lock);
 }
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/u_rmnet_ctrl_smd.c b/kernel/msm-3.18/drivers/usb/gadget/function/u_rmnet_ctrl_smd.c
index f5c9082a6..add4477ae 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/u_rmnet_ctrl_smd.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/u_rmnet_ctrl_smd.c
@@ -72,7 +72,7 @@ struct rmnet_ctrl_port {
 	unsigned int		port_num;
 	struct grmnet		*port_usb;
 
-	spinlock_t		port_lock;
+	raw_spinlock_t		port_lock;
 	struct delayed_work	connect_w;
 	struct delayed_work	disconnect_w;
 };
@@ -123,13 +123,13 @@ static void grmnet_ctrl_smd_read_w(struct work_struct *w)
 	void *buf;
 	unsigned long flags;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	while (c->ch) {
 		sz = smd_cur_packet_size(c->ch);
 		if (sz <= 0)
 			break;
 
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 		buf = kmalloc(sz, GFP_KERNEL);
 		if (!buf)
@@ -166,7 +166,7 @@ static void grmnet_ctrl_smd_read_w(struct work_struct *w)
 		}
 
 		/* send it to USB here */
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		if (port->port_usb && port->port_usb->send_cpkt_response) {
 			port->port_usb->send_cpkt_response(port->port_usb,
 							buf, sz);
@@ -174,7 +174,7 @@ static void grmnet_ctrl_smd_read_w(struct work_struct *w)
 		}
 		kfree(buf);
 	}
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 static void grmnet_ctrl_smd_write_w(struct work_struct *w)
@@ -185,7 +185,7 @@ static void grmnet_ctrl_smd_write_w(struct work_struct *w)
 	struct rmnet_ctrl_pkt *cpkt;
 	int ret;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	while (c->ch) {
 		if (list_empty(&c->tx_q))
 			break;
@@ -196,9 +196,9 @@ static void grmnet_ctrl_smd_write_w(struct work_struct *w)
 			break;
 
 		list_del(&cpkt->list);
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		ret = smd_write(c->ch, cpkt->buf, cpkt->len);
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		if (ret != cpkt->len) {
 			pr_err("%s: smd_write failed err:%d\n", __func__, ret);
 			free_rmnet_ctrl_pkt(cpkt);
@@ -207,7 +207,7 @@ static void grmnet_ctrl_smd_write_w(struct work_struct *w)
 		free_rmnet_ctrl_pkt(cpkt);
 		c->to_modem++;
 	}
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 static int is_legal_port_num(u8 portno)
 {
@@ -244,7 +244,7 @@ grmnet_ctrl_smd_send_cpkt_tomodem(u8 portno,
 	memcpy(cpkt->buf, buf, len);
 	cpkt->len = len;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	c = &port->ctrl_ch;
 
 	/* queue cpkt if ch is not open, would be sent once ch is opened */
@@ -257,13 +257,13 @@ grmnet_ctrl_smd_send_cpkt_tomodem(u8 portno,
 			pr_debug("%s: Dropping SMD CTRL packet: limit %u\n",
 					__func__, c->offline_pkt_for_modem);
 		}
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return 0;
 	}
 
 	list_add_tail(&cpkt->list, &c->tx_q);
 	queue_work(grmnet_ctrl_wq, &c->write_w);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	return 0;
 }
@@ -366,7 +366,7 @@ static void grmnet_ctrl_smd_notify(void *p, unsigned event)
 		if (port && port->port_usb && port->port_usb->disconnect)
 			port->port_usb->disconnect(port->port_usb);
 
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		while (!list_empty(&c->tx_q)) {
 			cpkt = list_first_entry(&c->tx_q,
 					struct rmnet_ctrl_pkt, list);
@@ -374,7 +374,7 @@ static void grmnet_ctrl_smd_notify(void *p, unsigned event)
 			list_del(&cpkt->list);
 			free_rmnet_ctrl_pkt(cpkt);
 		}
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 		break;
 	}
@@ -423,10 +423,10 @@ static void grmnet_ctrl_smd_connect_w(struct work_struct *w)
 
 	set_bits = c->cbits_tomodem;
 	clear_bits = ~(c->cbits_tomodem | TIOCM_RTS);
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (port->port_usb)
 		smd_tiocmset(c->ch, set_bits, clear_bits);
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 int gsmd_ctrl_connect(struct grmnet *gr, int port_num)
@@ -450,11 +450,11 @@ int gsmd_ctrl_connect(struct grmnet *gr, int port_num)
 	port = ctrl_smd_ports[port_num].port;
 	c = &port->ctrl_ch;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->port_usb = gr;
 	gr->send_encap_cmd = grmnet_ctrl_smd_send_cpkt_tomodem;
 	gr->notify_modem = gsmd_ctrl_send_cbits_tomodem;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	queue_delayed_work(grmnet_ctrl_wq, &port->connect_w, 0);
 
@@ -506,7 +506,7 @@ void gsmd_ctrl_disconnect(struct grmnet *gr, u8 port_num)
 	port = ctrl_smd_ports[port_num].port;
 	c = &port->ctrl_ch;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->port_usb = 0;
 	gr->send_encap_cmd = 0;
 	gr->notify_modem = 0;
@@ -520,7 +520,7 @@ void gsmd_ctrl_disconnect(struct grmnet *gr, u8 port_num)
 	}
 	c->offline_pkt_for_modem = 0;
 
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	if (test_and_clear_bit(CH_OPENED, &c->flags)) {
 		clear_bits = ~(c->cbits_tomodem | TIOCM_RTS);
@@ -553,11 +553,11 @@ static int grmnet_ctrl_smd_ch_probe(struct platform_device *pdev)
 			set_bit(CH_READY, &c->flags);
 
 			/* if usb is online, try opening smd_ch */
-			spin_lock_irqsave(&port->port_lock, flags);
+			raw_spin_lock_irqsave(&port->port_lock, flags);
 			if (port->port_usb)
 				queue_delayed_work(grmnet_ctrl_wq,
 							&port->connect_w, 0);
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 			break;
 		}
@@ -624,7 +624,7 @@ static int grmnet_ctrl_smd_port_alloc(int portno)
 
 	port->port_num = portno;
 
-	spin_lock_init(&port->port_lock);
+	raw_spin_lock_init(&port->port_lock);
 	INIT_DELAYED_WORK(&port->connect_w, grmnet_ctrl_smd_connect_w);
 	INIT_DELAYED_WORK(&port->disconnect_w, grmnet_ctrl_smd_disconnect_w);
 
@@ -730,7 +730,7 @@ static ssize_t gsmd_ctrl_read_stats(struct file *file, char __user *ubuf,
 			continue;
 		port = ctrl_smd_ports[i].port;
 
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 
 		c = &port->ctrl_ch;
 
@@ -751,7 +751,7 @@ static ssize_t gsmd_ctrl_read_stats(struct file *file, char __user *ubuf,
 				c->ch ? smd_read_avail(c->ch) : 0,
 				c->ch ? smd_write_avail(c->ch) : 0);
 
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	}
 
 	ret = simple_read_from_buffer(ubuf, count, ppos, buf, temp);
@@ -774,14 +774,14 @@ static ssize_t gsmd_ctrl_reset_stats(struct file *file, const char __user *buf,
 			continue;
 		port = ctrl_smd_ports[i].port;
 
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 
 		c = &port->ctrl_ch;
 
 		c->to_host = 0;
 		c->to_modem = 0;
 
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	}
 	return count;
 }
diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/u_smd.c b/kernel/msm-3.18/drivers/usb/gadget/function/u_smd.c
index 3efaaf27f..3247ef96e 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/u_smd.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/u_smd.c
@@ -58,7 +58,7 @@ struct smd_port_info smd_pi[SMD_N_PORTS] = {
 
 struct gsmd_port {
 	unsigned		port_num;
-	spinlock_t		port_lock;
+	raw_spinlock_t		port_lock;
 
 	unsigned		n_read;
 	struct list_head	read_pool;
@@ -177,7 +177,7 @@ static void gsmd_start_rx(struct gsmd_port *port)
 		return;
 	}
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 
 	if (!port->port_usb) {
 		pr_debug("%s: USB disconnected\n", __func__);
@@ -194,9 +194,9 @@ static void gsmd_start_rx(struct gsmd_port *port)
 		list_del(&req->list);
 		req->length = SMD_RX_BUF_SIZE;
 
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		ret = usb_ep_queue(out, req, GFP_KERNEL);
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		if (ret) {
 			pr_err("%s: usb ep out queue failed"
 					"port:%pK, port#%d\n",
@@ -206,7 +206,7 @@ static void gsmd_start_rx(struct gsmd_port *port)
 		}
 	}
 start_rx_end:
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 }
 
 static void gsmd_rx_push(struct work_struct *w)
@@ -217,7 +217,7 @@ static void gsmd_rx_push(struct work_struct *w)
 
 	pr_debug("%s: port:%pK port#%d", __func__, port, port->port_num);
 
-	spin_lock_irq(&port->port_lock);
+	raw_spin_lock_irq(&port->port_lock);
 
 	q = &port->read_queue;
 	while (pi->ch && !list_empty(q)) {
@@ -276,7 +276,7 @@ static void gsmd_rx_push(struct work_struct *w)
 	}
 
 rx_push_end:
-	spin_unlock_irq(&port->port_lock);
+	raw_spin_unlock_irq(&port->port_lock);
 
 	gsmd_start_rx(port);
 }
@@ -308,11 +308,11 @@ static void gsmd_tx_pull(struct work_struct *w)
 	pr_debug("%s: port:%pK port#%d pool:%pK\n", __func__,
 			port, port->port_num, pool);
 
-	spin_lock_irq(&port->port_lock);
+	raw_spin_lock_irq(&port->port_lock);
 
 	if (!port->port_usb) {
 		pr_debug("%s: usb is disconnected\n", __func__);
-		spin_unlock_irq(&port->port_lock);
+		raw_spin_unlock_irq(&port->port_lock);
 		gsmd_read_pending(port);
 		return;
 	}
@@ -321,7 +321,7 @@ static void gsmd_tx_pull(struct work_struct *w)
 	func = &port->port_usb->func;
 	gadget = func->config->cdev->gadget;
 	if (port->is_suspended) {
-		spin_unlock_irq(&port->port_lock);
+		raw_spin_unlock_irq(&port->port_lock);
 		if ((gadget->speed == USB_SPEED_SUPER) &&
 		    (func->func_is_suspended))
 			ret = usb_func_wakeup(func);
@@ -333,14 +333,14 @@ static void gsmd_tx_pull(struct work_struct *w)
 		else if (ret)
 			pr_err("Failed to wake up the USB core. ret=%d\n", ret);
 
-		spin_lock_irq(&port->port_lock);
+		raw_spin_lock_irq(&port->port_lock);
 		if (!port->port_usb) {
 			pr_debug("%s: USB disconnected\n", __func__);
-			spin_unlock_irq(&port->port_lock);
+			raw_spin_unlock_irq(&port->port_lock);
 			gsmd_read_pending(port);
 			return;
 		}
-		spin_unlock_irq(&port->port_lock);
+		raw_spin_unlock_irq(&port->port_lock);
 		return;
 	}
 
@@ -360,9 +360,9 @@ static void gsmd_tx_pull(struct work_struct *w)
 		req->length = smd_read(pi->ch, req->buf, avail);
 		req->zero = 1;
 
-		spin_unlock_irq(&port->port_lock);
+		raw_spin_unlock_irq(&port->port_lock);
 		ret = usb_ep_queue(in, req, GFP_KERNEL);
-		spin_lock_irq(&port->port_lock);
+		raw_spin_lock_irq(&port->port_lock);
 		if (ret) {
 			pr_err("%s: usb ep in queue failed"
 					"port:%pK, port#%d err:%d\n",
@@ -383,7 +383,7 @@ tx_pull_end:
 	if (port->port_usb && smd_read_avail(port->pi->ch) && !list_empty(pool))
 		queue_work(gsmd_wq, &port->pull);
 
-	spin_unlock_irq(&port->port_lock);
+	raw_spin_unlock_irq(&port->port_lock);
 
 	return;
 }
@@ -399,17 +399,17 @@ static void gsmd_read_complete(struct usb_ep *ep, struct usb_request *req)
 		return;
 	}
 
-	spin_lock(&port->port_lock);
+	raw_spin_lock(&port->port_lock);
 	if (!test_bit(CH_OPENED, &port->pi->flags) ||
 			req->status == -ESHUTDOWN) {
-		spin_unlock(&port->port_lock);
+		raw_spin_unlock(&port->port_lock);
 		gsmd_free_req(ep, req);
 		return;
 	}
 
 	list_add_tail(&req->list, &port->read_queue);
 	queue_work(gsmd_wq, &port->push);
-	spin_unlock(&port->port_lock);
+	raw_spin_unlock(&port->port_lock);
 
 	return;
 }
@@ -425,10 +425,10 @@ static void gsmd_write_complete(struct usb_ep *ep, struct usb_request *req)
 		return;
 	}
 
-	spin_lock(&port->port_lock);
+	raw_spin_lock(&port->port_lock);
 	if (!test_bit(CH_OPENED, &port->pi->flags) ||
 			req->status == -ESHUTDOWN) {
-		spin_unlock(&port->port_lock);
+		raw_spin_unlock(&port->port_lock);
 		gsmd_free_req(ep, req);
 		return;
 	}
@@ -440,7 +440,7 @@ static void gsmd_write_complete(struct usb_ep *ep, struct usb_request *req)
 
 	list_add(&req->list, &port->write_pool);
 	queue_work(gsmd_wq, &port->pull);
-	spin_unlock(&port->port_lock);
+	raw_spin_unlock(&port->port_lock);
 
 	return;
 }
@@ -451,7 +451,7 @@ static void gsmd_start_io(struct gsmd_port *port)
 
 	pr_debug("%s: port: %pK\n", __func__, port);
 
-	spin_lock(&port->port_lock);
+	raw_spin_lock(&port->port_lock);
 
 	if (!port->port_usb)
 		goto start_io_out;
@@ -482,7 +482,7 @@ static void gsmd_start_io(struct gsmd_port *port)
 	}
 
 start_io_out:
-	spin_unlock(&port->port_lock);
+	raw_spin_unlock(&port->port_lock);
 
 	if (ret)
 		return;
@@ -529,19 +529,19 @@ static void gsmd_stop_io(struct gsmd_port *port)
 	struct usb_ep	*out;
 	unsigned long	flags;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	if (!port->port_usb) {
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 		return;
 	}
 	in = port->port_usb->in;
 	out = port->port_usb->out;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	usb_ep_fifo_flush(in);
 	usb_ep_fifo_flush(out);
 
-	spin_lock(&port->port_lock);
+	raw_spin_lock(&port->port_lock);
 	if (port->port_usb) {
 		gsmd_free_requests(out, &port->read_pool);
 		gsmd_free_requests(out, &port->read_queue);
@@ -549,7 +549,7 @@ static void gsmd_stop_io(struct gsmd_port *port)
 		port->n_read = 0;
 		port->cbits_to_laptop = 0;
 	} else {
-		spin_unlock(&port->port_lock);
+		raw_spin_unlock(&port->port_lock);
 		return;
 	}
 
@@ -557,7 +557,7 @@ static void gsmd_stop_io(struct gsmd_port *port)
 		port->port_usb->send_modem_ctrl_bits(
 					port->port_usb,
 					port->cbits_to_laptop);
-	spin_unlock(&port->port_lock);
+	raw_spin_unlock(&port->port_lock);
 
 }
 
@@ -715,13 +715,13 @@ int gsmd_connect(struct gserial *gser, u8 portno)
 
 	port = smd_ports[portno].port;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->port_usb = gser;
 	gser->notify_modem = gsmd_notify_modem;
 	port->nbytes_tomodem = 0;
 	port->nbytes_tolaptop = 0;
 	port->is_suspended = false;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	ret = usb_ep_enable(gser->in);
 	if (ret) {
@@ -766,10 +766,10 @@ void gsmd_disconnect(struct gserial *gser, u8 portno)
 
 	port = smd_ports[portno].port;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	port->port_usb = 0;
 	port->is_suspended = false;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	/* disable endpoints, aborting down any active I/O */
 	usb_ep_disable(gser->out);
@@ -777,12 +777,12 @@ void gsmd_disconnect(struct gserial *gser, u8 portno)
 	usb_ep_disable(gser->in);
 	gser->in->driver_data = NULL;
 
-	spin_lock_irqsave(&port->port_lock, flags);
+	raw_spin_lock_irqsave(&port->port_lock, flags);
 	gsmd_free_requests(gser->out, &port->read_pool);
 	gsmd_free_requests(gser->out, &port->read_queue);
 	gsmd_free_requests(gser->in, &port->write_pool);
 	port->n_read = 0;
-	spin_unlock_irqrestore(&port->port_lock, flags);
+	raw_spin_unlock_irqrestore(&port->port_lock, flags);
 
 	if (test_and_clear_bit(CH_OPENED, &port->pi->flags)) {
 		/* lower the dtr */
@@ -814,11 +814,11 @@ static int gsmd_ch_probe(struct platform_device *pdev)
 
 		if (!strncmp(pi->name, pdev->name, SMD_CH_MAX_LEN)) {
 			set_bit(CH_READY, &pi->flags);
-			spin_lock_irqsave(&port->port_lock, flags);
+			raw_spin_lock_irqsave(&port->port_lock, flags);
 			if (port->port_usb)
 				queue_delayed_work(gsmd_wq, &port->connect_work,
 					msecs_to_jiffies(0));
-			spin_unlock_irqrestore(&port->port_lock, flags);
+			raw_spin_unlock_irqrestore(&port->port_lock, flags);
 			break;
 		}
 	}
@@ -870,7 +870,7 @@ static int gsmd_port_alloc(int portno, struct usb_cdc_line_coding *coding)
 	port->port_num = portno;
 	port->pi = &smd_pi[portno];
 
-	spin_lock_init(&port->port_lock);
+	raw_spin_lock_init(&port->port_lock);
 
 	INIT_LIST_HEAD(&port->read_pool);
 	INIT_LIST_HEAD(&port->read_queue);
@@ -914,7 +914,7 @@ static ssize_t debug_smd_read_stats(struct file *file, char __user *ubuf,
 	for (i = 0; i < n_smd_ports; i++) {
 		port = smd_ports[i].port;
 		pi = port->pi;
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		temp += scnprintf(buf + temp, 512 - temp,
 				"###PORT:%d###\n"
 				"nbytes_tolaptop: %lu\n"
@@ -933,7 +933,7 @@ static ssize_t debug_smd_read_stats(struct file *file, char __user *ubuf,
 				pi->ch ? smd_write_avail(pi->ch) : 0,
 				test_bit(CH_OPENED, &pi->flags),
 				test_bit(CH_READY, &pi->flags));
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	}
 
 	ret = simple_read_from_buffer(ubuf, count, ppos, buf, temp);
@@ -954,10 +954,10 @@ static ssize_t debug_smd_reset_stats(struct file *file, const char __user *buf,
 	for (i = 0; i < n_smd_ports; i++) {
 		port = smd_ports[i].port;
 
-		spin_lock_irqsave(&port->port_lock, flags);
+		raw_spin_lock_irqsave(&port->port_lock, flags);
 		port->nbytes_tolaptop = 0;
 		port->nbytes_tomodem = 0;
-		spin_unlock_irqrestore(&port->port_lock, flags);
+		raw_spin_unlock_irqrestore(&port->port_lock, flags);
 	}
 
 	return count;
@@ -1045,9 +1045,9 @@ void gsmd_suspend(struct gserial *gser, u8 portno)
 	pr_debug("%s: gserial:%pK portno:%u\n", __func__, gser, portno);
 
 	port = smd_ports[portno].port;
-	spin_lock(&port->port_lock);
+	raw_spin_lock(&port->port_lock);
 	port->is_suspended = true;
-	spin_unlock(&port->port_lock);
+	raw_spin_unlock(&port->port_lock);
 }
 
 void gsmd_resume(struct gserial *gser, u8 portno)
@@ -1057,9 +1057,9 @@ void gsmd_resume(struct gserial *gser, u8 portno)
 	pr_debug("%s: gserial:%pK portno:%u\n", __func__, gser, portno);
 
 	port = smd_ports[portno].port;
-	spin_lock(&port->port_lock);
+	raw_spin_lock(&port->port_lock);
 	port->is_suspended = false;
-	spin_unlock(&port->port_lock);
+	raw_spin_unlock(&port->port_lock);
 	queue_work(gsmd_wq, &port->pull);
 }
 
diff --git a/kernel/msm-3.18/drivers/usb/host/ehci-msm-hsic.c b/kernel/msm-3.18/drivers/usb/host/ehci-msm-hsic.c
index 5948a90c2..7ac1c5b8b 100644
--- a/kernel/msm-3.18/drivers/usb/host/ehci-msm-hsic.c
+++ b/kernel/msm-3.18/drivers/usb/host/ehci-msm-hsic.c
@@ -75,7 +75,7 @@ struct ehci_timer {
 
 struct msm_hsic_hcd {
 	struct ehci_hcd		ehci;
-	spinlock_t		wakeup_lock;
+	raw_spinlock_t		wakeup_lock;
 	struct device		*dev;
 	struct clk		*ahb_clk;
 	struct clk		*core_clk;
@@ -929,13 +929,13 @@ static int msm_hsic_resume(struct msm_hsic_hcd *mehci)
 			pdata->standalone_latency + 1);
 
 	if (mehci->wakeup_irq) {
-		spin_lock_irqsave(&mehci->wakeup_lock, flags);
+		raw_spin_lock_irqsave(&mehci->wakeup_lock, flags);
 		if (mehci->wakeup_irq_enabled) {
 			disable_irq_wake(mehci->wakeup_irq);
 			disable_irq_nosync(mehci->wakeup_irq);
 			mehci->wakeup_irq_enabled = 0;
 		}
-		spin_unlock_irqrestore(&mehci->wakeup_lock, flags);
+		raw_spin_unlock_irqrestore(&mehci->wakeup_lock, flags);
 	}
 
 	wake_lock(&mehci->wlock);
@@ -1204,7 +1204,7 @@ retry:
 	dbg_log_event(NULL, "RESET: start", retries);
 	pr_debug("reset begin %d\n", retries);
 	mehci->reset_again = 0;
-	spin_lock_irqsave(&ehci->lock, flags);
+	raw_spin_lock_irqsave(&ehci->lock, flags);
 	ehci_writel(ehci, val, status_reg);
 	ehci_writel(ehci, GPT_LD(RESET_SIGNAL_TIME_USEC - 1),
 					&mehci->timer->gptimer0_ld);
@@ -1218,7 +1218,7 @@ retry:
 	ehci_writel(ehci, GPT_RESET | GPT_RUN,
 		&mehci->timer->gptimer1_ctrl);
 
-	spin_unlock_irqrestore(&ehci->lock, flags);
+	raw_spin_unlock_irqrestore(&ehci->lock, flags);
 	wait_for_completion(&mehci->gpt0_completion);
 
 	if (!mehci->reset_again)
@@ -1237,12 +1237,12 @@ retry:
 	pr_info("RESET in tight loop\n");
 	dbg_log_event(NULL, "RESET: tight", 0);
 
-	spin_lock_irqsave(&ehci->lock, flags);
+	raw_spin_lock_irqsave(&ehci->lock, flags);
 	ehci_writel(ehci, val, status_reg);
 	while (cnt--)
 		udelay(1);
 	ret = msm_hsic_reset_done(hcd);
-	spin_unlock_irqrestore(&ehci->lock, flags);
+	raw_spin_unlock_irqrestore(&ehci->lock, flags);
 	if (ret) {
 		pr_err("RESET in tight loop failed\n");
 		dbg_log_event(NULL, "RESET: tight failed", 0);
@@ -1303,7 +1303,7 @@ static int msm_hsic_resume_thread(void *data)
 	if (time_before_eq(jiffies, ehci->next_statechange))
 		usleep_range(10000, 10000);
 
-	spin_lock_irq(&ehci->lock);
+	raw_spin_lock_irq(&ehci->lock);
 	if (!HCD_HW_ACCESSIBLE(hcd)) {
 		mehci->resume_status = -ESHUTDOWN;
 		goto exit;
@@ -1377,9 +1377,9 @@ resume_again:
 			ehci_writel(ehci, GPT_RESET | GPT_RUN,
 				&mehci->timer->gptimer1_ctrl);
 
-			spin_unlock_irq(&ehci->lock);
+			raw_spin_unlock_irq(&ehci->lock);
 			wait_for_completion(&mehci->gpt0_completion);
-			spin_lock_irq(&ehci->lock);
+			raw_spin_lock_irq(&ehci->lock);
 		} else {
 			dbg_log_event(NULL, "FPR: Tightloop", 0);
 			/* do the resume in a tight loop */
@@ -1395,7 +1395,7 @@ resume_again:
 
 			dbg_log_event(NULL, "FPR: Re-Resume", retry_cnt);
 			pr_info("FPR: retry count: %d\n", retry_cnt);
-			spin_unlock_irq(&ehci->lock);
+			raw_spin_unlock_irq(&ehci->lock);
 			temp = ehci_readl(ehci, &ehci->regs->port_status[0]);
 			temp &= ~PORT_RWC_BITS;
 			temp |= PORT_SUSPEND;
@@ -1407,7 +1407,7 @@ resume_again:
 			dbg_log_event(NULL,
 				"FPR: RResume",
 				ehci_readl(ehci, &ehci->regs->port_status[0]));
-			spin_lock_irq(&ehci->lock);
+			raw_spin_lock_irq(&ehci->lock);
 			mehci->resume_again = 0;
 			retry_cnt++;
 			goto resume_again;
@@ -1417,7 +1417,7 @@ resume_again:
 	dbg_log_event(NULL, "FPR: RT-Done", 0);
 	mehci->resume_status = 1;
 exit:
-	spin_unlock_irq(&ehci->lock);
+	raw_spin_unlock_irq(&ehci->lock);
 	complete(&mehci->rt_completion);
 	if (next_latency)
 		pm_qos_update_request(&mehci->pm_qos_req_dma, next_latency);
@@ -1453,7 +1453,7 @@ static int ehci_hsic_bus_resume(struct usb_hcd *hcd)
 			return mehci->resume_status;
 
 		dbg_log_event(NULL, "FPR: Wokeup", 0);
-		spin_lock_irq(&ehci->lock);
+		raw_spin_lock_irq(&ehci->lock);
 
 		ehci->next_statechange = jiffies + msecs_to_jiffies(5);
 		hcd->state = HC_STATE_RUNNING;
@@ -1464,7 +1464,7 @@ static int ehci_hsic_bus_resume(struct usb_hcd *hcd)
 		ehci_writel(ehci, INTR_MASK, &ehci->regs->intr_enable);
 		(void) ehci_readl(ehci, &ehci->regs->intr_enable);
 
-		spin_unlock_irq(&ehci->lock);
+		raw_spin_unlock_irq(&ehci->lock);
 	}
 
 	if (pdata->resume_gpio)
@@ -1704,13 +1704,13 @@ static irqreturn_t msm_hsic_wakeup_irq(int irq, void *data)
 	wake_lock(&mehci->wlock);
 
 	if (mehci->wakeup_irq) {
-		spin_lock(&mehci->wakeup_lock);
+		raw_spin_lock(&mehci->wakeup_lock);
 		if (mehci->wakeup_irq_enabled) {
 			mehci->wakeup_irq_enabled = 0;
 			disable_irq_wake(irq);
 			disable_irq_nosync(irq);
 		}
-		spin_unlock(&mehci->wakeup_lock);
+		raw_spin_unlock(&mehci->wakeup_lock);
 	}
 
 	if (!atomic_read(&mehci->pm_usage_cnt)) {
@@ -2071,7 +2071,7 @@ static int ehci_hsic_msm_probe(struct platform_device *pdev)
 		}
 	}
 
-	spin_lock_init(&mehci->wakeup_lock);
+	raw_spin_lock_init(&mehci->wakeup_lock);
 
 	if (pdata->phy_sof_workaround) {
 		/* Enable ALL workarounds related to PHY SOF bugs */
diff --git a/kernel/msm-3.18/drivers/usb/host/hbm.c b/kernel/msm-3.18/drivers/usb/host/hbm.c
index d5163bec7..060d24977 100644
--- a/kernel/msm-3.18/drivers/usb/host/hbm.c
+++ b/kernel/msm-3.18/drivers/usb/host/hbm.c
@@ -227,7 +227,7 @@ static int hbm_submit_async(struct ehci_hcd *ehci, struct urb *urb,
 
 	epnum = urb->ep->desc.bEndpointAddress;
 
-	spin_lock_irqsave(&ehci->lock, flags);
+	raw_spin_lock_irqsave(&ehci->lock, flags);
 	if (unlikely(!HCD_HW_ACCESSIBLE(ehci_to_hcd(ehci)))) {
 		rc = -ESHUTDOWN;
 		goto done;
@@ -249,7 +249,7 @@ static int hbm_submit_async(struct ehci_hcd *ehci, struct urb *urb,
 		qh_link_async(ehci, qh);
 
 done:
-	spin_unlock_irqrestore(&ehci->lock, flags);
+	raw_spin_unlock_irqrestore(&ehci->lock, flags);
 	if (unlikely(qh == NULL))
 		qtd_list_free(ehci, urb, qtd_list);
 	return rc;
diff --git a/kernel/msm-3.18/drivers/usb/misc/ks_bridge.c b/kernel/msm-3.18/drivers/usb/misc/ks_bridge.c
index e98fbde7b..c5029f2aa 100644
--- a/kernel/msm-3.18/drivers/usb/misc/ks_bridge.c
+++ b/kernel/msm-3.18/drivers/usb/misc/ks_bridge.c
@@ -76,7 +76,7 @@ struct ksb_dev_info {
 
 struct ks_bridge {
 	char			*name;
-	spinlock_t		lock;
+	raw_spinlock_t		lock;
 	struct workqueue_struct	*wq;
 	struct work_struct	to_mdm_work;
 	struct work_struct	start_rx_work;
@@ -187,9 +187,9 @@ read_start:
 	if (!test_bit(USB_DEV_CONNECTED, &ksb->flags))
 		return -ENODEV;
 
-	spin_lock_irqsave(&ksb->lock, flags);
+	raw_spin_lock_irqsave(&ksb->lock, flags);
 	if (list_empty(&ksb->to_ks_list)) {
-		spin_unlock_irqrestore(&ksb->lock, flags);
+		raw_spin_unlock_irqrestore(&ksb->lock, flags);
 		ret = wait_event_interruptible(ksb->ks_wait_q,
 				!list_empty(&ksb->to_ks_list) ||
 				!test_bit(USB_DEV_CONNECTED, &ksb->flags));
@@ -208,7 +208,7 @@ read_start:
 		pkt = list_first_entry(&ksb->to_ks_list, struct data_pkt, list);
 		list_del_init(&pkt->list);
 		len = min_t(size_t, space, pkt->len - pkt->n_read);
-		spin_unlock_irqrestore(&ksb->lock, flags);
+		raw_spin_unlock_irqrestore(&ksb->lock, flags);
 
 		ret = copy_to_user(buf + copied, pkt->buf + pkt->n_read, len);
 		if (ret) {
@@ -232,7 +232,7 @@ read_start:
 			submit_one_urb(ksb, GFP_KERNEL, pkt);
 			pkt = NULL;
 		}
-		spin_lock_irqsave(&ksb->lock, flags);
+		raw_spin_lock_irqsave(&ksb->lock, flags);
 	}
 
 	/* put the partial packet back in the list */
@@ -242,7 +242,7 @@ read_start:
 		else
 			ksb_free_data_pkt(pkt);
 	}
-	spin_unlock_irqrestore(&ksb->lock, flags);
+	raw_spin_unlock_irqrestore(&ksb->lock, flags);
 
 	dbg_log_event(ksb, "KS_READ", copied, 0);
 
@@ -281,13 +281,13 @@ static void ksb_tomdm_work(struct work_struct *w)
 	struct urb *urb;
 	int ret;
 
-	spin_lock_irqsave(&ksb->lock, flags);
+	raw_spin_lock_irqsave(&ksb->lock, flags);
 	while (!list_empty(&ksb->to_mdm_list)
 			&& test_bit(USB_DEV_CONNECTED, &ksb->flags)) {
 		pkt = list_first_entry(&ksb->to_mdm_list,
 				struct data_pkt, list);
 		list_del_init(&pkt->list);
-		spin_unlock_irqrestore(&ksb->lock, flags);
+		raw_spin_unlock_irqrestore(&ksb->lock, flags);
 
 		urb = usb_alloc_urb(0, GFP_KERNEL);
 		if (!urb) {
@@ -328,9 +328,9 @@ static void ksb_tomdm_work(struct work_struct *w)
 
 		usb_free_urb(urb);
 
-		spin_lock_irqsave(&ksb->lock, flags);
+		raw_spin_lock_irqsave(&ksb->lock, flags);
 	}
-	spin_unlock_irqrestore(&ksb->lock, flags);
+	raw_spin_unlock_irqrestore(&ksb->lock, flags);
 }
 
 static ssize_t ksb_fs_write(struct file *fp, const char __user *buf,
@@ -362,9 +362,9 @@ static ssize_t ksb_fs_write(struct file *fp, const char __user *buf,
 		return ret;
 	}
 
-	spin_lock_irqsave(&ksb->lock, flags);
+	raw_spin_lock_irqsave(&ksb->lock, flags);
 	list_add_tail(&pkt->list, &ksb->to_mdm_list);
-	spin_unlock_irqrestore(&ksb->lock, flags);
+	raw_spin_unlock_irqrestore(&ksb->lock, flags);
 
 	queue_work(ksb->wq, &ksb->to_mdm_work);
 
@@ -408,10 +408,10 @@ static unsigned int ksb_fs_poll(struct file *file, poll_table *wait)
 	if (!test_bit(USB_DEV_CONNECTED, &ksb->flags))
 		return POLLERR;
 
-	spin_lock_irqsave(&ksb->lock, flags);
+	raw_spin_lock_irqsave(&ksb->lock, flags);
 	if (!list_empty(&ksb->to_ks_list))
 		ret = POLLIN | POLLRDNORM;
-	spin_unlock_irqrestore(&ksb->lock, flags);
+	raw_spin_unlock_irqrestore(&ksb->lock, flags);
 
 	return ret;
 }
@@ -581,10 +581,10 @@ static void ksb_rx_cb(struct urb *urb)
 	}
 
 add_to_list:
-	spin_lock(&ksb->lock);
+	raw_spin_lock(&ksb->lock);
 	pkt->len = urb->actual_length;
 	list_add_tail(&pkt->list, &ksb->to_ks_list);
-	spin_unlock(&ksb->lock);
+	raw_spin_unlock(&ksb->lock);
 	/* wake up read thread */
 	if (wakeup)
 		wake_up(&ksb->ks_wait_q);
@@ -794,7 +794,7 @@ ksb_usb_probe(struct usb_interface *ifc, const struct usb_device_id *id)
 	dbg_log_event(ksb, "PID-ATT", id->idProduct, 0);
 
 	/*free up stale buffers if any from previous disconnect*/
-	spin_lock_irqsave(&ksb->lock, flags);
+	raw_spin_lock_irqsave(&ksb->lock, flags);
 	while (!list_empty(&ksb->to_ks_list)) {
 		pkt = list_first_entry(&ksb->to_ks_list,
 				struct data_pkt, list);
@@ -807,7 +807,7 @@ ksb_usb_probe(struct usb_interface *ifc, const struct usb_device_id *id)
 		list_del_init(&pkt->list);
 		ksb_free_data_pkt(pkt);
 	}
-	spin_unlock_irqrestore(&ksb->lock, flags);
+	raw_spin_unlock_irqrestore(&ksb->lock, flags);
 
 	ret = alloc_chrdev_region(&ksb->cdev_start_no, 0, 1, mdev->name);
 	if (ret < 0) {
@@ -875,9 +875,9 @@ static int ksb_usb_suspend(struct usb_interface *ifc, pm_message_t message)
 
 	usb_kill_anchored_urbs(&ksb->submitted);
 
-	spin_lock_irqsave(&ksb->lock, flags);
+	raw_spin_lock_irqsave(&ksb->lock, flags);
 	if (!list_empty(&ksb->to_ks_list)) {
-		spin_unlock_irqrestore(&ksb->lock, flags);
+		raw_spin_unlock_irqrestore(&ksb->lock, flags);
 		dbg_log_event(ksb, "SUSPEND ABORT", 0, 0);
 		/*
 		 * Now wakeup the reader process and queue
@@ -887,7 +887,7 @@ static int ksb_usb_suspend(struct usb_interface *ifc, pm_message_t message)
 		queue_work(ksb->wq, &ksb->start_rx_work);
 		return -EBUSY;
 	}
-	spin_unlock_irqrestore(&ksb->lock, flags);
+	raw_spin_unlock_irqrestore(&ksb->lock, flags);
 
 	return 0;
 }
@@ -930,7 +930,7 @@ static void ksb_usb_disconnect(struct usb_interface *ifc)
 					!atomic_read(&ksb->rx_pending_cnt),
 					msecs_to_jiffies(PENDING_URB_TIMEOUT));
 
-	spin_lock_irqsave(&ksb->lock, flags);
+	raw_spin_lock_irqsave(&ksb->lock, flags);
 	while (!list_empty(&ksb->to_ks_list)) {
 		pkt = list_first_entry(&ksb->to_ks_list,
 				struct data_pkt, list);
@@ -943,7 +943,7 @@ static void ksb_usb_disconnect(struct usb_interface *ifc)
 		list_del_init(&pkt->list);
 		ksb_free_data_pkt(pkt);
 	}
-	spin_unlock_irqrestore(&ksb->lock, flags);
+	raw_spin_unlock_irqrestore(&ksb->lock, flags);
 
 	ifc->needs_remote_wakeup = 0;
 	usb_put_dev(ksb->udev);
@@ -1024,7 +1024,7 @@ static int __init ksb_init(void)
 			goto dev_free;
 		}
 
-		spin_lock_init(&ksb->lock);
+		raw_spin_lock_init(&ksb->lock);
 		INIT_LIST_HEAD(&ksb->to_mdm_list);
 		INIT_LIST_HEAD(&ksb->to_ks_list);
 		init_waitqueue_head(&ksb->ks_wait_q);
diff --git a/kernel/msm-3.18/drivers/usb/phy/otg-wakelock.c b/kernel/msm-3.18/drivers/usb/phy/otg-wakelock.c
index 479376bfa..90784d482 100644
--- a/kernel/msm-3.18/drivers/usb/phy/otg-wakelock.c
+++ b/kernel/msm-3.18/drivers/usb/phy/otg-wakelock.c
@@ -81,11 +81,11 @@ static void otgwl_handle_event(unsigned long event)
 {
 	unsigned long irqflags;
 
-	spin_lock_irqsave(&otgwl_spinlock, irqflags);
+	raw_spin_lock_irqsave(&otgwl_spinlock, irqflags);
 
 	if (!enabled) {
 		otgwl_drop(&vbus_lock);
-		spin_unlock_irqrestore(&otgwl_spinlock, irqflags);
+		raw_spin_unlock_irqrestore(&otgwl_spinlock, irqflags);
 		return;
 	}
 
@@ -105,7 +105,7 @@ static void otgwl_handle_event(unsigned long event)
 		break;
 	}
 
-	spin_unlock_irqrestore(&otgwl_spinlock, irqflags);
+	raw_spin_unlock_irqrestore(&otgwl_spinlock, irqflags);
 }
 
 static int otgwl_otg_notifications(struct notifier_block *nb,
diff --git a/kernel/msm-3.18/drivers/usb/phy/phy-msm-qusb.c b/kernel/msm-3.18/drivers/usb/phy/phy-msm-qusb.c
index 15efbf6cf..48926ac53 100644
--- a/kernel/msm-3.18/drivers/usb/phy/phy-msm-qusb.c
+++ b/kernel/msm-3.18/drivers/usb/phy/phy-msm-qusb.c
@@ -166,7 +166,7 @@ struct qusb_phy {
 	int			phy_pll_reset_seq_len;
 	int			*emu_dcm_reset_seq;
 	int			emu_dcm_reset_seq_len;
-	spinlock_t		pulse_lock;
+	raw_spinlock_t		pulse_lock;
 	bool			put_into_high_z_state;
 	bool			scm_lvl_shifter_update;
 };
@@ -559,7 +559,7 @@ static int qusb_phy_update_dpdm(struct usb_phy *phy, int value)
 		if (ret)
 			goto clk_error;
 
-		spin_lock_irqsave(&qphy->pulse_lock, flags);
+		raw_spin_lock_irqsave(&qphy->pulse_lock, flags);
 		/*Set DP to 3.075v, sleep for .25 ms */
 		reg = readl_relaxed(qphy->base + QUSB2PHY_PORT_QC2);
 		reg |= (RDP_UP_EN | RPUP_LOW_EN);
@@ -584,7 +584,7 @@ static int qusb_phy_update_dpdm(struct usb_phy *phy, int value)
 		writel_relaxed(reg, qphy->base + QUSB2PHY_PORT_QC2);
 		/* complete above write */
 		mb();
-		spin_unlock_irqrestore(&qphy->pulse_lock, flags);
+		raw_spin_unlock_irqrestore(&qphy->pulse_lock, flags);
 		/*
 		 * It is recommended to wait here to get voltage change on
 		 * DP/DM line.
@@ -602,7 +602,7 @@ static int qusb_phy_update_dpdm(struct usb_phy *phy, int value)
 		if (ret)
 			goto clk_error;
 
-		spin_lock_irqsave(&qphy->pulse_lock, flags);
+		raw_spin_lock_irqsave(&qphy->pulse_lock, flags);
 		/* Set DM to 0.6v, sleep .25 ms */
 		reg = readl_relaxed(qphy->base + QUSB2PHY_PORT_QC1);
 		reg |= VDM_SRC_EN;
@@ -632,7 +632,7 @@ static int qusb_phy_update_dpdm(struct usb_phy *phy, int value)
 
 		/* complete above write */
 		mb();
-		spin_unlock_irqrestore(&qphy->pulse_lock, flags);
+		raw_spin_unlock_irqrestore(&qphy->pulse_lock, flags);
 
 		/*
 		 * It is recommended to wait here to get voltage change on
@@ -1108,7 +1108,7 @@ static int qusb_phy_probe(struct platform_device *pdev)
 		return -ENOMEM;
 
 	qphy->phy.dev = dev;
-	spin_lock_init(&qphy->pulse_lock);
+	raw_spin_lock_init(&qphy->pulse_lock);
 
 	res = platform_get_resource_byname(pdev, IORESOURCE_MEM,
 							"qusb_phy_base");
diff --git a/kernel/msm-3.18/fs/ext4/crypto.c b/kernel/msm-3.18/fs/ext4/crypto.c
index 79959f1cc..6af5003c1 100644
--- a/kernel/msm-3.18/fs/ext4/crypto.c
+++ b/kernel/msm-3.18/fs/ext4/crypto.c
@@ -79,9 +79,9 @@ void ext4_release_crypto_ctx(struct ext4_crypto_ctx *ctx)
 	if (ctx->flags & EXT4_CTX_REQUIRES_FREE_ENCRYPT_FL) {
 		kmem_cache_free(ext4_crypto_ctx_cachep, ctx);
 	} else {
-		spin_lock_irqsave(&ext4_crypto_ctx_lock, flags);
+		raw_spin_lock_irqsave(&ext4_crypto_ctx_lock, flags);
 		list_add(&ctx->free_list, &ext4_free_crypto_ctxs);
-		spin_unlock_irqrestore(&ext4_crypto_ctx_lock, flags);
+		raw_spin_unlock_irqrestore(&ext4_crypto_ctx_lock, flags);
 	}
 }
 
@@ -115,12 +115,12 @@ struct ext4_crypto_ctx *ext4_get_crypto_ctx(struct inode *inode,
 	 * should generally be a "last resort" option for a filesystem
 	 * to be able to do its job.
 	 */
-	spin_lock_irqsave(&ext4_crypto_ctx_lock, flags);
+	raw_spin_lock_irqsave(&ext4_crypto_ctx_lock, flags);
 	ctx = list_first_entry_or_null(&ext4_free_crypto_ctxs,
 				       struct ext4_crypto_ctx, free_list);
 	if (ctx)
 		list_del(&ctx->free_list);
-	spin_unlock_irqrestore(&ext4_crypto_ctx_lock, flags);
+	raw_spin_unlock_irqrestore(&ext4_crypto_ctx_lock, flags);
 	if (!ctx) {
 		ctx = kmem_cache_zalloc(ext4_crypto_ctx_cachep, gfp_flags);
 		if (!ctx) {
diff --git a/kernel/msm-3.18/fs/f2fs/extent_cache.c b/kernel/msm-3.18/fs/f2fs/extent_cache.c
index ff2352a0e..74642f6e6 100644
--- a/kernel/msm-3.18/fs/f2fs/extent_cache.c
+++ b/kernel/msm-3.18/fs/f2fs/extent_cache.c
@@ -236,10 +236,10 @@ static void __detach_extent_node(struct f2fs_sb_info *sbi,
 static void __release_extent_node(struct f2fs_sb_info *sbi,
 			struct extent_tree *et, struct extent_node *en)
 {
-	spin_lock(&sbi->extent_lock);
+	raw_spin_lock(&sbi->extent_lock);
 	f2fs_bug_on(sbi, list_empty(&en->list));
 	list_del_init(&en->list);
-	spin_unlock(&sbi->extent_lock);
+	raw_spin_unlock(&sbi->extent_lock);
 
 	__detach_extent_node(sbi, et, en);
 }
@@ -349,9 +349,9 @@ static bool __f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_e
 
 	en = __init_extent_tree(sbi, et, &ei);
 	if (en) {
-		spin_lock(&sbi->extent_lock);
+		raw_spin_lock(&sbi->extent_lock);
 		list_add_tail(&en->list, &sbi->extent_list);
-		spin_unlock(&sbi->extent_lock);
+		raw_spin_unlock(&sbi->extent_lock);
 	}
 out:
 	write_unlock(&et->lock);
@@ -401,12 +401,12 @@ static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 		stat_inc_rbtree_node_hit(sbi);
 
 	*ei = en->ei;
-	spin_lock(&sbi->extent_lock);
+	raw_spin_lock(&sbi->extent_lock);
 	if (!list_empty(&en->list)) {
 		list_move_tail(&en->list, &sbi->extent_list);
 		et->cached_en = en;
 	}
-	spin_unlock(&sbi->extent_lock);
+	raw_spin_unlock(&sbi->extent_lock);
 	ret = true;
 out:
 	stat_inc_total_hit(sbi);
@@ -445,12 +445,12 @@ static struct extent_node *__try_merge_extent_node(struct inode *inode,
 
 	__try_update_largest_extent(inode, et, en);
 
-	spin_lock(&sbi->extent_lock);
+	raw_spin_lock(&sbi->extent_lock);
 	if (!list_empty(&en->list)) {
 		list_move_tail(&en->list, &sbi->extent_list);
 		et->cached_en = en;
 	}
-	spin_unlock(&sbi->extent_lock);
+	raw_spin_unlock(&sbi->extent_lock);
 	return en;
 }
 
@@ -479,10 +479,10 @@ do_insert:
 	__try_update_largest_extent(inode, et, en);
 
 	/* update in global extent list */
-	spin_lock(&sbi->extent_lock);
+	raw_spin_lock(&sbi->extent_lock);
 	list_add_tail(&en->list, &sbi->extent_list);
 	et->cached_en = en;
-	spin_unlock(&sbi->extent_lock);
+	raw_spin_unlock(&sbi->extent_lock);
 	return en;
 }
 
@@ -653,7 +653,7 @@ free_node:
 
 	remained = nr_shrink - (node_cnt + tree_cnt);
 
-	spin_lock(&sbi->extent_lock);
+	raw_spin_lock(&sbi->extent_lock);
 	for (; remained > 0; remained--) {
 		if (list_empty(&sbi->extent_list))
 			break;
@@ -667,15 +667,15 @@ free_node:
 		}
 
 		list_del_init(&en->list);
-		spin_unlock(&sbi->extent_lock);
+		raw_spin_unlock(&sbi->extent_lock);
 
 		__detach_extent_node(sbi, et, en);
 
 		write_unlock(&et->lock);
 		node_cnt++;
-		spin_lock(&sbi->extent_lock);
+		raw_spin_lock(&sbi->extent_lock);
 	}
-	spin_unlock(&sbi->extent_lock);
+	raw_spin_unlock(&sbi->extent_lock);
 
 unlock_out:
 	mutex_unlock(&sbi->extent_tree_lock);
@@ -790,7 +790,7 @@ void init_extent_cache_info(struct f2fs_sb_info *sbi)
 	INIT_RADIX_TREE(&sbi->extent_tree_root, GFP_NOIO);
 	mutex_init(&sbi->extent_tree_lock);
 	INIT_LIST_HEAD(&sbi->extent_list);
-	spin_lock_init(&sbi->extent_lock);
+	raw_spin_lock_init(&sbi->extent_lock);
 	atomic_set(&sbi->total_ext_tree, 0);
 	INIT_LIST_HEAD(&sbi->zombie_list);
 	atomic_set(&sbi->total_zombie_tree, 0);
diff --git a/kernel/msm-3.18/fs/f2fs/shrinker.c b/kernel/msm-3.18/fs/f2fs/shrinker.c
index 5c60fc28e..cfc75bf11 100644
--- a/kernel/msm-3.18/fs/f2fs/shrinker.c
+++ b/kernel/msm-3.18/fs/f2fs/shrinker.c
@@ -46,7 +46,7 @@ unsigned long f2fs_shrink_count(struct shrinker *shrink,
 	struct list_head *p;
 	unsigned long count = 0;
 
-	spin_lock(&f2fs_list_lock);
+	raw_spin_lock(&f2fs_list_lock);
 	p = f2fs_list.next;
 	while (p != &f2fs_list) {
 		sbi = list_entry(p, struct f2fs_sb_info, s_list);
@@ -56,7 +56,7 @@ unsigned long f2fs_shrink_count(struct shrinker *shrink,
 			p = p->next;
 			continue;
 		}
-		spin_unlock(&f2fs_list_lock);
+		raw_spin_unlock(&f2fs_list_lock);
 
 		/* count extent cache entries */
 		count += __count_extent_cache(sbi);
@@ -67,11 +67,11 @@ unsigned long f2fs_shrink_count(struct shrinker *shrink,
 		/* count free nids cache entries */
 		count += __count_free_nids(sbi);
 
-		spin_lock(&f2fs_list_lock);
+		raw_spin_lock(&f2fs_list_lock);
 		p = p->next;
 		mutex_unlock(&sbi->umount_mutex);
 	}
-	spin_unlock(&f2fs_list_lock);
+	raw_spin_unlock(&f2fs_list_lock);
 	return count;
 }
 
@@ -84,7 +84,7 @@ unsigned long f2fs_shrink_scan(struct shrinker *shrink,
 	unsigned int run_no;
 	unsigned long freed = 0;
 
-	spin_lock(&f2fs_list_lock);
+	raw_spin_lock(&f2fs_list_lock);
 	do {
 		run_no = ++shrinker_run_no;
 	} while (run_no == 0);
@@ -100,7 +100,7 @@ unsigned long f2fs_shrink_scan(struct shrinker *shrink,
 			p = p->next;
 			continue;
 		}
-		spin_unlock(&f2fs_list_lock);
+		raw_spin_unlock(&f2fs_list_lock);
 
 		sbi->shrinker_run_no = run_no;
 
@@ -115,29 +115,29 @@ unsigned long f2fs_shrink_scan(struct shrinker *shrink,
 		if (freed < nr)
 			freed += try_to_free_nids(sbi, nr - freed);
 
-		spin_lock(&f2fs_list_lock);
+		raw_spin_lock(&f2fs_list_lock);
 		p = p->next;
 		list_move_tail(&sbi->s_list, &f2fs_list);
 		mutex_unlock(&sbi->umount_mutex);
 		if (freed >= nr)
 			break;
 	}
-	spin_unlock(&f2fs_list_lock);
+	raw_spin_unlock(&f2fs_list_lock);
 	return freed;
 }
 
 void f2fs_join_shrinker(struct f2fs_sb_info *sbi)
 {
-	spin_lock(&f2fs_list_lock);
+	raw_spin_lock(&f2fs_list_lock);
 	list_add_tail(&sbi->s_list, &f2fs_list);
-	spin_unlock(&f2fs_list_lock);
+	raw_spin_unlock(&f2fs_list_lock);
 }
 
 void f2fs_leave_shrinker(struct f2fs_sb_info *sbi)
 {
 	f2fs_shrink_extent_tree(sbi, __count_extent_cache(sbi));
 
-	spin_lock(&f2fs_list_lock);
+	raw_spin_lock(&f2fs_list_lock);
 	list_del(&sbi->s_list);
-	spin_unlock(&f2fs_list_lock);
+	raw_spin_unlock(&f2fs_list_lock);
 }
diff --git a/kernel/msm-3.18/fs/f2fs/sysfs.c b/kernel/msm-3.18/fs/f2fs/sysfs.c
index 71191d899..37c5834bf 100644
--- a/kernel/msm-3.18/fs/f2fs/sysfs.c
+++ b/kernel/msm-3.18/fs/f2fs/sysfs.c
@@ -114,14 +114,14 @@ static ssize_t f2fs_sbi_store(struct f2fs_attr *a,
 		return -EINVAL;
 #endif
 	if (a->struct_type == RESERVED_BLOCKS) {
-		spin_lock(&sbi->stat_lock);
+		raw_spin_lock(&sbi->stat_lock);
 		if ((unsigned long)sbi->total_valid_block_count + t >
 				(unsigned long)sbi->user_block_count) {
-			spin_unlock(&sbi->stat_lock);
+			raw_spin_unlock(&sbi->stat_lock);
 			return -EINVAL;
 		}
 		*ui = t;
-		spin_unlock(&sbi->stat_lock);
+		raw_spin_unlock(&sbi->stat_lock);
 		return count;
 	}
 	*ui = t;
diff --git a/kernel/msm-3.18/fs/f2fs/trace.c b/kernel/msm-3.18/fs/f2fs/trace.c
index bccbbf261..85ad9bad9 100644
--- a/kernel/msm-3.18/fs/f2fs/trace.c
+++ b/kernel/msm-3.18/fs/f2fs/trace.c
@@ -17,7 +17,7 @@
 #include "trace.h"
 
 static RADIX_TREE(pids, GFP_ATOMIC);
-static spinlock_t pids_lock;
+static raw_spinlock_t pids_lock;
 static struct last_io_info last_io;
 
 static inline void __print_last_io(void)
@@ -64,7 +64,7 @@ void f2fs_trace_pid(struct page *page)
 	if (radix_tree_preload(GFP_NOFS))
 		return;
 
-	spin_lock(&pids_lock);
+	raw_spin_lock(&pids_lock);
 	p = radix_tree_lookup(&pids, pid);
 	if (p == current)
 		goto out;
@@ -77,7 +77,7 @@ void f2fs_trace_pid(struct page *page)
 			MAJOR(inode->i_sb->s_dev), MINOR(inode->i_sb->s_dev),
 			pid, current->comm);
 out:
-	spin_unlock(&pids_lock);
+	raw_spin_unlock(&pids_lock);
 	radix_tree_preload_end();
 }
 
@@ -122,7 +122,7 @@ void f2fs_trace_ios(struct f2fs_io_info *fio, int flush)
 
 void f2fs_build_trace_ios(void)
 {
-	spin_lock_init(&pids_lock);
+	raw_spin_lock_init(&pids_lock);
 }
 
 #define PIDVEC_SIZE	128
@@ -150,7 +150,7 @@ void f2fs_destroy_trace_ios(void)
 	pid_t next_pid = 0;
 	unsigned int found;
 
-	spin_lock(&pids_lock);
+	raw_spin_lock(&pids_lock);
 	while ((found = gang_lookup_pids(pid, next_pid, PIDVEC_SIZE))) {
 		unsigned idx;
 
@@ -158,5 +158,5 @@ void f2fs_destroy_trace_ios(void)
 		for (idx = 0; idx < found; idx++)
 			radix_tree_delete(&pids, pid[idx]);
 	}
-	spin_unlock(&pids_lock);
+	raw_spin_unlock(&pids_lock);
 }
diff --git a/kernel/msm-3.18/fs/fuse/shortcircuit.c b/kernel/msm-3.18/fs/fuse/shortcircuit.c
index c657b170f..bbc588188 100644
--- a/kernel/msm-3.18/fs/fuse/shortcircuit.c
+++ b/kernel/msm-3.18/fs/fuse/shortcircuit.c
@@ -77,9 +77,9 @@ static ssize_t fuse_shortcircuit_read_write_iter(struct kiocb *iocb,
 		ret_val = lower_file->f_op->write_iter(iocb, iter);
 
 		if (ret_val >= 0 || ret_val == -EIOCBQUEUED) {
-			spin_lock(&fc->lock);
+			raw_spin_lock(&fc->lock);
 			fsstack_copy_inode_size(fuse_inode, lower_inode);
-			spin_unlock(&fc->lock);
+			raw_spin_unlock(&fc->lock);
 			fsstack_copy_attr_times(fuse_inode, lower_inode);
 		}
 	} else {
diff --git a/kernel/msm-3.18/include/linux/clk/msm-clk-provider.h b/kernel/msm-3.18/include/linux/clk/msm-clk-provider.h
index bc5095d28..0dbd5132f 100644
--- a/kernel/msm-3.18/include/linux/clk/msm-clk-provider.h
+++ b/kernel/msm-3.18/include/linux/clk/msm-clk-provider.h
@@ -189,7 +189,7 @@ struct clk {
 
 	unsigned count;
 	unsigned notifier_count;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	unsigned prepare_count;
 	struct mutex prepare_lock;
 
@@ -201,7 +201,7 @@ struct clk {
 };
 
 #define CLK_INIT(name) \
-	.lock = __SPIN_LOCK_UNLOCKED((name).lock), \
+	.lock = __RAW_SPIN_LOCK_UNLOCKED((name).lock), \
 	.prepare_lock = __MUTEX_INITIALIZER((name).prepare_lock), \
 	.children = LIST_HEAD_INIT((name).children), \
 	.siblings = LIST_HEAD_INIT((name).siblings), \
diff --git a/kernel/msm-3.18/include/linux/dma-mapping-fast.h b/kernel/msm-3.18/include/linux/dma-mapping-fast.h
index aa9fcfe73..d28b6c720 100644
--- a/kernel/msm-3.18/include/linux/dma-mapping-fast.h
+++ b/kernel/msm-3.18/include/linux/dma-mapping-fast.h
@@ -32,7 +32,7 @@ struct dma_fast_smmu_mapping {
 	dma_addr_t	pgtbl_dma_handle;
 	av8l_fast_iopte	*pgtbl_pmds;
 
-	spinlock_t	lock;
+	raw_spinlock_t	lock;
 	struct notifier_block notifier;
 };
 
diff --git a/kernel/msm-3.18/include/linux/fscrypt_supp.h b/kernel/msm-3.18/include/linux/fscrypt_supp.h
index 6828dc611..cf98d00d7 100644
--- a/kernel/msm-3.18/include/linux/fscrypt_supp.h
+++ b/kernel/msm-3.18/include/linux/fscrypt_supp.h
@@ -29,9 +29,9 @@ static inline void fscrypt_set_d_op(struct dentry *dentry)
 
 static inline void fscrypt_set_encrypted_dentry(struct dentry *dentry)
 {
-	spin_lock(&dentry->d_lock);
+	raw_spin_lock(&dentry->d_lock);
 	dentry->d_flags |= DCACHE_ENCRYPTED_WITH_KEY;
-	spin_unlock(&dentry->d_lock);
+	raw_spin_unlock(&dentry->d_lock);
 }
 
 /* policy.c */
diff --git a/kernel/msm-3.18/include/linux/ipc_router.h b/kernel/msm-3.18/include/linux/ipc_router.h
index ad2192d89..6d4e2bf65 100644
--- a/kernel/msm-3.18/include/linux/ipc_router.h
+++ b/kernel/msm-3.18/include/linux/ipc_router.h
@@ -131,7 +131,7 @@ struct msm_ipc_port {
 	wait_queue_head_t port_tx_wait_q;
 
 	int restart_state;
-	spinlock_t restart_lock;
+	raw_spinlock_t restart_lock;
 	wait_queue_head_t restart_wait;
 
 	void *rport_info;
diff --git a/kernel/msm-3.18/include/linux/mmc/ring_buffer.h b/kernel/msm-3.18/include/linux/mmc/ring_buffer.h
index 1bf564df7..172efdb8b 100644
--- a/kernel/msm-3.18/include/linux/mmc/ring_buffer.h
+++ b/kernel/msm-3.18/include/linux/mmc/ring_buffer.h
@@ -29,7 +29,7 @@ struct mmc_host;
 struct mmc_trace_buffer {
 	int	wr_idx;
 	bool stop_tracing;
-	spinlock_t trace_lock;
+	raw_spinlock_t trace_lock;
 	char *data;
 };
 
diff --git a/kernel/msm-3.18/include/linux/remote_spinlock.h b/kernel/msm-3.18/include/linux/remote_spinlock.h
index 591c1b24d..cdfb3a934 100644
--- a/kernel/msm-3.18/include/linux/remote_spinlock.h
+++ b/kernel/msm-3.18/include/linux/remote_spinlock.h
@@ -35,41 +35,41 @@
  *    CPU in some board/machine types.
  */
 typedef struct {
-	spinlock_t local;
+	raw_spinlock_t local;
 	_remote_spinlock_t remote;
 } remote_spinlock_t;
 
 #define remote_spin_lock_init(lock, id) \
 	({ \
-		spin_lock_init(&((lock)->local)); \
+		raw_spin_lock_init(&((lock)->local)); \
 		_remote_spin_lock_init(id, &((lock)->remote)); \
 	})
 #define remote_spin_lock(lock) \
 	do { \
-		spin_lock(&((lock)->local)); \
+		raw_spin_lock(&((lock)->local)); \
 		_remote_spin_lock(&((lock)->remote)); \
 	} while (0)
 #define remote_spin_unlock(lock) \
 	do { \
 		_remote_spin_unlock(&((lock)->remote)); \
-		spin_unlock(&((lock)->local)); \
+		raw_spin_unlock(&((lock)->local)); \
 	} while (0)
 #define remote_spin_lock_irqsave(lock, flags) \
 	do { \
-		spin_lock_irqsave(&((lock)->local), flags); \
+		raw_spin_lock_irqsave(&((lock)->local), flags); \
 		_remote_spin_lock(&((lock)->remote)); \
 	} while (0)
 #define remote_spin_unlock_irqrestore(lock, flags) \
 	do { \
 		_remote_spin_unlock(&((lock)->remote)); \
-		spin_unlock_irqrestore(&((lock)->local), flags); \
+		raw_spin_unlock_irqrestore(&((lock)->local), flags); \
 	} while (0)
 #define remote_spin_trylock(lock) \
 	({ \
 		spin_trylock(&((lock)->local)) \
 		? _remote_spin_trylock(&((lock)->remote)) \
 			? 1 \
-			: ({ spin_unlock(&((lock)->local)); 0; }) \
+			: ({ raw_spin_unlock(&((lock)->local)); 0; }) \
 		: 0; \
 	})
 #define remote_spin_trylock_irqsave(lock, flags) \
@@ -77,7 +77,7 @@ typedef struct {
 		spin_trylock_irqsave(&((lock)->local), flags) \
 		? _remote_spin_trylock(&((lock)->remote)) \
 			? 1 \
-			: ({ spin_unlock_irqrestore(&((lock)->local), flags); \
+			: ({ raw_spin_unlock_irqrestore(&((lock)->local), flags); \
 				0; }) \
 		: 0; \
 	})
diff --git a/kernel/msm-3.18/include/linux/rq_stats.h b/kernel/msm-3.18/include/linux/rq_stats.h
index 44cd8426d..08a752fd0 100644
--- a/kernel/msm-3.18/include/linux/rq_stats.h
+++ b/kernel/msm-3.18/include/linux/rq_stats.h
@@ -26,6 +26,6 @@ struct rq_data {
 	int init;
 };
 
-extern spinlock_t rq_lock;
+extern raw_spinlock_t rq_lock;
 extern struct rq_data rq_info;
 extern struct workqueue_struct *rq_wq;
diff --git a/kernel/msm-3.18/include/linux/sync.h b/kernel/msm-3.18/include/linux/sync.h
index a443b529e..1d1453082 100644
--- a/kernel/msm-3.18/include/linux/sync.h
+++ b/kernel/msm-3.18/include/linux/sync.h
@@ -110,10 +110,10 @@ struct sync_timeline {
 	bool			destroyed;
 
 	struct list_head	child_list_head;
-	spinlock_t		child_list_lock;
+	raw_spinlock_t		child_list_lock;
 
 	struct list_head	active_list_head;
-	spinlock_t		active_list_lock;
+	raw_spinlock_t		active_list_lock;
 
 	struct list_head	sync_timeline_list;
 };
@@ -169,7 +169,7 @@ struct sync_fence {
 	struct list_head	pt_list_head;
 
 	struct list_head	waiter_list_head;
-	spinlock_t		waiter_list_lock; /* also protects status */
+	raw_spinlock_t		waiter_list_lock; /* also protects status */
 	int			status;
 
 	wait_queue_head_t	wq;
diff --git a/kernel/msm-3.18/include/linux/test-iosched.h b/kernel/msm-3.18/include/linux/test-iosched.h
index 61c152141..4c60fe63f 100644
--- a/kernel/msm-3.18/include/linux/test-iosched.h
+++ b/kernel/msm-3.18/include/linux/test-iosched.h
@@ -234,7 +234,7 @@ struct test_iosched {
 	u32 sector_range;
 	int wr_rd_next_req_id;
 	int unique_next_req_id;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	struct test_info test_info;
 	bool fs_wr_reqs_during_test;
 	bool ignore_round;
diff --git a/kernel/msm-3.18/include/soc/qcom/msm_qmi_interface.h b/kernel/msm-3.18/include/soc/qcom/msm_qmi_interface.h
index 135265b74..e3ac5b0cf 100644
--- a/kernel/msm-3.18/include/soc/qcom/msm_qmi_interface.h
+++ b/kernel/msm-3.18/include/soc/qcom/msm_qmi_interface.h
@@ -63,7 +63,7 @@ struct qmi_handle {
 	uint16_t next_txn_id;
 	struct workqueue_struct *handle_wq;
 	struct mutex handle_lock;
-	spinlock_t notify_lock;
+	raw_spinlock_t notify_lock;
 	void (*notify)(struct qmi_handle *handle, enum qmi_event_type event,
 			void *notify_priv);
 	void *notify_priv;
diff --git a/kernel/msm-3.18/include/sound/q6afe-v2.h b/kernel/msm-3.18/include/sound/q6afe-v2.h
index 73cd5e521..8212d3359 100644
--- a/kernel/msm-3.18/include/sound/q6afe-v2.h
+++ b/kernel/msm-3.18/include/sound/q6afe-v2.h
@@ -214,7 +214,7 @@ struct afe_audio_port_data {
 	uint32_t	    tmp_hdl;
 	/* read or write locks */
 	struct mutex	    lock;
-	spinlock_t	    dsp_lock;
+	raw_spinlock_t	    dsp_lock;
 };
 
 struct afe_audio_client {
diff --git a/kernel/msm-3.18/include/sound/q6asm-v2.h b/kernel/msm-3.18/include/sound/q6asm-v2.h
index 21ecfb47d..6e37635de 100644
--- a/kernel/msm-3.18/include/sound/q6asm-v2.h
+++ b/kernel/msm-3.18/include/sound/q6asm-v2.h
@@ -175,7 +175,7 @@ struct audio_port_data {
 	uint32_t	    tmp_hdl;
 	/* read or write locks */
 	struct mutex	    lock;
-	spinlock_t	    dsp_lock;
+	raw_spinlock_t	    dsp_lock;
 };
 
 struct shared_io_config {
@@ -197,7 +197,7 @@ struct audio_client {
 	atomic_t	       time_flag;
 	atomic_t	       nowait_cmd_cnt;
 	struct list_head       no_wait_que;
-	spinlock_t             no_wait_que_spinlock;
+	raw_spinlock_t             no_wait_que_spinlock;
 	atomic_t               mem_state;
 	void		       *priv;
 	uint32_t               io_mode;
diff --git a/kernel/msm-3.18/kernel/kcov.c b/kernel/msm-3.18/kernel/kcov.c
index e228cb189..e1de64df7 100644
--- a/kernel/msm-3.18/kernel/kcov.c
+++ b/kernel/msm-3.18/kernel/kcov.c
@@ -30,7 +30,7 @@ struct kcov {
 	 */
 	atomic_t		refcount;
 	/* The lock protects mode, size, area and t. */
-	spinlock_t		lock;
+	raw_spinlock_t		lock;
 	enum kcov_mode		mode;
 	/* Size of arena (in long's for KCOV_MODE_TRACE). */
 	unsigned		size;
@@ -108,15 +108,15 @@ void kcov_task_exit(struct task_struct *t)
 	kcov = t->kcov;
 	if (kcov == NULL)
 		return;
-	spin_lock(&kcov->lock);
+	raw_spin_lock(&kcov->lock);
 	if (WARN_ON(kcov->t != t)) {
-		spin_unlock(&kcov->lock);
+		raw_spin_unlock(&kcov->lock);
 		return;
 	}
 	/* Just to not leave dangling references behind. */
 	kcov_task_init(t);
 	kcov->t = NULL;
-	spin_unlock(&kcov->lock);
+	raw_spin_unlock(&kcov->lock);
 	kcov_put(kcov);
 }
 
@@ -132,7 +132,7 @@ static int kcov_mmap(struct file *filep, struct vm_area_struct *vma)
 	if (!area)
 		return -ENOMEM;
 
-	spin_lock(&kcov->lock);
+	raw_spin_lock(&kcov->lock);
 	size = kcov->size * sizeof(unsigned long);
 	if (kcov->mode == KCOV_MODE_DISABLED || vma->vm_pgoff != 0 ||
 	    vma->vm_end - vma->vm_start != size) {
@@ -142,7 +142,7 @@ static int kcov_mmap(struct file *filep, struct vm_area_struct *vma)
 	if (!kcov->area) {
 		kcov->area = area;
 		vma->vm_flags |= VM_DONTEXPAND;
-		spin_unlock(&kcov->lock);
+		raw_spin_unlock(&kcov->lock);
 		for (off = 0; off < size; off += PAGE_SIZE) {
 			page = vmalloc_to_page(kcov->area + off);
 			if (vm_insert_page(vma, vma->vm_start + off, page))
@@ -151,7 +151,7 @@ static int kcov_mmap(struct file *filep, struct vm_area_struct *vma)
 		return 0;
 	}
 exit:
-	spin_unlock(&kcov->lock);
+	raw_spin_unlock(&kcov->lock);
 	vfree(area);
 	return res;
 }
@@ -164,7 +164,7 @@ static int kcov_open(struct inode *inode, struct file *filep)
 	if (!kcov)
 		return -ENOMEM;
 	atomic_set(&kcov->refcount, 1);
-	spin_lock_init(&kcov->lock);
+	raw_spin_lock_init(&kcov->lock);
 	filep->private_data = kcov;
 	return nonseekable_open(inode, filep);
 }
@@ -249,9 +249,9 @@ static long kcov_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)
 	int res;
 
 	kcov = filep->private_data;
-	spin_lock(&kcov->lock);
+	raw_spin_lock(&kcov->lock);
 	res = kcov_ioctl_locked(kcov, cmd, arg);
-	spin_unlock(&kcov->lock);
+	raw_spin_unlock(&kcov->lock);
 	return res;
 }
 
diff --git a/kernel/msm-3.18/kernel/power/wakeup_reason.c b/kernel/msm-3.18/kernel/power/wakeup_reason.c
index 4c4899feb..1f7345462 100644
--- a/kernel/msm-3.18/kernel/power/wakeup_reason.c
+++ b/kernel/msm-3.18/kernel/power/wakeup_reason.c
@@ -41,7 +41,7 @@ static LIST_HEAD(wakeup_irqs);
 
 static struct kmem_cache *wakeup_irq_nodes_cache;
 static struct kobject *wakeup_reason;
-static spinlock_t resume_reason_lock;
+static raw_spinlock_t resume_reason_lock;
 bool log_wakeups __read_mostly;
 struct completion wakeups_completion;
 
@@ -249,12 +249,12 @@ static ssize_t last_resume_reason_show(struct kobject *kobj,
 		.buf_offset = 0
 	};
 
-	spin_lock_irqsave(&resume_reason_lock, flags);
+	raw_spin_lock_irqsave(&resume_reason_lock, flags);
 	if (suspend_abort)
 		b.buf_offset = snprintf(buf, PAGE_SIZE, "Abort: %s", abort_reason);
 	else
 		walk_irq_node_tree(base_irq_nodes, print_leaf_node, &b);
-	spin_unlock_irqrestore(&resume_reason_lock, flags);
+	raw_spin_unlock_irqrestore(&resume_reason_lock, flags);
 
 	return b.buf_offset;
 }
@@ -442,11 +442,11 @@ void log_suspend_abort_reason(const char *fmt, ...)
 {
 	va_list args;
 
-	spin_lock(&resume_reason_lock);
+	raw_spin_lock(&resume_reason_lock);
 
 	//Suspend abort reason has already been logged.
 	if (suspend_abort) {
-		spin_unlock(&resume_reason_lock);
+		raw_spin_unlock(&resume_reason_lock);
 		return;
 	}
 
@@ -455,7 +455,7 @@ void log_suspend_abort_reason(const char *fmt, ...)
 	vsnprintf(abort_reason, MAX_SUSPEND_ABORT_LEN, fmt, args);
 	va_end(args);
 
-	spin_unlock(&resume_reason_lock);
+	raw_spin_unlock(&resume_reason_lock);
 }
 
 static bool match_node(struct wakeup_irq_node *n, void *_p)
@@ -467,9 +467,9 @@ static bool match_node(struct wakeup_irq_node *n, void *_p)
 int check_wakeup_reason(int irq)
 {
 	bool found;
-	spin_lock(&resume_reason_lock);
+	raw_spin_lock(&resume_reason_lock);
 	found = !walk_irq_node_tree(base_irq_nodes, match_node, &irq);
-	spin_unlock(&resume_reason_lock);
+	raw_spin_unlock(&resume_reason_lock);
 	return found;
 }
 
@@ -532,7 +532,7 @@ static bool delete_node(struct wakeup_irq_node *n, void *unused)
 void clear_wakeup_reasons(void)
 {
 	unsigned long flags;
-	spin_lock_irqsave(&resume_reason_lock, flags);
+	raw_spin_lock_irqsave(&resume_reason_lock, flags);
 
 	BUG_ON(logging_wakeup_reasons());
 	walk_irq_node_tree(base_irq_nodes, delete_node, NULL);
@@ -542,7 +542,7 @@ void clear_wakeup_reasons(void)
 	INIT_LIST_HEAD(&wakeup_irqs);
 	suspend_abort = false;
 
-	spin_unlock_irqrestore(&resume_reason_lock, flags);
+	raw_spin_unlock_irqrestore(&resume_reason_lock, flags);
 }
 
 /* Detects a suspend and clears all the previous wake up reasons*/
@@ -551,9 +551,9 @@ static int wakeup_reason_pm_event(struct notifier_block *notifier,
 {
 	switch (pm_event) {
 	case PM_SUSPEND_PREPARE:
-		spin_lock(&resume_reason_lock);
+		raw_spin_lock(&resume_reason_lock);
 		suspend_abort = false;
-		spin_unlock(&resume_reason_lock);
+		raw_spin_unlock(&resume_reason_lock);
 		/* monotonic time since boot */
 		last_monotime = ktime_get();
 		/* monotonic time since boot including the time spent in suspend */
@@ -636,7 +636,7 @@ late_initcall(suspend_time_debug_init);
  */
 int __init wakeup_reason_init(void)
 {
-	spin_lock_init(&resume_reason_lock);
+	raw_spin_lock_init(&resume_reason_lock);
 
 	if (register_pm_notifier(&wakeup_reason_pm_notifier_block)) {
 		pr_warning("[%s] failed to register PM notifier\n",
diff --git a/kernel/msm-3.18/kernel/sched/core_ctl.c b/kernel/msm-3.18/kernel/sched/core_ctl.c
index 0897b8d76..00cdedbb9 100644
--- a/kernel/msm-3.18/kernel/sched/core_ctl.c
+++ b/kernel/msm-3.18/kernel/sched/core_ctl.c
@@ -52,7 +52,7 @@ struct cpu_data {
 	s64 need_ts;
 	struct list_head lru;
 	bool pending;
-	spinlock_t pending_lock;
+	raw_spinlock_t pending_lock;
 	bool is_big_cluster;
 	int nrrun;
 	bool nrrun_changed;
@@ -243,13 +243,13 @@ static ssize_t show_cpus(struct cpu_data *state, char *buf)
 	ssize_t count = 0;
 	unsigned long flags;
 
-	spin_lock_irqsave(&state_lock, flags);
+	raw_spin_lock_irqsave(&state_lock, flags);
 	list_for_each_entry(c, &state->lru, sib) {
 		count += snprintf(buf + count, PAGE_SIZE - count,
 					"CPU%u (%s)\n", c->cpu,
 					c->online ? "Online" : "Offline");
 	}
-	spin_unlock_irqrestore(&state_lock, flags);
+	raw_spin_unlock_irqrestore(&state_lock, flags);
 	return count;
 }
 
@@ -471,17 +471,17 @@ static void update_running_avg(bool trigger_update)
 	s64 now;
 	unsigned long flags;
 
-	spin_lock_irqsave(&state_lock, flags);
+	raw_spin_lock_irqsave(&state_lock, flags);
 
 	now = ktime_to_ms(ktime_get());
 	if (now - rq_avg_timestamp_ms < rq_avg_period_ms - RQ_AVG_TOLERANCE) {
-		spin_unlock_irqrestore(&state_lock, flags);
+		raw_spin_unlock_irqrestore(&state_lock, flags);
 		return;
 	}
 	rq_avg_timestamp_ms = now;
 	sched_get_nr_running_avg(&avg, &iowait_avg, &big_avg);
 
-	spin_unlock_irqrestore(&state_lock, flags);
+	raw_spin_unlock_irqrestore(&state_lock, flags);
 
 	/*
 	 * Round up to the next integer if the average nr running tasks
@@ -570,7 +570,7 @@ static bool eval_need(struct cpu_data *f)
 	if (unlikely(!f->inited))
 		return 0;
 
-	spin_lock_irqsave(&state_lock, flags);
+	raw_spin_lock_irqsave(&state_lock, flags);
 	thres_idx = f->online_cpus ? f->online_cpus - 1 : 0;
 	list_for_each_entry(c, &f->lru, sib) {
 		if (c->busy >= f->busy_up_thres[thres_idx])
@@ -587,7 +587,7 @@ static bool eval_need(struct cpu_data *f)
 
 	if (need_cpus == last_need) {
 		f->need_ts = now;
-		spin_unlock_irqrestore(&state_lock, flags);
+		raw_spin_unlock_irqrestore(&state_lock, flags);
 		return 0;
 	}
 
@@ -611,7 +611,7 @@ static bool eval_need(struct cpu_data *f)
 
 	trace_core_ctl_eval_need(f->cpu, last_need, need_cpus,
 				 ret && need_flag);
-	spin_unlock_irqrestore(&state_lock, flags);
+	raw_spin_unlock_irqrestore(&state_lock, flags);
 
 	return ret && need_flag;
 }
@@ -670,14 +670,14 @@ static void wake_up_hotplug_thread(struct cpu_data *state)
 		}
 	}
 
-	spin_lock_irqsave(&state->pending_lock, flags);
+	raw_spin_lock_irqsave(&state->pending_lock, flags);
 	state->pending = true;
-	spin_unlock_irqrestore(&state->pending_lock, flags);
+	raw_spin_unlock_irqrestore(&state->pending_lock, flags);
 
 	if (no_wakeup) {
-		spin_lock_irqsave(&state_lock, flags);
+		raw_spin_lock_irqsave(&state_lock, flags);
 		mod_timer(&state->timer, jiffies);
-		spin_unlock_irqrestore(&state_lock, flags);
+		raw_spin_unlock_irqrestore(&state_lock, flags);
 	} else {
 		wake_up_process(state->hotplug_thread);
 	}
@@ -689,9 +689,9 @@ static void core_ctl_timer_func(unsigned long cpu)
 	unsigned long flags;
 
 	if (eval_need(state) && !state->disabled) {
-		spin_lock_irqsave(&state->pending_lock, flags);
+		raw_spin_lock_irqsave(&state->pending_lock, flags);
 		state->pending = true;
-		spin_unlock_irqrestore(&state->pending_lock, flags);
+		raw_spin_unlock_irqrestore(&state->pending_lock, flags);
 		wake_up_process(state->hotplug_thread);
 	}
 
@@ -736,8 +736,8 @@ static void update_lru(struct cpu_data *f)
 	struct cpu_data *c, *tmp;
 	unsigned long flags;
 
-	spin_lock_irqsave(&pending_lru_lock, flags);
-	spin_lock(&state_lock);
+	raw_spin_lock_irqsave(&pending_lru_lock, flags);
+	raw_spin_lock(&state_lock);
 
 	list_for_each_entry_safe(c, tmp, &f->pending_lru, pending_sib) {
 		list_del_init(&c->pending_sib);
@@ -745,8 +745,8 @@ static void update_lru(struct cpu_data *f)
 		list_add_tail(&c->sib, &f->lru);
 	}
 
-	spin_unlock(&state_lock);
-	spin_unlock_irqrestore(&pending_lru_lock, flags);
+	raw_spin_unlock(&state_lock);
+	raw_spin_unlock_irqrestore(&pending_lru_lock, flags);
 }
 
 static void __ref do_hotplug(struct cpu_data *f)
@@ -832,17 +832,17 @@ static int __ref try_hotplug(void *data)
 
 	while (1) {
 		set_current_state(TASK_INTERRUPTIBLE);
-		spin_lock_irqsave(&f->pending_lock, flags);
+		raw_spin_lock_irqsave(&f->pending_lock, flags);
 		if (!f->pending) {
-			spin_unlock_irqrestore(&f->pending_lock, flags);
+			raw_spin_unlock_irqrestore(&f->pending_lock, flags);
 			schedule();
 			if (kthread_should_stop())
 				break;
-			spin_lock_irqsave(&f->pending_lock, flags);
+			raw_spin_lock_irqsave(&f->pending_lock, flags);
 		}
 		set_current_state(TASK_RUNNING);
 		f->pending = false;
-		spin_unlock_irqrestore(&f->pending_lock, flags);
+		raw_spin_unlock_irqrestore(&f->pending_lock, flags);
 
 		do_hotplug(f);
 	}
@@ -855,13 +855,13 @@ static void add_to_pending_lru(struct cpu_data *state)
 	unsigned long flags;
 	struct cpu_data *f = &per_cpu(cpu_state, state->first_cpu);
 
-	spin_lock_irqsave(&pending_lru_lock, flags);
+	raw_spin_lock_irqsave(&pending_lru_lock, flags);
 
 	if (!list_empty(&state->pending_sib))
 		list_del(&state->pending_sib);
 	list_add_tail(&state->pending_sib, &f->pending_lru);
 
-	spin_unlock_irqrestore(&pending_lru_lock, flags);
+	raw_spin_unlock_irqrestore(&pending_lru_lock, flags);
 }
 
 static int __ref cpu_callback(struct notifier_block *nfb,
@@ -921,10 +921,10 @@ static int __ref cpu_callback(struct notifier_block *nfb,
 		 */
 		ret = mutex_trylock(&lru_lock);
 		if (ret) {
-			spin_lock_irqsave(&state_lock, flags);
+			raw_spin_lock_irqsave(&state_lock, flags);
 			list_del(&state->sib);
 			list_add_tail(&state->sib, &f->lru);
-			spin_unlock_irqrestore(&state_lock, flags);
+			raw_spin_unlock_irqrestore(&state_lock, flags);
 			mutex_unlock(&lru_lock);
 		} else {
 			/*
@@ -941,10 +941,10 @@ static int __ref cpu_callback(struct notifier_block *nfb,
 		/* Move a CPU to the end of the LRU when it goes offline. */
 		ret = mutex_trylock(&lru_lock);
 		if (ret) {
-			spin_lock_irqsave(&state_lock, flags);
+			raw_spin_lock_irqsave(&state_lock, flags);
 			list_del(&state->sib);
 			list_add_tail(&state->sib, &f->lru);
-			spin_unlock_irqrestore(&state_lock, flags);
+			raw_spin_unlock_irqrestore(&state_lock, flags);
 			mutex_unlock(&lru_lock);
 		} else {
 			add_to_pending_lru(state);
@@ -1017,7 +1017,7 @@ static int group_init(struct cpumask *mask)
 	INIT_LIST_HEAD(&f->lru);
 	INIT_LIST_HEAD(&f->pending_lru);
 	init_timer(&f->timer);
-	spin_lock_init(&f->pending_lock);
+	raw_spin_lock_init(&f->pending_lock);
 	f->timer.function = core_ctl_timer_func;
 	f->timer.data = first_cpu;
 
diff --git a/kernel/msm-3.18/kernel/sched/qhmp_core.c b/kernel/msm-3.18/kernel/sched/qhmp_core.c
index 865edbab5..a0f7bc8b9 100644
--- a/kernel/msm-3.18/kernel/sched/qhmp_core.c
+++ b/kernel/msm-3.18/kernel/sched/qhmp_core.c
@@ -4612,7 +4612,7 @@ again:
  *
  *         - in syscall or exception context, at the next outmost
  *           preempt_enable(). (this might be as soon as the wake_up()'s
- *           spin_unlock()!)
+ *           raw_spin_unlock()!)
  *
  *         - in IRQ context, return from interrupt-handler to
  *           preemptible context
@@ -5294,7 +5294,7 @@ static int __sched_setscheduler(struct task_struct *p,
 	int reset_on_fork;
 	int queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE;
 
-	/* may grab non-irq protected spin_locks */
+	/* may grab non-irq protected raw_spin_locks */
 	BUG_ON(in_interrupt());
 recheck:
 	/* double check policy once rq lock held */
@@ -6123,7 +6123,7 @@ EXPORT_SYMBOL(_cond_resched);
  *
  * This works OK both with and without CONFIG_PREEMPT. We do strange low-level
  * operations here to prevent schedule() from being called twice (once via
- * spin_unlock(), once by hand).
+ * raw_spin_unlock(), once by hand).
  */
 int __cond_resched_lock(spinlock_t *lock)
 {
@@ -6133,13 +6133,13 @@ int __cond_resched_lock(spinlock_t *lock)
 	lockdep_assert_held(lock);
 
 	if (spin_needbreak(lock) || resched) {
-		spin_unlock(lock);
+		raw_spin_unlock(lock);
 		if (resched)
 			__cond_resched();
 		else
 			cpu_relax();
 		ret = 1;
-		spin_lock(lock);
+		raw_spin_lock(lock);
 	}
 	return ret;
 }
@@ -9344,7 +9344,7 @@ void sched_online_group(struct task_group *tg, struct task_group *parent)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&task_group_lock, flags);
+	raw_spin_lock_irqsave(&task_group_lock, flags);
 	list_add_rcu(&tg->list, &task_groups);
 
 	WARN_ON(!parent); /* root should already exist */
@@ -9352,7 +9352,7 @@ void sched_online_group(struct task_group *tg, struct task_group *parent)
 	tg->parent = parent;
 	INIT_LIST_HEAD(&tg->children);
 	list_add_rcu(&tg->siblings, &parent->children);
-	spin_unlock_irqrestore(&task_group_lock, flags);
+	raw_spin_unlock_irqrestore(&task_group_lock, flags);
 }
 
 /* rcu callback to free various structures associated with a task group */
@@ -9378,10 +9378,10 @@ void sched_offline_group(struct task_group *tg)
 	for_each_possible_cpu(i)
 		unregister_fair_sched_group(tg, i);
 
-	spin_lock_irqsave(&task_group_lock, flags);
+	raw_spin_lock_irqsave(&task_group_lock, flags);
 	list_del_rcu(&tg->list);
 	list_del_rcu(&tg->siblings);
-	spin_unlock_irqrestore(&task_group_lock, flags);
+	raw_spin_unlock_irqrestore(&task_group_lock, flags);
 }
 
 /* change task's runqueue when it moves between groups.
diff --git a/kernel/msm-3.18/kernel/sched/qhmp_fair.c b/kernel/msm-3.18/kernel/sched/qhmp_fair.c
index f19c331cf..be9ed9074 100644
--- a/kernel/msm-3.18/kernel/sched/qhmp_fair.c
+++ b/kernel/msm-3.18/kernel/sched/qhmp_fair.c
@@ -996,7 +996,7 @@ static void account_numa_dequeue(struct rq *rq, struct task_struct *p)
 struct numa_group {
 	atomic_t refcount;
 
-	spinlock_t lock; /* nr_tasks, tasks */
+	raw_spinlock_t lock; /* nr_tasks, tasks */
 	int nr_tasks;
 	pid_t gid;
 	struct list_head task_list;
@@ -1713,7 +1713,7 @@ static void task_numa_placement(struct task_struct *p)
 	unsigned long fault_types[2] = { 0, 0 };
 	unsigned long total_faults;
 	u64 runtime, period;
-	spinlock_t *group_lock = NULL;
+	raw_spinlock_t *group_lock = NULL;
 
 	seq = ACCESS_ONCE(p->mm->numa_scan_seq);
 	if (p->numa_scan_seq == seq)
@@ -1728,7 +1728,7 @@ static void task_numa_placement(struct task_struct *p)
 	/* If the task is part of a group prevent parallel updates to group stats */
 	if (p->numa_group) {
 		group_lock = &p->numa_group->lock;
-		spin_lock_irq(group_lock);
+		raw_spin_lock_irq(group_lock);
 	}
 
 	/* Find the node with the highest number of faults */
@@ -1787,7 +1787,7 @@ static void task_numa_placement(struct task_struct *p)
 
 	if (p->numa_group) {
 		update_numa_active_node_mask(p->numa_group);
-		spin_unlock_irq(group_lock);
+		raw_spin_unlock_irq(group_lock);
 		max_nid = max_group_nid;
 	}
 
@@ -1830,7 +1830,7 @@ static void task_numa_group(struct task_struct *p, int cpupid, int flags,
 			return;
 
 		atomic_set(&grp->refcount, 1);
-		spin_lock_init(&grp->lock);
+		raw_spin_lock_init(&grp->lock);
 		INIT_LIST_HEAD(&grp->task_list);
 		grp->gid = p->pid;
 		/* Second half of the array tracks nids where faults happen */
@@ -1909,8 +1909,8 @@ static void task_numa_group(struct task_struct *p, int cpupid, int flags,
 	my_grp->nr_tasks--;
 	grp->nr_tasks++;
 
-	spin_unlock(&my_grp->lock);
-	spin_unlock_irq(&grp->lock);
+	raw_spin_unlock(&my_grp->lock);
+	raw_spin_unlock_irq(&grp->lock);
 
 	rcu_assign_pointer(p->numa_group, grp);
 
@@ -1930,14 +1930,14 @@ void task_numa_free(struct task_struct *p)
 	int i;
 
 	if (grp) {
-		spin_lock_irqsave(&grp->lock, flags);
+		raw_spin_lock_irqsave(&grp->lock, flags);
 		for (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)
 			grp->faults[i] -= p->numa_faults_memory[i];
 		grp->total_faults -= p->total_numa_faults;
 
 		list_del(&p->numa_entry);
 		grp->nr_tasks--;
-		spin_unlock_irqrestore(&grp->lock, flags);
+		raw_spin_unlock_irqrestore(&grp->lock, flags);
 		RCU_INIT_POINTER(p->numa_group, NULL);
 		put_numa_group(grp);
 	}
@@ -2853,7 +2853,7 @@ int sched_set_boost(int enable)
 	if (!sched_enable_hmp)
 		return -EINVAL;
 
-	spin_lock_irqsave(&boost_lock, flags);
+	raw_spin_lock_irqsave(&boost_lock, flags);
 
 	old_refcount = boost_refcount;
 
@@ -2872,7 +2872,7 @@ int sched_set_boost(int enable)
 		boost_kick_cpus();
 
 	trace_sched_set_boost(boost_refcount);
-	spin_unlock_irqrestore(&boost_lock, flags);
+	raw_spin_unlock_irqrestore(&boost_lock, flags);
 
 	return ret;
 }
@@ -9956,7 +9956,7 @@ static void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)
 			interval = get_sd_balance_interval(sd, idle != CPU_IDLE);
 		}
 		if (need_serialize)
-			spin_unlock(&balancing);
+			raw_spin_unlock(&balancing);
 out:
 		if (time_after(next_balance, sd->last_balance + interval)) {
 			next_balance = sd->last_balance + interval;
diff --git a/kernel/msm-3.18/kernel/sched/qhmp_sched.h b/kernel/msm-3.18/kernel/sched/qhmp_sched.h
index 3a3a048b0..9ff5f7c44 100644
--- a/kernel/msm-3.18/kernel/sched/qhmp_sched.h
+++ b/kernel/msm-3.18/kernel/sched/qhmp_sched.h
@@ -1724,7 +1724,7 @@ static inline void double_rq_lock(struct rq *rq1, struct rq *rq2);
  * fair double_lock_balance: Safely acquires both rq->locks in a fair
  * way at the expense of forcing extra atomic operations in all
  * invocations.  This assures that the double_lock is acquired using the
- * same underlying policy as the spinlock_t on this architecture, which
+ * same underlying policy as the raw_spinlock_t on this architecture, which
  * reduces latency compared to the unfair variant below.  However, it
  * also adds more overhead and therefore may reduce throughput.
  */
@@ -1791,22 +1791,22 @@ static inline void double_unlock_balance(struct rq *this_rq, struct rq *busiest)
 	lock_set_subclass(&this_rq->lock.dep_map, 0, _RET_IP_);
 }
 
-static inline void double_lock(spinlock_t *l1, spinlock_t *l2)
+static inline void double_lock(spinlock_t *l1, raw_spinlock_t *l2)
 {
 	if (l1 > l2)
 		swap(l1, l2);
 
-	spin_lock(l1);
-	spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
+	raw_spin_lock(l1);
+	raw_spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
 }
 
-static inline void double_lock_irq(spinlock_t *l1, spinlock_t *l2)
+static inline void double_lock_irq(spinlock_t *l1, raw_spinlock_t *l2)
 {
 	if (l1 > l2)
 		swap(l1, l2);
 
-	spin_lock_irq(l1);
-	spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
+	raw_spin_lock_irq(l1);
+	raw_spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
 }
 
 static inline void double_raw_lock(raw_spinlock_t *l1, raw_spinlock_t *l2)
diff --git a/kernel/msm-3.18/kernel/sched/sched_avg.c b/kernel/msm-3.18/kernel/sched/sched_avg.c
index 29d8a26a7..b383a5775 100644
--- a/kernel/msm-3.18/kernel/sched/sched_avg.c
+++ b/kernel/msm-3.18/kernel/sched/sched_avg.c
@@ -58,7 +58,7 @@ void sched_get_nr_running_avg(int *avg, int *iowait_avg, int *big_avg)
 	for_each_possible_cpu(cpu) {
 		unsigned long flags;
 
-		spin_lock_irqsave(&per_cpu(nr_lock, cpu), flags);
+		raw_spin_lock_irqsave(&per_cpu(nr_lock, cpu), flags);
 		curr_time = sched_clock();
 		diff = curr_time - per_cpu(last_time, cpu);
 		BUG_ON((s64)diff < 0);
@@ -78,7 +78,7 @@ void sched_get_nr_running_avg(int *avg, int *iowait_avg, int *big_avg)
 		per_cpu(nr_big_prod_sum, cpu) = 0;
 		per_cpu(iowait_prod_sum, cpu) = 0;
 
-		spin_unlock_irqrestore(&per_cpu(nr_lock, cpu), flags);
+		raw_spin_unlock_irqrestore(&per_cpu(nr_lock, cpu), flags);
 	}
 
 	diff = curr_time - last_get_time;
@@ -111,7 +111,7 @@ void sched_update_nr_prod(int cpu, long delta, bool inc)
 	u64 curr_time;
 	unsigned long flags, nr_running;
 
-	spin_lock_irqsave(&per_cpu(nr_lock, cpu), flags);
+	raw_spin_lock_irqsave(&per_cpu(nr_lock, cpu), flags);
 	nr_running = per_cpu(nr, cpu);
 	curr_time = sched_clock();
 	diff = curr_time - per_cpu(last_time, cpu);
@@ -124,6 +124,6 @@ void sched_update_nr_prod(int cpu, long delta, bool inc)
 	per_cpu(nr_prod_sum, cpu) += nr_running * diff;
 	per_cpu(nr_big_prod_sum, cpu) += nr_eligible_big_tasks(cpu) * diff;
 	per_cpu(iowait_prod_sum, cpu) += nr_iowait_cpu(cpu) * diff;
-	spin_unlock_irqrestore(&per_cpu(nr_lock, cpu), flags);
+	raw_spin_unlock_irqrestore(&per_cpu(nr_lock, cpu), flags);
 }
 EXPORT_SYMBOL(sched_update_nr_prod);
diff --git a/kernel/msm-3.18/kernel/time/tick-sched.c b/kernel/msm-3.18/kernel/time/tick-sched.c
index 15852119a..fae5e1a3b 100644
--- a/kernel/msm-3.18/kernel/time/tick-sched.c
+++ b/kernel/msm-3.18/kernel/time/tick-sched.c
@@ -34,7 +34,7 @@
 
 struct rq_data rq_info;
 struct workqueue_struct *rq_wq;
-spinlock_t rq_lock;
+raw_spinlock_t rq_lock;
 
 /*
  * Per cpu nohz control structure
diff --git a/kernel/msm-3.18/kernel/time/timer.c.rej b/kernel/msm-3.18/kernel/time/timer.c.rej
index a95b84576..73649fe18 100644
--- a/kernel/msm-3.18/kernel/time/timer.c.rej
+++ b/kernel/msm-3.18/kernel/time/timer.c.rej
@@ -21,9 +21,9 @@
 -			if (likely(base->running_timer != timer)) {
 -				/* See the comment in lock_timer_base() */
 -				timer_set_base(timer, NULL);
--				spin_unlock(&base->lock);
+-				raw_spin_unlock(&base->lock);
 -				base = new_base;
--				spin_lock(&base->lock);
+-				raw_spin_lock(&base->lock);
 -				timer_set_base(timer, base);
 -			}
 +	if (base != new_base) {
@@ -37,9 +37,9 @@
 +		if (likely(base->running_timer != timer)) {
 +			/* See the comment in lock_timer_base() */
 +			timer_set_base(timer, NULL);
-+			spin_unlock(&base->lock);
++			raw_spin_unlock(&base->lock);
 +			base = new_base;
-+			spin_lock(&base->lock);
++			raw_spin_lock(&base->lock);
 +			timer_set_base(timer, base);
  		}
  	}
diff --git a/kernel/msm-3.18/kernel/trace/ipc_logging.c b/kernel/msm-3.18/kernel/trace/ipc_logging.c
index c31bd90ad..356312a55 100644
--- a/kernel/msm-3.18/kernel/trace/ipc_logging.c
+++ b/kernel/msm-3.18/kernel/trace/ipc_logging.c
@@ -293,7 +293,7 @@ void ipc_log_write(void *ctxt, struct encode_context *ectxt)
 	}
 
 	read_lock_irqsave(&context_list_lock_lha1, flags);
-	spin_lock(&ilctxt->context_lock_lhb1);
+	raw_spin_lock(&ilctxt->context_lock_lhb1);
 	while (ilctxt->write_avail <= ectxt->offset)
 		msg_drop(ilctxt);
 
@@ -323,7 +323,7 @@ void ipc_log_write(void *ctxt, struct encode_context *ectxt)
 	ilctxt->write_page->hdr.write_offset += bytes_to_write;
 	ilctxt->write_avail -= ectxt->offset;
 	complete(&ilctxt->read_avail);
-	spin_unlock(&ilctxt->context_lock_lhb1);
+	raw_spin_unlock(&ilctxt->context_lock_lhb1);
 	read_unlock_irqrestore(&context_list_lock_lha1, flags);
 }
 EXPORT_SYMBOL(ipc_log_write);
@@ -546,13 +546,13 @@ int ipc_log_extract(void *ctxt, char *buff, int size)
 	dctxt.buff = buff;
 	dctxt.size = size;
 	read_lock_irqsave(&context_list_lock_lha1, flags);
-	spin_lock(&ilctxt->context_lock_lhb1);
+	raw_spin_lock(&ilctxt->context_lock_lhb1);
 	while (dctxt.size >= MAX_MSG_DECODED_SIZE &&
 	       !is_nd_read_empty(ilctxt)) {
 		msg_read(ilctxt, &ectxt);
 		deserialize_func = get_deserialization_func(ilctxt,
 							ectxt.hdr.type);
-		spin_unlock(&ilctxt->context_lock_lhb1);
+		raw_spin_unlock(&ilctxt->context_lock_lhb1);
 		read_unlock_irqrestore(&context_list_lock_lha1, flags);
 		if (deserialize_func)
 			deserialize_func(&ectxt, &dctxt);
@@ -560,11 +560,11 @@ int ipc_log_extract(void *ctxt, char *buff, int size)
 			pr_err("%s: unknown message 0x%x\n",
 				__func__, ectxt.hdr.type);
 		read_lock_irqsave(&context_list_lock_lha1, flags);
-		spin_lock(&ilctxt->context_lock_lhb1);
+		raw_spin_lock(&ilctxt->context_lock_lhb1);
 	}
 	if ((size - dctxt.size) == 0)
 		reinit_completion(&ilctxt->read_avail);
-	spin_unlock(&ilctxt->context_lock_lhb1);
+	raw_spin_unlock(&ilctxt->context_lock_lhb1);
 	read_unlock_irqrestore(&context_list_lock_lha1, flags);
 	return size - dctxt.size;
 }
@@ -727,11 +727,11 @@ int add_deserialization_func(void *ctxt, int type,
 		return -ENOSPC;
 
 	read_lock_irqsave(&context_list_lock_lha1, flags);
-	spin_lock(&ilctxt->context_lock_lhb1);
+	raw_spin_lock(&ilctxt->context_lock_lhb1);
 	df_info->type = type;
 	df_info->dfunc = dfunc;
 	list_add_tail(&df_info->list, &ilctxt->dfunc_info_list);
-	spin_unlock(&ilctxt->context_lock_lhb1);
+	raw_spin_unlock(&ilctxt->context_lock_lhb1);
 	read_unlock_irqrestore(&context_list_lock_lha1, flags);
 	return 0;
 }
@@ -779,7 +779,7 @@ void *ipc_log_context_create(int max_num_pages,
 	init_completion(&ctxt->read_avail);
 	INIT_LIST_HEAD(&ctxt->page_list);
 	INIT_LIST_HEAD(&ctxt->dfunc_info_list);
-	spin_lock_init(&ctxt->context_lock_lhb1);
+	raw_spin_lock_init(&ctxt->context_lock_lhb1);
 	for (page_cnt = 0; page_cnt < max_num_pages; page_cnt++) {
 		pg = kzalloc(sizeof(struct ipc_log_page), GFP_KERNEL);
 		if (!pg) {
@@ -795,9 +795,9 @@ void *ipc_log_context_create(int max_num_pages,
 		pg->hdr.magic = IPC_LOGGING_MAGIC_NUM;
 		pg->hdr.nmagic = ~(IPC_LOGGING_MAGIC_NUM);
 
-		spin_lock_irqsave(&ctxt->context_lock_lhb1, flags);
+		raw_spin_lock_irqsave(&ctxt->context_lock_lhb1, flags);
 		list_add_tail(&pg->hdr.list, &ctxt->page_list);
-		spin_unlock_irqrestore(&ctxt->context_lock_lhb1, flags);
+		raw_spin_unlock_irqrestore(&ctxt->context_lock_lhb1, flags);
 	}
 
 	ctxt->log_id = (uint64_t)(uintptr_t)ctxt;
diff --git a/kernel/msm-3.18/kernel/trace/ipc_logging_private.h b/kernel/msm-3.18/kernel/trace/ipc_logging_private.h
index 3ac950695..ce5e7d1e2 100644
--- a/kernel/msm-3.18/kernel/trace/ipc_logging_private.h
+++ b/kernel/msm-3.18/kernel/trace/ipc_logging_private.h
@@ -117,7 +117,7 @@ struct ipc_log_context {
 	uint32_t write_avail;
 	struct dentry *dent;
 	struct list_head dfunc_info_list;
-	spinlock_t context_lock_lhb1;
+	raw_spinlock_t context_lock_lhb1;
 	struct completion read_avail;
 };
 
diff --git a/kernel/msm-3.18/kernel/trace/trace_cpu_freq_switch.c b/kernel/msm-3.18/kernel/trace/trace_cpu_freq_switch.c
index 18d2390cc..a04bc4d09 100644
--- a/kernel/msm-3.18/kernel/trace/trace_cpu_freq_switch.c
+++ b/kernel/msm-3.18/kernel/trace/trace_cpu_freq_switch.c
@@ -92,7 +92,7 @@ static int tr_insert(struct rb_root *root, struct trans *tr)
 }
 
 struct trans_state {
-	spinlock_t lock;
+	raw_spinlock_t lock;
 	unsigned int start_freq;
 	unsigned int end_freq;
 	ktime_t start_t;
@@ -107,12 +107,12 @@ static void probe_start(void *ignore, unsigned int start_freq,
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&state_lock, flags);
+	raw_spin_lock_irqsave(&state_lock, flags);
 	per_cpu(freq_trans_state, cpu).start_freq = start_freq;
 	per_cpu(freq_trans_state, cpu).end_freq = end_freq;
 	per_cpu(freq_trans_state, cpu).start_t = ktime_get();
 	per_cpu(freq_trans_state, cpu).started = true;
-	spin_unlock_irqrestore(&state_lock, flags);
+	raw_spin_unlock_irqrestore(&state_lock, flags);
 }
 
 static void probe_end(void *ignore, unsigned int cpu)
@@ -122,7 +122,7 @@ static void probe_end(void *ignore, unsigned int cpu)
 	s64 dur_us;
 	ktime_t dur_t, end_t = ktime_get();
 
-	spin_lock_irqsave(&state_lock, flags);
+	raw_spin_lock_irqsave(&state_lock, flags);
 
 	if (!per_cpu(freq_trans_state, cpu).started)
 		goto out;
@@ -156,7 +156,7 @@ static void probe_end(void *ignore, unsigned int cpu)
 
 	per_cpu(freq_trans_state, cpu).started = false;
 out:
-	spin_unlock_irqrestore(&state_lock, flags);
+	raw_spin_unlock_irqrestore(&state_lock, flags);
 }
 
 static void *freq_switch_stat_start(struct tracer_stat *trace)
@@ -164,9 +164,9 @@ static void *freq_switch_stat_start(struct tracer_stat *trace)
 	struct rb_node *n;
 	unsigned long flags;
 
-	spin_lock_irqsave(&state_lock, flags);
+	raw_spin_lock_irqsave(&state_lock, flags);
 	n = rb_first(&freq_trans_tree);
-	spin_unlock_irqrestore(&state_lock, flags);
+	raw_spin_unlock_irqrestore(&state_lock, flags);
 
 	return n;
 }
@@ -176,9 +176,9 @@ static void *freq_switch_stat_next(void *prev, int idx)
 	struct rb_node *n;
 	unsigned long flags;
 
-	spin_lock_irqsave(&state_lock, flags);
+	raw_spin_lock_irqsave(&state_lock, flags);
 	n = rb_next(prev);
-	spin_unlock_irqrestore(&state_lock, flags);
+	raw_spin_unlock_irqrestore(&state_lock, flags);
 
 	return n;
 }
@@ -188,12 +188,12 @@ static int freq_switch_stat_show(struct seq_file *s, void *p)
 	unsigned long flags;
 	struct trans *tr = p;
 
-	spin_lock_irqsave(&state_lock, flags);
+	raw_spin_lock_irqsave(&state_lock, flags);
 	seq_printf(s, "%3d %9d %8d %5d %6lld %6d %6d\n", tr->cpu,
 		   tr->start_freq, tr->end_freq, tr->count,
 		   div_s64(ktime_to_us(tr->total_t), tr->count),
 		   tr->min_us, tr->max_us);
-	spin_unlock_irqrestore(&state_lock, flags);
+	raw_spin_unlock_irqrestore(&state_lock, flags);
 
 	return 0;
 }
@@ -203,9 +203,9 @@ static void freq_switch_stat_release(void *stat)
 	struct trans *tr = stat;
 	unsigned long flags;
 
-	spin_lock_irqsave(&state_lock, flags);
+	raw_spin_lock_irqsave(&state_lock, flags);
 	rb_erase(&tr->node, &freq_trans_tree);
-	spin_unlock_irqrestore(&state_lock, flags);
+	raw_spin_unlock_irqrestore(&state_lock, flags);
 	kfree(tr);
 }
 
diff --git a/kernel/msm-3.18/lib/ubsan.c b/kernel/msm-3.18/lib/ubsan.c
index 8799ae5e2..e41cd8eab 100644
--- a/kernel/msm-3.18/lib/ubsan.c
+++ b/kernel/msm-3.18/lib/ubsan.c
@@ -152,7 +152,7 @@ static void ubsan_prologue(struct source_location *location,
 			unsigned long *flags)
 {
 	current->in_ubsan++;
-	spin_lock_irqsave(&report_lock, *flags);
+	raw_spin_lock_irqsave(&report_lock, *flags);
 
 	pr_err("========================================"
 		"========================================\n");
@@ -164,7 +164,7 @@ static void ubsan_epilogue(unsigned long *flags)
 	dump_stack();
 	pr_err("========================================"
 		"========================================\n");
-	spin_unlock_irqrestore(&report_lock, *flags);
+	raw_spin_unlock_irqrestore(&report_lock, *flags);
 	current->in_ubsan--;
 }
 
diff --git a/kernel/msm-3.18/mm/cma.h b/kernel/msm-3.18/mm/cma.h
index 1132d7335..254cceb8b 100644
--- a/kernel/msm-3.18/mm/cma.h
+++ b/kernel/msm-3.18/mm/cma.h
@@ -9,7 +9,7 @@ struct cma {
 	struct mutex    lock;
 #ifdef CONFIG_CMA_DEBUGFS
 	struct hlist_head mem_head;
-	spinlock_t mem_head_lock;
+	raw_spinlock_t mem_head_lock;
 #endif
 };
 
diff --git a/kernel/msm-3.18/mm/cma_debug.c b/kernel/msm-3.18/mm/cma_debug.c
index f8e4b60db..2f2a5942e 100644
--- a/kernel/msm-3.18/mm/cma_debug.c
+++ b/kernel/msm-3.18/mm/cma_debug.c
@@ -71,21 +71,21 @@ DEFINE_SIMPLE_ATTRIBUTE(cma_maxchunk_fops, cma_maxchunk_get, NULL, "%llu\n");
 
 static void cma_add_to_cma_mem_list(struct cma *cma, struct cma_mem *mem)
 {
-	spin_lock(&cma->mem_head_lock);
+	raw_spin_lock(&cma->mem_head_lock);
 	hlist_add_head(&mem->node, &cma->mem_head);
-	spin_unlock(&cma->mem_head_lock);
+	raw_spin_unlock(&cma->mem_head_lock);
 }
 
 static struct cma_mem *cma_get_entry_from_list(struct cma *cma)
 {
 	struct cma_mem *mem = NULL;
 
-	spin_lock(&cma->mem_head_lock);
+	raw_spin_lock(&cma->mem_head_lock);
 	if (!hlist_empty(&cma->mem_head)) {
 		mem = hlist_entry(cma->mem_head.first, struct cma_mem, node);
 		hlist_del_init(&mem->node);
 	}
-	spin_unlock(&cma->mem_head_lock);
+	raw_spin_unlock(&cma->mem_head_lock);
 
 	return mem;
 }
diff --git a/kernel/msm-3.18/mm/swap_ratio.c b/kernel/msm-3.18/mm/swap_ratio.c
index cf2a6e2ae..f2902d307 100644
--- a/kernel/msm-3.18/mm/swap_ratio.c
+++ b/kernel/msm-3.18/mm/swap_ratio.c
@@ -77,8 +77,8 @@ static int swap_ratio_slow(struct swap_info_struct **si)
 	struct swap_info_struct *n = NULL;
 	int ret = 0;
 
-	spin_lock(&(*si)->lock);
-	spin_lock(&swap_avail_lock);
+	raw_spin_lock(&(*si)->lock);
+	raw_spin_lock(&swap_avail_lock);
 	if (&(*si)->avail_list == plist_last(&swap_avail_head)) {
 		/* just to make skip work */
 		n = *si;
@@ -94,9 +94,9 @@ static int swap_ratio_slow(struct swap_info_struct **si)
 		goto skip;
 	}
 
-	spin_unlock(&swap_avail_lock);
-	spin_lock(&n->lock);
-	spin_lock(&swap_avail_lock);
+	raw_spin_unlock(&swap_avail_lock);
+	raw_spin_lock(&n->lock);
+	raw_spin_lock(&swap_avail_lock);
 
 	if ((*si)->flags & SWP_FAST) {
 		if ((*si)->write_pending) {
@@ -114,7 +114,7 @@ static int swap_ratio_slow(struct swap_info_struct **si)
 				plist_requeue(&(*si)->avail_list,
 					&swap_avail_head);
 				n->write_pending--;
-				spin_unlock(&(*si)->lock);
+				raw_spin_unlock(&(*si)->lock);
 				*si = n;
 				goto skip;
 			} else {
@@ -139,7 +139,7 @@ static int swap_ratio_slow(struct swap_info_struct **si)
 			/* requeue slow device to the end */
 			plist_requeue(&(*si)->avail_list, &swap_avail_head);
 			n->write_pending--;
-			spin_unlock(&(*si)->lock);
+			raw_spin_unlock(&(*si)->lock);
 			*si = n;
 			goto skip;
 		} else {
@@ -153,18 +153,18 @@ static int swap_ratio_slow(struct swap_info_struct **si)
 				n->write_pending--;
 				plist_requeue(&(*si)->avail_list,
 					&swap_avail_head);
-				spin_unlock(&(*si)->lock);
+				raw_spin_unlock(&(*si)->lock);
 				*si = n;
 				goto skip;
 			}
 		}
 	}
 exit:
-	spin_unlock(&(*si)->lock);
+	raw_spin_unlock(&(*si)->lock);
 skip:
-	spin_unlock(&swap_avail_lock);
+	raw_spin_unlock(&swap_avail_lock);
 	/* n and si would have got interchanged */
-	spin_unlock(&n->lock);
+	raw_spin_unlock(&n->lock);
 	return ret;
 }
 
diff --git a/kernel/msm-3.18/mm/zcache.c b/kernel/msm-3.18/mm/zcache.c
index 3df9879ee..53b77f9ad 100644
--- a/kernel/msm-3.18/mm/zcache.c
+++ b/kernel/msm-3.18/mm/zcache.c
@@ -127,7 +127,7 @@ struct zcache_pool {
 struct _zcache {
 	struct zcache_pool *pools[MAX_ZCACHE_POOLS];
 	u32 num_pools;			/* Current no. of zcache pools */
-	spinlock_t pool_lock;		/* Protects pools[] and num_pools */
+	raw_spinlock_t pool_lock;		/* Protects pools[] and num_pools */
 };
 struct _zcache zcache;
 
@@ -139,7 +139,7 @@ struct zcache_rbnode {
 	struct rb_node rb_node;
 	int rb_index;
 	struct radix_tree_root ratree; /* Page radix tree per inode rbtree */
-	spinlock_t ra_lock;		/* Protects radix tree */
+	raw_spinlock_t ra_lock;		/* Protects radix tree */
 	struct kref refcount;
 };
 
@@ -528,7 +528,7 @@ static int zcache_store_zaddr(struct zcache_pool *zpool,
 			return -ENOMEM;
 
 		INIT_RADIX_TREE(&rbnode->ratree, GFP_ATOMIC|__GFP_NOWARN);
-		spin_lock_init(&rbnode->ra_lock);
+		raw_spin_lock_init(&rbnode->ra_lock);
 		rbnode->rb_index = rb_index;
 		kref_init(&rbnode->refcount);
 		RB_CLEAR_NODE(&rbnode->rb_node);
@@ -552,7 +552,7 @@ static int zcache_store_zaddr(struct zcache_pool *zpool,
 	}
 
 	/* Succfully got a zcache_rbnode when arriving here */
-	spin_lock_irqsave(&rbnode->ra_lock, flags);
+	raw_spin_lock_irqsave(&rbnode->ra_lock, flags);
 	dup_zaddr = radix_tree_delete(&rbnode->ratree, ra_index);
 	if (unlikely(dup_zaddr)) {
 		WARN_ON("duplicated, will be replaced!\n");
@@ -569,15 +569,15 @@ static int zcache_store_zaddr(struct zcache_pool *zpool,
 	/* Insert zcache_ra_handle to ratree */
 	ret = radix_tree_insert(&rbnode->ratree, ra_index,
 				(void *)zaddr);
-	spin_unlock_irqrestore(&rbnode->ra_lock, flags);
+	raw_spin_unlock_irqrestore(&rbnode->ra_lock, flags);
 	if (unlikely(ret)) {
 		write_lock_irqsave(&zpool->rb_lock, flags);
-		spin_lock(&rbnode->ra_lock);
+		raw_spin_lock(&rbnode->ra_lock);
 
 		if (zcache_rbnode_empty(rbnode))
 			zcache_rbnode_isolate(zpool, rbnode, 1);
 
-		spin_unlock(&rbnode->ra_lock);
+		raw_spin_unlock(&rbnode->ra_lock);
 		write_unlock_irqrestore(&zpool->rb_lock, flags);
 	}
 
@@ -603,16 +603,16 @@ static void *zcache_load_delete_zaddr(struct zcache_pool *zpool,
 
 	BUG_ON(rbnode->rb_index != rb_index);
 
-	spin_lock_irqsave(&rbnode->ra_lock, flags);
+	raw_spin_lock_irqsave(&rbnode->ra_lock, flags);
 	zaddr = radix_tree_delete(&rbnode->ratree, ra_index);
-	spin_unlock_irqrestore(&rbnode->ra_lock, flags);
+	raw_spin_unlock_irqrestore(&rbnode->ra_lock, flags);
 
 	/* rb_lock and ra_lock must be taken again in the given sequence */
 	write_lock_irqsave(&zpool->rb_lock, flags);
-	spin_lock(&rbnode->ra_lock);
+	raw_spin_lock(&rbnode->ra_lock);
 	if (zcache_rbnode_empty(rbnode))
 		zcache_rbnode_isolate(zpool, rbnode, 1);
-	spin_unlock(&rbnode->ra_lock);
+	raw_spin_unlock(&rbnode->ra_lock);
 	write_unlock_irqrestore(&zpool->rb_lock, flags);
 
 	kref_put(&rbnode->refcount, zcache_rbnode_release);
@@ -861,14 +861,14 @@ static void zcache_flush_inode(int pool_id, struct cleancache_filekey key)
 	}
 
 	kref_get(&rbnode->refcount);
-	spin_lock_irqsave(&rbnode->ra_lock, flags2);
+	raw_spin_lock_irqsave(&rbnode->ra_lock, flags2);
 
 	zcache_flush_ratree(zpool, rbnode);
 	if (zcache_rbnode_empty(rbnode))
 		/* When arrvied here, we already hold rb_lock */
 		zcache_rbnode_isolate(zpool, rbnode, 1);
 
-	spin_unlock_irqrestore(&rbnode->ra_lock, flags2);
+	raw_spin_unlock_irqrestore(&rbnode->ra_lock, flags2);
 	write_unlock_irqrestore(&zpool->rb_lock, flags1);
 	kref_put(&rbnode->refcount, zcache_rbnode_release);
 }
@@ -899,11 +899,11 @@ static void zcache_flush_fs(int pool_id)
 		rbnode = rb_next(rbnode);
 		if (z_rbnode) {
 			kref_get(&z_rbnode->refcount);
-			spin_lock_irqsave(&z_rbnode->ra_lock, flags2);
+			raw_spin_lock_irqsave(&z_rbnode->ra_lock, flags2);
 			zcache_flush_ratree(zpool, z_rbnode);
 			if (zcache_rbnode_empty(z_rbnode))
 				zcache_rbnode_isolate(zpool, z_rbnode, 1);
-			spin_unlock_irqrestore(&z_rbnode->ra_lock, flags2);
+			raw_spin_unlock_irqrestore(&z_rbnode->ra_lock, flags2);
 			kref_put(&z_rbnode->refcount, zcache_rbnode_release);
 		}
 	}
@@ -969,7 +969,7 @@ static int zcache_create_pool(void)
 		goto out;
 	}
 
-	spin_lock(&zcache.pool_lock);
+	raw_spin_lock(&zcache.pool_lock);
 	if (zcache.num_pools == MAX_ZCACHE_POOLS) {
 		pr_err("Cannot create new pool (limit:%u)\n", MAX_ZCACHE_POOLS);
 		zbud_destroy_pool(zpool->pool);
@@ -989,7 +989,7 @@ static int zcache_create_pool(void)
 	pr_info("New pool created id:%d\n", ret);
 
 out_unlock:
-	spin_unlock(&zcache.pool_lock);
+	raw_spin_unlock(&zcache.pool_lock);
 out:
 	return ret;
 }
@@ -1001,13 +1001,13 @@ static void zcache_destroy_pool(struct zcache_pool *zpool)
 	if (!zpool)
 		return;
 
-	spin_lock(&zcache.pool_lock);
+	raw_spin_lock(&zcache.pool_lock);
 	zcache.num_pools--;
 	for (i = 0; i < MAX_ZCACHE_POOLS; i++)
 		if (zcache.pools[i] == zpool)
 			break;
 	zcache.pools[i] = NULL;
-	spin_unlock(&zcache.pool_lock);
+	raw_spin_unlock(&zcache.pool_lock);
 
 	if (!RB_EMPTY_ROOT(&zpool->rbtree))
 		WARN_ON("Memory leak detected. Freeing non-empty pool!\n");
@@ -1145,7 +1145,7 @@ static int __init init_zcache(void)
 		goto pcpufail;
 	}
 
-	spin_lock_init(&zcache.pool_lock);
+	raw_spin_lock_init(&zcache.pool_lock);
 	cleancache_register_ops(&zcache_ops);
 
 	if (zcache_debugfs_init())
diff --git a/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6afe.c b/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6afe.c
index 9c60419df..4c6c6040b 100644
--- a/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6afe.c
+++ b/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6afe.c
@@ -3633,7 +3633,7 @@ struct afe_audio_client *q6afe_audio_client_alloc(void *priv)
 	mutex_init(&ac->cmd_lock);
 	for (lcnt = 0; lcnt <= OUT; lcnt++) {
 		mutex_init(&ac->port[lcnt].lock);
-		spin_lock_init(&ac->port[lcnt].dsp_lock);
+		raw_spin_lock_init(&ac->port[lcnt].dsp_lock);
 	}
 	atomic_set(&ac->cmd_state, 0);
 
diff --git a/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6asm.c b/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6asm.c
index 25d4c275c..f974df5f3 100644
--- a/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6asm.c
+++ b/kernel/msm-3.18/sound/soc/msm/qdsp6v2/q6asm.c
@@ -432,13 +432,13 @@ static void q6asm_session_free(struct audio_client *ac)
 	ac->perf_mode = LEGACY_PCM_MODE;
 	ac->fptr_cache_ops = NULL;
 
-	spin_lock_irqsave(&ac->no_wait_que_spinlock, flags);
+	raw_spin_lock_irqsave(&ac->no_wait_que_spinlock, flags);
 	list_for_each_safe(ptr, next, &ac->no_wait_que) {
 		node = list_entry(ptr, struct asm_no_wait_node, list);
 		list_del(&node->list);
 		kfree(node);
 	}
-	spin_unlock_irqrestore(&ac->no_wait_que_spinlock, flags);
+	raw_spin_unlock_irqrestore(&ac->no_wait_que_spinlock, flags);
 	return;
 }
 
@@ -456,9 +456,9 @@ static int q6asm_add_nowait_opcode(struct audio_client *ac, uint32_t opcode)
 	new_node->opcode = opcode;
 	INIT_LIST_HEAD(&new_node->list);
 
-	spin_lock_irqsave(&ac->no_wait_que_spinlock, flags);
+	raw_spin_lock_irqsave(&ac->no_wait_que_spinlock, flags);
 	list_add_tail(&new_node->list, &ac->no_wait_que);
-	spin_unlock_irqrestore(&ac->no_wait_que_spinlock, flags);
+	raw_spin_unlock_irqrestore(&ac->no_wait_que_spinlock, flags);
 
 done:
 	return ret;
@@ -472,7 +472,7 @@ static bool q6asm_remove_nowait_opcode(struct audio_client *ac,
 	unsigned long flags;
 	bool ret = false;
 
-	spin_lock_irqsave(&ac->no_wait_que_spinlock, flags);
+	raw_spin_lock_irqsave(&ac->no_wait_que_spinlock, flags);
 	list_for_each_safe(ptr, next, &ac->no_wait_que) {
 		node = list_entry(ptr,
 			struct asm_no_wait_node, list);
@@ -486,7 +486,7 @@ static bool q6asm_remove_nowait_opcode(struct audio_client *ac,
 
 	pr_debug("%s: nowait opcode NOT found 0x%x\n", __func__, opcode);
 done:
-	spin_unlock_irqrestore(&ac->no_wait_que_spinlock, flags);
+	raw_spin_unlock_irqrestore(&ac->no_wait_que_spinlock, flags);
 	return ret;
 }
 
@@ -1079,7 +1079,7 @@ struct audio_client *q6asm_audio_client_alloc(app_cb cb, void *priv)
 	/* DSP expects stream id from 1 */
 	ac->stream_id = 1;
 	INIT_LIST_HEAD(&ac->no_wait_que);
-	spin_lock_init(&ac->no_wait_que_spinlock);
+	raw_spin_lock_init(&ac->no_wait_que_spinlock);
 	ac->apr = apr_register("ADSP", "ASM", \
 			(apr_fn)q6asm_callback,\
 			((ac->session) << 8 | 0x0001),\
@@ -1121,7 +1121,7 @@ struct audio_client *q6asm_audio_client_alloc(app_cb cb, void *priv)
 	mutex_init(&ac->cmd_lock);
 	for (lcnt = 0; lcnt <= OUT; lcnt++) {
 		mutex_init(&ac->port[lcnt].lock);
-		spin_lock_init(&ac->port[lcnt].dsp_lock);
+		raw_spin_lock_init(&ac->port[lcnt].dsp_lock);
 	}
 	atomic_set(&ac->cmd_state, 0);
 	atomic_set(&ac->cmd_state_pp, 0);
@@ -1480,21 +1480,21 @@ static int32_t q6asm_srvc_callback(struct apr_client_data *data, void *priv)
 	case ASM_CMDRSP_SHARED_MEM_MAP_REGIONS:{
 		pr_debug("%s:PL#0[0x%x] dir=0x%x s_id=0x%x\n",
 				__func__, payload[0], dir, sid);
-		spin_lock_irqsave(&port->dsp_lock, dsp_flags);
+		raw_spin_lock_irqsave(&port->dsp_lock, dsp_flags);
 		if (atomic_cmpxchg(&ac->mem_state, -1, 0) == -1) {
 			ac->port[dir].tmp_hdl = payload[0];
 			swait_wake(&ac->mem_wait);
 		}
-		spin_unlock_irqrestore(&port->dsp_lock, dsp_flags);
+		raw_spin_unlock_irqrestore(&port->dsp_lock, dsp_flags);
 		break;
 	}
 	case ASM_CMD_SHARED_MEM_UNMAP_REGIONS:{
 		pr_debug("%s: PL#0[0x%x]PL#1 [0x%x]\n",
 					__func__, payload[0], payload[1]);
-		spin_lock_irqsave(&port->dsp_lock, dsp_flags);
+		raw_spin_lock_irqsave(&port->dsp_lock, dsp_flags);
 		if (atomic_cmpxchg(&ac->mem_state, -1, 0) == -1)
 			swait_wake(&ac->mem_wait);
-		spin_unlock_irqrestore(&port->dsp_lock, dsp_flags);
+		raw_spin_unlock_irqrestore(&port->dsp_lock, dsp_flags);
 
 		break;
 	}
@@ -1782,7 +1782,7 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 								__func__);
 				return -EINVAL;
 			}
-			spin_lock_irqsave(&port->dsp_lock, dsp_flags);
+			raw_spin_lock_irqsave(&port->dsp_lock, dsp_flags);
 			if (lower_32_bits(port->buf[data->token].phys) !=
 			payload[0] ||
 			msm_audio_populate_upper_32_bits(
@@ -1791,13 +1791,13 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 				__func__, &port->buf[data->token].phys);
 				pr_err("%s: rxedl[0x%x] rxedu [0x%x]\n",
 					__func__, payload[0], payload[1]);
-				spin_unlock_irqrestore(&port->dsp_lock,
+				raw_spin_unlock_irqrestore(&port->dsp_lock,
 								dsp_flags);
 				return -EINVAL;
 			}
 			token = data->token;
 			port->buf[token].used = 1;
-			spin_unlock_irqrestore(&port->dsp_lock, dsp_flags);
+			raw_spin_unlock_irqrestore(&port->dsp_lock, dsp_flags);
 
 			config_debug_fs_write_cb();
 
@@ -1866,7 +1866,7 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 				pr_err("%s: Unexpected Write Done\n", __func__);
 				return -EINVAL;
 			}
-			spin_lock_irqsave(&port->dsp_lock, dsp_flags);
+			raw_spin_lock_irqsave(&port->dsp_lock, dsp_flags);
 			token = data->token;
 			port->buf[token].used = 0;
 			if (lower_32_bits(port->buf[token].phys) !=
@@ -1880,13 +1880,13 @@ static int32_t q6asm_callback(struct apr_client_data *data, void *priv)
 					__func__,
 				payload[READDONE_IDX_BUFADD_LSW],
 				payload[READDONE_IDX_BUFADD_MSW]);
-				spin_unlock_irqrestore(&port->dsp_lock,
+				raw_spin_unlock_irqrestore(&port->dsp_lock,
 							dsp_flags);
 				break;
 			}
 			port->buf[token].actual_size =
 				payload[READDONE_IDX_SIZE];
-			spin_unlock_irqrestore(&port->dsp_lock, dsp_flags);
+			raw_spin_unlock_irqrestore(&port->dsp_lock, dsp_flags);
 		}
 		break;
 	}
@@ -7960,11 +7960,11 @@ static int __init q6asm_init(void)
 	mutex_init(&common_client.cmd_lock);
 	for (lcnt = 0; lcnt <= OUT; lcnt++) {
 		mutex_init(&common_client.port[lcnt].lock);
-		spin_lock_init(&common_client.port[lcnt].dsp_lock);
+		raw_spin_lock_init(&common_client.port[lcnt].dsp_lock);
 	}
 	atomic_set(&common_client.cmd_state, 0);
 	atomic_set(&common_client.nowait_cmd_cnt, 0);
-	spin_lock_init(&common_client.no_wait_que_spinlock);
+	raw_spin_lock_init(&common_client.no_wait_que_spinlock);
 	INIT_LIST_HEAD(&common_client.no_wait_que);
 	atomic_set(&common_client.mem_state, 0);
 
-- 
2.49.0


From b3bd0a0768646cc3924abc9f87392db817754287 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Wed, 9 Jan 2019 15:42:59 -0800
Subject: [PATCH 04/20] More raw_spinlock changes

---
 kernel/msm-3.18/drivers/soc/qcom/smd.c          |  4 ++--
 kernel/msm-3.18/include/linux/remote_spinlock.h | 16 ++++++++--------
 2 files changed, 10 insertions(+), 10 deletions(-)

diff --git a/kernel/msm-3.18/drivers/soc/qcom/smd.c b/kernel/msm-3.18/drivers/soc/qcom/smd.c
index 79df626a6..0f271d36d 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smd.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/smd.c
@@ -70,7 +70,7 @@ static struct smsm_shared_info smsm_info;
 static struct kfifo smsm_snapshot_fifo;
 static struct wakeup_source smsm_snapshot_ws;
 static int smsm_snapshot_count;
-static DEFINE_SPINLOCK(smsm_snapshot_count_lock);
+static DEFINE_RAW_SPINLOCK(smsm_snapshot_count_lock);
 
 struct smsm_size_info_type {
 	uint32_t num_hosts;
@@ -526,7 +526,7 @@ static struct notifier_block smsm_pm_nb = {
  * irq handler and code that mutates the channel
  * list or fiddles with channel state
  */
-static DEFINE_SPINLOCK(smd_lock);
+static DEFINE_RAW_SPINLOCK(smd_lock);
 DEFINE_RAW_SPINLOCK(smem_lock);
 
 /* the mutex is used during open() and close()
diff --git a/kernel/msm-3.18/include/linux/remote_spinlock.h b/kernel/msm-3.18/include/linux/remote_spinlock.h
index cdfb3a934..591c1b24d 100644
--- a/kernel/msm-3.18/include/linux/remote_spinlock.h
+++ b/kernel/msm-3.18/include/linux/remote_spinlock.h
@@ -35,41 +35,41 @@
  *    CPU in some board/machine types.
  */
 typedef struct {
-	raw_spinlock_t local;
+	spinlock_t local;
 	_remote_spinlock_t remote;
 } remote_spinlock_t;
 
 #define remote_spin_lock_init(lock, id) \
 	({ \
-		raw_spin_lock_init(&((lock)->local)); \
+		spin_lock_init(&((lock)->local)); \
 		_remote_spin_lock_init(id, &((lock)->remote)); \
 	})
 #define remote_spin_lock(lock) \
 	do { \
-		raw_spin_lock(&((lock)->local)); \
+		spin_lock(&((lock)->local)); \
 		_remote_spin_lock(&((lock)->remote)); \
 	} while (0)
 #define remote_spin_unlock(lock) \
 	do { \
 		_remote_spin_unlock(&((lock)->remote)); \
-		raw_spin_unlock(&((lock)->local)); \
+		spin_unlock(&((lock)->local)); \
 	} while (0)
 #define remote_spin_lock_irqsave(lock, flags) \
 	do { \
-		raw_spin_lock_irqsave(&((lock)->local), flags); \
+		spin_lock_irqsave(&((lock)->local), flags); \
 		_remote_spin_lock(&((lock)->remote)); \
 	} while (0)
 #define remote_spin_unlock_irqrestore(lock, flags) \
 	do { \
 		_remote_spin_unlock(&((lock)->remote)); \
-		raw_spin_unlock_irqrestore(&((lock)->local), flags); \
+		spin_unlock_irqrestore(&((lock)->local), flags); \
 	} while (0)
 #define remote_spin_trylock(lock) \
 	({ \
 		spin_trylock(&((lock)->local)) \
 		? _remote_spin_trylock(&((lock)->remote)) \
 			? 1 \
-			: ({ raw_spin_unlock(&((lock)->local)); 0; }) \
+			: ({ spin_unlock(&((lock)->local)); 0; }) \
 		: 0; \
 	})
 #define remote_spin_trylock_irqsave(lock, flags) \
@@ -77,7 +77,7 @@ typedef struct {
 		spin_trylock_irqsave(&((lock)->local), flags) \
 		? _remote_spin_trylock(&((lock)->remote)) \
 			? 1 \
-			: ({ raw_spin_unlock_irqrestore(&((lock)->local), flags); \
+			: ({ spin_unlock_irqrestore(&((lock)->local), flags); \
 				0; }) \
 		: 0; \
 	})
-- 
2.49.0


From 62746039b559799e790c89398f57a6d751ca8d75 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Thu, 10 Jan 2019 11:49:46 -0800
Subject: [PATCH 05/20] Turn off cpufreq_interactive Revert MSM_RUN_QUEUE_STATS

---
 kernel/msm-3.18/drivers/cpufreq/Makefile   |  1 -
 kernel/msm-3.18/drivers/soc/qcom/Kconfig   |  8 ---
 kernel/msm-3.18/kernel/time/tick-sched.c   | 66 +---------------------
 kernel/msm-3.18/kernel/trace/ipc_logging.c | 26 ++++-----
 4 files changed, 14 insertions(+), 87 deletions(-)

diff --git a/kernel/msm-3.18/drivers/cpufreq/Makefile b/kernel/msm-3.18/drivers/cpufreq/Makefile
index 805294ead..516e5e845 100644
--- a/kernel/msm-3.18/drivers/cpufreq/Makefile
+++ b/kernel/msm-3.18/drivers/cpufreq/Makefile
@@ -10,7 +10,6 @@ obj-$(CONFIG_CPU_FREQ_GOV_POWERSAVE)	+= cpufreq_powersave.o
 obj-$(CONFIG_CPU_FREQ_GOV_USERSPACE)	+= cpufreq_userspace.o
 obj-$(CONFIG_CPU_FREQ_GOV_ONDEMAND)	+= cpufreq_ondemand.o
 obj-$(CONFIG_CPU_FREQ_GOV_CONSERVATIVE)	+= cpufreq_conservative.o
-obj-$(CONFIG_CPU_FREQ_GOV_INTERACTIVE)	+= cpufreq_interactive.o
 obj-$(CONFIG_CPU_FREQ_GOV_COMMON)		+= cpufreq_governor.o
 obj-$(CONFIG_CPU_BOOST)			+= cpu-boost.o
 
diff --git a/kernel/msm-3.18/drivers/soc/qcom/Kconfig b/kernel/msm-3.18/drivers/soc/qcom/Kconfig
index c0d99ba84..2876b7923 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/Kconfig
+++ b/kernel/msm-3.18/drivers/soc/qcom/Kconfig
@@ -344,14 +344,6 @@ config MSM_RPM_STATS_LOG
           the low power modes that RPM enters. The drivers outputs the message
           via a debugfs node.
 
-config MSM_RUN_QUEUE_STATS
-	bool "Enable collection and exporting of MSM Run Queue stats to userspace"
-	help
-	 This option enables the driver to periodically collecting the statistics
-	 of kernel run queue information and calculate the load of the system.
-	 This information is exported to usespace via sysfs entries and userspace
-	 algorithms uses info and decide when to turn on/off the cpu cores.
-
 config MSM_SCM
        bool "Secure Channel Manager (SCM) support"
        default n
diff --git a/kernel/msm-3.18/kernel/time/tick-sched.c b/kernel/msm-3.18/kernel/time/tick-sched.c
index fae5e1a3b..a17285893 100644
--- a/kernel/msm-3.18/kernel/time/tick-sched.c
+++ b/kernel/msm-3.18/kernel/time/tick-sched.c
@@ -24,7 +24,6 @@
 #include <linux/posix-timers.h>
 #include <linux/perf_event.h>
 #include <linux/context_tracking.h>
-#include <linux/rq_stats.h>
 
 #include <asm/irq_regs.h>
 
@@ -32,10 +31,6 @@
 
 #include <trace/events/timer.h>
 
-struct rq_data rq_info;
-struct workqueue_struct *rq_wq;
-raw_spinlock_t rq_lock;
-
 /*
  * Per cpu nohz control structure
  */
@@ -1131,51 +1126,6 @@ void tick_irq_enter(void)
  * High resolution timer specific code
  */
 #ifdef CONFIG_HIGH_RES_TIMERS
-static void update_rq_stats(void)
-{
-	unsigned long jiffy_gap = 0;
-	unsigned int rq_avg = 0;
-	unsigned long flags = 0;
-
-	jiffy_gap = jiffies - rq_info.rq_poll_last_jiffy;
-
-	if (jiffy_gap >= rq_info.rq_poll_jiffies) {
-
-		spin_lock_irqsave(&rq_lock, flags);
-
-		if (!rq_info.rq_avg)
-			rq_info.rq_poll_total_jiffies = 0;
-
-		rq_avg = nr_running() * 10;
-
-		if (rq_info.rq_poll_total_jiffies) {
-			rq_avg = (rq_avg * jiffy_gap) +
-				(rq_info.rq_avg *
-				 rq_info.rq_poll_total_jiffies);
-			do_div(rq_avg,
-			       rq_info.rq_poll_total_jiffies + jiffy_gap);
-		}
-
-		rq_info.rq_avg =  rq_avg;
-		rq_info.rq_poll_total_jiffies += jiffy_gap;
-		rq_info.rq_poll_last_jiffy = jiffies;
-
-		spin_unlock_irqrestore(&rq_lock, flags);
-	}
-}
-
-static void wakeup_user(void)
-{
-	unsigned long jiffy_gap;
-
-	jiffy_gap = jiffies - rq_info.def_timer_last_jiffy;
-
-	if (jiffy_gap >= rq_info.def_timer_jiffies) {
-		rq_info.def_timer_last_jiffy = jiffies;
-		queue_work(rq_wq, &rq_info.def_timer_work);
-	}
-}
-
 /*
  * We rearm the timer until we get disabled by the idle code.
  * Called with interrupts disabled.
@@ -1193,23 +1143,9 @@ static enum hrtimer_restart tick_sched_timer(struct hrtimer *timer)
 	 * Do not call, when we are not in irq context and have
 	 * no valid regs pointer
 	 */
-	if (regs) {
+	if (regs)
 		tick_sched_handle(ts, regs);
 
-		if (rq_info.init == 1 &&
-				tick_do_timer_cpu == smp_processor_id()) {
-			/*
-			 * update run queue statistics
-			 */
-			update_rq_stats();
-
-			/*
-			 * wakeup user if needed
-			 */
-			wakeup_user();
-		}
-	}
-
 	/* No need to reprogram if we are in idle or full dynticks mode */
 	if (unlikely(ts->tick_stopped))
 		return HRTIMER_NORESTART;
diff --git a/kernel/msm-3.18/kernel/trace/ipc_logging.c b/kernel/msm-3.18/kernel/trace/ipc_logging.c
index 356312a55..63275c606 100644
--- a/kernel/msm-3.18/kernel/trace/ipc_logging.c
+++ b/kernel/msm-3.18/kernel/trace/ipc_logging.c
@@ -35,7 +35,7 @@
 #define LOG_PAGE_FLAG (1 << 31)
 
 static LIST_HEAD(ipc_log_context_list);
-static DEFINE_RWLOCK(context_list_lock_lha1);
+static DEFINE_RAW_SPINLOCK(context_list_lock_lha1);
 static void *get_deserialization_func(struct ipc_log_context *ilctxt,
 				      int type);
 
@@ -292,7 +292,7 @@ void ipc_log_write(void *ctxt, struct encode_context *ectxt)
 		return;
 	}
 
-	read_lock_irqsave(&context_list_lock_lha1, flags);
+	raw_spin_lock_irqsave(&context_list_lock_lha1, flags);
 	raw_spin_lock(&ilctxt->context_lock_lhb1);
 	while (ilctxt->write_avail <= ectxt->offset)
 		msg_drop(ilctxt);
@@ -324,7 +324,7 @@ void ipc_log_write(void *ctxt, struct encode_context *ectxt)
 	ilctxt->write_avail -= ectxt->offset;
 	complete(&ilctxt->read_avail);
 	raw_spin_unlock(&ilctxt->context_lock_lhb1);
-	read_unlock_irqrestore(&context_list_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&context_list_lock_lha1, flags);
 }
 EXPORT_SYMBOL(ipc_log_write);
 
@@ -545,7 +545,7 @@ int ipc_log_extract(void *ctxt, char *buff, int size)
 	dctxt.output_format = OUTPUT_DEBUGFS;
 	dctxt.buff = buff;
 	dctxt.size = size;
-	read_lock_irqsave(&context_list_lock_lha1, flags);
+	raw_spin_lock_irqsave(&context_list_lock_lha1, flags);
 	raw_spin_lock(&ilctxt->context_lock_lhb1);
 	while (dctxt.size >= MAX_MSG_DECODED_SIZE &&
 	       !is_nd_read_empty(ilctxt)) {
@@ -553,19 +553,19 @@ int ipc_log_extract(void *ctxt, char *buff, int size)
 		deserialize_func = get_deserialization_func(ilctxt,
 							ectxt.hdr.type);
 		raw_spin_unlock(&ilctxt->context_lock_lhb1);
-		read_unlock_irqrestore(&context_list_lock_lha1, flags);
+		raw_spin_unlock_irqrestore(&context_list_lock_lha1, flags);
 		if (deserialize_func)
 			deserialize_func(&ectxt, &dctxt);
 		else
 			pr_err("%s: unknown message 0x%x\n",
 				__func__, ectxt.hdr.type);
-		read_lock_irqsave(&context_list_lock_lha1, flags);
+		raw_spin_lock_irqsave(&context_list_lock_lha1, flags);
 		raw_spin_lock(&ilctxt->context_lock_lhb1);
 	}
 	if ((size - dctxt.size) == 0)
 		reinit_completion(&ilctxt->read_avail);
 	raw_spin_unlock(&ilctxt->context_lock_lhb1);
-	read_unlock_irqrestore(&context_list_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&context_list_lock_lha1, flags);
 	return size - dctxt.size;
 }
 EXPORT_SYMBOL(ipc_log_extract);
@@ -726,13 +726,13 @@ int add_deserialization_func(void *ctxt, int type,
 	if (!df_info)
 		return -ENOSPC;
 
-	read_lock_irqsave(&context_list_lock_lha1, flags);
+	raw_spin_lock_irqsave(&context_list_lock_lha1, flags);
 	raw_spin_lock(&ilctxt->context_lock_lhb1);
 	df_info->type = type;
 	df_info->dfunc = dfunc;
 	list_add_tail(&df_info->list, &ilctxt->dfunc_info_list);
 	raw_spin_unlock(&ilctxt->context_lock_lhb1);
-	read_unlock_irqrestore(&context_list_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&context_list_lock_lha1, flags);
 	return 0;
 }
 EXPORT_SYMBOL(add_deserialization_func);
@@ -817,9 +817,9 @@ void *ipc_log_context_create(int max_num_pages,
 	ctxt->magic = IPC_LOG_CONTEXT_MAGIC_NUM;
 	ctxt->nmagic = ~(IPC_LOG_CONTEXT_MAGIC_NUM);
 
-	write_lock_irqsave(&context_list_lock_lha1, flags);
+	raw_spin_lock_irqsave(&context_list_lock_lha1, flags);
 	list_add_tail(&ctxt->list, &ipc_log_context_list);
-	write_unlock_irqrestore(&context_list_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&context_list_lock_lha1, flags);
 	return (void *)ctxt;
 
 release_ipc_log_context:
@@ -853,9 +853,9 @@ int ipc_log_context_destroy(void *ctxt)
 		kfree(pg);
 	}
 
-	write_lock_irqsave(&context_list_lock_lha1, flags);
+	raw_spin_lock_irqsave(&context_list_lock_lha1, flags);
 	list_del(&ilctxt->list);
-	write_unlock_irqrestore(&context_list_lock_lha1, flags);
+	raw_spin_unlock_irqrestore(&context_list_lock_lha1, flags);
 
 	debugfs_remove_recursive(ilctxt->dent);
 
-- 
2.49.0


From 0eda2c6154c35916504c4e577b0bfe960b78800f Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Thu, 10 Jan 2019 11:50:29 -0800
Subject: [PATCH 06/20] Make msm_serial_hs_light use threaded irq

---
 kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
index f5ed8b142..fbabf4af4 100644
--- a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
+++ b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
@@ -1027,7 +1027,7 @@ static int msm_hsl_startup(struct uart_port *port)
 	msm_hsl_write(port, data, regmap[vid][UARTDM_MR1]);
 	raw_spin_unlock_irqrestore(&port->lock, flags);
 
-	ret = request_irq(port->irq, msm_hsl_irq, IRQF_TRIGGER_HIGH,
+	ret = request_threaded_irq(port->irq, NULL, msm_hsl_irq, IRQF_TRIGGER_HIGH,
 			  msm_hsl_port->name, port);
 	if (unlikely(ret)) {
 		pr_err("failed to request_irq\n");
-- 
2.49.0


From b3ea3664cbe4e292302a08c6aa9a5b2ab51eec3f Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Fri, 11 Jan 2019 09:36:32 -0800
Subject: [PATCH 07/20] mm, vmstat: make quiet_vmstat lighter

Mike has reported a considerable overhead of refresh_cpu_vm_stats from
the idle entry during pipe test:

    12.89%  [kernel]       [k] refresh_cpu_vm_stats.isra.12
     4.75%  [kernel]       [k] __schedule
     4.70%  [kernel]       [k] mutex_unlock
     3.14%  [kernel]       [k] __switch_to

This is caused by commit 0eb77e988032 ("vmstat: make vmstat_updater
deferrable again and shut down on idle") which has placed quiet_vmstat
into cpu_idle_loop.  The main reason here seems to be that the idle
entry has to get over all zones and perform atomic operations for each
vmstat entry even though there might be no per cpu diffs.  This is a
pointless overhead for _each_ idle entry.

Make sure that quiet_vmstat is as light as possible.

First of all it doesn't make any sense to do any local sync if the
current cpu is already set in oncpu_stat_off because vmstat_update puts
itself there only if there is nothing to do.

Then we can check need_update which should be a cheap way to check for
potential per-cpu diffs and only then do refresh_cpu_vm_stats.

The original patch also did cancel_delayed_work which we are not doing
here.  There are two reasons for that.  Firstly cancel_delayed_work from
idle context will blow up on RT kernels (reported by Mike):

  CPU: 1 PID: 0 Comm: swapper/1 Not tainted 4.5.0-rt3 #7
  Hardware name: MEDION MS-7848/MS-7848, BIOS M7848W08.20C 09/23/2013
  Call Trace:
    dump_stack+0x49/0x67
    ___might_sleep+0xf5/0x180
    rt_spin_lock+0x20/0x50
    try_to_grab_pending+0x69/0x240
    cancel_delayed_work+0x26/0xe0
    quiet_vmstat+0x75/0xa0
    cpu_idle_loop+0x38/0x3e0
    cpu_startup_entry+0x13/0x20
    start_secondary+0x114/0x140

And secondly, even on !RT kernels it might add some non trivial overhead
which is not necessary.  Even if the vmstat worker wakes up and preempts
idle then it will be most likely a single shot noop because the stats
were already synced and so it would end up on the oncpu_stat_off anyway.
We just need to teach both vmstat_shepherd and vmstat_update to stop
scheduling the worker if there is nothing to do.

[mgalbraith@suse.de: cancel pending work of the cpu_stat_off CPU]
Signed-off-by: Michal Hocko <mhocko@suse.com>
Reported-by: Mike Galbraith <umgwanakikbuti@gmail.com>
Acked-by: Christoph Lameter <cl@linux.com>
Signed-off-by: Mike Galbraith <mgalbraith@suse.de>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
---
 kernel/msm-3.18/mm/backing-dev.c | 26 ++++++---
 kernel/msm-3.18/mm/vmstat.c      | 93 +++++++++++++++++++-------------
 2 files changed, 76 insertions(+), 43 deletions(-)

diff --git a/kernel/msm-3.18/mm/backing-dev.c b/kernel/msm-3.18/mm/backing-dev.c
index 0ae0df550..e2ae5aea3 100644
--- a/kernel/msm-3.18/mm/backing-dev.c
+++ b/kernel/msm-3.18/mm/backing-dev.c
@@ -603,8 +603,9 @@ EXPORT_SYMBOL(congestion_wait);
  * jiffies for either a BDI to exit congestion of the given @sync queue
  * or a write to complete.
  *
- * In the absence of zone congestion, cond_resched() is called to yield
- * the processor if necessary but otherwise does not sleep.
+ * In the absence of zone congestion, a short sleep or a cond_resched is
+ * performed to yield the processor and to allow other subsystems to make
+ * a forward progress.
  *
  * The return value is 0 if the sleep is for the full timeout. Otherwise,
  * it is the number of jiffies that were still remaining when the function
@@ -623,11 +624,22 @@ long wait_iff_congested(struct zone *zone, int sync, long timeout)
 	 * of sleeping on the congestion queue
 	 */
 	if (atomic_read(&nr_bdi_congested[sync]) == 0 ||
-	    !test_bit(ZONE_CONGESTED, &zone->flags)) {
-		cond_resched();
-
-		/* In case we scheduled, work out time remaining */
-		ret = timeout - (jiffies - start);
+      !test_bit(ZONE_CONGESTED, &zone->flags)) {
+    /*
+      +		 * Memory allocation/reclaim might be called from a WQ
+      +		 * context and the current implementation of the WQ
+      +		 * concurrency control doesn't recognize that a particular
+      +		 * WQ is congested if the worker thread is looping without
+      +		 * ever sleeping. Therefore we have to do a short sleep
+      +		 * here rather than calling cond_resched().
+      +		 */
+    if (current->flags & PF_WQ_WORKER)
+      schedule_timeout(1);
+    else
+      cond_resched();
+
+    /* In case we scheduled, work out time remaining */
+    ret = timeout - (jiffies - start);
 		if (ret < 0)
 			ret = 0;
 
diff --git a/kernel/msm-3.18/mm/vmstat.c b/kernel/msm-3.18/mm/vmstat.c
index f7bdd77bf..6ef6afefc 100644
--- a/kernel/msm-3.18/mm/vmstat.c
+++ b/kernel/msm-3.18/mm/vmstat.c
@@ -1358,6 +1358,7 @@ static const struct file_operations proc_vmstat_file_operations = {
 #endif /* CONFIG_PROC_FS */
 
 #ifdef CONFIG_SMP
+static struct workqueue_struct *vmstat_wq;
 static DEFINE_PER_CPU(struct delayed_work, vmstat_work);
 int sysctl_stat_interval __read_mostly = HZ;
 static cpumask_var_t cpu_stat_off;
@@ -1369,13 +1370,18 @@ static void vmstat_update(struct work_struct *w)
 		 * Counters were updated so we expect more updates
 		 * to occur in the future. Keep on running the
 		 * update worker thread.
-		 */
-		schedule_delayed_work_on(smp_processor_id(),
-				this_cpu_ptr(&vmstat_work),
-			round_jiffies_relative(sysctl_stat_interval));
-	} else {
-		/*
-		 * We did not update any counters so the app may be in
+     * If we were marked on cpu_stat_off clear the flag
+     * so that vmstat_shepherd doesn't schedule us again.
+     */
+    if (!cpumask_test_and_clear_cpu(smp_processor_id(),
+                                    cpu_stat_off)) {
+      queue_delayed_work_on(smp_processor_id(), vmstat_wq,
+                            this_cpu_ptr(&vmstat_work),
+                            round_jiffies_relative(sysctl_stat_interval));
+    }
+  } else {
+    /*
+     * We did not update any counters so the app may be in
 		 * a mode where it does not cause counter updates.
 		 * We may be uselessly running vmstat_update.
 		 * Defer the checking for differentials to the
@@ -1385,23 +1391,6 @@ static void vmstat_update(struct work_struct *w)
 	}
 }
 
-/*
- * Switch off vmstat processing and then fold all the remaining differentials
- * until the diffs stay at zero. The function is used by NOHZ and can only be
- * invoked when tick processing is not active.
- */
-void quiet_vmstat(void)
-{
-	if (system_state != SYSTEM_RUNNING)
-		return;
-
-	do {
-		if (!cpumask_test_and_set_cpu(smp_processor_id(), cpu_stat_off))
-			cancel_delayed_work(this_cpu_ptr(&vmstat_work));
-
-	} while (refresh_cpu_vm_stats(false));
-}
-
 /*
  * Check if the diffs for a certain cpu indicate that
  * an update is needed.
@@ -1426,6 +1415,31 @@ static bool need_update(int cpu)
 }
 
 
+/*
+ * Switch off vmstat processing and then fold all the remaining differentials
+ * until the diffs stay at zero. The function is used by NOHZ and can only be
+ * invoked when tick processing is not active.
+ */
+void quiet_vmstat(void)
+{
+	if (system_state != SYSTEM_RUNNING)
+		return;
+
+	if (!delayed_work_pending(this_cpu_ptr(&vmstat_work)))
+		return;
+
+	if (!need_update(smp_processor_id()))
+		return;
+
+	/*
+	 * Just refresh counters and do not care about the pending delayed
+	 * vmstat_update. It doesn't fire that often to matter and canceling
+	 * it would be too expensive from this path.
+	 * vmstat_shepherd will take care about that for us.
+	 */
+	refresh_cpu_vm_stats(false);
+}
+
 /*
  * Shepherd worker thread that checks the
  * differentials of processors that have their worker
@@ -1442,18 +1456,24 @@ static void vmstat_shepherd(struct work_struct *w)
 
 	get_online_cpus();
 	/* Check processors whose vmstat worker threads have been disabled */
-	for_each_cpu(cpu, cpu_stat_off)
-		if (need_update(cpu) &&
-			cpumask_test_and_clear_cpu(cpu, cpu_stat_off))
-
-			schedule_delayed_work_on(cpu, &per_cpu(vmstat_work, cpu),
-				__round_jiffies_relative(sysctl_stat_interval, cpu));
-
-	put_online_cpus();
-
-	schedule_delayed_work(&shepherd,
-		round_jiffies_relative(sysctl_stat_interval));
-
+  for_each_cpu(cpu, cpu_stat_off) {
+    struct delayed_work *dw = &per_cpu(vmstat_work, cpu);
+    if (need_update(cpu)) {
+      if (cpumask_test_and_clear_cpu(cpu, cpu_stat_off))
+        queue_delayed_work_on(cpu, vmstat_wq, dw, 0);
+    } else {
+      /*
+        +			 * Cancel the work if quiet_vmstat has put this
+        +			 * cpu on cpu_stat_off because the work item might
+        +			 * be still scheduled
+        +			 */
+      cancel_delayed_work(dw);
+    }
+  }
+  put_online_cpus();
+
+  schedule_delayed_work(&shepherd,
+    round_jiffies_relative(sysctl_stat_interval));
 }
 
 static void __init start_shepherd_timer(void)
@@ -1468,6 +1488,7 @@ static void __init start_shepherd_timer(void)
 		BUG();
 	cpumask_copy(cpu_stat_off, cpu_online_mask);
 
+	vmstat_wq = alloc_workqueue("vmstat", WQ_FREEZABLE|WQ_MEM_RECLAIM, 0);
 	schedule_delayed_work(&shepherd,
 		round_jiffies_relative(sysctl_stat_interval));
 }
-- 
2.49.0


From b04b540172887dd2b7376d1567a306d4ef6d876f Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Fri, 11 Jan 2019 15:59:01 -0800
Subject: [PATCH 08/20] more raw spinlocks

---
 .../drivers/platform/msm/spmi/qpnp-int.c      | 12 +++++-----
 .../drivers/platform/msm/spmi/spmi-pmic-arb.c | 24 +++++++++----------
 .../drivers/soc/qcom/qdsp6v2/apr_tal.c        | 14 +++++------
 3 files changed, 25 insertions(+), 25 deletions(-)

diff --git a/kernel/msm-3.18/drivers/platform/msm/spmi/qpnp-int.c b/kernel/msm-3.18/drivers/platform/msm/spmi/qpnp-int.c
index 5c8d0c867..99dc27827 100644
--- a/kernel/msm-3.18/drivers/platform/msm/spmi/qpnp-int.c
+++ b/kernel/msm-3.18/drivers/platform/msm/spmi/qpnp-int.c
@@ -53,7 +53,7 @@ struct q_perip_data {
 	uint8_t pol_low;    /* bitmap */
 	uint8_t int_en;     /* bitmap */
 	uint8_t use_count;
-	spinlock_t lock;
+	raw_spinlock_t lock;
 };
 
 struct q_irq_data {
@@ -217,7 +217,7 @@ static void qpnpint_irq_mask(struct irq_data *d)
 		return;
 	}
 
-	spin_lock(&per_d->lock);
+	raw_spin_lock(&per_d->lock);
 	prev_int_en = per_d->int_en;
 	per_d->int_en &= ~irq_d->mask_shift;
 
@@ -228,7 +228,7 @@ static void qpnpint_irq_mask(struct irq_data *d)
 		 */
 		qpnpint_arbiter_op(d, irq_d, chip_d->cb->mask);
 	}
-	spin_unlock(&per_d->lock);
+	raw_spin_unlock(&per_d->lock);
 
 	rc = qpnpint_spmi_write(irq_d, QPNPINT_REG_EN_CLR,
 					(u8 *)&irq_d->mask_shift, 1);
@@ -266,7 +266,7 @@ static void qpnpint_irq_unmask(struct irq_data *d)
 		return;
 	}
 
-	spin_lock(&per_d->lock);
+	raw_spin_lock(&per_d->lock);
 	prev_int_en = per_d->int_en;
 	per_d->int_en |= irq_d->mask_shift;
 	if (!prev_int_en && per_d->int_en) {
@@ -277,7 +277,7 @@ static void qpnpint_irq_unmask(struct irq_data *d)
 		 */
 		qpnpint_arbiter_op(d, irq_d, chip_d->cb->unmask);
 	}
-	spin_unlock(&per_d->lock);
+	raw_spin_unlock(&per_d->lock);
 
 	/* Check the current state of the interrupt enable bit. */
 	rc = qpnpint_spmi_read(irq_d, QPNPINT_REG_EN_SET, buf, 1);
@@ -436,7 +436,7 @@ static struct q_irq_data *qpnpint_alloc_irq_data(
 			rc = -ENOMEM;
 			goto alloc_fail;
 		}
-		spin_lock_init(&per_d->lock);
+		raw_spin_lock_init(&per_d->lock);
 		rc = radix_tree_preload(GFP_KERNEL);
 		if (rc)
 			goto alloc_fail;
diff --git a/kernel/msm-3.18/drivers/platform/msm/spmi/spmi-pmic-arb.c b/kernel/msm-3.18/drivers/platform/msm/spmi/spmi-pmic-arb.c
index 88961ae2b..6e2e75fd7 100644
--- a/kernel/msm-3.18/drivers/platform/msm/spmi/spmi-pmic-arb.c
+++ b/kernel/msm-3.18/drivers/platform/msm/spmi/spmi-pmic-arb.c
@@ -210,7 +210,7 @@ struct spmi_pmic_arb_dev {
 	struct spmi_pmic_arb_dbg dbg;
 	int			pic_irq;
 	bool			allow_wakeup;
-	spinlock_t		lock;
+	raw_spinlock_t		lock;
 	u8			ee;
 	u8			channel;
 	u16			max_peripherals;
@@ -499,12 +499,12 @@ pmic_arb_non_data_cmd_v1(struct spmi_pmic_arb_dev *pmic_arb, u8 opc, u8 sid)
 
 	cmd = (opc << 27) | ((sid & 0xf) << 20);
 
-	spin_lock_irqsave(&pmic_arb->lock, flags);
+	raw_spin_lock_irqsave(&pmic_arb->lock, flags);
 	pmic_arb_save_stat_before_txn(pmic_arb);
 	pmic_arb_write(pmic_arb, chnl_ofst + PMIC_ARB_CMD, cmd);
 	/* sid and addr are don't-care for pmic_arb_wait_for_done() HW-v1 */
 	rc = pmic_arb_wait_for_done(pmic_arb, pmic_arb->wrbase, 0, 0);
-	spin_unlock_irqrestore(&pmic_arb->lock, flags);
+	raw_spin_unlock_irqrestore(&pmic_arb->lock, flags);
 
 	if (rc)
 		pmic_arb_dbg_err_dump(pmic_arb, rc, "cmd", opc, sid, 0, 0, 0);
@@ -588,7 +588,7 @@ static int pmic_arb_read_cmd(struct spmi_controller *ctrl,
 
 	cmd = pmic_arb->ver->fmt_cmd(opc, sid, addr, bc);
 
-	spin_lock_irqsave(&pmic_arb->lock, flags);
+	raw_spin_lock_irqsave(&pmic_arb->lock, flags);
 	pmic_arb_save_stat_before_txn(pmic_arb);
 
 	pmic_arb_set_rd_cmd(pmic_arb, chnl_ofst + PMIC_ARB_CMD, cmd);
@@ -605,7 +605,7 @@ static int pmic_arb_read_cmd(struct spmi_controller *ctrl,
 				chnl_ofst + PMIC_ARB_RDATA1, bc - 4);
 
 done:
-	spin_unlock_irqrestore(&pmic_arb->lock, flags);
+	raw_spin_unlock_irqrestore(&pmic_arb->lock, flags);
 	if (rc)
 		pmic_arb_dbg_err_dump(pmic_arb, rc, "read", opc, sid, addr, bc,
 									buf);
@@ -648,7 +648,7 @@ static int pmic_arb_write_cmd(struct spmi_controller *ctrl,
 	cmd = pmic_arb->ver->fmt_cmd(opc, sid, addr, bc);
 
 	/* Write data to FIFOs */
-	spin_lock_irqsave(&pmic_arb->lock, flags);
+	raw_spin_lock_irqsave(&pmic_arb->lock, flags);
 	pmic_arb_save_stat_before_txn(pmic_arb);
 	pa_write_data(pmic_arb, buf, chnl_ofst + PMIC_ARB_WDATA0,
 							min_t(u8, bc, 3));
@@ -661,7 +661,7 @@ static int pmic_arb_write_cmd(struct spmi_controller *ctrl,
 	pmic_arb_write(pmic_arb, chnl_ofst + PMIC_ARB_CMD, cmd);
 
 	rc = pmic_arb_wait_for_done(pmic_arb, pmic_arb->wrbase, sid, addr);
-	spin_unlock_irqrestore(&pmic_arb->lock, flags);
+	raw_spin_unlock_irqrestore(&pmic_arb->lock, flags);
 
 	if (rc)
 		pmic_arb_dbg_err_dump(pmic_arb, rc, "write", opc, sid, addr, bc,
@@ -803,7 +803,7 @@ static int pmic_arb_pic_enable(struct spmi_controller *ctrl,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&pmic_arb->lock, flags);
+	raw_spin_lock_irqsave(&pmic_arb->lock, flags);
 	status = spmi_pic_acc_en_rd(pmic_arb, spec->slave, spec->per, apid,
 								"pic-en");
 	if (!(status & SPMI_PIC_ACC_ENABLE_BIT)) {
@@ -813,7 +813,7 @@ static int pmic_arb_pic_enable(struct spmi_controller *ctrl,
 		/* Interrupt needs to be enabled before returning to caller */
 		wmb();
 	}
-	spin_unlock_irqrestore(&pmic_arb->lock, flags);
+	raw_spin_unlock_irqrestore(&pmic_arb->lock, flags);
 	return 0;
 }
 
@@ -842,7 +842,7 @@ static int pmic_arb_pic_disable(struct spmi_controller *ctrl,
 		return -EINVAL;
 	}
 
-	spin_lock_irqsave(&pmic_arb->lock, flags);
+	raw_spin_lock_irqsave(&pmic_arb->lock, flags);
 	status = spmi_pic_acc_en_rd(pmic_arb, spec->slave, spec->per, apid,
 								"pic-en");
 	if (status & SPMI_PIC_ACC_ENABLE_BIT) {
@@ -853,7 +853,7 @@ static int pmic_arb_pic_disable(struct spmi_controller *ctrl,
 		/* Interrupt needs to be disabled before returning to caller */
 		wmb();
 	}
-	spin_unlock_irqrestore(&pmic_arb->lock, flags);
+	raw_spin_unlock_irqrestore(&pmic_arb->lock, flags);
 	return 0;
 }
 
@@ -1285,7 +1285,7 @@ static int spmi_pmic_arb_probe(struct platform_device *pdev)
 	platform_set_drvdata(pdev, pmic_arb);
 	spmi_set_ctrldata(&pmic_arb->controller, pmic_arb);
 
-	spin_lock_init(&pmic_arb->lock);
+	raw_spin_lock_init(&pmic_arb->lock);
 
 	pmic_arb->controller.nr = cell_index;
 	pmic_arb->controller.dev.parent = pdev->dev.parent;
diff --git a/kernel/msm-3.18/drivers/soc/qcom/qdsp6v2/apr_tal.c b/kernel/msm-3.18/drivers/soc/qcom/qdsp6v2/apr_tal.c
index 6cffe7be6..eda77e497 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/qdsp6v2/apr_tal.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/qdsp6v2/apr_tal.c
@@ -98,31 +98,31 @@ static void apr_tal_notify(void *priv, unsigned event)
 	switch (event) {
 	case SMD_EVENT_DATA:
 		pkt_cnt = 0;
-		spin_lock_irqsave(&apr_ch->lock, flags);
+		raw_spin_lock_irqsave(&apr_ch->lock, flags);
 check_pending:
 		len = smd_read_avail(apr_ch->ch);
 		if (len < 0) {
 			pr_err("apr_tal: Invalid Read Event :%d\n", len);
-			spin_unlock_irqrestore(&apr_ch->lock, flags);
+			raw_spin_unlock_irqrestore(&apr_ch->lock, flags);
 			return;
 		}
 		sz = smd_cur_packet_size(apr_ch->ch);
 		if (sz < 0) {
 			pr_debug("pkt size is zero\n");
-			spin_unlock_irqrestore(&apr_ch->lock, flags);
+			raw_spin_unlock_irqrestore(&apr_ch->lock, flags);
 			return;
 		}
 		if (!len && !sz && !pkt_cnt)
 			goto check_write_avail;
 		if (!len) {
 			pr_debug("len = %d pkt_cnt = %d\n", len, pkt_cnt);
-			spin_unlock_irqrestore(&apr_ch->lock, flags);
+			raw_spin_unlock_irqrestore(&apr_ch->lock, flags);
 			return;
 		}
 		r_len = smd_read_from_cb(apr_ch->ch, apr_ch->data, len);
 		if (len != r_len) {
 			pr_err("apr_tal: Invalid Read\n");
-			spin_unlock_irqrestore(&apr_ch->lock, flags);
+			raw_spin_unlock_irqrestore(&apr_ch->lock, flags);
 			return;
 		}
 		pkt_cnt++;
@@ -133,7 +133,7 @@ check_pending:
 check_write_avail:
 		if (smd_write_avail(apr_ch->ch))
 			wake_up(&apr_ch->wait);
-		spin_unlock_irqrestore(&apr_ch->lock, flags);
+		raw_spin_unlock_irqrestore(&apr_ch->lock, flags);
 		break;
 	case SMD_EVENT_OPEN:
 		pr_debug("apr_tal: SMD_EVENT_OPEN\n");
@@ -287,7 +287,7 @@ static int __init apr_tal_init(void)
 			for (k = 0; k < APR_CLIENT_MAX; k++) {
 				init_waitqueue_head(&apr_svc_ch[i][j][k].wait);
 				init_waitqueue_head(&apr_svc_ch[i][j][k].dest);
-				spin_lock_init(&apr_svc_ch[i][j][k].lock);
+				raw_spin_lock_init(&apr_svc_ch[i][j][k].lock);
 				spin_lock_init(&apr_svc_ch[i][j][k].w_lock);
 				mutex_init(&apr_svc_ch[i][j][k].m_lock);
 			}
-- 
2.49.0


From 7b6f4fd62e4d2f89297ea56cf00402f1432916ed Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Fri, 11 Jan 2019 16:00:04 -0800
Subject: [PATCH 09/20] more spinlocks

---
 .../drivers/usb/gadget/function/f_diag.c      | 90 +++++++++----------
 .../msm-3.18/include/linux/qdsp6v2/apr_tal.h  |  2 +-
 2 files changed, 46 insertions(+), 46 deletions(-)

diff --git a/kernel/msm-3.18/drivers/usb/gadget/function/f_diag.c b/kernel/msm-3.18/drivers/usb/gadget/function/f_diag.c
index 0df91d22a..7594fb143 100644
--- a/kernel/msm-3.18/drivers/usb/gadget/function/f_diag.c
+++ b/kernel/msm-3.18/drivers/usb/gadget/function/f_diag.c
@@ -151,7 +151,7 @@ struct diag_context {
 	struct usb_ep *in;
 	struct list_head read_pool;
 	struct list_head write_pool;
-	raw_spinlock_t lock;
+	spinlock_t lock;
 	unsigned configured;
 	struct usb_composite_dev *cdev;
 	int (*update_pid_and_serial_num)(uint32_t, const char *);
@@ -180,7 +180,7 @@ static void diag_context_release(struct kref *kref)
 	struct diag_context *ctxt =
 		container_of(kref, struct diag_context, kref);
 
-	raw_spin_unlock(&ctxt->lock);
+	spin_unlock(&ctxt->lock);
 	kfree(ctxt);
 }
 
@@ -246,13 +246,13 @@ static void diag_write_complete(struct usb_ep *ep,
 		}
 	}
 
-	raw_spin_lock_irqsave(&ctxt->lock, flags);
+	spin_lock_irqsave(&ctxt->lock, flags);
 	list_add_tail(&req->list, &ctxt->write_pool);
 	if (req->length != 0) {
 		d_req->actual = req->actual;
 		d_req->status = req->status;
 	}
-	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+	spin_unlock_irqrestore(&ctxt->lock, flags);
 
 	if (ctxt->ch && ctxt->ch->notify)
 		ctxt->ch->notify(ctxt->ch->priv, USB_DIAG_WRITE_DONE, d_req);
@@ -271,9 +271,9 @@ static void diag_read_complete(struct usb_ep *ep,
 	d_req->actual = req->actual;
 	d_req->status = req->status;
 
-	raw_spin_lock_irqsave(&ctxt->lock, flags);
+	spin_lock_irqsave(&ctxt->lock, flags);
 	list_add_tail(&req->list, &ctxt->read_pool);
-	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+	spin_unlock_irqrestore(&ctxt->lock, flags);
 
 	ctxt->dpkts_tomodem++;
 
@@ -302,7 +302,7 @@ struct usb_diag_ch *usb_diag_open(const char *name, void *priv,
 	unsigned long flags;
 	int found = 0;
 
-	raw_spin_lock_irqsave(&ch_lock, flags);
+	spin_lock_irqsave(&ch_lock, flags);
 	/* Check if we already have a channel with this name */
 	list_for_each_entry(ch, &usb_diag_ch_list, list) {
 		if (!strcmp(name, ch->name)) {
@@ -310,7 +310,7 @@ struct usb_diag_ch *usb_diag_open(const char *name, void *priv,
 			break;
 		}
 	}
-	raw_spin_unlock_irqrestore(&ch_lock, flags);
+	spin_unlock_irqrestore(&ch_lock, flags);
 
 	if (!found) {
 		ch = kzalloc(sizeof(*ch), GFP_KERNEL);
@@ -323,9 +323,9 @@ struct usb_diag_ch *usb_diag_open(const char *name, void *priv,
 	ch->notify = notify;
 
 	if (!found) {
-		raw_spin_lock_irqsave(&ch_lock, flags);
+		spin_lock_irqsave(&ch_lock, flags);
 		list_add_tail(&ch->list, &usb_diag_ch_list);
-		raw_spin_unlock_irqrestore(&ch_lock, flags);
+		spin_unlock_irqrestore(&ch_lock, flags);
 	}
 
 	return ch;
@@ -344,7 +344,7 @@ void usb_diag_close(struct usb_diag_ch *ch)
 	struct diag_context *dev = NULL;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&ch_lock, flags);
+	spin_lock_irqsave(&ch_lock, flags);
 	ch->priv = NULL;
 	ch->notify = NULL;
 	/* Free-up the resources if channel is no more active */
@@ -354,7 +354,7 @@ void usb_diag_close(struct usb_diag_ch *ch)
 			dev->ch = NULL;
 	kfree(ch);
 
-	raw_spin_unlock_irqrestore(&ch_lock, flags);
+	spin_unlock_irqrestore(&ch_lock, flags);
 }
 EXPORT_SYMBOL(usb_diag_close);
 
@@ -397,7 +397,7 @@ int usb_diag_alloc_req(struct usb_diag_ch *ch, int n_write, int n_read)
 	if (!ctxt)
 		return -ENODEV;
 
-	raw_spin_lock_irqsave(&ctxt->lock, flags);
+	spin_lock_irqsave(&ctxt->lock, flags);
 	/* Free previous session's stale requests */
 	free_reqs(ctxt);
 	for (i = 0; i < n_write; i++) {
@@ -417,11 +417,11 @@ int usb_diag_alloc_req(struct usb_diag_ch *ch, int n_write, int n_read)
 		req->complete = diag_read_complete;
 		list_add_tail(&req->list, &ctxt->read_pool);
 	}
-	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+	spin_unlock_irqrestore(&ctxt->lock, flags);
 	return 0;
 fail:
 	free_reqs(ctxt);
-	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+	spin_unlock_irqrestore(&ctxt->lock, flags);
 	return -ENOMEM;
 
 }
@@ -472,17 +472,17 @@ int usb_diag_read(struct usb_diag_ch *ch, struct diag_request *d_req)
 	if (!ctxt)
 		return -ENODEV;
 
-	raw_spin_lock_irqsave(&ctxt->lock, flags);
+	spin_lock_irqsave(&ctxt->lock, flags);
 
 	if (!ctxt->configured || !ctxt->out) {
-		raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+		spin_unlock_irqrestore(&ctxt->lock, flags);
 		return -EIO;
 	}
 
 	out = ctxt->out;
 
 	if (list_empty(&ctxt->read_pool)) {
-		raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+		spin_unlock_irqrestore(&ctxt->lock, flags);
 		ERROR(ctxt->cdev, "%s: no requests available\n", __func__);
 		return -EAGAIN;
 	}
@@ -490,7 +490,7 @@ int usb_diag_read(struct usb_diag_ch *ch, struct diag_request *d_req)
 	req = list_first_entry(&ctxt->read_pool, struct usb_request, list);
 	list_del(&req->list);
 	kref_get(&ctxt->kref); /* put called in complete callback */
-	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+	spin_unlock_irqrestore(&ctxt->lock, flags);
 
 	req->buf = d_req->buf;
 	req->length = d_req->length;
@@ -506,7 +506,7 @@ int usb_diag_read(struct usb_diag_ch *ch, struct diag_request *d_req)
 
 	if (usb_ep_queue(out, req, GFP_ATOMIC)) {
 		/* If error add the link to linked list again*/
-		raw_spin_lock_irqsave(&ctxt->lock, flags);
+		spin_lock_irqsave(&ctxt->lock, flags);
 		list_add_tail(&req->list, &ctxt->read_pool);
 		/* 1 error message for every 10 sec */
 		if (__ratelimit(&rl))
@@ -514,10 +514,10 @@ int usb_diag_read(struct usb_diag_ch *ch, struct diag_request *d_req)
 				" read request\n", __func__);
 
 		if (kref_put(&ctxt->kref, diag_context_release))
-			/* diag_context_release called raw_spin_unlock already */
+			/* diag_context_release called spin_unlock already */
 			local_irq_restore(flags);
 		else
-			raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+			spin_unlock_irqrestore(&ctxt->lock, flags);
 		return -EIO;
 	}
 
@@ -549,17 +549,17 @@ int usb_diag_write(struct usb_diag_ch *ch, struct diag_request *d_req)
 	if (!ctxt)
 		return -ENODEV;
 
-	raw_spin_lock_irqsave(&ctxt->lock, flags);
+	spin_lock_irqsave(&ctxt->lock, flags);
 
 	if (!ctxt->configured || !ctxt->in) {
-		raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+		spin_unlock_irqrestore(&ctxt->lock, flags);
 		return -EIO;
 	}
 
 	in = ctxt->in;
 
 	if (list_empty(&ctxt->write_pool)) {
-		raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+		spin_unlock_irqrestore(&ctxt->lock, flags);
 		ERROR(ctxt->cdev, "%s: no requests available\n", __func__);
 		return -EAGAIN;
 	}
@@ -567,7 +567,7 @@ int usb_diag_write(struct usb_diag_ch *ch, struct diag_request *d_req)
 	req = list_first_entry(&ctxt->write_pool, struct usb_request, list);
 	list_del(&req->list);
 	kref_get(&ctxt->kref); /* put called in complete callback */
-	raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+	spin_unlock_irqrestore(&ctxt->lock, flags);
 
 	req->buf = d_req->buf;
 	req->length = d_req->length;
@@ -584,7 +584,7 @@ int usb_diag_write(struct usb_diag_ch *ch, struct diag_request *d_req)
 	ctxt->dpkts_tolaptop_pending++;
 	if (usb_ep_queue(in, req, GFP_ATOMIC)) {
 		/* If error add the link to linked list again*/
-		raw_spin_lock_irqsave(&ctxt->lock, flags);
+		spin_lock_irqsave(&ctxt->lock, flags);
 		list_add_tail(&req->list, &ctxt->write_pool);
 		ctxt->dpkts_tolaptop_pending--;
 		/* 1 error message for every 10 sec */
@@ -593,10 +593,10 @@ int usb_diag_write(struct usb_diag_ch *ch, struct diag_request *d_req)
 				" read request\n", __func__);
 
 		if (kref_put(&ctxt->kref, diag_context_release))
-			/* diag_context_release called raw_spin_unlock already */
+			/* diag_context_release called spin_unlock already */
 			local_irq_restore(flags);
 		else
-			raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+			spin_unlock_irqrestore(&ctxt->lock, flags);
 		return -EIO;
 	}
 
@@ -617,9 +617,9 @@ static void diag_function_disable(struct usb_function *f)
 
 	DBG(dev->cdev, "diag_function_disable\n");
 
-	raw_spin_lock_irqsave(&dev->lock, flags);
+	spin_lock_irqsave(&dev->lock, flags);
 	dev->configured = 0;
-	raw_spin_unlock_irqrestore(&dev->lock, flags);
+	spin_unlock_irqrestore(&dev->lock, flags);
 
 	if (dev->ch && dev->ch->notify)
 		dev->ch->notify(dev->ch->priv, USB_DIAG_DISCONNECT, NULL);
@@ -677,9 +677,9 @@ static int diag_function_set_alt(struct usb_function *f,
 	dev->dpkts_tomodem = 0;
 	dev->dpkts_tolaptop_pending = 0;
 
-	raw_spin_lock_irqsave(&dev->lock, flags);
+	spin_lock_irqsave(&dev->lock, flags);
 	dev->configured = 1;
-	raw_spin_unlock_irqrestore(&dev->lock, flags);
+	spin_unlock_irqrestore(&dev->lock, flags);
 
 	if (dev->ch->notify)
 		dev->ch->notify(dev->ch->priv, USB_DIAG_CONNECT, NULL);
@@ -709,14 +709,14 @@ static void diag_function_unbind(struct usb_configuration *c,
 		ctxt->ch->priv_usb = NULL;
 	list_del(&ctxt->list_item);
 	/* Free any pending USB requests from last session */
-	raw_spin_lock_irqsave(&ctxt->lock, flags);
+	spin_lock_irqsave(&ctxt->lock, flags);
 	free_reqs(ctxt);
 
 	if (kref_put(&ctxt->kref, diag_context_release))
-		/* diag_context_release called raw_spin_unlock already */
+		/* diag_context_release called spin_unlock already */
 		local_irq_restore(flags);
 	else
-		raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+		spin_unlock_irqrestore(&ctxt->lock, flags);
 }
 
 static int diag_function_bind(struct usb_configuration *c,
@@ -813,9 +813,9 @@ int diag_function_add(struct usb_configuration *c, const char *name,
 
 		_ch->name = name;
 
-		raw_spin_lock_irqsave(&ch_lock, flags);
+		spin_lock_irqsave(&ch_lock, flags);
 		list_add_tail(&_ch->list, &usb_diag_ch_list);
-		raw_spin_unlock_irqrestore(&ch_lock, flags);
+		spin_unlock_irqrestore(&ch_lock, flags);
 	}
 
 	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
@@ -842,7 +842,7 @@ int diag_function_add(struct usb_configuration *c, const char *name,
 	dev->function.set_alt = diag_function_set_alt;
 	dev->function.disable = diag_function_disable;
 	kref_init(&dev->kref);
-	raw_spin_lock_init(&dev->lock);
+	spin_lock_init(&dev->lock);
 	INIT_LIST_HEAD(&dev->read_pool);
 	INIT_LIST_HEAD(&dev->write_pool);
 
@@ -871,7 +871,7 @@ static ssize_t debug_read_stats(struct file *file, char __user *ubuf,
 		unsigned long flags;
 
 		if (ctxt) {
-			raw_spin_lock_irqsave(&ctxt->lock, flags);
+			spin_lock_irqsave(&ctxt->lock, flags);
 			temp += scnprintf(buf + temp, PAGE_SIZE - temp,
 					"---Name: %s---\n"
 					"endpoints: %s, %s\n"
@@ -883,7 +883,7 @@ static ssize_t debug_read_stats(struct file *file, char __user *ubuf,
 					ctxt->dpkts_tolaptop,
 					ctxt->dpkts_tomodem,
 					ctxt->dpkts_tolaptop_pending);
-			raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+			spin_unlock_irqrestore(&ctxt->lock, flags);
 		}
 	}
 
@@ -900,11 +900,11 @@ static ssize_t debug_reset_stats(struct file *file, const char __user *buf,
 		unsigned long flags;
 
 		if (ctxt) {
-			raw_spin_lock_irqsave(&ctxt->lock, flags);
+			spin_lock_irqsave(&ctxt->lock, flags);
 			ctxt->dpkts_tolaptop = 0;
 			ctxt->dpkts_tomodem = 0;
 			ctxt->dpkts_tolaptop_pending = 0;
-			raw_spin_unlock_irqrestore(&ctxt->lock, flags);
+			spin_unlock_irqrestore(&ctxt->lock, flags);
 		}
 	}
 
@@ -960,13 +960,13 @@ static void diag_cleanup(void)
 	list_for_each_safe(act, tmp, &usb_diag_ch_list) {
 		_ch = list_entry(act, struct usb_diag_ch, list);
 
-		raw_spin_lock_irqsave(&ch_lock, flags);
+		spin_lock_irqsave(&ch_lock, flags);
 		/* Free if diagchar is not using the channel anymore */
 		if (!_ch->priv) {
 			list_del(&_ch->list);
 			kfree(_ch);
 		}
-		raw_spin_unlock_irqrestore(&ch_lock, flags);
+		spin_unlock_irqrestore(&ch_lock, flags);
 	}
 }
 
diff --git a/kernel/msm-3.18/include/linux/qdsp6v2/apr_tal.h b/kernel/msm-3.18/include/linux/qdsp6v2/apr_tal.h
index ee8b2f5a8..52e7ce08a 100644
--- a/kernel/msm-3.18/include/linux/qdsp6v2/apr_tal.h
+++ b/kernel/msm-3.18/include/linux/qdsp6v2/apr_tal.h
@@ -92,7 +92,7 @@ struct apr_svc_ch_dev {
 #else
 struct apr_svc_ch_dev {
 	struct smd_channel *ch;
-	spinlock_t         lock;
+	raw_spinlock_t     lock;
 	spinlock_t         w_lock;
 	struct mutex       m_lock;
 	apr_svc_cb_fn      func;
-- 
2.49.0


From 864efccaef998d1b3a7392fef3190196870bbe0f Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Fri, 11 Jan 2019 16:00:19 -0800
Subject: [PATCH 10/20] msm_serial_hs threaded irq

---
 .../drivers/tty/serial/msm_serial_hs.c        | 10 +++++-
 .../drivers/tty/serial/msm_serial_hs_lite.c   | 33 ++++++++++++-------
 2 files changed, 30 insertions(+), 13 deletions(-)

diff --git a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c
index 53068614c..3ee6773f5 100644
--- a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c
+++ b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c
@@ -2389,6 +2389,14 @@ exit_request_clock_on:
 }
 EXPORT_SYMBOL(msm_hs_request_clock_on);
 
+static irqreturn_t msm_hsl_handler(int irq, void *dev) {
+	struct msm_hs_port *msm_uport = (struct msm_hs_port *)dev;
+	struct uart_port *uport = &msm_uport->uport;
+  disable_irq(msm_uport->wakeup.irq);
+  disable_irq(uport->irq);
+  return IRQ_WAKE_THREAD;
+}
+
 static irqreturn_t msm_hs_wakeup_isr(int irq, void *dev)
 {
 	unsigned int wakeup = 0;
@@ -2614,7 +2622,7 @@ static int msm_hs_startup(struct uart_port *uport)
 	msm_hs_resource_vote(msm_uport);
 
 	if (is_use_low_power_wakeup(msm_uport)) {
-		ret = request_irq(msm_uport->wakeup.irq, msm_hs_wakeup_isr,
+		ret = request_threaded_irq(msm_uport->wakeup.irq, NULL , msm_hs_wakeup_isr,
 					IRQF_TRIGGER_FALLING | IRQF_ONESHOT,
 					"msm_hs_wakeup", msm_uport);
 		if (unlikely(ret)) {
diff --git a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
index fbabf4af4..71bc52b78 100644
--- a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
+++ b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
@@ -444,17 +444,17 @@ static int msm_hsl_loopback_enable_set(void *data, u64 val)
 
 	vid = msm_hsl_port->ver_id;
 	if (val) {
-		raw_spin_lock_irqsave(&port->lock, flags);
+		spin_lock_irqsave(&port->lock, flags);
 		ret = msm_hsl_read(port, regmap[vid][UARTDM_MR2]);
 		ret |= UARTDM_MR2_LOOP_MODE_BMSK;
 		msm_hsl_write(port, ret, regmap[vid][UARTDM_MR2]);
-		raw_spin_unlock_irqrestore(&port->lock, flags);
+		spin_unlock_irqrestore(&port->lock, flags);
 	} else {
-		raw_spin_lock_irqsave(&port->lock, flags);
+		spin_lock_irqsave(&port->lock, flags);
 		ret = msm_hsl_read(port, regmap[vid][UARTDM_MR2]);
 		ret &= ~UARTDM_MR2_LOOP_MODE_BMSK;
 		msm_hsl_write(port, ret, regmap[vid][UARTDM_MR2]);
-		raw_spin_unlock_irqrestore(&port->lock, flags);
+		spin_unlock_irqrestore(&port->lock, flags);
 	}
 
 	clk_en(port, 0);
@@ -476,9 +476,9 @@ static int msm_hsl_loopback_enable_get(void *data, u64 *val)
 		return -EINVAL;
 	}
 
-	raw_spin_lock_irqsave(&port->lock, flags);
+	spin_lock_irqsave(&port->lock, flags);
 	ret = msm_hsl_read(port, regmap[msm_hsl_port->ver_id][UARTDM_MR2]);
-	raw_spin_unlock_irqrestore(&port->lock, flags);
+	spin_unlock_irqrestore(&port->lock, flags);
 	clk_en(port, 0);
 
 	*val = (ret & UARTDM_MR2_LOOP_MODE_BMSK) ? 1 : 0;
@@ -706,6 +706,15 @@ static void handle_delta_cts(struct uart_port *port)
 	port->icount.cts++;
 	wake_up_interruptible(&port->state->port.delta_msr_wait);
 }
+static irqreturn_t msm_hsl_handler(int irq, void *dev_id) {
+	unsigned int vid;
+	struct uart_port *port = dev_id;
+	struct msm_hsl_port *msm_hsl_port = UART_TO_MSM(port);
+
+	vid = msm_hsl_port->ver_id;
+	msm_hsl_write(port, 0, regmap[vid][UARTDM_IMR]);
+  return IRQ_WAKE_THREAD;
+}
 
 static irqreturn_t msm_hsl_irq(int irq, void *dev_id)
 {
@@ -715,7 +724,7 @@ static irqreturn_t msm_hsl_irq(int irq, void *dev_id)
 	unsigned int misr;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&port->lock, flags);
+	spin_lock_irqsave(&port->lock, flags);
 	vid = msm_hsl_port->ver_id;
 	misr = msm_hsl_read(port, regmap[vid][UARTDM_MISR]);
 	/* disable interrupt */
@@ -737,7 +746,7 @@ static irqreturn_t msm_hsl_irq(int irq, void *dev_id)
 
 	/* restore interrupt */
 	msm_hsl_write(port, msm_hsl_port->imr, regmap[vid][UARTDM_IMR]);
-	raw_spin_unlock_irqrestore(&port->lock, flags);
+	spin_unlock_irqrestore(&port->lock, flags);
 
 	return IRQ_HANDLED;
 }
@@ -1015,7 +1024,7 @@ static int msm_hsl_startup(struct uart_port *port)
 	else
 		rfr_level = port->fifosize;
 
-	raw_spin_lock_irqsave(&port->lock, flags);
+	spin_lock_irqsave(&port->lock, flags);
 
 	vid = msm_hsl_port->ver_id;
 	/* set automatic RFR level */
@@ -1025,7 +1034,7 @@ static int msm_hsl_startup(struct uart_port *port)
 	data |= UARTDM_MR1_AUTO_RFR_LEVEL1_BMSK & (rfr_level << 2);
 	data |= UARTDM_MR1_AUTO_RFR_LEVEL0_BMSK & rfr_level;
 	msm_hsl_write(port, data, regmap[vid][UARTDM_MR1]);
-	raw_spin_unlock_irqrestore(&port->lock, flags);
+	spin_unlock_irqrestore(&port->lock, flags);
 
 	ret = request_threaded_irq(port->irq, NULL, msm_hsl_irq, IRQF_TRIGGER_HIGH,
 			  msm_hsl_port->name, port);
@@ -1458,13 +1467,13 @@ static void msm_hsl_console_write(struct console *co, const char *s,
 		locked = spin_trylock(&port->lock);
 	else {
 		locked = 1;
-		raw_spin_lock(&port->lock);
+		spin_lock(&port->lock);
 	}
 	msm_hsl_write(port, 0, regmap[vid][UARTDM_IMR]);
 	uart_console_write(port, s, count, msm_hsl_console_putchar);
 	msm_hsl_write(port, msm_hsl_port->imr, regmap[vid][UARTDM_IMR]);
 	if (locked == 1)
-		raw_spin_unlock(&port->lock);
+		spin_unlock(&port->lock);
 }
 
 static int msm_hsl_console_setup(struct console *co, char *options)
-- 
2.49.0


From bf68ed00211bc5148ebd66616bd2aaecd6c931a6 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Fri, 11 Jan 2019 16:01:19 -0800
Subject: [PATCH 11/20] config and turn off irqdebug

---
 .../conf/machine/apq8009-robot.conf           |  2 +-
 .../meta/cfg/debug/apq8009-robot/defconfig    | 28 +++++++++----------
 2 files changed, 15 insertions(+), 15 deletions(-)

diff --git a/poky/meta-qti-bsp/conf/machine/apq8009-robot.conf b/poky/meta-qti-bsp/conf/machine/apq8009-robot.conf
index a4b62adaa..5591524fd 100644
--- a/poky/meta-qti-bsp/conf/machine/apq8009-robot.conf
+++ b/poky/meta-qti-bsp/conf/machine/apq8009-robot.conf
@@ -13,7 +13,7 @@ KERNEL_BASE = "0x81C00000"
 KERNEL_TAGS_OFFSET = "0x81900000"
 
 # setting mem=511M as the last MB is reserved for panic reporting
-KERNEL_CMD_PARAMS = "noinitrd ro console=ttyHSL0,115200,n8 androidboot.hardware=qcom ehci-hcd.park=3 msm_rtb.filter=0x37 lpm_levels.sleep_disabled=1 rootwait androidboot.bootdevice=7824900.sdhci mem=511M isolcpus=3"
+KERNEL_CMD_PARAMS = "noinitrd ro console=ttyHSL0,115200,n8 androidboot.hardware=qcom ehci-hcd.park=3 msm_rtb.filter=0x37 lpm_levels.sleep_disabled=1 rootwait androidboot.bootdevice=7824900.sdhci mem=511M isolcpus=3 noirqdebug"
 
 # Use systemd init manager.
 DISTRO_FEATURES_append = " systemd"
diff --git a/poky/meta-qti-bsp/recipes-kernel/linux-quic/meta/cfg/debug/apq8009-robot/defconfig b/poky/meta-qti-bsp/recipes-kernel/linux-quic/meta/cfg/debug/apq8009-robot/defconfig
index 15f9ef75a..043382ca4 100644
--- a/poky/meta-qti-bsp/recipes-kernel/linux-quic/meta/cfg/debug/apq8009-robot/defconfig
+++ b/poky/meta-qti-bsp/recipes-kernel/linux-quic/meta/cfg/debug/apq8009-robot/defconfig
@@ -109,9 +109,10 @@ CONFIG_RCU_STALL_COMMON=y
 CONFIG_RCU_FANOUT=32
 CONFIG_RCU_FANOUT_LEAF=16
 # CONFIG_RCU_FANOUT_EXACT is not set
-CONFIG_RCU_FAST_NO_HZ=y
 # CONFIG_TREE_RCU_TRACE is not set
-# CONFIG_RCU_BOOST is not set
+CONFIG_RCU_BOOST=y
+CONFIG_RCU_BOOST_PRIO=1
+CONFIG_RCU_BOOST_DELAY=500
 # CONFIG_RCU_NOCB_CPU is not set
 CONFIG_BUILD_BIN2C=y
 CONFIG_IKCONFIG=y
@@ -132,7 +133,6 @@ CONFIG_RESOURCE_COUNTERS=y
 CONFIG_CGROUP_SCHED=y
 CONFIG_FAIR_GROUP_SCHED=y
 # CONFIG_CFS_BANDWIDTH is not set
-CONFIG_RT_GROUP_SCHED=y
 # CONFIG_BLK_CGROUP is not set
 # CONFIG_SCHED_HMP is not set
 # CONFIG_SCHED_CORE_CTL is not set
@@ -191,13 +191,10 @@ CONFIG_PERF_EVENTS=y
 CONFIG_VM_EVENT_COUNTERS=y
 CONFIG_SLUB_DEBUG=y
 CONFIG_COMPAT_BRK=y
-# CONFIG_SLAB is not set
 CONFIG_SLUB=y
-# CONFIG_SLOB is not set
 CONFIG_SLUB_CPU_PARTIAL=y
 CONFIG_PROFILING=y
 CONFIG_TRACEPOINTS=y
-CONFIG_OPROFILE=m
 CONFIG_HAVE_OPROFILE=y
 CONFIG_KPROBES=y
 # CONFIG_JUMP_LABEL is not set
@@ -300,10 +297,7 @@ CONFIG_IOSCHED_CFQ=y
 CONFIG_DEFAULT_CFQ=y
 # CONFIG_DEFAULT_NOOP is not set
 CONFIG_DEFAULT_IOSCHED="cfq"
-CONFIG_UNINLINE_SPIN_UNLOCK=y
 CONFIG_ARCH_SUPPORTS_ATOMIC_RMW=y
-CONFIG_MUTEX_SPIN_ON_OWNER=y
-CONFIG_RWSEM_SPIN_ON_OWNER=y
 CONFIG_FREEZER=y
 
 #
@@ -437,9 +431,15 @@ CONFIG_NR_CPUS=4
 CONFIG_HOTPLUG_CPU=y
 # CONFIG_ARM_PSCI is not set
 CONFIG_ARCH_NR_GPIO=1024
+CONFIG_PREEMPT=y
+CONFIG_PREEMPT_RT_BASE=y
+CONFIG_HAVE_PREEMPT_LAZY=y
+CONFIG_PREEMPT_LAZY=y
 # CONFIG_PREEMPT_NONE is not set
 # CONFIG_PREEMPT_VOLUNTARY is not set
-CONFIG_PREEMPT=y
+# CONFIG_PREEMPT__LL is not set
+# CONFIG_PREEMPT_RTB is not set
+CONFIG_PREEMPT_RT_FULL=y
 CONFIG_PREEMPT_COUNT=y
 CONFIG_HZ_FIXED=0
 CONFIG_HZ_100=y
@@ -1290,6 +1290,7 @@ CONFIG_BLK_DEV_RAM_SIZE=4096
 # CONFIG_SENSORS_LIS3LV02D is not set
 # CONFIG_AD525X_DPOT is not set
 # CONFIG_DUMMY_IRQ is not set
+CONFIG_HWLAT_DETECTOR=m
 # CONFIG_ICS932S401 is not set
 # CONFIG_ENCLOSURE_SERVICES is not set
 # CONFIG_APDS9802ALS is not set
@@ -1402,7 +1403,6 @@ CONFIG_SCSI_LOWLEVEL=y
 # CONFIG_ATA is not set
 CONFIG_MD=y
 # CONFIG_BLK_DEV_MD is not set
-# CONFIG_BCACHE is not set
 CONFIG_BLK_DEV_DM_BUILTIN=y
 CONFIG_BLK_DEV_DM=y
 # CONFIG_DM_DEBUG is not set
@@ -3140,7 +3140,6 @@ CONFIG_LEDS_TRIGGERS=y
 # CONFIG_LEDS_TRIGGER_ONESHOT is not set
 # CONFIG_LEDS_TRIGGER_HEARTBEAT is not set
 # CONFIG_LEDS_TRIGGER_BACKLIGHT is not set
-# CONFIG_LEDS_TRIGGER_CPU is not set
 # CONFIG_LEDS_TRIGGER_GPIO is not set
 # CONFIG_LEDS_TRIGGER_DEFAULT_ON is not set
 
@@ -3785,7 +3784,6 @@ CONFIG_DEBUG_VM=y
 # CONFIG_DEBUG_VM_RB is not set
 CONFIG_DEBUG_MEMORY_INIT=y
 # CONFIG_DEBUG_PER_CPU_MAPS is not set
-# CONFIG_DEBUG_SHIRQ is not set
 
 #
 # Debug Lockups and Hangs
@@ -3877,8 +3875,10 @@ CONFIG_FTRACE=y
 CONFIG_FUNCTION_TRACER=y
 CONFIG_FUNCTION_GRAPH_TRACER=y
 CONFIG_IRQSOFF_TRACER=y
-CONFIG_PREEMPT_TRACER=y
+# CONFIG_INTERRUPT_OFF_HIST is not set
+# CONFIG_PREEMPT_TRACER is not set
 # CONFIG_SCHED_TRACER is not set
+# CONFIG_MISSED_TIMER_OFFSETS_HIST is not set
 # CONFIG_FTRACE_SYSCALLS is not set
 CONFIG_TRACER_SNAPSHOT=y
 CONFIG_TRACER_SNAPSHOT_PER_CPU_SWAP=y
-- 
2.49.0


From a338b6608bb9b63123cd672a4889983336f32fb3 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Tue, 15 Jan 2019 14:08:28 -0800
Subject: [PATCH 12/20] Revert "Make msm_serial_hs_light use threaded irq"

This reverts commit 0eda2c6154c35916504c4e577b0bfe960b78800f.
---
 kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
index 71bc52b78..6dae09a9d 100644
--- a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
+++ b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs_lite.c
@@ -1036,7 +1036,7 @@ static int msm_hsl_startup(struct uart_port *port)
 	msm_hsl_write(port, data, regmap[vid][UARTDM_MR1]);
 	spin_unlock_irqrestore(&port->lock, flags);
 
-	ret = request_threaded_irq(port->irq, NULL, msm_hsl_irq, IRQF_TRIGGER_HIGH,
+	ret = request_irq(port->irq, msm_hsl_irq, IRQF_TRIGGER_HIGH,
 			  msm_hsl_port->name, port);
 	if (unlikely(ret)) {
 		pr_err("failed to request_irq\n");
-- 
2.49.0


From dc80e98e602b7fbce00bfd7c161d192818fe24c2 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Wed, 16 Jan 2019 13:09:15 -0800
Subject: [PATCH 13/20] more locks

---
 kernel/msm-3.18/drivers/iommu/msm_iommu-v1.c  | 48 +++++++++----------
 kernel/msm-3.18/drivers/usb/phy/phy-msm-usb.c |  5 +-
 2 files changed, 27 insertions(+), 26 deletions(-)

diff --git a/kernel/msm-3.18/drivers/iommu/msm_iommu-v1.c b/kernel/msm-3.18/drivers/iommu/msm_iommu-v1.c
index 650d019c0..8380c87c4 100644
--- a/kernel/msm-3.18/drivers/iommu/msm_iommu-v1.c
+++ b/kernel/msm-3.18/drivers/iommu/msm_iommu-v1.c
@@ -974,7 +974,7 @@ static void msm_iommu_domain_destroy(struct iommu_domain *domain)
 	unsigned long flags;
 
 	mutex_lock(&msm_iommu_lock);
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	priv = domain->priv;
 	domain->priv = NULL;
 
@@ -982,7 +982,7 @@ static void msm_iommu_domain_destroy(struct iommu_domain *domain)
 		msm_iommu_pagetable_free(&priv->pt);
 
 	kfree(priv);
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	mutex_unlock(&msm_iommu_lock);
 }
 
@@ -1055,21 +1055,21 @@ static int msm_iommu_attach_dev(struct iommu_domain *domain, struct device *dev)
 	if (ctx_drvdata->attach_count > 1)
 		goto already_attached;
 
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	if (!list_empty(&ctx_drvdata->attached_elm)) {
 		ret = -EBUSY;
-		raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+		spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 		goto unlock;
 	}
 
 	list_for_each_entry(tmp_drvdata, &priv->list_attached, attached_elm)
 		if (tmp_drvdata == ctx_drvdata) {
 			ret = -EBUSY;
-			raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+			spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 			goto unlock;
 		}
 
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 
 	is_secure = iommu_drvdata->sec_id != -1;
 
@@ -1121,9 +1121,9 @@ static int msm_iommu_attach_dev(struct iommu_domain *domain, struct device *dev)
 
 	__disable_clocks(iommu_drvdata);
 
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	list_add(&(ctx_drvdata->attached_elm), &priv->list_attached);
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 
 	ctx_drvdata->attached_domain = domain;
 	++iommu_drvdata->ctx_attach_count;
@@ -1235,9 +1235,9 @@ static void msm_iommu_detach_dev(struct iommu_domain *domain,
 
 	__disable_regulators(iommu_drvdata);
 
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	list_del_init(&ctx_drvdata->attached_elm);
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 
 	ctx_drvdata->attached_domain = NULL;
 	BUG_ON(iommu_drvdata->ctx_attach_count == 0);
@@ -1253,7 +1253,7 @@ static int msm_iommu_map(struct iommu_domain *domain, unsigned long va,
 	int ret = 0;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	priv = domain->priv;
 	if (!priv) {
 		ret = -EINVAL;
@@ -1266,7 +1266,7 @@ static int msm_iommu_map(struct iommu_domain *domain, unsigned long va,
 
 	msm_iommu_flush_pagetable(&priv->pt, va, len);
 fail:
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	return ret;
 }
 
@@ -1277,7 +1277,7 @@ static size_t msm_iommu_unmap(struct iommu_domain *domain, unsigned long va,
 	int ret = -ENODEV;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	priv = domain->priv;
 	if (!priv)
 		goto fail;
@@ -1294,7 +1294,7 @@ static size_t msm_iommu_unmap(struct iommu_domain *domain, unsigned long va,
 
 	msm_iommu_pagetable_free_tables(&priv->pt, va, len);
 fail:
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	/* the IOMMU API requires us to return how many bytes were unmapped */
 	len = ret ? 0 : len;
 	return len;
@@ -1308,7 +1308,7 @@ static int msm_iommu_map_range(struct iommu_domain *domain, unsigned long va,
 	struct msm_iommu_priv *priv;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	priv = domain->priv;
 	if (!priv) {
 		ret = -EINVAL;
@@ -1319,7 +1319,7 @@ static int msm_iommu_map_range(struct iommu_domain *domain, unsigned long va,
 	msm_iommu_flush_pagetable(&priv->pt, va, len);
 
 fail:
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	return ret;
 }
 
@@ -1330,7 +1330,7 @@ static int msm_iommu_unmap_range(struct iommu_domain *domain, unsigned long va,
 	struct msm_iommu_priv *priv;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	priv = domain->priv;
 	msm_iommu_pagetable_unmap_range(&priv->pt, va, len);
 
@@ -1338,7 +1338,7 @@ static int msm_iommu_unmap_range(struct iommu_domain *domain, unsigned long va,
 	__flush_iotlb(domain);
 
 	msm_iommu_pagetable_free_tables(&priv->pt, va, len);
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	return 0;
 }
 
@@ -1370,9 +1370,9 @@ static phys_addr_t msm_iommu_iova_to_phys(struct iommu_domain *domain,
 	phys_addr_t ret;
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	ret = msm_iommu_iova_to_phys_soft(domain, va);
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 	return ret;
 }
 
@@ -1396,10 +1396,10 @@ static phys_addr_t msm_iommu_iova_to_phys_hard(struct iommu_domain *domain,
 		goto fail;
 
 
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	ctx_drvdata = list_entry(priv->list_attached.next,
 				 struct msm_iommu_ctx_drvdata, attached_elm);
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 
 	if (is_domain_dynamic(priv) || ctx_drvdata->dynamic)
 		goto fail;
@@ -1421,7 +1421,7 @@ static phys_addr_t msm_iommu_iova_to_phys_hard(struct iommu_domain *domain,
 		goto fail;
 	}
 
-	raw_spin_lock_irqsave(&msm_iommu_spin_lock, flags);
+	spin_lock_irqsave(&msm_iommu_spin_lock, flags);
 	SET_ATS1PR(base, ctx, va & CB_ATS1PR_ADDR);
 	/* make sure ATS1PR is visible */
 	mb();
@@ -1439,7 +1439,7 @@ static phys_addr_t msm_iommu_iova_to_phys_hard(struct iommu_domain *domain,
 	}
 
 	par = GET_PAR(base, ctx);
-	raw_spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
+	spin_unlock_irqrestore(&msm_iommu_spin_lock, flags);
 
 	__disable_clocks(iommu_drvdata);
 
diff --git a/kernel/msm-3.18/drivers/usb/phy/phy-msm-usb.c b/kernel/msm-3.18/drivers/usb/phy/phy-msm-usb.c
index 0c6bc9522..74ec01c58 100644
--- a/kernel/msm-3.18/drivers/usb/phy/phy-msm-usb.c
+++ b/kernel/msm-3.18/drivers/usb/phy/phy-msm-usb.c
@@ -4722,7 +4722,7 @@ static int msm_otg_probe(struct platform_device *pdev)
 		goto destroy_wlock;
 	}
 
-	ret = request_irq(motg->irq, msm_otg_irq, IRQF_SHARED,
+	ret = request_threaded_irq(motg->irq,NULL, msm_otg_irq, IRQF_SHARED,
 					"msm_otg", motg);
 	if (ret) {
 		dev_err(&pdev->dev, "request irq failed\n");
@@ -4755,8 +4755,9 @@ static int msm_otg_probe(struct platform_device *pdev)
 		}
 	}
 
-	ret = request_irq(motg->async_irq, msm_otg_irq,
+	ret = request_threaded_irq(motg->async_irq, NULL, msm_otg_irq,
 				IRQF_TRIGGER_RISING, "msm_otg", motg);
+
 	if (ret) {
 		dev_err(&pdev->dev, "request irq failed (ASYNC INT)\n");
 		goto free_phy_irq;
-- 
2.49.0


From 25325e65da4d16969ea0aa193d07aee6bc616590 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Thu, 17 Jan 2019 10:16:53 -0800
Subject: [PATCH 14/20] RT-PATCH testing

Remove wlan
Add tty terminal
Remove lttng kernel tracing
Remove isolate cpu 3
---
 poky/build/conf/local.conf                        | 10 +++++++---
 poky/meta-anki/recipes/ankiinit/files/ankiinit.sh |  2 +-
 poky/meta-anki/recipes/ankitrace/files/ankitrace  | 14 +-------------
 poky/meta-qti-bsp/conf/machine/apq8009-robot.conf |  2 +-
 poky/meta/recipes-core/systemd/systemd_225.bb     |  1 +
 5 files changed, 11 insertions(+), 18 deletions(-)

diff --git a/poky/build/conf/local.conf b/poky/build/conf/local.conf
index 69284f7b3..06ee19712 100644
--- a/poky/build/conf/local.conf
+++ b/poky/build/conf/local.conf
@@ -17,12 +17,12 @@
 # These two options control how much parallelism BitBake should use. The first
 # option determines how many tasks bitbake should run in parallel:
 #
-BB_NUMBER_THREADS ?= " 12"
+BB_NUMBER_THREADS ?= " 32"
 #
 # The second option controls how many processes make should run in parallel when
 # running compile tasks:
 #
-PARALLEL_MAKE ?= "-j 12"
+PARALLEL_MAKE ?= "-j 32"
 #
 # For a quadcore, BB_NUMBER_THREADS = "4", PARALLEL_MAKE = "-j 4" would
 # be appropriate for example.
@@ -138,7 +138,7 @@ PACKAGE_DEBUG_SPLIT_STYLE = "debug-without-src"
 # There are other application targets that can be uses here too, see
 # meta/classes/core-image.bbclass and meta/recipes-core/tasks/task-core.bb for more details.
 # We default to enabling the debugging tweaks.
-#EXTRA_IMAGE_FEATURES = "debug-tweaks"
+EXTRA_IMAGE_FEATURES = "debug-tweaks"
 
 #
 # Additional image features
@@ -255,3 +255,7 @@ ANKI_PRODUCT_NAME = "Vector"
 
 # Set the default hostname to our product name
 hostname_pn-base-files = "${ANKI_PRODUCT_NAME}"
+
+IMAGE_INSTALL_append = " systemd-serialgetty"
+
+IMAGE_INSTALL_remove += "wlan-opensource"
\ No newline at end of file
diff --git a/poky/meta-anki/recipes/ankiinit/files/ankiinit.sh b/poky/meta-anki/recipes/ankiinit/files/ankiinit.sh
index e21c5e4b9..d21fb0f02 100644
--- a/poky/meta-anki/recipes/ankiinit/files/ankiinit.sh
+++ b/poky/meta-anki/recipes/ankiinit/files/ankiinit.sh
@@ -16,7 +16,7 @@ echo 1 > /sys/kernel/debug/msm-bus-dbg/shell-client/update_request
 
 # Pin serial and spi kthreads to core 0
 pgrep msm_serial_hs_0 | xargs -I PID taskset -p 8 PID
-pgrep spi[01] | xargs -I PID taskset -p 8 PID
+pgrep spi0 | xargs -I PID taskset -p 8 PID
 
 # TODO Move this power rail controll into the camera driver
 CAM_REG_GPIO=83
diff --git a/poky/meta-anki/recipes/ankitrace/files/ankitrace b/poky/meta-anki/recipes/ankitrace/files/ankitrace
index e0d9c70a0..bad8a009f 100644
--- a/poky/meta-anki/recipes/ankitrace/files/ankitrace
+++ b/poky/meta-anki/recipes/ankitrace/files/ankitrace
@@ -10,21 +10,9 @@ function trace_start {
     /bin/mkdir -p $LTTNG_URL
     /sbin/modprobe lttng-tracer
     $LTTNG create --snapshot --set-url=$LTTNG_URL
-    $LTTNG enable-channel -k --subbuf-size=512k --num-subbuf=2 channel0
-    $LTTNG enable-channel -u --subbuf-size=128K --num-subbuf=2 channel0
+    $LTTNG enable-channel -u --subbuf-size=512K --num-subbuf=3 channel0
     $LTTNG enable-event -u anki_ust* -c channel0
     $LTTNG enable-event -u lttng_ust_tracelog* -c channel0
-    $LTTNG enable-event -k -a --syscall  -c channel0
-    $LTTNG enable-event -k sched_switch -c channel0
-    $LTTNG enable-event -k power_cpu_frequency -c channel0
-    $LTTNG enable-event -k irq_handler_entry -c channel0
-    $LTTNG enable-event -k irq_softirq_raise -c channel0
-    $LTTNG enable-event -k asoc_snd* -c channel0
-    $LTTNG enable-event -k compaction* -c channel0
-    $LTTNG enable-event -k writeback* -c channel0
-    $LTTNG enable-event -k workqueue* -c channel0
-    $LTTNG enable-event -k mm_vmscan* -c channel0
-    $LTTNG enable-event -k lttng_logger -c channel0
     $LTTNG start
     /bin/touch $TRACE_ACTIVE
 }
diff --git a/poky/meta-qti-bsp/conf/machine/apq8009-robot.conf b/poky/meta-qti-bsp/conf/machine/apq8009-robot.conf
index 5591524fd..b51e2912b 100644
--- a/poky/meta-qti-bsp/conf/machine/apq8009-robot.conf
+++ b/poky/meta-qti-bsp/conf/machine/apq8009-robot.conf
@@ -13,7 +13,7 @@ KERNEL_BASE = "0x81C00000"
 KERNEL_TAGS_OFFSET = "0x81900000"
 
 # setting mem=511M as the last MB is reserved for panic reporting
-KERNEL_CMD_PARAMS = "noinitrd ro console=ttyHSL0,115200,n8 androidboot.hardware=qcom ehci-hcd.park=3 msm_rtb.filter=0x37 lpm_levels.sleep_disabled=1 rootwait androidboot.bootdevice=7824900.sdhci mem=511M isolcpus=3 noirqdebug"
+KERNEL_CMD_PARAMS = "noinitrd ro console=ttyHSL0,115200,n8 androidboot.hardware=qcom ehci-hcd.park=3 msm_rtb.filter=0x37 lpm_levels.sleep_disabled=1 rootwait androidboot.bootdevice=7824900.sdhci mem=511M noirqdebug"
 
 # Use systemd init manager.
 DISTRO_FEATURES_append = " systemd"
diff --git a/poky/meta/recipes-core/systemd/systemd_225.bb b/poky/meta/recipes-core/systemd/systemd_225.bb
index fcfa5ec67..7d472f921 100644
--- a/poky/meta/recipes-core/systemd/systemd_225.bb
+++ b/poky/meta/recipes-core/systemd/systemd_225.bb
@@ -329,6 +329,7 @@ RDEPENDS_${PN} += "kmod dbus util-linux-mount udev (= ${EXTENDPKGV})"
 RDEPENDS_${PN} += "volatile-binds update-rc.d"
 
 RRECOMMENDS_${PN} += "systemd-vconsole-setup \
+                      systemd-serialgetty \
                       systemd-compat-units udev-hwdb \
                       util-linux-agetty  util-linux-fsck e2fsprogs-e2fsck \
                       kernel-module-autofs4 kernel-module-unix kernel-module-ipv6 \
-- 
2.49.0


From de78b6b9cddd6d3e3faeeb4b33637134a2c8dd9e Mon Sep 17 00:00:00 2001
From: Steven Walter <stevenrwalter@xxxxxxxxx>
Date: Fri, 14 Dec 2018 11:05:33 -0800
Subject: [PATCH 15/20] [PATCH v3 1/2] drivers/tty: refactor functions for
 flushing/queuing work

Preparation for re-iplementing low-latency port support.  Replace queue_work
and schedule_work with tty_buffer_queue_work, and calls to flush_work with
tty_buffer_flush_work.  cancel_work_sync are replaced by tty_buffer_flush_work
as well, because kthread workqueues don't have a method of cancellation other
than to flush.  flush could potentially take longer, but we would already delay
if the callback were in progress.  Since it's only used from the release /
destroy path, the difference should be negligible.

Signed-off-by: Steven Walter <stevenrwalter@xxxxxxxxx>
---
 kernel/msm-3.18/drivers/tty/n_tty.c      |  2 +-
 kernel/msm-3.18/drivers/tty/tty_buffer.c | 18 +++++++++++++++---
 kernel/msm-3.18/drivers/tty/tty_ldisc.c  |  4 ++--
 kernel/msm-3.18/drivers/tty/tty_port.c   |  2 +-
 kernel/msm-3.18/include/linux/tty.h      |  2 ++
 5 files changed, 21 insertions(+), 7 deletions(-)

diff --git a/kernel/msm-3.18/drivers/tty/n_tty.c b/kernel/msm-3.18/drivers/tty/n_tty.c
index d6144cbe2..e74475b65 100644
--- a/kernel/msm-3.18/drivers/tty/n_tty.c
+++ b/kernel/msm-3.18/drivers/tty/n_tty.c
@@ -228,7 +228,7 @@ static void n_tty_set_room(struct tty_struct *tty)
 		 */
 		WARN_RATELIMIT(test_bit(TTY_LDISC_HALTED, &tty->flags),
 			       "scheduling buffer work for halted ldisc\n");
-		queue_work(system_unbound_wq, &tty->port->buf.work);
+		tty_buffer_queue_work(tty->port);
 	}
 }
 
diff --git a/kernel/msm-3.18/drivers/tty/tty_buffer.c b/kernel/msm-3.18/drivers/tty/tty_buffer.c
index 9b240b3ef..3bfb4738a 100644
--- a/kernel/msm-3.18/drivers/tty/tty_buffer.c
+++ b/kernel/msm-3.18/drivers/tty/tty_buffer.c
@@ -72,7 +72,7 @@ void tty_buffer_unlock_exclusive(struct tty_port *port)
 	atomic_dec(&buf->priority);
 	mutex_unlock(&buf->lock);
 	if (restart)
-		queue_work(system_unbound_wq, &buf->work);
+		tty_buffer_queue_work(port);
 }
 EXPORT_SYMBOL_GPL(tty_buffer_unlock_exclusive);
 
@@ -364,7 +364,7 @@ void tty_schedule_flip(struct tty_port *port)
 	struct tty_bufhead *buf = &port->buf;
 
 	buf->tail->commit = buf->tail->used;
-	schedule_work(&buf->work);
+	tty_buffer_queue_work(port);
 }
 EXPORT_SYMBOL(tty_schedule_flip);
 
@@ -492,7 +492,19 @@ static void flush_to_ldisc(struct work_struct *work)
  */
 void tty_flush_to_ldisc(struct tty_struct *tty)
 {
-	flush_work(&tty->port->buf.work);
+	tty_buffer_flush_work(tty->port);
+}
+
+void tty_buffer_queue_work(struct tty_port *port)
+{
+	struct tty_bufhead *buf = &port->buf;
+	schedule_work(&buf->work);
+}
+
+void tty_buffer_flush_work(struct tty_port *port)
+{
+	struct tty_bufhead *buf = &port->buf;
+	flush_work(&buf->work);
 }
 
 /**
diff --git a/kernel/msm-3.18/drivers/tty/tty_ldisc.c b/kernel/msm-3.18/drivers/tty/tty_ldisc.c
index 2bf08366c..6a2d2d9cc 100644
--- a/kernel/msm-3.18/drivers/tty/tty_ldisc.c
+++ b/kernel/msm-3.18/drivers/tty/tty_ldisc.c
@@ -591,9 +591,9 @@ int tty_set_ldisc(struct tty_struct *tty, int ldisc)
 
 	/* Restart the work queue in case no characters kick it off. Safe if
 	   already running */
-	schedule_work(&tty->port->buf.work);
+	tty_buffer_queue_work(tty->port);
 	if (o_tty)
-		schedule_work(&o_tty->port->buf.work);
+		tty_buffer_queue_work(o_tty->port);
 
 	tty_unlock(tty);
 	return retval;
diff --git a/kernel/msm-3.18/drivers/tty/tty_port.c b/kernel/msm-3.18/drivers/tty/tty_port.c
index 1b9335796..3bc07a976 100644
--- a/kernel/msm-3.18/drivers/tty/tty_port.c
+++ b/kernel/msm-3.18/drivers/tty/tty_port.c
@@ -132,7 +132,7 @@ EXPORT_SYMBOL(tty_port_free_xmit_buf);
 void tty_port_destroy(struct tty_port *port)
 {
 	cancel_work_sync(&port->buf.work);
-	tty_buffer_free_all(port);
+	tty_buffer_flush_work(port);
 }
 EXPORT_SYMBOL(tty_port_destroy);
 
diff --git a/kernel/msm-3.18/include/linux/tty.h b/kernel/msm-3.18/include/linux/tty.h
index 4858a3b79..2adb3dd25 100644
--- a/kernel/msm-3.18/include/linux/tty.h
+++ b/kernel/msm-3.18/include/linux/tty.h
@@ -446,6 +446,8 @@ extern void tty_flush_to_ldisc(struct tty_struct *tty);
 extern void tty_buffer_free_all(struct tty_port *port);
 extern void tty_buffer_flush(struct tty_struct *tty);
 extern void tty_buffer_init(struct tty_port *port);
+extern void tty_buffer_queue_work(struct tty_port *port);
+extern void tty_buffer_flush_work(struct tty_port *port);
 extern speed_t tty_termios_baud_rate(struct ktermios *termios);
 extern speed_t tty_termios_input_baud_rate(struct ktermios *termios);
 extern void tty_termios_encode_baud_rate(struct ktermios *termios,
-- 
2.49.0


From 07b54dc411bdcf23c8c10677c05afa3a0e81cc88 Mon Sep 17 00:00:00 2001
From: Steven Walter <stevenrwalter@xxxxxxxxx>
Date: Fri, 14 Dec 2018 11:15:48 -0800
Subject: [PATCH 16/20] [PATCH v3 2/2] drivers/tty: use a kthread_worker for
 low-latency

Use a dedicated kthread for handling ports marked as low_latency.  Since
this thread is RT_FIFO, it is not subject to the same types of
starvation as the normal priority kworker threads.

Some UART applications are very latency sensitive.  In at least one case
(motor controller card controlled over RS-232), a reply needs to be sent
within about 1ms after receiving a byte.  Without this patch, normal
priority kworker threads can be delayed substantially by userspace
real-time threads.  This has been observed to cause latencies up to
100ms.  With this patch, no occurences of communications failure were
observed, meaning the 1ms latency is reliably achieved.

Signed-off-by: Steven Walter <stevenrwalter@xxxxxxxxx>
---
 kernel/msm-3.18/drivers/tty/tty_buffer.c | 40 +++++++++++++++++++++---
 kernel/msm-3.18/drivers/tty/tty_io.c     |  1 +
 kernel/msm-3.18/include/linux/tty.h      |  3 ++
 3 files changed, 40 insertions(+), 4 deletions(-)

diff --git a/kernel/msm-3.18/drivers/tty/tty_buffer.c b/kernel/msm-3.18/drivers/tty/tty_buffer.c
index 3bfb4738a..6719d0390 100644
--- a/kernel/msm-3.18/drivers/tty/tty_buffer.c
+++ b/kernel/msm-3.18/drivers/tty/tty_buffer.c
@@ -3,6 +3,7 @@
  */
 
 #include <linux/types.h>
+#include <linux/kthread.h>
 #include <linux/errno.h>
 #include <linux/tty.h>
 #include <linux/tty_driver.h>
@@ -11,6 +12,7 @@
 #include <linux/string.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
+#include <linux/sched/prio.h>
 #include <linux/wait.h>
 #include <linux/bitops.h>
 #include <linux/delay.h>
@@ -431,9 +433,8 @@ receive_buf(struct tty_struct *tty, struct tty_buffer *head, int count)
  *		 'consumer'
  */
 
-static void flush_to_ldisc(struct work_struct *work)
+static void flush_to_ldisc(struct tty_port *port)
 {
-	struct tty_port *port = container_of(work, struct tty_port, buf.work);
 	struct tty_bufhead *buf = &port->buf;
 	struct tty_struct *tty;
 	struct tty_ldisc *disc;
@@ -482,6 +483,18 @@ static void flush_to_ldisc(struct work_struct *work)
 	tty_ldisc_deref(disc);
 }
 
+static void flush_to_ldisc_kthread(struct kthread_work *work)
+{
+	struct tty_port *port = container_of(work, struct tty_port, buf.kwork);
+	flush_to_ldisc(port);
+}
+
+static void flush_to_ldisc_workq(struct work_struct *work)
+{
+	struct tty_port *port = container_of(work, struct tty_port, buf.work);
+	flush_to_ldisc(port);
+}
+
 /**
  *	tty_flush_to_ldisc
  *	@tty: tty to push
@@ -495,18 +508,36 @@ void tty_flush_to_ldisc(struct tty_struct *tty)
 	tty_buffer_flush_work(tty->port);
 }
 
+static DEFINE_KTHREAD_WORKER(tty_buffer_worker);
+
 void tty_buffer_queue_work(struct tty_port *port)
 {
 	struct tty_bufhead *buf = &port->buf;
-	schedule_work(&buf->work);
+	if (port->low_latency)
+		queue_kthread_work(&tty_buffer_worker, &buf->kwork);
+	else
+		schedule_work(&buf->work);
 }
 
 void tty_buffer_flush_work(struct tty_port *port)
 {
 	struct tty_bufhead *buf = &port->buf;
+	/* Flush both so we don't have to worry about racing with
+	 changes to port->low_latency */
+	flush_kthread_work(&buf->kwork);
 	flush_work(&buf->work);
 }
 
+void tty_buffer_init_kthread()
+{
+	struct task_struct *task;
+	/* Use same default priority as threaded irq handlers */
+	struct sched_param param = { .sched_priority = MAX_USER_RT_PRIO/2 };
+
+	task = kthread_run(kthread_worker_fn, &tty_buffer_worker, "tty");
+	sched_setscheduler(task, SCHED_FIFO, &param);
+}
+
 /**
  *	tty_flip_buffer_push	-	terminal
  *	@port: tty port to push
@@ -543,7 +574,8 @@ void tty_buffer_init(struct tty_port *port)
 	init_llist_head(&buf->free);
 	atomic_set(&buf->mem_used, 0);
 	atomic_set(&buf->priority, 0);
-	INIT_WORK(&buf->work, flush_to_ldisc);
+	INIT_WORK(&buf->work, flush_to_ldisc_workq);
+	init_kthread_work(&buf->kwork, flush_to_ldisc_kthread);
 	buf->mem_limit = TTYB_DEFAULT_MEM_LIMIT;
 }
 
diff --git a/kernel/msm-3.18/drivers/tty/tty_io.c b/kernel/msm-3.18/drivers/tty/tty_io.c
index dacf8d5a0..617186d93 100644
--- a/kernel/msm-3.18/drivers/tty/tty_io.c
+++ b/kernel/msm-3.18/drivers/tty/tty_io.c
@@ -3624,6 +3624,7 @@ void console_sysfs_notify(void)
  */
 int __init tty_init(void)
 {
+	tty_buffer_init_kthread();
 	cdev_init(&tty_cdev, &tty_fops);
 	if (cdev_add(&tty_cdev, MKDEV(TTYAUX_MAJOR, 0), 1) ||
 	    register_chrdev_region(MKDEV(TTYAUX_MAJOR, 0), 1, "/dev/tty") < 0)
diff --git a/kernel/msm-3.18/include/linux/tty.h b/kernel/msm-3.18/include/linux/tty.h
index 2adb3dd25..59f60d0e4 100644
--- a/kernel/msm-3.18/include/linux/tty.h
+++ b/kernel/msm-3.18/include/linux/tty.h
@@ -5,6 +5,7 @@
 #include <linux/major.h>
 #include <linux/termios.h>
 #include <linux/workqueue.h>
+#include <linux/kthread.h>
 #include <linux/tty_driver.h>
 #include <linux/tty_ldisc.h>
 #include <linux/mutex.h>
@@ -60,6 +61,7 @@ static inline char *flag_buf_ptr(struct tty_buffer *b, int ofs)
 struct tty_bufhead {
 	struct tty_buffer *head;	/* Queue head */
 	struct work_struct work;
+	struct kthread_work kwork;
 	struct mutex	   lock;
 	atomic_t	   priority;
 	struct tty_buffer sentinel;
@@ -448,6 +450,7 @@ extern void tty_buffer_flush(struct tty_struct *tty);
 extern void tty_buffer_init(struct tty_port *port);
 extern void tty_buffer_queue_work(struct tty_port *port);
 extern void tty_buffer_flush_work(struct tty_port *port);
+extern void tty_buffer_init_kthread(void);
 extern speed_t tty_termios_baud_rate(struct ktermios *termios);
 extern speed_t tty_termios_input_baud_rate(struct ktermios *termios);
 extern void tty_termios_encode_baud_rate(struct ktermios *termios,
-- 
2.49.0


From 75070cc83a0666a55cbef4c19ff6b62139769233 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Fri, 18 Jan 2019 12:03:07 -0800
Subject: [PATCH 17/20] more config for RT

---
 kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c | 2 +-
 poky/meta-anki/recipes/ankiinit/files/ankiinit.sh  | 7 +++++--
 2 files changed, 6 insertions(+), 3 deletions(-)

diff --git a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c
index 3ee6773f5..263d6499a 100644
--- a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c
+++ b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c
@@ -2721,7 +2721,7 @@ static int msm_hs_startup(struct uart_port *uport)
 	 */
 	mb();
 
-	ret = request_irq(uport->irq, msm_hs_isr, IRQF_TRIGGER_HIGH,
+	ret = request_threaded_irq(uport->irq, msm_hsl_handler,  msm_hs_isr, IRQF_TRIGGER_HIGH,
 			  "msm_hs_uart", msm_uport);
 	if (unlikely(ret)) {
 		MSM_HS_ERR("%s():Error %d getting uart irq\n", __func__, ret);
diff --git a/poky/meta-anki/recipes/ankiinit/files/ankiinit.sh b/poky/meta-anki/recipes/ankiinit/files/ankiinit.sh
index d21fb0f02..8f1f0abd1 100644
--- a/poky/meta-anki/recipes/ankiinit/files/ankiinit.sh
+++ b/poky/meta-anki/recipes/ankiinit/files/ankiinit.sh
@@ -15,8 +15,11 @@ echo 1 > /sys/kernel/debug/msm-bus-dbg/shell-client/update_request
 #End clock speed limits
 
 # Pin serial and spi kthreads to core 0
-pgrep msm_serial_hs_0 | xargs -I PID taskset -p 8 PID
-pgrep spi0 | xargs -I PID taskset -p 8 PID
+#pgrep msm_serial_hs_0 | xargs -I PID taskset -p 8 PID
+#pgrep spi0 | xargs -I PID taskset -p 8 PID
+
+pgrep msm_ser | xargs -I PID renice -n -20 -p PID
+pgrep irq.*sps | xargs -I PID renice -n -20 -p PID
 
 # TODO Move this power rail controll into the camera driver
 CAM_REG_GPIO=83
-- 
2.49.0


From d0b726a3d951691b7ce5d3908138a4e4095c33ce Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Fri, 18 Jan 2019 15:27:03 -0800
Subject: [PATCH 18/20] remove iso on vic-robot

---
 anki/victor                                               | 2 +-
 poky/build/conf/local.conf                                | 2 +-
 poky/meta-anki/recipes/anki-robot/files/vic-robot.service | 2 +-
 3 files changed, 3 insertions(+), 3 deletions(-)

diff --git a/anki/victor b/anki/victor
index fbd79b6a2..7d0446a4e 160000
--- a/anki/victor
+++ b/anki/victor
@@ -1 +1 @@
-Subproject commit fbd79b6a2b4d0e6f5dc83b4abf574d76023a1539
+Subproject commit 7d0446a4e2f9717d2b42927d17e665e80fcafb73
diff --git a/poky/build/conf/local.conf b/poky/build/conf/local.conf
index 06ee19712..d74ba9316 100644
--- a/poky/build/conf/local.conf
+++ b/poky/build/conf/local.conf
@@ -258,4 +258,4 @@ hostname_pn-base-files = "${ANKI_PRODUCT_NAME}"
 
 IMAGE_INSTALL_append = " systemd-serialgetty"
 
-IMAGE_INSTALL_remove += "wlan-opensource"
\ No newline at end of file
+IMAGE_INSTALL_remove += "wlan-opensource"
diff --git a/poky/meta-anki/recipes/anki-robot/files/vic-robot.service b/poky/meta-anki/recipes/anki-robot/files/vic-robot.service
index 87607157b..038c4d567 100755
--- a/poky/meta-anki/recipes/anki-robot/files/vic-robot.service
+++ b/poky/meta-anki/recipes/anki-robot/files/vic-robot.service
@@ -22,7 +22,7 @@ LimitMEMLOCK=infinity
 Nice=-20
 
 # Set CPUAffinity (required isolcpus kernel arg)
-CPUAffinity=3
+#CPUAffinity=3
 
 # VIC-1884: Hard kill at 5 seconds if process doesn't respond to SIGTERM
 TimeoutStopSec=5
-- 
2.49.0


From d2a0b22329ebe74ccfca8981cfe0775228cd9e53 Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Tue, 22 Jan 2019 09:12:48 -0800
Subject: [PATCH 19/20] Always use low_latency tty

---
 kernel/msm-3.18/drivers/tty/tty_buffer.c | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/kernel/msm-3.18/drivers/tty/tty_buffer.c b/kernel/msm-3.18/drivers/tty/tty_buffer.c
index 6719d0390..2262ebca8 100644
--- a/kernel/msm-3.18/drivers/tty/tty_buffer.c
+++ b/kernel/msm-3.18/drivers/tty/tty_buffer.c
@@ -513,10 +513,10 @@ static DEFINE_KTHREAD_WORKER(tty_buffer_worker);
 void tty_buffer_queue_work(struct tty_port *port)
 {
 	struct tty_bufhead *buf = &port->buf;
-	if (port->low_latency)
+//	if (port->low_latency)
 		queue_kthread_work(&tty_buffer_worker, &buf->kwork);
-	else
-		schedule_work(&buf->work);
+//	else
+//		schedule_work(&buf->work);
 }
 
 void tty_buffer_flush_work(struct tty_port *port)
-- 
2.49.0


From b597989057fd2002348a225a3452b06eede5d54a Mon Sep 17 00:00:00 2001
From: Mathew Prokos <mprokos@anki.com>
Date: Tue, 22 Jan 2019 09:13:11 -0800
Subject: [PATCH 20/20] Add more spinlock changes

---
 kernel/msm-3.18/drivers/soc/qcom/smd.c        | 90 +++++++++----------
 .../msm-3.18/drivers/soc/qcom/smd_init_dt.c   |  2 +-
 .../msm-3.18/drivers/soc/qcom/smd_private.h   |  2 +-
 .../drivers/soc/qcom/smp2p_spinlock_test.c    |  2 +-
 .../drivers/tty/serial/msm_serial_hs.c        |  2 +-
 5 files changed, 49 insertions(+), 49 deletions(-)

diff --git a/kernel/msm-3.18/drivers/soc/qcom/smd.c b/kernel/msm-3.18/drivers/soc/qcom/smd.c
index 0f271d36d..d4efd42c8 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smd.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/smd.c
@@ -70,7 +70,7 @@ static struct smsm_shared_info smsm_info;
 static struct kfifo smsm_snapshot_fifo;
 static struct wakeup_source smsm_snapshot_ws;
 static int smsm_snapshot_count;
-static DEFINE_RAW_SPINLOCK(smsm_snapshot_count_lock);
+static DEFINE_SPINLOCK(smsm_snapshot_count_lock);
 
 struct smsm_size_info_type {
 	uint32_t num_hosts;
@@ -526,8 +526,8 @@ static struct notifier_block smsm_pm_nb = {
  * irq handler and code that mutates the channel
  * list or fiddles with channel state
  */
-static DEFINE_RAW_SPINLOCK(smd_lock);
-DEFINE_RAW_SPINLOCK(smem_lock);
+static DEFINE_SPINLOCK(smd_lock);
+DEFINE_SPINLOCK(smem_lock);
 
 /* the mutex is used during open() and close()
  * operations to avoid races while creating or
@@ -993,14 +993,14 @@ void smd_channel_reset(uint32_t restart_pid)
 
 	/* change all remote states to CLOSING */
 	mutex_lock(&smd_probe_lock);
-	raw_spin_lock_irqsave(&smd_lock, flags);
+	spin_lock_irqsave(&smd_lock, flags);
 	smd_channel_reset_state(shared_pri, PRI_ALLOC_TBL, SMD_SS_CLOSING,
 				restart_pid, pri_size / sizeof(*shared_pri));
 	if (shared_sec)
 		smd_channel_reset_state(shared_sec, SEC_ALLOC_TBL,
 						SMD_SS_CLOSING, restart_pid,
 						sec_size / sizeof(*shared_sec));
-	raw_spin_unlock_irqrestore(&smd_lock, flags);
+	spin_unlock_irqrestore(&smd_lock, flags);
 	mutex_unlock(&smd_probe_lock);
 
 	mb();
@@ -1008,14 +1008,14 @@ void smd_channel_reset(uint32_t restart_pid)
 
 	/* change all remote states to CLOSED */
 	mutex_lock(&smd_probe_lock);
-	raw_spin_lock_irqsave(&smd_lock, flags);
+	spin_lock_irqsave(&smd_lock, flags);
 	smd_channel_reset_state(shared_pri, PRI_ALLOC_TBL, SMD_SS_CLOSED,
 				restart_pid, pri_size / sizeof(*shared_pri));
 	if (shared_sec)
 		smd_channel_reset_state(shared_sec, SEC_ALLOC_TBL,
 						SMD_SS_CLOSED, restart_pid,
 						sec_size / sizeof(*shared_sec));
-	raw_spin_unlock_irqrestore(&smd_lock, flags);
+	spin_unlock_irqrestore(&smd_lock, flags);
 	mutex_unlock(&smd_probe_lock);
 
 	mb();
@@ -1339,7 +1339,7 @@ static void handle_smd_irq_closing_list(void)
 	struct smd_channel *index;
 	unsigned tmp;
 
-	raw_spin_lock_irqsave(&smd_lock, flags);
+	spin_lock_irqsave(&smd_lock, flags);
 	list_for_each_entry_safe(ch, index, &smd_ch_closing_list, ch_list) {
 		if (ch->half_ch->get_fSTATE(ch->recv))
 			ch->half_ch->set_fSTATE(ch->recv, 0);
@@ -1347,7 +1347,7 @@ static void handle_smd_irq_closing_list(void)
 		if (tmp != ch->last_state)
 			smd_state_change(ch, ch->last_state, tmp);
 	}
-	raw_spin_unlock_irqrestore(&smd_lock, flags);
+	spin_unlock_irqrestore(&smd_lock, flags);
 }
 
 static void handle_smd_irq(struct remote_proc_info *r_info,
@@ -1362,7 +1362,7 @@ static void handle_smd_irq(struct remote_proc_info *r_info,
 
 	list = &r_info->ch_list;
 
-	raw_spin_lock_irqsave(&smd_lock, flags);
+	spin_lock_irqsave(&smd_lock, flags);
 	list_for_each_entry(ch, list, ch_list) {
 		state_change = 0;
 		ch_flags = 0;
@@ -1409,7 +1409,7 @@ static void handle_smd_irq(struct remote_proc_info *r_info,
 			ch->notify(ch->priv, SMD_EVENT_STATUS);
 		}
 	}
-	raw_spin_unlock_irqrestore(&smd_lock, flags);
+	spin_unlock_irqrestore(&smd_lock, flags);
 	do_smd_probe(r_info->remote_pid);
 }
 
@@ -1622,10 +1622,10 @@ static int smd_packet_read(smd_channel_t *ch, void *data, int len)
 		if (!read_intr_blocked(ch))
 			ch->notify_other_cpu(ch);
 
-	raw_spin_lock_irqsave(&smd_lock, flags);
+	spin_lock_irqsave(&smd_lock, flags);
 	ch->current_packet -= r;
 	update_packet_state(ch);
-	raw_spin_unlock_irqrestore(&smd_lock, flags);
+	spin_unlock_irqrestore(&smd_lock, flags);
 
 	return r;
 }
@@ -1836,14 +1836,14 @@ static void finalize_channel_close_fn(struct work_struct *work)
 	struct smd_channel *index;
 
 	mutex_lock(&smd_creation_mutex);
-	raw_spin_lock_irqsave(&smd_lock, flags);
+	spin_lock_irqsave(&smd_lock, flags);
 	list_for_each_entry_safe(ch, index,  &smd_ch_to_close_list, ch_list) {
 		list_del(&ch->ch_list);
 		list_add(&ch->ch_list, &smd_ch_closed_list);
 		ch->notify(ch->priv, SMD_EVENT_REOPEN_READY);
 		ch->notify = do_nothing_notify;
 	}
-	raw_spin_unlock_irqrestore(&smd_lock, flags);
+	spin_unlock_irqrestore(&smd_lock, flags);
 	mutex_unlock(&smd_creation_mutex);
 }
 
@@ -1886,14 +1886,14 @@ int smd_named_open_on_edge(const char *name, uint32_t edge,
 
 	ch = smd_get_channel(name, edge);
 	if (!ch) {
-		raw_spin_lock_irqsave(&smd_lock, flags);
+		spin_lock_irqsave(&smd_lock, flags);
 		/* check opened list for port */
 		list_for_each_entry(ch,
 			&remote_info[edge_to_pids[edge].remote_pid].ch_list,
 			ch_list) {
 			if (!strcmp(name, ch->name)) {
 				/* channel is already open */
-				raw_spin_unlock_irqrestore(&smd_lock, flags);
+				spin_unlock_irqrestore(&smd_lock, flags);
 				SMD_DBG("smd_open: channel '%s' already open\n",
 					ch->name);
 				return -EBUSY;
@@ -1905,7 +1905,7 @@ int smd_named_open_on_edge(const char *name, uint32_t edge,
 			if (!strncmp(name, ch->name, 20) &&
 				(edge == ch->type)) {
 				/* channel exists, but is being closed */
-				raw_spin_unlock_irqrestore(&smd_lock, flags);
+				spin_unlock_irqrestore(&smd_lock, flags);
 				return -EAGAIN;
 			}
 		}
@@ -1915,11 +1915,11 @@ int smd_named_open_on_edge(const char *name, uint32_t edge,
 			if (!strncmp(name, ch->name, 20) &&
 				(edge == ch->type)) {
 				/* channel exists, but is being closed */
-				raw_spin_unlock_irqrestore(&smd_lock, flags);
+				spin_unlock_irqrestore(&smd_lock, flags);
 				return -EAGAIN;
 			}
 		}
-		raw_spin_unlock_irqrestore(&smd_lock, flags);
+		spin_unlock_irqrestore(&smd_lock, flags);
 
 		/* one final check to handle closing->closed race condition */
 		ch = smd_get_channel(name, edge);
@@ -1949,7 +1949,7 @@ int smd_named_open_on_edge(const char *name, uint32_t edge,
 
 	SMD_DBG("smd_open: opening '%s'\n", ch->name);
 
-	raw_spin_lock_irqsave(&smd_lock, flags);
+	spin_lock_irqsave(&smd_lock, flags);
 	list_add(&ch->ch_list,
 		       &remote_info[edge_to_pids[ch->type].remote_pid].ch_list);
 
@@ -1957,7 +1957,7 @@ int smd_named_open_on_edge(const char *name, uint32_t edge,
 
 	smd_state_change(ch, ch->last_state, SMD_SS_OPENING);
 
-	raw_spin_unlock_irqrestore(&smd_lock, flags);
+	spin_unlock_irqrestore(&smd_lock, flags);
 
 	return 0;
 }
@@ -1973,7 +1973,7 @@ int smd_close(smd_channel_t *ch)
 
 	SMD_INFO("smd_close(%s)\n", ch->name);
 
-	raw_spin_lock_irqsave(&smd_lock, flags);
+	spin_lock_irqsave(&smd_lock, flags);
 	list_del(&ch->ch_list);
 
 	was_opened = ch->half_ch->get_state(ch->recv) == SMD_SS_OPENED;
@@ -1981,9 +1981,9 @@ int smd_close(smd_channel_t *ch)
 
 	if (was_opened) {
 		list_add(&ch->ch_list, &smd_ch_closing_list);
-		raw_spin_unlock_irqrestore(&smd_lock, flags);
+		spin_unlock_irqrestore(&smd_lock, flags);
 	} else {
-		raw_spin_unlock_irqrestore(&smd_lock, flags);
+		spin_unlock_irqrestore(&smd_lock, flags);
 		ch->notify = do_nothing_notify;
 		mutex_lock(&smd_creation_mutex);
 		list_add(&ch->ch_list, &smd_ch_closed_list);
@@ -2307,9 +2307,9 @@ int smd_tiocmset(smd_channel_t *ch, unsigned int set, unsigned int clear)
 		return -ENODEV;
 	}
 
-	raw_spin_lock_irqsave(&smd_lock, flags);
+	spin_lock_irqsave(&smd_lock, flags);
 	smd_tiocmset_from_cb(ch, set, clear);
-	raw_spin_unlock_irqrestore(&smd_lock, flags);
+	spin_unlock_irqrestore(&smd_lock, flags);
 
 	return 0;
 }
@@ -2325,9 +2325,9 @@ int smd_is_pkt_avail(smd_channel_t *ch)
 	if (ch->current_packet)
 		return 1;
 
-	raw_spin_lock_irqsave(&smd_lock, flags);
+	spin_lock_irqsave(&smd_lock, flags);
 	update_packet_state(ch);
-	raw_spin_unlock_irqrestore(&smd_lock, flags);
+	spin_unlock_irqrestore(&smd_lock, flags);
 
 	return ch->current_packet ? 1 : 0;
 }
@@ -2480,13 +2480,13 @@ static void smsm_cb_snapshot(uint32_t use_wakeup_source)
 	 *   This order ensures that 1 will always occur before abc.
 	 */
 	if (use_wakeup_source) {
-		raw_spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
+		spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
 		if (smsm_snapshot_count == 0) {
 			SMSM_POWER_INFO("SMSM snapshot wake lock\n");
 			__pm_stay_awake(&smsm_snapshot_ws);
 		}
 		++smsm_snapshot_count;
-		raw_spin_unlock_irqrestore(&smsm_snapshot_count_lock, flags);
+		spin_unlock_irqrestore(&smsm_snapshot_count_lock, flags);
 	}
 
 	/* queue state entries */
@@ -2520,7 +2520,7 @@ static void smsm_cb_snapshot(uint32_t use_wakeup_source)
 
 restore_snapshot_count:
 	if (use_wakeup_source) {
-		raw_spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
+		spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
 		if (smsm_snapshot_count) {
 			--smsm_snapshot_count;
 			if (smsm_snapshot_count == 0) {
@@ -2530,7 +2530,7 @@ restore_snapshot_count:
 		} else {
 			pr_err("%s: invalid snapshot count\n", __func__);
 		}
-		raw_spin_unlock_irqrestore(&smsm_snapshot_count_lock, flags);
+		spin_unlock_irqrestore(&smsm_snapshot_count_lock, flags);
 	}
 }
 
@@ -2538,7 +2538,7 @@ static irqreturn_t smsm_irq_handler(int irq, void *data)
 {
 	unsigned long flags;
 
-	raw_spin_lock_irqsave(&smem_lock, flags);
+	spin_lock_irqsave(&smem_lock, flags);
 	if (!smsm_info.state) {
 		SMSM_INFO("<SM NO STATE>\n");
 	} else {
@@ -2565,7 +2565,7 @@ static irqreturn_t smsm_irq_handler(int irq, void *data)
 
 		smsm_cb_snapshot(1);
 	}
-	raw_spin_unlock_irqrestore(&smem_lock, flags);
+	spin_unlock_irqrestore(&smem_lock, flags);
 	return IRQ_HANDLED;
 }
 
@@ -2628,7 +2628,7 @@ int smsm_change_intr_mask(uint32_t smsm_entry,
 		return -EIO;
 	}
 
-	raw_spin_lock_irqsave(&smem_lock, flags);
+	spin_lock_irqsave(&smem_lock, flags);
 	smsm_states[smsm_entry].intr_mask_clear = clear_mask;
 	smsm_states[smsm_entry].intr_mask_set = set_mask;
 
@@ -2637,7 +2637,7 @@ int smsm_change_intr_mask(uint32_t smsm_entry,
 	__raw_writel(new_mask, SMSM_INTR_MASK_ADDR(smsm_entry, SMSM_APPS));
 
 	wmb();
-	raw_spin_unlock_irqrestore(&smem_lock, flags);
+	spin_unlock_irqrestore(&smem_lock, flags);
 
 	return 0;
 }
@@ -2677,7 +2677,7 @@ int smsm_change_state(uint32_t smsm_entry,
 		pr_err("smsm_change_state <SM NO STATE>\n");
 		return -EIO;
 	}
-	raw_spin_lock_irqsave(&smem_lock, flags);
+	spin_lock_irqsave(&smem_lock, flags);
 
 	old_state = __raw_readl(SMSM_STATE_ADDR(smsm_entry));
 	new_state = (old_state & ~clear_mask) | set_mask;
@@ -2686,7 +2686,7 @@ int smsm_change_state(uint32_t smsm_entry,
 			old_state, new_state);
 	notify_other_smsm(SMSM_APPS_STATE, (old_state ^ new_state));
 
-	raw_spin_unlock_irqrestore(&smem_lock, flags);
+	spin_unlock_irqrestore(&smem_lock, flags);
 
 	return 0;
 }
@@ -2782,7 +2782,7 @@ void notify_smsm_cb_clients_worker(struct work_struct *work)
 		mutex_unlock(&smsm_lock);
 
 		if (use_wakeup_source) {
-			raw_spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
+			spin_lock_irqsave(&smsm_snapshot_count_lock, flags);
 			if (smsm_snapshot_count) {
 				--smsm_snapshot_count;
 				if (smsm_snapshot_count == 0) {
@@ -2794,7 +2794,7 @@ void notify_smsm_cb_clients_worker(struct work_struct *work)
 				pr_err("%s: invalid snapshot count\n",
 						__func__);
 			}
-			raw_spin_unlock_irqrestore(&smsm_snapshot_count_lock,
+			spin_unlock_irqrestore(&smsm_snapshot_count_lock,
 					flags);
 		}
 
@@ -2877,13 +2877,13 @@ int smsm_state_cb_register(uint32_t smsm_entry, uint32_t mask,
 	if (smsm_info.intr_mask) {
 		unsigned long flags;
 
-		raw_spin_lock_irqsave(&smem_lock, flags);
+		spin_lock_irqsave(&smem_lock, flags);
 		new_mask = (new_mask & ~state->intr_mask_clear)
 				| state->intr_mask_set;
 		__raw_writel(new_mask,
 				SMSM_INTR_MASK_ADDR(smsm_entry, SMSM_APPS));
 		wmb();
-		raw_spin_unlock_irqrestore(&smem_lock, flags);
+		spin_unlock_irqrestore(&smem_lock, flags);
 	}
 
 cleanup:
@@ -2952,13 +2952,13 @@ int smsm_state_cb_deregister(uint32_t smsm_entry, uint32_t mask,
 	if (smsm_info.intr_mask) {
 		unsigned long flags;
 
-		raw_spin_lock_irqsave(&smem_lock, flags);
+		spin_lock_irqsave(&smem_lock, flags);
 		new_mask = (new_mask & ~state->intr_mask_clear)
 				| state->intr_mask_set;
 		__raw_writel(new_mask,
 				SMSM_INTR_MASK_ADDR(smsm_entry, SMSM_APPS));
 		wmb();
-		raw_spin_unlock_irqrestore(&smem_lock, flags);
+		spin_unlock_irqrestore(&smem_lock, flags);
 	}
 
 	mutex_unlock(&smsm_lock);
diff --git a/kernel/msm-3.18/drivers/soc/qcom/smd_init_dt.c b/kernel/msm-3.18/drivers/soc/qcom/smd_init_dt.c
index 1d722d88e..00cce874e 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smd_init_dt.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/smd_init_dt.c
@@ -124,7 +124,7 @@ static int msm_smsm_probe(struct platform_device *pdev)
 	remote_pid = smd_edge_to_remote_pid(edge);
 	interrupt_stats[remote_pid].smsm_interrupt_id = irq_line;
 
-	ret = request_irq(irq_line,
+	ret = request_threaded_irq(irq_line, NULL,
 				private_irq->irq_handler,
 				IRQF_TRIGGER_RISING | IRQF_NO_SUSPEND,
 				node->name,
diff --git a/kernel/msm-3.18/drivers/soc/qcom/smd_private.h b/kernel/msm-3.18/drivers/soc/qcom/smd_private.h
index decaf606c..4beb6b8c1 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smd_private.h
+++ b/kernel/msm-3.18/drivers/soc/qcom/smd_private.h
@@ -169,7 +169,7 @@ struct smd_channel {
 	struct smd_half_channel_access *half_ch;
 };
 
-extern raw_spinlock_t smem_lock;
+extern spinlock_t smem_lock;
 
 struct interrupt_stat {
 	uint32_t smd_in_count;
diff --git a/kernel/msm-3.18/drivers/soc/qcom/smp2p_spinlock_test.c b/kernel/msm-3.18/drivers/soc/qcom/smp2p_spinlock_test.c
index 418a3da72..27ad4f4c5 100644
--- a/kernel/msm-3.18/drivers/soc/qcom/smp2p_spinlock_test.c
+++ b/kernel/msm-3.18/drivers/soc/qcom/smp2p_spinlock_test.c
@@ -532,7 +532,7 @@ static void smp2p_ut_remote_spinlock_ssr(struct seq_file *s)
 
 		remote_spin_lock_irqsave(smem_spinlock, flags);
 		/* Unlock local spin lock and hold HW spinlock */
-		raw_spin_unlock_irqrestore(&((smem_spinlock)->local), flags);
+		spin_unlock_irqrestore(&((smem_spinlock)->local), flags);
 
 		queue_work(ws, &work_item.work);
 		UT_ASSERT_INT(
diff --git a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c
index 263d6499a..3ee6773f5 100644
--- a/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c
+++ b/kernel/msm-3.18/drivers/tty/serial/msm_serial_hs.c
@@ -2721,7 +2721,7 @@ static int msm_hs_startup(struct uart_port *uport)
 	 */
 	mb();
 
-	ret = request_threaded_irq(uport->irq, msm_hsl_handler,  msm_hs_isr, IRQF_TRIGGER_HIGH,
+	ret = request_irq(uport->irq, msm_hs_isr, IRQF_TRIGGER_HIGH,
 			  "msm_hs_uart", msm_uport);
 	if (unlikely(ret)) {
 		MSM_HS_ERR("%s():Error %d getting uart irq\n", __func__, ret);
-- 
2.49.0

